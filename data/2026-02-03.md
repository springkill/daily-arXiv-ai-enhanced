<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 23]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.LG](#cs.LG) [Total: 128]
- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Learning to Recommend Multi-Agent Subgraphs from Calling Trees](https://arxiv.org/abs/2601.22209)
*Xinyuan Song,Liang Zhao*

Main category: cs.MA

TL;DR: 该论文将多智能体系统中的智能体推荐问题形式化为约束决策问题，提出了一个基于历史调用树的两阶段推荐框架，支持智能体级和系统级推荐。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统市场扩大，智能体功能重叠加剧，选择问题不仅涉及相关性检索，还需考虑可靠性、兼容性和协作性。现有推荐系统基于扁平的用户-物品日志，无法处理智能体编排的结构化、顺序性和交互依赖性特点。

Method: 提出约束推荐框架：1）基于当前子任务和上下文通过检索构建紧凑候选集；2）在可行集内使用学习到的评分器进行效用优化，考虑相关性、可靠性和交互效应。框架基于历史调用树，支持智能体级推荐（选择下一个智能体/工具）和系统级推荐（选择小型连接智能体团队/子图）。

Result: 通过将八个异构多智能体语料库的调用日志归一化为统一的结构化表示，构建了统一的调用树基准数据集，用于系统评估。

Conclusion: 该研究填补了多智能体系统中智能体推荐的理论与实践空白，通过约束决策框架和基于调用树的学习信号，为复杂任务编排提供了更有效的智能体选择方法。

Abstract: Multi-agent systems (MAS) increasingly solve complex tasks by orchestrating agents and tools selected from rapidly growing marketplaces. As these marketplaces expand, many candidates become functionally overlapping, making selection not just a retrieval problem: beyond filtering relevant agents, an orchestrator must choose options that are reliable, compatible with the current execution context, and able to cooperate with other selected agents. Existing recommender systems -- largely built for item-level ranking from flat user-item logs -- do not directly address the structured, sequential, and interaction-dependent nature of agent orchestration. We address this gap by \textbf{formulating agent recommendation in MAS as a constrained decision problem} and introducing a generic \textbf{constrained recommendation framework} that first uses retrieval to build a compact candidate set conditioned on the current subtask and context, and then performs \textbf{utility optimization} within this feasible set using a learned scorer that accounts for relevance, reliability, and interaction effects. We ground both the formulation and learning signals in \textbf{historical calling trees}, which capture the execution structure of MAS (parent-child calls, branching dependencies, and local cooperation patterns) beyond what flat logs provide. The framework supports two complementary settings: \textbf{agent-level recommendation} (select the next agent/tool) and \textbf{system-level recommendation} (select a small, connected agent team/subgraph for coordinated execution). To enable systematic evaluation, we construct a unified calling-tree benchmark by normalizing invocation logs from eight heterogeneous multi-agent corpora into a shared structured representation.

</details>


### [2] [Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data](https://arxiv.org/abs/2601.22242)
*Zhihao Zhang,Keith Redmill,Chengyang Peng,Bowen Weng*

Main category: cs.MA

TL;DR: 提出一个从宏观观测重建微观状态的框架，通过微观数据锚定车辆行为，学习一个在微观层面与部分观测轨迹一致、在宏观层面与目标交通统计对齐的共享策略。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶算法需要与人类驾驶实践对齐或有效协作，但现有方法（监督学习和强化学习）都依赖高质量的真实驾驶行为观测数据，这些数据获取困难且成本高。车载传感器能获取微观数据但缺乏环境上下文，路侧传感器能捕捉宏观交通流特征但无法关联到微观车辆层面。

Method: 提出一个框架，利用宏观观测重建未观测的微观状态，使用微观数据锚定观测到的车辆行为，学习一个共享策略。该策略在微观层面与部分观测的轨迹和动作保持一致，在宏观层面当部署到整个群体时与目标交通统计对齐。

Result: 这种受约束和正则化的策略能够促进现实的交通流模式，并在大规模部署时与人类驾驶员实现安全协调。

Conclusion: 通过结合微观和宏观观测的互补性，提出的框架能够克服单一数据源的局限性，学习到既符合微观驾驶行为又能在宏观层面产生理想交通流模式的自动驾驶策略。

Abstract: A driving algorithm that aligns with good human driving practices, or at the very least collaborates effectively with human drivers, is crucial for developing safe and efficient autonomous vehicles. In practice, two main approaches are commonly adopted: (i) supervised or imitation learning, which requires comprehensive naturalistic driving data capturing all states that influence a vehicle's decisions and corresponding actions, and (ii) reinforcement learning (RL), where the simulated driving environment either matches or is intentionally more challenging than real-world conditions. Both methods depend on high-quality observations of real-world driving behavior, which are often difficult and costly to obtain. State-of-the-art sensors on individual vehicles can gather microscopic data, but they lack context about the surrounding conditions. Conversely, roadside sensors can capture traffic flow and other macroscopic characteristics, but they cannot associate this information with individual vehicles on a microscopic level. Motivated by this complementarity, we propose a framework that reconstructs unobserved microscopic states from macroscopic observations, using microscopic data to anchor observed vehicle behaviors, and learns a shared policy whose behavior is microscopically consistent with the partially observed trajectories and actions and macroscopically aligned with target traffic statistics when deployed population-wide. Such constrained and regularized policies promote realistic flow patterns and safe coordination with human drivers at scale.

</details>


### [3] [MonoScale: Scaling Multi-Agent System with Monotonic Improvement](https://arxiv.org/abs/2601.23219)
*Shuai Shao,Yixiang Liu,Bingwei Lu,Weinan Zhang*

Main category: cs.MA

TL;DR: MonoScale框架解决LLM多智能体系统扩展时的性能崩溃问题，通过生成熟悉化任务、收集交互证据并蒸馏为可审计的自然语言记忆来指导路由，保证性能单调不减


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统通过扩展智能体池来增强能力，但简单扩展会导致新智能体冷启动、异构性和不可靠性引发性能崩溃，需要一种能保证扩展稳定性的框架

Method: 提出MonoScale框架：1)主动生成少量智能体条件化熟悉化任务；2)收集成功和失败交互证据；3)蒸馏为可审计的自然语言记忆指导路由；4)将顺序增强形式化为上下文赌博机问题；5)执行信任区域记忆更新

Result: 在GAIA和Humanity's Last Exam基准测试中，随着智能体池增长保持稳定增益，优于简单扩展和强路由固定池基线方法

Conclusion: MonoScale通过主动熟悉化和可审计记忆蒸馏，为LLM多智能体系统扩展提供了理论保证的稳定性能提升框架，解决了扩展时的性能崩溃问题

Abstract: In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous, and unreliable agents. We propose MonoScale, an expansion-aware update framework that proactively generates a small set of agent-conditioned familiarization tasks, harvests evidence from both successful and failed interactions, and distills it into auditable natural-language memory to guide future routing. We formalize sequential augmentation as a contextual bandit and perform trust-region memory updates, yielding a monotonic non-decreasing performance guarantee across onboarding rounds. Experiments on GAIA and Humanity's Last Exam show stable gains as the agent pool grows, outperforming naive scale-up and strong-router fixed-pool baselines.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [4] [An innovating approach to teaching applied to database design. Improvement of Action Learning in Lifelong Learning](https://arxiv.org/abs/2601.22175)
*Christophe Béchade*

Main category: cs.DB

TL;DR: 本文介绍了昂热大学通过行动学习模式，让企业员工参与数据库设计项目培训的创新教学方法，结合了法国技术课程的成功因素、职业培训的适应性以及服务提供商的专长。


<details>
  <summary>Details</summary>
Motivation: 传统数据库设计通常仅限于专业数据处理人员（来自法国技术课程或工程师学校），而行动学习旨在将这一专业知识扩展到没有数据处理能力但具备业务流程知识的企业员工，促进大学与企业合作。

Method: 采用行动学习模式，教师担任项目管理的监督者角色，通过由专业机构资助的实际项目进行培训。该方法结合了法国技术课程的横向学期项目经验、职业培训的适应性教学法以及服务提供商的咨询经验。

Result: 行动学习成功地将三种互补方法的优势结合起来：法国技术课程的成功因素、职业培训的适应性教学、以及服务提供商的专长。这种培训模式实现了可评估且持久的教学与职业目标。

Conclusion: 行动学习是一种有效的创新培训方法，它整合了教育机构、职业培训和企业实践的优势，符合法国促进大学与企业合作的政策目标，能够提高合作的数量和质量。

Abstract: For now 10 years, the Action Learning has allowed employees of University of Angers, private and public Companies to be initiated with the design of database, on projects financed by professional structures. These innovating training periods are carried out within the framework of the University College of Further Education of the University of Angers. Database design is a process initially reserved to the professional data processing specialists, coming from French Level-2 technological courses (2-year degrees) or Engineer Schools (Master). The pedagogical model of technological courses has integrated for more than 20 years transverse semester projects, in order to give the students the opportunity to apply newly acquired knowledge, coordinated by teachers. Action Learning requires teachers to assume the role of supervisors for the project management. The objective of Action Learning is to transmit not only knowledge from teachers, but also the experience of consultants to trainees having no competence in data processing, but who have the knowledge of their business process. The present paper shows that Action Learning puts together the factors for success of French technological courses, the adaptability of pedagogy provided to the vocational training, and finally the competence of service provider, Keeping the best parts of those three complementary approaches makes it possible for this kind of formation to achieve teaching and professional, assessable and long lasting goals. Action Learning belongs to the French policy that aims to improve the volume and the quality of the contracts between Universities and companies.

</details>


### [5] [Discovering High-utility Sequential Rules with Increasing Utility Ratio](https://arxiv.org/abs/2601.22178)
*Zhenqiang Ye,Wensheng Gan,Gengsen Huang,Tianlong Gu,Philip S. Yu*

Main category: cs.DB

TL;DR: 该论文提出了一种新的算法SRIU，用于挖掘具有递增效用比的高效用序列规则，通过两种扩展方法和多种优化策略提高挖掘效率和结果相关性。


<details>
  <summary>Details</summary>
Motivation: 当前高效用序列规则挖掘方法中，规则与其生成过程之间的关联不明确，无法确定新项目的添加如何影响规则的效用或置信度变化。需要解决如何挖掘具有递增效用比的规则的问题。

Method: 提出SRIU算法，采用左右扩展和右左扩展两种方法，引入项目对估计效用剪枝策略(IPEUP)减少搜索空间，为两种扩展方法设计上界和剪枝策略，使用Bitmap减少内存消耗，设计紧凑的效用表。

Result: 在真实世界和合成数据集上的广泛实验证明了该方法的有效性。使用置信度和确信度等指标评估生成的序列规则质量，表明SRIU能够提高挖掘结果的相关性。

Conclusion: SRIU算法能够有效挖掘具有递增效用比的高效用序列规则，通过创新的扩展方法和优化策略提高了挖掘效率和结果质量，为决策提供更可靠的信息。

Abstract: Utility-driven mining is an essential task in data science, as it can provide deeper insight into the real world. High-utility sequential rule mining (HUSRM) aims at discovering sequential rules with high utility and high confidence. It can certainly provide reliable information for decision-making because it uses confidence as an evaluation metric, as well as some algorithms like HUSRM and US-Rule. However, in current rule-growth mining methods, the linkage between HUSRs and their generation remains ambiguous. Specifically, it is unclear whether the addition of new items affects the utility or confidence of the former rule, leading to an increase or decrease in their values. Therefore, in this paper, we formulate the problem of mining HUSRs with an increasing utility ratio. To address this, we introduce a novel algorithm called SRIU for discovering all HUSRs with an increasing utility ratio using two distinct expansion methods, including left-right expansion and right-left expansion. SRIU also utilizes the item pair estimated utility pruning strategy (IPEUP) to reduce the search space. Moreover, for the two expansion methods, two sets of upper bounds and corresponding pruning strategies are introduced. To enhance the efficiency of SRIU, several optimizations are incorporated. These include utilizing the Bitmap to reduce memory consumption and designing a compact utility table for the mining procedure. Finally, extensive experimental results from both real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Moreover, to better assess the quality of the generated sequential rules, metrics such as confidence and conviction are employed, which further demonstrate that SRIU can improve the relevance of mining results.

</details>


### [6] [High-utility Sequential Rule Mining Utilizing Segmentation Guided by Confidence](https://arxiv.org/abs/2601.22179)
*Chunkai Zhang,Jiarui Deng,Maohua Lyu,Wensheng Gan,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出基于置信度引导分割的高效用序列规则挖掘算法RSC，通过预计算置信度和分段策略减少冗余效用计算，提高挖掘效率


<details>
  <summary>Details</summary>
Motivation: 现有高效用序列规则挖掘算法存在冗余效用计算问题，当相同项目序列可以形成多个不同规则时，需要重复计算效用值，导致计算效率低下

Method: 提出RSC算法：1) 采用置信度引导的分割策略，预计算分段规则的置信度；2) 使用效用链接表加速候选序列生成；3) 引入更严格的效用上界——序列的缩减剩余效用，处理重复项目序列

Result: 在多个数据集上评估RSC算法，结果显示相比现有最优方法有显著改进，验证了算法在减少冗余计算和提高挖掘效率方面的有效性

Conclusion: RSC算法通过置信度引导的分割策略有效减少了高效用序列规则挖掘中的冗余计算，提高了算法效率，为实际应用提供了更高效的解决方案

Abstract: Within the domain of data mining, one critical objective is the discovery of sequential rules with high utility. The goal is to discover sequential rules that exhibit both high utility and strong confidence, which are valuable in real-world applications. However, existing high-utility sequential rule mining algorithms suffer from redundant utility computations, as different rules may consist of the same sequence of items. When these items can form multiple distinct rules, additional utility calculations are required. To address this issue, this study proposes a sequential rule mining algorithm that utilizes segmentation guided by confidence (RSC), which employs confidence-guided segmentation to reduce redundant utility computation. It adopts a method that precomputes the confidence of segmented rules by leveraging the support of candidate subsequences in advance. Once the segmentation point is determined, all rules with different antecedents and consequents are generated simultaneously. RSC uses a utility-linked table to accelerate candidate sequence generation and introduces a stricter utility upper bound, called the reduced remaining utility of a sequence, to address sequences with duplicate items. Finally, the proposed RSC method was evaluated on multiple datasets, and the results demonstrate improvements over state-of-the-art approaches.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [7] [Toward Non-Expert Customized Congestion Control](https://arxiv.org/abs/2601.22461)
*Mingrui Zhang,Hamid Bagheri,Lisong Xu*

Main category: cs.NI

TL;DR: NECC框架利用大语言模型和BPF接口，让非专业用户能够轻松建模、实现和部署定制化的拥塞控制算法


<details>
  <summary>Details</summary>
Motivation: 通用拥塞控制算法无法满足特定用户需求，而定制化算法需要专业知识，非专业用户难以实现

Method: 提出NECC框架，结合大语言模型和伯克利包过滤器接口，为非专业用户提供定制化拥塞控制算法的建模、实现和部署能力

Result: 在真实世界拥塞控制算法上的评估显示NECC性能很有前景，并讨论了相关见解和未来研究方向

Conclusion: NECC是首个解决定制化拥塞控制算法实现问题的框架，为非专业用户提供了便捷的定制化解决方案

Abstract: General-purpose congestion control algorithms (CCAs) are designed to achieve general congestion control goals, but they may not meet the specific requirements of certain users. Customized CCAs can meet certain users' specific requirements; however, non-expert users often lack the expertise to implement them. In this paper, we present an exploratory non-expert customized CCA framework, named NECC, which enables non-expert users to easily model, implement, and deploy their customized CCAs by leveraging Large Language Models and the Berkeley Packet Filter (BPF) interface. To the best of our knowledge, we are the first to address the customized CCA implementation problem. Our evaluations using real-world CCAs show that the performance of NECC is very promising, and we discuss the insights that we find and possible future research directions.

</details>


### [8] [Chance-Constrained Secrecy Optimization in Hybrid RIS-Empowered and UAV-Assisted Networks](https://arxiv.org/abs/2601.22499)
*Elhadj Moustapha Diallo,Mamadou Aliou Diallo,Abusaeed B. M. Adam,Muhammad Naeem Shah*

Main category: cs.NI

TL;DR: 论文提出了一种混合可重构环境，包含无人机RIS、室外STAR-RIS和室内全息RIS，用于增强室内外用户的安全下行通信，解决了移动性、遮挡、窃听和硬件损伤等问题。


<details>
  <summary>Details</summary>
Motivation: 现有RIS系统在应对用户移动、动态遮挡、共谋窃听、硬件损伤等复杂场景时存在局限性，需要开发更鲁棒的混合可重构环境来保障室内外用户的通信安全。

Method: 开发了3GPP/ITU兼容的随机信道模型，采用Bernstein型确定性近似处理机会约束，提出基于交替优化的框架，使用SCA方法分别求解基站波束成形、RIS配置和无人机位置子问题。

Result: 仿真基于3GPP TR 38.901、TR 36.873和ITU-R P.2109标准，显示所提混合RIS方案相比基准方案显著降低了安全中断概率，对信道不确定性、遮挡、窃听和硬件损伤具有强鲁棒性。

Conclusion: 无人机RIS、STAR-RIS和全息RIS的集成能有效提升室内外通信安全性能，所提算法能单调降低安全中断成本并收敛到鲁棒问题的稳定点，为复杂环境下的安全通信提供了有效解决方案。

Abstract: This paper considers a hybrid reconfigurable environment comprising a UAV-mounted reflecting RIS, an outdoor STAR-RIS enabling simultaneous transmission and reflection, and an indoor holographic RIS (H-RIS), jointly enhancing secure downlink communication for indoor and outdoor users. The system operates under user mobility, dynamic blockages, colluding idle and active eavesdroppers, and transceiver and surface hardware impairments. A 3GPP and ITU-compliant stochastic channel model is developed, capturing mobility-induced covariance evolution, outdoor-indoor penetration losses, and distortion-aware noise due to practical EVM-based impairments. We aim to minimize the aggregate secrecy-outage probability subject to secrecy-rate constraints, QoS requirements, power limitations, and statistical CSI uncertainty. The resulting problem contains coupled secrecy and QoS chance constraints and nonlinear interactions among the BS beamforming vectors, multi-surface phase coefficients, and UAV position. To handle these difficulties, we derive rigorous Bernstein-type deterministic approximations for all chance constraints, yielding a distributionally robust reformulation. Building on this, we propose an alternating optimization framework that employs successive convex approximation (SCA) to convexify each block and solve the BS beamforming, RIS, STAR-RIS, H-RIS configuration, and UAV placement subproblems efficiently. The proposed algorithm is shown to monotonically decrease a smooth surrogate of the secrecy-outage cost and converge to a stationary point of the robustified problem. Simulations based on 3GPP TR 38.901, TR 36.873, and ITU-R P.2109 demonstrate that integrating UAV-RIS, STAR-RIS, and H-RIS significantly reduces secrecy-outage probability compared with benchmark schemes and provides strong robustness to channel uncertainty, blockages, colluding eavesdroppers, and hardware impairments.

</details>


### [9] [Digital Twin Synchronization: towards a data-centric architecture](https://arxiv.org/abs/2601.23051)
*Eduardo Freitas,Assis T. de Oliveira Filho,Pedro R. X. do Carmo,Djamel Sadok,Judith Kelner*

Main category: cs.NI

TL;DR: 该论文综述了数字孪生同步技术，分析了现有方法，识别了技术挑战，并提出了统一的同步架构以满足工业应用的安全和互操作性需求。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术作为工业4.0的关键使能技术，能够提升生产力和运营效率。然而，尽管中间件设计和低延迟通信技术有所进展，确保数字孪生与其物理对应物准确同步仍然面临挑战。论文旨在解决这一同步问题，推动数字孪生环境的稳健同步。

Method: 论文采用综述研究方法：1) 回顾当前采用的同步技术和架构；2) 识别关键的技术挑战；3) 提出统一的同步架构，该架构考虑了各种工业应用需求，并解决了安全性和互操作性要求。

Result: 论文通过系统分析，识别了数字孪生同步领域的关键技术挑战，并提出了一种统一的同步架构。该架构旨在为不同工业应用提供通用解决方案，同时满足安全标准和互操作性需求，填补了现有研究的空白。

Conclusion: 数字孪生同步是工业4.0环境中的关键挑战。论文通过综述现有技术、识别挑战并提出统一架构，为推进稳健的数字孪生同步提供了重要贡献。标准化架构对于确保工业系统无缝运行和持续改进至关重要。

Abstract: Digital Twin (DT) technology revolutionizes industrial processes by enabling the representation of physical entities and their dynamics to enhance productivity and operational efficiency. It has emerged as a vital enabling technology in the Industry 4.0 context. The present article examines the particular issue of synchronizing a digital twin while ensuring an accurate reflection of its physical counterpart. Despite the reported recent advances in the design of middleware and low delay communication technologies, effective synchronization between both worlds remains challenging. This paper reviews currently adopted synchronization technologies and architectures, identifies vital outstanding technical challenges, and proposes a unified synchronization architecture for use by various industrial applications while addressing security and interoperability requirements. As such, this study aims to bridges gaps and advance robust synchronization in DT environments, emphasizing the need for a standardized architecture to ensure seamless operation and continuous improvement of industrial systems.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [10] [MiTa: A Hierarchical Multi-Agent Collaboration Framework with Memory-integrated and Task Allocation](https://arxiv.org/abs/2601.22974)
*XiaoJie Zhang,JianHan Wu,Xiaoyang Qu,Jianzong Wang*

Main category: cs.ET

TL;DR: MiTa是一个分层记忆集成任务分配框架，通过管理器-成员层次结构和全局任务分配与情景记忆集成，解决多智能体系统中的记忆不一致和行为冲突问题。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大语言模型的多智能体系统缓解了单智能体在复杂任务中的低效问题，但仍存在记忆不一致和智能体行为冲突等挑战，需要更有效的协作框架。

Method: 提出MiTa框架，采用管理器-成员分层结构。管理器包含分配模块和总结模块：分配模块从全局视角分配任务以避免冲突；总结模块在任务进度更新时触发，将近期协作历史压缩为简洁摘要以保持长期上下文。

Result: 实验结果表明，MiTa在复杂多智能体协作中实现了优于强基线方法的效率和适应性。

Conclusion: 通过结合任务分配和情景记忆，MiTa能够更清晰地理解任务并促进全局一致的任务分配，有效提升多智能体协作效率。

Abstract: Recent advances in large language models (LLMs) have substantially accelerated the development of embodied agents. LLM-based multi-agent systems mitigate the inefficiency of single agents in complex tasks. However, they still suffer from issues such as memory inconsistency and agent behavioral conflicts. To address these challenges, we propose MiTa, a hierarchical memory-integrated task allocative framework to enhance collaborative efficiency. MiTa organizes agents into a manager-member hierarchy, where the manager incorporates additional allocation and summary modules that enable (1) global task allocation and (2) episodic memory integration. The allocation module enables the manager to allocate tasks from a global perspective, thereby avoiding potential inter-agent conflicts. The summary module, triggered by task progress updates, performs episodic memory integration by condensing recent collaboration history into a concise summary that preserves long-horizon context. By combining task allocation with episodic memory, MiTa attains a clearer understanding of the task and facilitates globally consistent task distribution. Experimental results confirm that MiTa achieves superior efficiency and adaptability in complex multi-agent cooperation over strong baseline methods.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [11] [SPARK: Real-Time Monitoring of Multi-Faceted Programming Exercises](https://arxiv.org/abs/2601.22256)
*Yinuo Yang,Ashley Ge Zhang,Steve Oney,April Yi Wang*

Main category: cs.HC

TL;DR: SPARK是一个编程练习监控仪表板，通过将子步骤分组为检查点、提供自动测试建议和可视化进度跟踪，帮助教师监控学生在复杂编程任务中的进展。


<details>
  <summary>Details</summary>
Motivation: 监控课堂编程练习有助于识别困难学生和常见挑战，但对于多步骤、复杂依赖关系、无固定完成顺序或难以总结评估标准的复杂问题（如构建交互式Web界面），理解学生进展极其困难。

Method: SPARK允许教师根据练习要求灵活地将子步骤分组为检查点，为这些检查点建议自动测试，并生成可视化来跟踪跨步骤的进度。系统还允许教师检查中间输出以深入了解解决方案的变体。

Result: 构建了包含N=22名学习者解决两个Web编程练习的40分钟击键编码数据集，并通过16名编程教师的受试者内评估提供了SPARK感知有用性的实证见解。

Conclusion: SPARK通过提供灵活的检查点分组、自动测试建议和进度可视化，有效解决了监控复杂编程练习的挑战，为教师提供了深入了解学生解决方案变体的能力。

Abstract: Monitoring in-class programming exercises can help instructors identify struggling students and common challenges. However, understanding students' progress can be prohibitively difficult, particularly for multi-faceted problems that include multiple steps with complex interdependencies, have no predictable completion order, or involve evaluation criteria that are difficult to summarize across many students (e.g., exercises building interactive web-based user interfaces). We introduce SPARK, a coding exercise monitoring dashboard designed to address these challenges. SPARK allows instructors to flexibly group substeps into checkpoints based on exercise requirements, suggests automated tests for these checkpoints, and generates visualizations to track progress across steps. SPARK also allows instructors to inspect intermediate outputs, providing deeper insights into solution variations. We also construct a dataset of 40-minute keystroke coding data from N=22 learners solving two web programming exercises and provide empirical insights into the perceived usefulness of SPARK through a within-subjects evaluation with 16 programming instructors.

</details>


### [12] [PersonaCite: VoC-Grounded Interviewable Agentic Synthetic AI Personas for Verifiable User and Design Research](https://arxiv.org/abs/2601.22288)
*Mario Truss*

Main category: cs.HC

TL;DR: PersonaCite：基于检索增强的AI角色系统，通过检索真实客户反馈作为证据基础，提供可验证的响应和来源归属，解决传统提示式角色生成不可验证响应的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于提示的AI角色生成方法虽然能产生有说服力的响应，但缺乏可验证的证据基础，导致在设计和产品决策中难以信任。需要开发一种能够提供可验证证据的AI角色系统。

Method: PersonaCite采用检索增强的交互方法：1) 在每次对话轮次中检索真实的客户反馈文档；2) 将响应限制在检索到的证据范围内；3) 当证据缺失时明确弃权；4) 提供响应级别的来源归属。通过半结构化访谈和部署研究验证系统。

Result: 通过对14位行业专家的研究，识别了初步发现：感知到的益处、有效性关注点和设计张力。提出了Persona Provenance Cards作为负责任AI角色使用在以人为本设计工作流中的文档模式。

Conclusion: PersonaCite将AI角色重新定义为证据受限的研究工具，通过检索增强的交互和明确的来源归属，提高了AI角色响应的可验证性和可信度，为负责任地在设计工作流中使用AI角色提供了框架。

Abstract: LLM-based and agent-based synthetic personas are increasingly used in design and product decision-making, yet prior work shows that prompt-based personas often produce persuasive but unverifiable responses that obscure their evidentiary basis. We present PersonaCite, an agentic system that reframes AI personas as evidence-bounded research instruments through retrieval-augmented interaction. Unlike prior approaches that rely on prompt-based roleplaying, PersonaCite retrieves actual voice-of-customer artifacts during each conversation turn, constrains responses to retrieved evidence, explicitly abstains when evidence is missing, and provides response-level source attribution. Through semi-structured interviews and deployment study with 14 industry experts, we identify preliminary findings on perceived benefits, validity concerns, and design tensions, and propose Persona Provenance Cards as a documentation pattern for responsible AI persona use in human-centered design workflows.

</details>


### [13] [From Retrieving Information to Reasoning with AI: Exploring Different Interaction Modalities to Support Human-AI Coordination in Clinical Decision-Making](https://arxiv.org/abs/2601.22338)
*Behnam Rahdari,Sameer Shaikh,Jonathan H Chen,Tobias Gerstenberg,Shriti Raj*

Main category: cs.HC

TL;DR: 该研究通过定性方法探讨临床医生对LLM决策支持不同交互模式（文本对话、交互/静态UI、语音）的感知，发现医生主要将LLM用作信息检索工具而非复杂问题处理伙伴，且交互设置和认知风格影响参与度，没有单一最佳交互模式。


<details>
  <summary>Details</summary>
Motivation: LLM在临床决策支持中应用广泛但效果不明确，不清楚临床医生如何使用这项新技术以及如何与传统CDSS比较，这限制了设计能克服现有工具局限、提升性能和体验的新机制。

Method: 定性研究，12名临床医生参与，考察他们对不同交互模式（LLM文本对话、交互/静态UI、语音）在决策支持中的感知，采用开放式使用场景观察。

Result: 1. 参与者采取工具中心方法，将LLM主要用于信息检索和确认，使用简单提示而非作为处理复杂问题的主动审议伙伴；2. 交互设置改变会引发批判性参与；3. 参与度因个体认知风格而异；4. 文本、语音和传统UI各有优缺点，不存在适用于所有情况的单一交互模式。

Conclusion: 临床医生对LLM的使用模式限制了其作为复杂问题处理伙伴的潜力，交互设计和个体差异显著影响使用效果，未来决策支持系统需要提供多样化交互模式以适应不同使用场景和认知风格。

Abstract: LLMs are popular among clinicians for decision-support because of simple text-based interaction. However, their impact on clinicians' performance is ambiguous. Not knowing how clinicians use this new technology and how they compare it to traditional clinical decision-support systems (CDSS) restricts designing novel mechanisms that overcome existing tool limitations and enhance performance and experience. This qualitative study examines how clinicians (n=12) perceive different interaction modalities (text-based conversation with LLMs, interactive and static UI, and voice) for decision-support. In open-ended use of LLM-based tools, our participants took a tool-centric approach using them for information retrieval and confirmation with simple prompts instead of use as active deliberation partners that can handle complex questions. Critical engagement emerged with changes to the interaction setup. Engagement also differed with individual cognitive styles. Lastly, benefits and drawbacks of interaction with text, voice and traditional UIs for clinical decision-support show the lack of a one-size-fits-all interaction modality.

</details>


### [14] [Conversational Inoculation to Enhance Resistance to Misinformation](https://arxiv.org/abs/2601.22394)
*Dániel Szabó,Chi-Lan Yang,Aku Visuri,Jonas Oppenlaender,Bharathi Sekar,Koji Yatani,Simo Hosio*

Main category: cs.HC

TL;DR: 对话式免疫：通过聊天机器人对话帮助人们建立对虚假信息抵抗力的新方法


<details>
  <summary>Details</summary>
Motivation: 虚假信息传播是全球性问题，认知免疫有助于建立对不同形式说服（包括虚假信息）的抵抗力。传统免疫方法存在局限性，需要探索更有效、可扩展的干预方式。

Method: 提出对话式免疫方法，通过动态对话帮助人们建立对虚假信息的抵抗力。开发了基于Web的系统实现该方法，并进行了被试内实验，与两种传统免疫方法进行比较。

Result: 验证了对话式免疫作为一种可行新方法的有效性，显示其能增强参与者对虚假信息的抵抗力。定性分析揭示了独立性和信任是提升效率的因素，交互摩擦是阻碍因素。

Conclusion: 对话式免疫为对抗虚假信息提供了及时的研究方向和有前景的途径，讨论了该方法的机会与挑战，为可扩展的虚假信息对抗方法做出了贡献。

Abstract: Proliferation of misinformation is a globally acknowledged problem. Cognitive Inoculation helps build resistance to different forms of persuasion, such as misinformation. We investigate Conversational Inoculation, a method to help people build resistance to misinformation through dynamic conversations with a chatbot. We built a Web-based system to implement the method, and conducted a within-subject user experiment to compare it with two traditional inoculation methods. Our results validate Conversational Inoculation as a viable novel method, and show how it was able to enhance participants' resistance to misinformation. A qualitative analysis of the conversations between participants and the chatbot reveal independence and trust as factors that boosted the efficiency of Conversational Inoculation, and friction of interaction as a factor hindering it. We discuss the opportunities and challenges of using Conversational Inoculation to combat misinformation. Our work contributes a timely investigation and a promising research direction in scalable ways to combat misinformation.

</details>


### [15] [Does My Chatbot Have an Agenda? Understanding Human and AI Agency in Human-Human-like Chatbot Interaction](https://arxiv.org/abs/2601.22452)
*Bhada Yun,Evgenia Taranova,April Yi Wang*

Main category: cs.HC

TL;DR: 研究通过为期一个月的纵向研究发现，人机聊天室中的主体性是一种涌现的共享体验，控制权在人类设定边界、AI引导意图的过程中被逐轮共同建构，提出了3×5框架来映射主体性行动。


<details>
  <summary>Details</summary>
Motivation: 随着AI聊天机器人从工具转变为伴侣，引发了关于主体性的关键问题：在人机聊天室中，谁驱动对话并设定边界？需要理解人机互动中控制权的动态分配和共同建构过程。

Method: 对22名成年人进行了为期一个月的纵向研究，参与者与研究人员构建的LLM伴侣Day聊天，随后进行半结构化访谈，包括事后引发显著时刻、跨参与者聊天回顾，以及揭示Day垂直（深度寻求）与水平（广度寻求）模式的"策略揭示"。

Result: 发现人机聊天室中的主体性是一种涌现的共享体验：参与者通过设定边界和提供反馈来主张主体性，而AI则被感知为引导意图和驱动执行，控制权在逐轮对话中转移并被共同建构。提出了3×5框架，映射谁（人类、AI、混合）×主体性行动（意图、执行、适应、界定、协商），受个体和环境因素调节。

Conclusion: 主张采用半透明设计（按需透明）、为主体性协商提供空间，以及制定面向主体性感知的对话AI指南，以支持人机互动中健康的主体性动态。

Abstract: AI chatbots are shifting from tools to companions. This raises critical questions about agency: who drives conversations and sets boundaries in human-AI chatrooms? We report a month-long longitudinal study with 22 adults who chatted with Day, an LLM companion we built, followed by a semi-structured interview with post-hoc elicitation of notable moments, cross-participant chat reviews, and a 'strategy reveal' disclosing Day's vertical (depth-seeking) vs. horizontal (breadth-seeking) modes. We discover that agency in human-AI chatrooms is an emergent, shared experience: as participants claimed agency by setting boundaries and providing feedback, and the AI was perceived to steer intentions and drive execution, control shifted and was co-constructed turn-by-turn. We introduce a 3-by-5 framework mapping who (human, AI, hybrid) x agency action (Intention, Execution, Adaptation, Delimitation, Negotiation), modulated by individual and environmental factors. Ultimately, we argue for translucent design (i.e. transparency-on-demand), spaces for agency negotiation, and guidelines toward agency-aware conversational AI.

</details>


### [16] [Design Perspective on Materials Experience: A CiteSpace-Based Bibliometric and Visual Analysis of Interdisciplinary Research](https://arxiv.org/abs/2601.22518)
*Yuxin Zhang,Fan Zhang*

Main category: cs.HC

TL;DR: 基于2005-2024年文献计量分析，材料体验研究正经历深刻变革：材料定义扩展至虚拟与生物媒介，方法论转向数据驱动量化模型，体验中心范式融合设计、科学与技术。


<details>
  <summary>Details</summary>
Motivation: 探究材料体验研究领域的演变趋势，分析其定义扩展、方法论进步和跨学科整合的现状，识别关键驱动因素和全球研究格局，为未来发展方向提供依据。

Method: 采用文献计量分析方法，对2005年至2024年期间的相关文献进行系统性分析，考察材料类型、方法论演变、跨学科整合、国家/地区贡献、机构表现等维度。

Result: 材料定义从传统物质扩展到虚拟和生物媒介；方法论从主观描述转向数据驱动的量化模型；美国、中国、日本、德国、荷兰为领先贡献国；代尔夫特理工大学等机构表现突出；材料驱动设计理论产生基础性影响。

Conclusion: 材料体验研究处于关键转折点，未来发展取决于材料创新、技术整合、感知量化等进展，以及社会文化价值的建立，需要通过设计有效统一这些要素以应对复杂需求。

Abstract: Based on a bibliometric analysis of literature from 2005 to 2024, this study reveals that material experience is undergoing a profound transformation characterized by evolving material definitions, methodological advances, and increasing interdisciplinary integration. Material types now extend beyond traditional substances to encompass virtual and biological media, underscoring a growing emphasis on perception and interaction. Methodologically, the field has transitioned from subjective descriptions to data-driven, quantifiable models focused on objective sensory analysis and multisensory integration to enhance immersion. Key drivers, including human-machine perception convergence, material-driven interface interactions, and the embedding of intelligent interactive functions, propel the discipline toward an experience-centered paradigm reflecting a deep convergence of design, science, and technology. At the national/regional level, the United States, China, Japan, Germany, and the Netherlands lead in contributions, while France, the United Kingdom, and Romania demonstrate significant interdisciplinary progress. At the institutional level, Delft University of Technology, Justus Liebig University Giessen, and the Centre National de la Recherche Scientifique show significant advantages. In particular, the Material-Driven Design theory has established a foundational impact on the discipline, while, regarding general research trends, scholars from the United States, the Netherlands, and Germany maintain the highest academic visibility. Overall, material experience research is at a critical juncture, its future development will depend on progress in material innovation, technological integration, and perceptual quantification, as well as the establishment of socio-cultural values, all of which must be effectively unified through design to address complex evolving needs.

</details>


### [17] [LEAP -- Live Experiments for Active Pedagogy](https://arxiv.org/abs/2601.22534)
*Sumedh Karajagi,Sampad Bhusan Mohanty,Bhaskar Krishnamachari*

Main category: cs.HC

TL;DR: LEAP是一个轻量级软件框架，通过远程可调用函数实现课堂互动计算实验，促进学生主动参与并提供学习过程洞察


<details>
  <summary>Details</summary>
Motivation: 传统课堂演示工具存在局限性：静态演示限制学生参与度；交互式可视化通常由教师单向控制，学生被动观察；工具分散且不统一，难以跨课程共享和复用

Method: 提出LEAP框架，核心思想是远程可调用的教师定义函数。通过API端点，学生可以从编码环境远程调用这些函数，所有调用时间戳记录并持久化到数据库，支持实时可视化参与情况、解决方案路径和常见错误

Result: 开发了示例实验室，涵盖数值分析、机器学习、算法课程等应用领域，以及电气工程、经济学和物理学。框架增强了课堂参与度，为教师提供了可操作的学习过程洞察

Conclusion: LEAP通过标准化实验室格式和在线社区贡献目录，旨在建立全球生态系统，促进互动教学法的交流与扩展，解决传统课堂演示工具的局限性

Abstract: Interactive computational environments can help students explore algorithmic concepts through collaborative hands-on experimentation. However, static and instructor controlled demos in lectures limit engagement. Even when interactive visualizations are used, interactions are solely controlled by the instructor, leaving students as passive observers. In addition, the tools used for demonstration often vary significantly, as they are typically developed by individual instructors. Consequently, the visualizations remain confined to a single classroom, rather than being shared and adapted across courses or reused by other instructors. To address this gap and foster active engagement in live classrooms, we present a lightweight and seamless software framework named LEAP for developing interactive computational lab exercises using a simple idea: remotely callable instructor-defined functions. Using API endpoints and a provided client, students can discover and then call instructor defined functions remotely from their coding environment using scripts or interactive notebooks. Each function call is time-stamped and persistently logged in a database, allowing real-time visualization of participation, diverse solution paths, common pitfalls, and live feedback through collaboration, gamification, and quizzes. Labs are packaged as self-contained folders, each containing their own remotely callable functions. We provide example labs to demonstrate applications relevant for numerical analysis, machine learning, algorithms courses and mention some in electrical engineering (EE), economics, and physics. These capabilities enhance engagement and provide instructors with actionable insights into learning processes. With a standardized lab format and an online directory for community-contributed labs, we aim to foster a global ecosystem for exchanging and expanding interactive pedagogy enabled by LEAP.

</details>


### [18] [Assistive Robots and Reasonable Work Assignment Reduce Perceived Stigma toward Persons with Disabilities](https://arxiv.org/abs/2601.22689)
*Stina Klein,Birgit Prodinger,Elisabeth André,Lars Mikelsons,Nils Mandischer*

Main category: cs.HC

TL;DR: 辅助机器人能减少对残疾同事的认知偏见，特别是在工作适配或提供通用机器人辅助时效果更佳


<details>
  <summary>Details</summary>
Motivation: 虽然机器人能帮助残疾人克服身体障碍，但其对促进社会包容的作用尚不明确。本研究旨在探讨机器人辅助是否会减少或增加工作场所中对残疾同事的偏见感知。

Method: 采用情景实验设计，设置四种实验条件：工作过载、工作适配、仅残疾人使用机器人辅助、以及为所有人提供机器人辅助（通用设计）。通过问卷调查测量参与者对残疾同事的认知和行为偏见。

Result: 当工作适配个人能力或通过辅助机器人增强时，认知偏见显著降低。为所有人提供机器人辅助（通用设计）进一步减少了认知偏见。行为偏见的变化不显著。

Conclusion: 辅助机器人能有效减少对残疾人的认知偏见，支持在工作场景中使用协作机器人来促进残疾人的社会包容。

Abstract: Robots are becoming more prominent in assisting persons with disabilities (PwD). Whilst there is broad consensus that robots can assist in mitigating physical impairments, the extent to which they can facilitate social inclusion remains equivocal. In fact, the exposed status of assisted workers could likewise lead to reduced or increased perceived stigma by other workers. We present a vignette study on the perceived cognitive and behavioral stigma toward PwD in the workplace. We designed four experimental conditions depicting a coworker with an impairment in work scenarios: overburdened work, suitable work, and robot-assisted work only for the coworker, and an offer of robot-assisted work for everyone. Our results show that cognitive stigma is significantly reduced when the work task is adapted to the person's abilities or augmented by an assistive robot. In addition, offering robot-assisted work for everyone, in the sense of universal design, further reduces perceived cognitive stigma. Thus, we conclude that assistive robots reduce perceived cognitive stigma, thereby supporting the use of collaborative robots in work scenarios involving PwDs.

</details>


### [19] [Qualitative Evaluation of LLM-Designed GUI](https://arxiv.org/abs/2601.22759)
*Bartosz Sawicki,Tomasz Les,Dariusz Parzych,Aleksandra Wycisk-Ficek,Pawel Trebacz,Pawel Zawadzki*

Main category: cs.HC

TL;DR: LLMs在GUI设计中的可用性和适应性研究：虽然能创建结构化布局，但在可访问性标准和交互功能方面存在挑战，需要人工干预确保可用性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能的发展，大型语言模型（LLMs）被探索用于自动化图形用户界面（GUI）设计。本研究旨在调查LLM生成界面的可用性和适应性，分析其满足多样化用户需求的能力。

Method: 使用2025年1月的三种最先进模型（OpenAI GPT o3-mini-high、DeepSeek R1和Anthropic Claude 3.5 Sonnet）为三种界面类型生成原型：聊天系统、技术团队面板和经理仪表板。通过专家评估分析LLM生成界面的表现。

Result: 专家评估显示：LLMs能有效创建结构化布局，但在满足可访问性标准和提供交互功能方面面临挑战。进一步测试表明，LLMs能部分针对不同用户角色定制界面，但缺乏更深层次的上下文理解。

Conclusion: LLMs是早期UI原型设计的有前途工具，但人工干预对于确保可用性、可访问性和用户满意度仍然至关重要。LLMs目前更适合作为辅助工具而非完全自动化解决方案。

Abstract: As generative artificial intelligence advances, Large Language Models (LLMs) are being explored for automated graphical user interface (GUI) design. This study investigates the usability and adaptability of LLM-generated interfaces by analysing their ability to meet diverse user needs. The experiments included utilization of three state-of-the-art models from January 2025 (OpenAI GPT o3-mini-high, DeepSeek R1, and Anthropic Claude 3.5 Sonnet) generating mockups for three interface types: a chat system, a technical team panel, and a manager dashboard. Expert evaluations revealed that while LLMs are effective at creating structured layouts, they face challenges in meeting accessibility standards and providing interactive functionality. Further testing showed that LLMs could partially tailor interfaces for different user personas but lacked deeper contextual understanding. The results suggest that while LLMs are promising tools for early-stage UI prototyping, human intervention remains critical to ensure usability, accessibility, and user satisfaction.

</details>


### [20] [FACET: Multi-Agent AI Supporting Teachers in Scaling Differentiated Learning for Diverse Students](https://arxiv.org/abs/2601.22788)
*Jana Gonnermann-Müller,Jennifer Haase,Nicolas Leins,Moritz Igel,Konstantin Fackeldey,Sebastian Pokutta*

Main category: cs.HC

TL;DR: FACET是一个面向教师的多智能体框架，旨在支持差异化教学，考虑动机、表现和学习差异，通过教师参与的设计解决课堂异质性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 课堂日益异质化，包含不同表现水平、动机水平、语言能力和学习差异（如阅读障碍和ADHD）的学生。教师认识到差异化教学的必要性，但工作负荷增加造成实质性障碍，使差异化教学成为实践中难以实现的理想。当前AI教育工具主要面向学生且以表现为中心，忽视了影响学习结果的其他方面。

Method: 开发了FACET框架，采用教师参与的多智能体设计，协调四个专门智能体：学习者模拟、诊断评估、材料生成和评估。通过参与式工作坊与学校校长（N=30）共同制定系统需求，由在职K-12教师（N=70）评估材料质量，采用混合方法进行评估。

Result: 混合方法评估显示对包容性差异化教学的强烈感知价值。从业者强调课堂异质性带来的紧迫需求，以及保持教学自主权作为采用前提的重要性。框架表现出良好的实用性和接受度。

Conclusion: FACET框架为解决课堂异质性挑战提供了有前景的解决方案，通过教师参与的设计平衡了技术支持和教学自主权。研究讨论了未来学校部署的影响，并概述了纵向课堂实施的合作伙伴关系。

Abstract: Classrooms are becoming increasingly heterogeneous, comprising learners with diverse performance and motivation levels, language proficiencies, and learning differences such as dyslexia and ADHD. While teachers recognize the need for differentiated instruction, growing workloads create substantial barriers, making differentiated instruction an ideal that is often unrealized in practice. Current AI educational tools, which promise differentiated materials, are predominantly student-facing and performance-centric, ignoring other aspects that shape learning outcomes. We introduce FACET, a teacher-facing multi-agent framework designed to address these gaps by supporting differentiation that accounts for motivation, performance, and learning differences. Developed with educational stakeholders from the outset, the framework coordinates four specialized agents, including learner simulation, diagnostic assessment, material generation, and evaluation within a teacher-in-the-loop design. School principals (N = 30) shaped system requirements through participatory workshops, while in-service K-12 teachers (N = 70) evaluated material quality. Mixed-methods evaluation demonstrates strong perceived value for inclusive differentiation. Practitioners emphasized both the urgent need arising from classroom heterogeneity and the importance of maintaining pedagogical autonomy as a prerequisite for adoption. We discuss implications for future school deployment and outline partnerships for longitudinal classroom implementation.

</details>


### [21] [μTouch: Enabling Accurate, Lightweight Self-Touch Sensing with Passive Magnets](https://arxiv.org/abs/2601.22864)
*Siyuan Wang,Ke Li,Jingyuan Huang,Jike Wang,Cheng Zhang,Alanson Sample,Dongyao Chen*

Main category: cs.HC

TL;DR: μTouch：基于磁感应的自触手势识别平台，通过紧凑硬件设计、轻量级半监督框架和环境干扰抑制，实现高精度微手势检测，适用于卫生监测和皮肤健康应用。


<details>
  <summary>Details</summary>
Motivation: 自触手势（如面部触摸和手指抓挠）能提供丰富的人类行为洞察，但现有方法难以检测这些微手势，因其运动模式多样且复杂。

Method: 提出μTouch磁感应平台，包含：1）紧凑硬件设计（低功耗磁力计和磁性硅）；2）轻量级半监督框架，仅需少量用户数据；3）环境场检测模块以减轻干扰。

Result: 在11和12名参与者的用户研究中，μTouch仅需每个手势3秒微调数据，新用户使用前准备时间少于1分钟。能区分8种面部触摸行为（平均准确率93.41%），可靠检测身体抓挠行为（平均准确率94.63%），一个月后仍保持准确稳健。

Conclusion: μTouch展示了作为卫生监测和皮肤健康应用实用工具的潜力，通过磁感应技术实现了准确、稳健的自触手势识别。

Abstract: Self-touch gestures (e.g., nuanced facial touches and subtle finger scratches) provide rich insights into human behaviors, from hygiene practices to health monitoring. However, existing approaches fall short in detecting such micro gestures due to their diverse movement patterns.
  This paper presents μTouch, a novel magnetic sensing platform for self-touch gesture recognition. μTouch features (1) a compact hardware design with low-power magnetometers and magnetic silicon, (2) a lightweight semi-supervised framework requiring minimal user data, and (3) an ambient field detection module to mitigate environmental interference. We evaluated μTouch in two representative applications in user studies with 11 and 12 participants. μTouch only requires three-second fine-tuning data for each gesture, and new users need less than one minute before starting to use the system. μTouch can distinguish eight different face-touching behaviors with an average accuracy of 93.41%, and reliably detect body-scratch behaviors with an average accuracy of 94.63%. μTouch demonstrates accurate and robust sensing performance even after a month, showcasing its potential as a practical tool for hygiene monitoring and dermatological health applications.

</details>


### [22] [Integrating Multi-Label Classification and Generative AI for Scalable Analysis of User Feedback](https://arxiv.org/abs/2601.23018)
*Sandra Loop,Erik Bertram,Sebastian Juhl,Martin Schrepp*

Main category: cs.HC

TL;DR: 该论文提出了在大型软件公司用户体验测量项目中开发的用户评论分析技术，包括基于监督学习的主题分类、生成式AI的反馈摘要，以及验证情感分析不能可靠反映用户满意度。


<details>
  <summary>Details</summary>
Motivation: 在竞争激烈的软件市场中，用户体验评估对确保软件质量和促进产品长期成功至关重要。开放式用户反馈虽然能提供有价值的改进见解并帮助解释定量结果，但分析大量用户评论既困难又耗时。

Method: 1. 采用监督机器学习方法为每条评论分配预定义的主题标签，提供高层次概览；2. 利用生成式AI创建用户反馈的简洁信息摘要，便于向组织特别是高层管理人员传达发现；3. 研究用户评论中的情感表达是否能作为整体产品满意度的指标。

Result: 研究结果表明，仅靠情感分析不能可靠地反映用户满意度。产品满意度需要在调查中明确评估，以衡量用户对产品的感知。

Conclusion: 论文提出了有效的用户评论分析技术组合：监督学习用于主题分类，生成式AI用于摘要生成，并明确指出情感分析不能替代明确的满意度测量。这些方法有助于高效处理大量用户反馈，支持组织决策。

Abstract: In highly competitive software markets, user experience (UX) evaluation is crucial for ensuring software quality and fostering long-term product success. Such UX evaluations typically combine quantitative metrics from standardized questionnaires with qualitative feedback collected through open-ended questions. While open-ended feedback offers valuable insights for improvement and helps explain quantitative results, analyzing large volumes of user comments is challenging and time-consuming. In this paper, we present techniques developed during a long-term UX measurement project at a major software company to efficiently process and interpret extensive volumes of user comments. To provide a high-level overview of the collected comments, we employ a supervised machine learning approach that assigns meaningful, pre-defined topic labels to each comment. Additionally, we demonstrate how generative AI (GenAI) can be leveraged to create concise and informative summaries of user feedback, facilitating effective communication of findings to the organization and especially upper management. Finally, we investigate whether the sentiment expressed in user comments can serve as an indicator for overall product satisfaction. Our results show that sentiment analysis alone does not reliably reflect user satisfaction. Instead, product satisfaction needs to be assessed explicitly in surveys to measure the user's perception of the product.

</details>


### [23] [Exploring Sidewalk Sheds in New York City through Chatbot Surveys and Human Computer Interaction](https://arxiv.org/abs/2601.23095)
*Junyi Li,Zhaoxi Zhang,Tamir Mendel,Takahiro Yabe*

Main category: cs.HC

TL;DR: 开发基于AI聊天机器人的调查方法，通过图像标注和路线选择评估纽约市人行道棚架对行人可见性和移动行为的影响


<details>
  <summary>Details</summary>
Motivation: 纽约市人行道棚架虽然用于安全目的，但政策制定者和商家担心其降低店面可见性并改变行人导航。当前规划实践中缺乏对行人可见性和移动影响的直接测量方法

Method: 开发集成大型语言模型（Google Gemini-1.5-flash-001）的图像标注聊天机器人调查，收集行人图像标注和路线选择数据。采用网格分析入口标注，应用逻辑混合效应模型评估人行道选择模式

Result: 脚手架显著降低行人识别底层零售入口的能力；天气条件和棚架设计特征（净空高度、柱间距、颜色）显著影响人行道选择行为

Conclusion: 通过将生成式AI整合到城市研究中，本研究展示了一种评估人行道棚架设计的新方法，为调整棚架指南提供实证证据，可在不损害安全性的前提下改善行人体验

Abstract: Sidewalk sheds are a common feature of the streetscape in New York City, reflecting ongoing construction and maintenance activities. However, policymakers and local business owners have raised concerns about reduced storefront visibility and altered pedestrian navigation. Although sidewalk sheds are widely used for safety, their effects on pedestrian visibility and movement are not directly measured in current planning practices. To address this, we developed an AI-based chatbot survey that collects image-based annotations and route choices from pedestrians, linking these responses to specific shed design features, including clearance height, post spacing, and color. This AI chatbot survey integrates a large language model (e.g., Google's Gemini-1.5-flash-001 model) with an image-annotation interface, allowing users to interact with street images, mark visual elements, and provide structured feedback through guided dialogue. To explore pedestrian perceptions and behaviors, this paper conducts a grid-based analysis of entrance annotations and applies logistic mixed-effects modeling to assess sidewalk choice patterns. Analysis of the dataset (n = 25) shows that: (1) the presence of scaffolding significantly reduces pedestrians' ability to identify ground-floor retail entrances, and (2) variations in weather conditions and shed design features significantly influence sidewalk selection behavior. By integrating generative AI into urban research, this study demonstrates a novel method for evaluating sidewalk shed designs and provides empirical evidence to support adjustments to shed guidelines that improve the pedestrian experience without compromising safety.

</details>


### [24] ["I Choose to Live, for Life Itself": Understanding Agency of Home-Based Care Patients Through Information Practices and Relational Dynamics in Care Networks](https://arxiv.org/abs/2601.23127)
*Sung-In Kim,Joonyoung Park,Bogoan Kim,Hwajung Hong*

Main category: cs.HC

TL;DR: 研究探讨家庭护理中患者能动性的表现及在共享规划中代表性不足的原因，提出设计考虑以弥合这一差距


<details>
  <summary>Details</summary>
Motivation: 家庭护理为患者中心护理提供独特机会，但患者能动性在共享规划过程中往往代表性不足，需要理解这一现象的原因和机制

Method: 通过23次多利益相关方访谈（患者、医疗专业人员、护理人员）和60小时民族志观察，研究患者能动性在家庭护理中的表现

Result: 患者能动性不是静态个体属性，而是通过维持日常连续性、获得护理提供者相互认可、与物质家庭环境互动形成的关系能力；结构化文档系统过滤情境知识，非正式沟通渠道碎片化患者声音，医生中心层级将患者定位为被动接受者

Conclusion: 提出设计考虑以弥合代表性差距，将患者能动性整合到共享家庭护理计划中

Abstract: Home-based care (HBC) delivers medical and care services in patients' living environments, offering unique opportunities for patient-centered care. However, patient agency is often inadequately represented in shared HBC planning processes. Through 23 multi-stakeholder interviews with HBC patients, healthcare professionals, and care workers, alongside 60 hours of ethnographic observations, we examined how patient agency manifests in HBC and why this representation gap occurs. Our findings reveal that patient agency is not a static individual attribute but a relational capacity shaped through maintaining everyday continuity, mutual recognition from care providers, and engagement with material home environments. Furthermore, we identified that structured documentation systems filter out contextual knowledge, informal communication channels fragment patient voices, and doctor-centered hierarchies position patients as passive recipients. Drawing on these insights, we propose design considerations to bridge this representation gap and to integrate patient agency into shared HBC plans.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [25] [Partial Rewriting and Value Interpretation of Logically Constrained Terms (Full Version)](https://arxiv.org/abs/2601.22191)
*Takahito Aoto,Naoki Nishida,Jonas Schöpf*

Main category: cs.LO

TL;DR: 本文引入部分约束重写作为存在约束项重写的一种变体，研究其与最一般约束重写的差异，建立两者对应关系并提供特征化描述


<details>
  <summary>Details</summary>
Motivation: 逻辑约束项重写系统（LCTRSs）支持内置数据结构，最近的存在约束项和最一般约束重写框架相比原始方法有优势。部分约束重写已在先前LCTRSs分析和应用中隐式出现，需要系统研究其与最一般约束重写的差异

Method: 引入部分约束重写概念，建立与最一般约束重写的直接对应关系（利用约束项的子包含和等价性），通过实例化解释存在约束项来特征化两种重写，并引入新的值解释概念

Result: 建立了部分约束重写与最一般约束重写之间的对应关系，提供了两种重写形式的特征化描述，通过值解释概念揭示了部分重写与最一般重写之间的细微差异

Conclusion: 部分约束重写作为存在约束项重写的变体，与最一般约束重写存在系统差异但可通过子包含和等价性建立对应关系，值解释概念有助于理解这些差异，为LCTRSs的分析和应用提供理论基础

Abstract: Logically constrained term rewrite systems (LCTRSs) are a rewriting formalism that naturally supports built-in data structures, including integers and bit-vectors. The recent framework of existentially constrained terms and most general constrained rewriting on them (Takahata et al., 2025) has many advantages over the original approach of rewriting constrained terms. In this paper, we introduce partial constrained rewriting, a variant of rewriting existentially constrained terms whose underlying idea has already appeared implicitly in previous analyses and applications of LCTRSs. We examine the differences between these two notions of constrained rewriting. First, we establish a direct correspondence between them, leveraging subsumption and equivalence of constrained terms where appropriate. Then we give characterizations of each of them, using the interpretation of existentially constrained terms by instantiation. We further introduce the novel notion of value interpretation, that highlights subtle differences between partial and most general rewriting.

</details>


### [26] [Proof Complexity of Linear Logics](https://arxiv.org/abs/2601.22393)
*Amirhossein Akbar Tabatabai,Raheleh Jalali*

Main category: cs.LO

TL;DR: 该论文研究了经典命题逻辑序列演算LK的证明规模下界问题，通过分离结构规则（收缩和弱化）的作用，证明了在缺少特定结构规则时，某些公式需要指数级或亚指数级证明规模，而恢复规则后则有多项式规模证明。


<details>
  <summary>Details</summary>
Motivation: 证明经典命题逻辑序列演算LK的证明规模下界是证明复杂性领域的主要开放问题。本文旨在通过分离结构规则（收缩和弱化）的各自作用来深入理解这一挑战，揭示这些规则的组合效应远强于任何单个规则。

Method: 使用带有交换的Full Lambek演算FLe及其收缩扩展变体FLec作为子结构系统基础。构造了在FLe（或FLec）中可证的公式族，这些公式在仿射线性逻辑ALL（或相关线性逻辑RLL）中需要指数级（或亚指数级）证明规模，但一旦恢复收缩（或弱化）规则，就存在多项式规模证明。

Result: 1. 对于缺少收缩的LK，建立了指数级证明规模下界；对于缺少弱化的LK，建立了亚指数级下界。2. 证明了FLe可证公式在ALL中需要指数级证明规模，从而也适用于MALL、AMALL和完整经典线性逻辑CLL。3. 展示了具有多项式规模FLe证明的公式在无切割LK中需要指数级证明，建立了各种线性演算与其无切割对应系统之间的指数级加速。

Conclusion: 结构规则的组合效应远强于任何单个规则，这一发现为理解LK证明规模下界问题提供了新视角。通过分离收缩和弱化规则的作用，论文建立了在缺少特定结构规则时的指数级和亚指数级下界，并展示了线性演算与无切割系统之间的指数级加速现象。

Abstract: Proving proof-size lower bounds for $\mathbf{LK}$, the sequent calculus for classical propositional logic, remains a major open problem in proof complexity. We shed new light on this challenge by isolating the power of structural rules, showing that their combination is extremely stronger than any single rule alone. We establish exponential (resp. sub-exponential) proof-size lower bounds for $\mathbf{LK}$ without contraction (resp. weakening) for formulas with short $\mathbf{LK}$-proofs. Concretely, we work with the Full Lambek calculus with exchange, $\mathbf{FL_e}$, and its contraction-extended variant, $\mathbf{FL_{ec}}$, substructural systems underlying linear logic. We construct families of $\mathbf{FL_e}$-provable (resp. $\mathbf{FL_{ec}}$-provable) formulas that require exponential-size (resp. sub-exponential-size) proofs in affine linear logic $\mathbf{ALL}$ (resp. relevant linear logic $\mathbf{RLL}$), but admit polynomial-size proofs once contraction (resp. weakening) is restored. This yields exponential lower bounds on proof-size of $\mathbf{FL_e}$-provable formulas in $\mathbf{ALL}$ and hence for $\mathbf{MALL}$, $\mathbf{AMALL}$, and full classical linear logic $\mathbf{CLL}$. Finally, we exhibit formulas with polynomial-size $\mathbf{FL_e}$-proofs that nevertheless require exponential-size proofs in cut-free $\mathbf{LK}$, establishing exponential speed-ups between various linear calculi and their cut-free counterparts.

</details>


### [27] [LeanArchitect: Automating Blueprint Generation for Humans and AI](https://arxiv.org/abs/2601.22554)
*Thomas Zhu,Pietro Monticone,Jeremy Avigad,Sean Welleck*

Main category: cs.LO

TL;DR: LeanArchitect：一个直接从Lean代码中提取、管理和导出蓝图数据的Lean包，通过声明式注解机制将形式声明与蓝图元数据关联，自动推断依赖关系并生成与Lean开发同步的LaTeX蓝图内容。


<details>
  <summary>Details</summary>
Motivation: 现有工具将非正式（LaTeX）和正式（Lean）组件视为基本解耦的工件，导致维护开销并限制了与AI自动化的集成。蓝图在人类协作中至关重要，但缺乏与形式代码的紧密集成。

Method: 引入声明式注解机制，将形式声明与蓝图元数据关联；自动推断依赖信息；从Lean代码生成同步的LaTeX蓝图内容；消除形式和非正式表示之间的重复。

Result: 成功自动化转换了几个大型现有蓝图驱动项目；通过人机协作案例研究形式化了多元泰勒定理；提高了可维护性，暴露了现有蓝图中的潜在不一致性，为AI工具集成提供了有效接口。

Conclusion: LeanArchitect通过紧密集成形式和非正式数学表示，改善了大型形式化项目的可维护性，促进了人机协作，并为AI工具集成提供了实用框架。

Abstract: Large-scale formalization projects in Lean rely on blueprints: structured dependency graphs linking informal mathematical exposition to formal declarations. While blueprints are central to human collaboration, existing tooling treats the informal ($\LaTeX$) and formal (Lean) components as largely decoupled artifacts, leading to maintenance overhead and limiting integration with AI automation. We present LeanArchitect, a Lean package for extracting, managing, and exporting blueprint data directly from Lean code. LeanArchitect introduces a declarative annotation mechanism that associates formal declarations with blueprint metadata, automatically infers dependency information, and generates $\LaTeX$ blueprint content synchronized with the Lean development. This design eliminates duplication between formal and informal representations and eases fine-grained progress tracking for both human contributors and AI-based theorem provers. We demonstrate the practicality of LeanArchitect through the automated conversion of several large existing blueprint-driven projects, and through a human--AI collaboration case study formalizing a multivariate Taylor theorem. Our results show that LeanArchitect improves maintainability, exposes latent inconsistencies in existing blueprints, and provides an effective interface for integrating AI tools into real-world formalization workflows.

</details>


### [28] [A Complete Finitary Refinement Type System for Scott-Open Properties](https://arxiv.org/abs/2601.23082)
*Colin Riba,Adam Donadille*

Main category: cs.LO

TL;DR: 提出一个用于证明处理无限数据（如流或非良基树）的函数输入输出性质的有限精化类型系统，该系统对Scott开性质是完备的


<details>
  <summary>Details</summary>
Motivation: 需要证明处理无限数据结构的函数的输入输出性质，传统方法难以处理这类无限数据，需要建立基于域理论的逻辑形式化方法

Method: 基于Abramsky的域理论逻辑形式，利用Scott域是谱空间的性质，构建有限精化类型系统，通过正负公式的极性对称性（正公式对应最小不动点定义Scott开性质，负公式对应最大不动点定义紧饱和性质）和可实现性蕴含来表述函数类型的非平凡输入输出性质

Result: 开发了一个对Scott开性质完备的有限精化类型系统，能够通过正公式在函数类型上表述非平凡的输入输出性质，系统具有完备性

Conclusion: 该工作为处理无限数据结构的函数性质验证提供了理论基础，通过域理论和逻辑极性的结合，实现了对Scott开性质的完备验证

Abstract: We are interested in proving input-output properties of functions that handle infinite data such as streams or non-wellfounded trees. We provide a finitary refinement type system which is sound and complete for Scott-open properties defined in a fixpoint-like logic. Working on top of Abramsky's Domain Theory in Logical Form, we build from the well-known fact that the Scott domains interpreting recursive types are spectral spaces. The usual symmetry between Scott-open and compact-saturated sets is reflected in logical polarities: positive formulae allow for least fixpoints and define Scott-open properties, while negative formulae allow for greatest fixpoints and define compact-saturated properties. A realizability implication with the usual (contra)variance on polarities allows for non-trivial input-output properties to be formulated as positive formulae on function types.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [29] [Recursive Mutexes in Separation Logic](https://arxiv.org/abs/2601.22557)
*Ke Du,William Mansky,Paolo G. Giarrusso,Gregory Malecha*

Main category: cs.PL

TL;DR: 该论文为递归互斥锁开发了分离逻辑规范，类似于普通互斥锁的两种规范风格（保护不变量或原子状态变化），使客户端能够统一处理所有获取/释放操作。


<details>
  <summary>Details</summary>
Motivation: 递归互斥锁在C++、Java等面向对象语言中很常见，但现有的分离逻辑规范主要针对普通互斥锁。需要为递归互斥锁开发类似的规范，以支持同一线程多次获取锁的特性，并提供统一的获取/释放处理方式。

Method: 基于分离逻辑，为递归互斥锁开发两种规范风格：1）保护不变量的规范，类似于普通互斥锁；2）原子状态变化的规范。规范设计使客户端能够统一处理所有获取和释放操作，只需在访问锁不变量时确定是否持有该互斥锁。

Result: 成功为递归互斥锁开发了分离逻辑规范，这些规范能够：1）支持同一线程多次获取锁的特性；2）提供统一的获取/释放操作处理；3）使客户端只需关注访问锁不变量时的持有状态判断。

Conclusion: 递归互斥锁可以在分离逻辑中获得与普通互斥锁类似的规范支持，通过统一的获取/释放处理简化客户端代码，为面向对象语言中的并发编程提供形式化验证基础。

Abstract: Mutexes (i.e., locks) are well understood in separation logic, and can be specified in terms of either protecting an invariant or atomically changing the state of the lock. In this abstract, we develop the same styles of specifications for \emph{recursive} mutexes, a common variant of mutexes in object-oriented languages such as C++ and Java. A recursive mutex can be acquired any number of times by the same thread, and our specifications treat all acquires/releases uniformly, with clients only needing to determine whether they hold the mutex when accessing the lock invariant.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [Linux Kernel Recency Matters, CVE Severity Doesn't, and History Fades](https://arxiv.org/abs/2601.22196)
*Piotr Przymus,Witold Weiner,Krzysztof Rykaczewski,Gunnar Kudrjavets*

Main category: cs.SE

TL;DR: Linux内核成为CVE编号机构后，研究发现补丁延迟与严重性评分关联微弱，内核新近度是更好的预测因子，旧内核CVE修复更慢，引入漏洞的提交比修复更复杂。


<details>
  <summary>Details</summary>
Motivation: 分析Linux内核作为CVE编号机构后，内核漏洞的解剖结构和动态变化，理解驱动补丁修复的因素，特别是补丁延迟的决定因素。

Method: 使用元数据、相关提交和补丁延迟分析内核CVE，通过生存模型研究补丁延迟的预测因子，比较引入漏洞的提交与修复提交的复杂性。

Result: 严重性和CVSS评分与补丁延迟关联微弱；内核新近度是合理的预测因子，新内核修复更快，旧内核保留未解决的CVE；引入漏洞的提交通常比修复更广泛和复杂。

Conclusion: Linux内核的CVE流程独特，补丁延迟主要受内核版本新近度影响而非传统安全指标，开发历史的重建通常只是近似，内核作为开源项目具有特殊性。

Abstract: In 2024, the Linux kernel became its own Common Vulnerabilities and Exposures (CVE) Numbering Authority (CNA), formalizing how kernel vulnerabilities are identified and tracked. We analyze the anatomy and dynamics of kernel CVEs using metadata, associated commits, and patch latency to understand what drives patching. Results show that severity and Common Vulnerability Scoring System (CVSS) metrics have a negligible association with patch latency, whereas kernel recency is a reasonable predictor in survival models. Kernel developers fix newer kernels sooner, while older ones retain unresolved CVEs. Commits introducing vulnerabilities are typically broader and more complex than their fixes, though often only approximate reconstructions of development history. The Linux kernel remains a unique open-source project -- its CVE process is no exception.

</details>


### [31] [Stalled, Biased, and Confused: Uncovering Reasoning Failures in LLMs for Cloud-Based Root Cause Analysis](https://arxiv.org/abs/2601.22208)
*Evelien Riddell,James Riddell,Gengyi Sun,Michał Antkiewicz,Krzysztof Czarnecki*

Main category: cs.SE

TL;DR: 该研究通过控制实验框架评估LLM在多跳根因分析中的推理能力，测试了6个LLM在两种智能体工作流下的表现，发现了16种常见推理失败模式。


<details>
  <summary>Details</summary>
Motivation: 现代云系统的分布式和相互依赖特性使根因分析变得复杂，特别是多跳故障传播场景。虽然LLM为自动化RCA提供了新机会，但现有方法依赖历史事件语料库、处理超出LLM容量的海量遥测数据，或将推理嵌入复杂多智能体管道，这些设计选择模糊了失败是源于推理本身还是外围设计。

Method: 设计控制实验框架，简化实验设置以突出LLM推理行为。评估6个LLM在两种智能体工作流（ReAct和Plan-and-Execute）和非智能体基线下的表现，使用两个真实案例研究（GAIA和OpenRCA）。执行48,000个模拟故障场景，测量根因准确性和中间推理轨迹质量，创建16种常见RCA推理失败的标记分类法，使用LLM-as-a-Judge进行标注。

Result: 执行了总计228天的执行时间，明确了当前开源LLM在多跳RCA中的成功和失败之处，量化了对输入数据模态的敏感性，识别了预测最终正确性的推理失败模式。

Conclusion: 该研究提供了透明可复现的实证结果和失败分类法，为未来基于推理的系统诊断研究提供指导，有助于理解LLM在复杂故障诊断场景中的实际推理能力。

Abstract: Root cause analysis (RCA) is essential for diagnosing failures within complex software systems to ensure system reliability. The highly distributed and interdependent nature of modern cloud-based systems often complicates RCA efforts, particularly for multi-hop fault propagation, where symptoms appear far from their true causes. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance automated RCA. However, their practical value for RCA depends on the fidelity of reasoning and decision-making. Existing work relies on historical incident corpora, operates directly on high-volume telemetry beyond current LLM capacity, or embeds reasoning inside complex multi-agent pipelines -- conditions that obscure whether failures arise from reasoning itself or from peripheral design choices.
  We present a focused empirical evaluation that isolates an LLM's reasoning behavior. We design a controlled experimental framework that foregrounds the LLM by using a simplified experimental setting. We evaluate six LLMs under two agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline on two real-world case studies (GAIA and OpenRCA). In total, we executed 48,000 simulated failure scenarios, totaling 228 days of execution time. We measure both root-cause accuracy and the quality of intermediate reasoning traces. We produce a labeled taxonomy of 16 common RCA reasoning failures and use an LLM-as-a-Judge for annotation. Our results clarify where current open-source LLMs succeed and fail in multi-hop RCA, quantify sensitivity to input data modalities, and identify reasoning failures that predict final correctness. Together, these contributions provide transparent and reproducible empirical results and a failure taxonomy to guide future work on reasoning-driven system diagnosis.

</details>


### [32] [Predicting Intermittent Job Failure Categories for Diagnosis Using Few-Shot Fine-Tuned Language Models](https://arxiv.org/abs/2601.22264)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: FlaXifyer是一个基于预训练语言模型的少样本学习方法，用于预测CI流水线中间歇性作业失败类别，仅需作业执行日志和少量标注数据，同时提出LogSift解释性技术加速故障诊断。


<details>
  <summary>Details</summary>
Motivation: CI流水线中的间歇性（flaky）作业失败导致计算资源浪费和开发人员诊断时间消耗，现有机器学习方法仅检测间歇性失败而未解决后续诊断挑战。

Method: 提出FlaXifyer：基于预训练语言模型的少样本学习方法，仅使用作业执行日志预测间歇性作业失败类别；提出LogSift：解释性技术，快速识别有影响力的日志语句。

Result: FlaXifyer仅需每类别12个标注样本，达到84.3% Macro F1和92.0% Top-2准确率；LogSift在1秒内识别有影响力日志语句，减少74.4%审查工作量，在87%情况下发现相关故障信息。

Conclusion: FlaXifyer和LogSift实现了有效的自动化故障分类，加速了故障诊断，为自动化解决间歇性作业失败铺平了道路。

Abstract: In principle, Continuous Integration (CI) pipeline failures provide valuable feedback to developers on code-related errors. In practice, however, pipeline jobs often fail intermittently due to non-deterministic tests, network outages, infrastructure failures, resource exhaustion, and other reliability issues. These intermittent (flaky) job failures lead to substantial inefficiencies: wasted computational resources from repeated reruns and significant diagnosis time that distracts developers from core activities and often requires intervention from specialized teams. Prior work has proposed machine learning techniques to detect intermittent failures, but does not address the subsequent diagnosis challenge. To fill this gap, we introduce FlaXifyer, a few-shot learning approach for predicting intermittent job failure categories using pre-trained language models. FlaXifyer requires only job execution logs and achieves 84.3% Macro F1 and 92.0% Top-2 accuracy with just 12 labeled examples per category. We also propose LogSift, an interpretability technique that identifies influential log statements in under one second, reducing review effort by 74.4% while surfacing relevant failure information in 87% of cases. Evaluation on 2,458 job failures from TELUS demonstrates that FlaXifyer and LogSift enable effective automated triage, accelerate failure diagnosis, and pave the way towards the automated resolution of intermittent job failures.

</details>


### [33] [PriviSense: A Frida-Based Framework for Multi-Sensor Spoofing on Android](https://arxiv.org/abs/2601.22414)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.SE

TL;DR: PriviSense是一个基于Frida的工具包，可在已root的Android设备上实时伪造传感器和系统信号，支持对未修改应用进行可重复的上下文敏感行为测试。


<details>
  <summary>Details</summary>
Motivation: 移动应用越来越依赖实时传感器和系统数据来适应用户上下文，但模拟器和插桩构建通常无法支持在物理设备上对上下文敏感应用行为进行可重复测试。

Method: 基于Frida开发，在已root的Android设备上运行时伪造传感器和系统信号。能够脚本化注入随时间变化的传感器流（加速度计、陀螺仪、步数计数器）和系统值（电池电量、系统时间、设备元数据）到未修改的应用中。

Result: 在已root的Android设备上对五个代表性传感器可视化应用进行了实时伪造验证。支持脚本化和可逆的数值操作，便于测试应用逻辑、发现基于上下文的行为以及进行隐私分析。

Conclusion: PriviSense无需模拟器或应用重写即可在物理设备上实现可重复的上下文敏感应用测试，代码在验证研究者请求后共享，确保符合伦理使用。

Abstract: Mobile apps increasingly rely on real-time sensor and system data to adapt their behavior to user context. While emulators and instrumented builds offer partial solutions, they often fail to support reproducible testing of context-sensitive app behavior on physical devices. We present PriviSense, a Frida-based, on-device toolkit for runtime spoofing of sensor and system signals on rooted Android devices. PriviSense can script and inject time-varying sensor streams (accelerometer, gyroscope, step counter) and system values (battery level, system time, device metadata) into unmodified apps, enabling reproducible on-device experiments without emulators or app rewrites. Our demo validates real-time spoofing on a rooted Android device across five representative sensor-visualization apps. By supporting scriptable and reversible manipulation of these values, PriviSense facilitates testing of app logic, uncovering of context-based behaviors, and privacy-focused analysis. To ensure ethical use, the code is shared upon request with verified researchers.
  Tool Guide: How to Run PriviSense on Rooted Android https://bit.ly/privisense-guide Demonstration video: https://www.youtube.com/watch?v=4Qwnogcc3pw

</details>


### [34] [Small is Beautiful: A Practical and Efficient Log Parsing Framework](https://arxiv.org/abs/2601.22590)
*Minxing Wang,Yintong Huo*

Main category: cs.SE

TL;DR: EFParser是一个基于LLM的无监督日志解析器，通过双缓存系统和校正模块提升小型LLM的性能，解决了语义解析器对模型规模的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的语义日志解析器虽然具有优越的泛化能力，但其性能严重依赖模型规模，导致在使用更小、更资源高效的LLM时出现显著性能下降。这在需要数据隐私保护和计算资源受限的实际部署场景中构成了主要障碍。

Method: EFParser引入了双缓存系统，包含自适应更新机制，能够区分新模板模式和现有模板的变体，从而合并冗余模板并修正先前错误。此外，专门的校正模块作为门控器，在缓存前验证和优化每个LLM生成的模板，防止错误注入。

Result: 在公开大规模数据集上的实证评估表明，EFParser在小型LLM上运行时，在所有指标上平均优于最先进的基线方法12.5%，甚至超过了一些使用大规模模型的基线方法。尽管增加了验证步骤，EFParser仍保持了高计算效率。

Conclusion: EFParser通过系统架构创新，有效提升了小型LLM在日志解析任务中的能力，为实际日志分析部署提供了强大且实用的解决方案，解决了语义解析器对模型规模的依赖问题。

Abstract: Log parsing is a fundamental step in log analysis, partitioning raw logs into constant templates and dynamic variables. While recent semantic-based parsers leveraging Large Language Models (LLMs) exhibit superior generalizability over traditional syntax-based methods, their effectiveness is heavily contingent on model scale. This dependency leads to significant performance collapse when employing smaller, more resource-efficient LLMs. Such degradation creates a major barrier to real-world adoption, where data privacy requirements and computational constraints necessitate the use of succinct models. To bridge this gap, we propose EFParser, an unsupervised LLM-based log parser designed to enhance the capabilities of smaller models through systematic architectural innovation. EFParser introduces a dual-cache system with an adaptive updating mechanism that distinguishes between novel patterns and variations of existing templates. This allows the parser to merge redundant templates and rectify prior errors, maintaining cache consistency. Furthermore, a dedicated correction module acts as a gatekeeper, validating and refining every LLM-generated template before caching to prevent error injection. Empirical evaluations on public large-scale datasets demonstrate that EFParser outperforms state-of-the-art baselines by an average of 12.5% across all metrics when running on smaller LLMs, even surpassing some baselines utilizing large-scale models. Despite its additional validation steps, EFParser maintains high computational efficiency, offering a robust and practical solution for real-world log analysis deployment.

</details>


### [35] [TimeMachine-bench: A Benchmark for Evaluating Model Capabilities in Repository-Level Migration Tasks](https://arxiv.org/abs/2601.22597)
*Ryo Fujii,Makoto Morishita,Kazuki Yano,Jun Suzuki*

Main category: cs.SE

TL;DR: TimeMachine-bench是一个用于评估Python项目软件迁移任务的自动化基准测试，包含因依赖更新导致测试失败的GitHub仓库，并评估了11个LLM模型的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 随着自动化软件工程的发展，研究重点正转向反映软件工程师日常工作的实际任务。软件迁移作为适应环境变化的关键过程，在现有研究中被忽视，需要专门的评估基准。

Method: 构建了TimeMachine-bench基准测试，包含因依赖更新导致测试失败的GitHub仓库，采用全自动化构建流程支持实时更新，并创建了人工验证的子集确保问题可解性。在此基础上评估了基于11个模型（包括开源权重模型和最先进LLM）的智能体基线。

Result: 评估结果显示，LLM在迁移任务中表现出一定潜力，但仍面临重大可靠性挑战：包括利用低测试覆盖率产生虚假解决方案，以及由于次优工具使用策略导致的不必要编辑。

Conclusion: TimeMachine-bench为软件迁移任务提供了实用的评估基准，揭示了当前LLM在该领域的局限性，特别是可靠性和工具使用方面的问题，为未来研究指明了方向。

Abstract: With the advancement of automated software engineering, research focus is increasingly shifting toward practical tasks reflecting the day-to-day work of software engineers. Among these tasks, software migration, a critical process of adapting code to evolving environments, has been largely overlooked. In this study, we introduce TimeMachine-bench, a benchmark designed to evaluate software migration in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to ensure problem solvability. We evaluated agent-based baselines built on top of 11 models, including both strong open-weight and state-of-the-art LLMs on this verified subset. Our results indicated that, while LLMs show some promise for migration tasks, they continue to face substantial reliability challenges, including spurious solutions that exploit low test coverage and unnecessary edits stemming from suboptimal tool-use strategies. Our dataset and implementation are available at https://github.com/tohoku-nlp/timemachine-bench.

</details>


### [36] [Elderly HealthMag: Systematic Building and Calibrating a Tool for Identifying and Evaluating Senior User Digital Health Software](https://arxiv.org/abs/2601.22627)
*Yuqing Xiao,John Grundy,Anuradha Madugalla,Elizabeth Manias*

Main category: cs.SE

TL;DR: 研究人员开发了HealthMag工具，用于帮助数字健康软件更好地识别、建模和评估针对健康条件用户的需求，并进一步结合AgeMag方法创建了Elderly HealthMag，专门用于老年用户的移动健康软件需求分析和评估。


<details>
  <summary>Details</summary>
Motivation: 数字健康软件在开发过程中经常基于对健康条件用户的隐含错误假设，导致产品无法满足特定年龄和健康状况带来的特殊需求。虽然软件在理论上可能达到临床目标，但在实际用户交互中往往缺乏包容性。

Method: 基于GenderMag启发，开发HealthMag工具，通过系统映射和校准遵循InclusiveMag框架。进一步整合现有AgeMag方法的校准版本，创建双重视角的Elderly HealthMag方法，专门用于老年用户的移动健康软件需求、设计和评估。

Result: 通过认知走查方法，展示了Age HealthMag在识别当前面向老年用户的数字健康应用程序中的包容性偏见方面的应用和效用。

Conclusion: HealthMag和Elderly HealthMag工具能够有效帮助数字健康软件开发团队更好地理解和满足健康条件用户及老年用户的特定需求，提高软件的包容性和实际使用效果。

Abstract: Digital health (DH) software is increasingly deployed to populations where many end users live with one or more health conditions. Yet, DH software development teams frequently operate using implicit, incorrect assumptions about these users, resulting in products that under-serve the specific requirements imposed by their age and health conditions. Consequently, while software may meet clinical objectives on paper, it often fails to be inclusive during actual user interaction. To address this, we propose \textbf{\textit{HealthMag}}, a tool inspired by GenderMag designed to help better elicit, model and evaluate requirements for digital health software. We developed HealthMag through systematic mapping and calibration following the InclusiveMag framework. Furthermore, we integrated this with a calibrated version of an existing AgeMag method to create a dual-lens approach: \textbf{\textit{Elderly HealthMag}}, designed to aid requirements, design and evaluation of mHealth software for senior end users. We demonstrate application and utility of Age HealthMag via cognitive walkthroughs in identifying inclusivity biases in current senior user-oriented digital health applications.

</details>


### [37] [From Horizontal Layering to Vertical Integration: A Comparative Study of the AI-Driven Software Development Paradigm](https://arxiv.org/abs/2601.22667)
*Chi Zhang,Zehan Li,Ziqian Zhong,Haibing Ma,Dan Xiao,Chen Lin,Ming Dong*

Main category: cs.SE

TL;DR: 该研究通过对比传统企业和AI原生初创公司的案例，发现生成式AI在软件工程中的应用促使组织结构从水平分层转向垂直整合，带来8-33倍的资源节约，主要得益于"超级员工"的出现和跨职能协调成本的消除。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在软件工程中的组织影响，对比传统企业（棕地）和AI原生初创公司（绿地）的不同采用模式，理解AI如何改变工程组织的结构和效率。

Method: 采用多案例比较研究方法，对比分析传统企业和AI原生初创公司两种开发环境，通过比较组织结构、资源配置和效率指标来研究生成式AI的组织影响。

Result: 研究发现从水平分层（职能专业化）转向垂直整合（端到端所有权）能带来8-33倍的资源消耗减少；识别出"超级员工"现象（AI增强的工程师跨越传统角色边界）；提出人机协作效能应成为工程组织的主要优化目标；发现AI扭曲效应会降低劳动规模回报同时放大技术杠杆。

Conclusion: 生成式AI促使软件工程组织需要重新设计，从水平分层转向垂直整合；管理者应关注激活高级工程师的闲置认知带宽，抑制盲目的规模扩张，以人机协作效能为核心优化目标进行组织重构。

Abstract: This paper examines the organizational implications of Generative AI adoption in software engineering through a multiple-case comparative study. We contrast two development environments: a traditional enterprise (brownfield) and an AI-native startup (greenfield). Our analysis reveals that transitioning from Horizontal Layering (functional specialization) to Vertical Integration (end-to-end ownership) yields 8-fold to 33-fold reductions in resource consumption. We attribute these gains to the emergence of Super Employees, AI-augmented engineers who span traditional role boundaries, and the elimination of inter-functional coordination overhead. Theoretically, we propose Human-AI Collaboration Efficacy as the primary optimization target for engineering organizations, supplanting individual productivity metrics. Our Total Factor Productivity analysis identifies an AI Distortion Effect that diminishes returns to labor scale while amplifying technological leverage. We conclude with managerial strategies for organizational redesign, including the reactivation of idle cognitive bandwidth in senior engineers and the suppression of blind scale expansion.

</details>


### [38] [VarParser: Unleashing the Neglected Power of Variables for LLM-based Log Parsing](https://arxiv.org/abs/2601.22676)
*Jinrui Sun,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: VarParser提出了一种以变量为中心的日志解析策略，通过利用日志中的变量部分提升解析准确性和效率，相比现有仅关注常量部分的LLM方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型(LLM)的日志解析器都只关注日志的常量部分，忽略了变量部分对日志解析的潜在贡献。这种以常量为中心的策略带来四个关键问题：1) 仅使用常量信息导致日志分组和采样效率低下；2) 基于常量的缓存导致LLM调用次数较多，降低解析准确性和效率；3) 提示中消耗的常量令牌数较多，增加LLM调用成本；4) 结果中只保留占位符，丢失了日志中变量信息带来的系统可见性。

Method: 提出VarParser变量中心日志解析策略，包含三个核心技术：1) 变量贡献采样：高效捕获日志的变量部分并利用其贡献；2) 变量中心解析缓存：减少LLM调用次数；3) 自适应变量感知上下文学习。通过引入变量单元，保留丰富的变量信息，增强日志解析结果的完整性。

Result: 在大规模数据集上的广泛评估表明，VarParser相比现有方法实现了更高的准确性，显著提高了解析效率，同时降低了LLM调用成本。

Conclusion: VarParser通过以变量为中心的日志解析策略，有效解决了现有LLM方法忽略变量部分的问题，在准确性、效率和成本方面均有显著提升，为下游异常检测和故障诊断任务提供了更完整的日志解析结果。

Abstract: Logs serve as a primary source of information for engineers to diagnose failures in large-scale online service systems. Log parsing, which extracts structured events from massive unstructured log data, is a critical first step for downstream tasks like anomaly detection and failure diagnosis. With advances in large language models (LLMs), leveraging their strong text understanding capabilities has proven effective for accurate log parsing. However, existing LLM-based log parsers all focus on the constant part of logs, ignoring the potential contribution of the variable part to log parsing. This constant-centric strategy brings four key problems. First, inefficient log grouping and sampling with only constant information. Second, a relatively large number of LLM invocations due to constant-based cache, leading to low log parsing accuracy and efficiency. Third, a relatively large number of consumed constant tokens in prompts leads to high LLM invocation costs. At last, these methods only retain placeholders in the results, losing the system visibility brought by variable information in logs.
  Facing these problems, we propose a variable-centric log parsing strategy named VarParser. Through variable contribution sampling, variable-centric parsing cache, and adaptive variable-aware in-context learning, our approach can efficiently capture the variable parts of logs and leverage their contributions to parsing. By introducing variable units, we preserve rich variable information, enhancing the integrity of log parsing results. Extensive evaluations on large-scale datasets demonstrate that VarParser achieves higher accuracy compared to existing methods, significantly improving parsing efficiency while reducing the LLM invocation costs.

</details>


### [39] [AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse](https://arxiv.org/abs/2601.22748)
*You Lu,Jiyang Zhang,Bihuan Chen,Chaofeng Sha,Dingji Wang,Xin Peng*

Main category: cs.SE

TL;DR: 该论文首次系统研究了模型合并技术在LLMs之外的其他深度学习模型架构和领域的应用效果，发现现有方法存在不一致性、对异构结构处理能力有限、超参数敏感等问题，并提出了AutoMerge框架来解决这些限制。


<details>
  <summary>Details</summary>
Motivation: 软件重用是软件工程中的重要课题，模型合并作为LLMs中的免训练重用方法已显示出效果。然而，目前缺乏对模型合并技术在其他深度学习模型架构和领域应用效果的系统研究，需要填补这一空白。

Method: 1) 在LLMs、图像分类和自动驾驶三个领域，对三种不同模型架构评估五种模型合并技术；2) 提出AutoMerge框架：首先将复杂模型分割为多个异构块，然后系统探索合并空间以确定最佳合并技术及其超参数配置。

Result: 研究发现：1) 直接应用现有模型合并技术导致高度不一致的结果，远不如在LLMs中的成功；2) 单一合并技术难以处理模型内的异构结构特性；3) 合并技术的有效性对超参数配置高度敏感，限制了更广泛的应用。

Conclusion: 模型合并技术不能直接推广到LLMs之外的其他模型架构和领域，需要更系统的方法。提出的AutoMerge框架通过模型分割和合并空间搜索，为解决现有方法的局限性提供了有效途径。

Abstract: Software reuse has long been recognized as a critical and widely studied topic in software engineering, offering substantial benefits in reducing development costs, improving software quality, and enhancing operational efficiency. This paradigm extends into deep learning through model reuse. Recently, model merging has emerged in the domain of large language models (LLMs) as a training-free approach that takes multiple task-specific models with the same architecture as source models and merges them without retraining, enhancing model reuse within LLMs. However, no prior work has systematically investigated whether such an approach can be effectively applied to other deep learning models with different architectures across domains. To bridge this gap, we present the first systematic study that evaluates five model merging techniques on three distinct model architectures across three domains: LLMs, image classification, and autonomous driving. Our findings reveal that directly applying existing model merging techniques leads to highly inconsistent results and falls notably short of their success within LLMs. Moreover, a single model merging technique often fails to handle the heterogeneous structural properties within a model, limiting its applicability to different model architectures across domains. Furthermore, the effectiveness of model merging techniques is highly sensitive to hyperparameter configurations, thereby constraining their potential for broader adoption. Inspired by these insights, we propose AutoMerge, a novel search-based model merging framework that first segments complex models into multiple heterogeneous blocks and then systematically explores the merging space to identify the merging technique and its hyperparameter configuration.

</details>


### [40] [Constructing Safety Cases for AI Systems: A Reusable Template Framework](https://arxiv.org/abs/2601.22773)
*Sung Une Lee,Liming Zhu,Md Shamsujjoha,Liming Dong,Qinghua Lu,Jieshan Chen*

Main category: cs.SE

TL;DR: 提出针对AI系统的可复用安全案例模板框架，解决传统安全案例方法不适应生成式和智能体AI动态特性的问题


<details>
  <summary>Details</summary>
Motivation: 传统安全案例方法（源自航空、核工程）依赖于明确的系统边界、稳定架构和已知故障模式，而现代AI系统（生成式、智能体AI）具有能力不可预测涌现、行为随提示变化、风险特征随微调/部署环境变化等动态特性，导致传统方法失效

Method: 提出可复用安全案例模板框架，包含：1) AI特定声明类型分类（断言型、约束型、能力型）；2) 论证类型分类（演示性、比较性、因果/解释性、风险型、规范性）；3) 证据家族分类（经验性、机制性、比较性、专家驱动、形式化方法、操作/现场数据、模型基础）。通过端到端模式展示模板，解决无真实标签评估、动态模型更新、基于阈值的风险决策等挑战

Result: 开发了系统性、可组合、可复用的安全案例构建和维护方法，使安全案例对生成式和前沿AI系统具有可信性、可审计性和适应性

Conclusion: 该框架为AI系统安全案例提供了结构化方法，能够应对AI系统的动态特性和新兴风险，支持可信、可审计且适应AI行为演化的安全论证

Abstract: Safety cases, structured arguments that a system is acceptably safe, are becoming central to the governance of AI systems. Yet, traditional safety-case practices from aviation or nuclear engineering rely on well-specified system boundaries, stable architectures, and known failure modes. Modern AI systems such as generative and agentic AI are the opposite. Their capabilities emerge unpredictably from low-level training objectives, their behaviour varies with prompts, and their risk profiles shift through fine-tuning, scaffolding, or deployment context. This study examines how safety cases are currently constructed for AI systems and why classical approaches fail to capture these dynamics. It then proposes a framework of reusable safety-case templates, each following a predefined structure of claims, arguments, and evidence tailored for AI systems. The framework introduces comprehensive taxonomies for AI-specific claim types (assertion-based, constrained-based, capability-based), argument types (demonstrative, comparative, causal/explanatory, risk-based, and normative), and evidence families (empirical, mechanistic, comparative, expert-driven, formal methods, operational/field data, and model-based). Each template is illustrated through end-to-end patterns addressing distinctive challenges such as evaluation without ground truth, dynamic model updates, and threshold-based risk decisions. The result is a systematic, composable, and reusable approach to constructing and maintaining safety cases that are credible, auditable, and adaptive to the evolving behaviour of generative and frontier AI systems.

</details>


### [41] [Understanding on the Edge: LLM-generated Boundary Test Explanations](https://arxiv.org/abs/2601.22791)
*Sabinakhon Akbarova,Felix Dobslaw,Robert Feldt*

Main category: cs.SE

TL;DR: GPT-4.1生成的边界值分析解释在软件专业人员中获得了63.5%的正面评价，但需要结构化、引用权威来源、适应读者专业水平并提供可操作的示例才能更有效。


<details>
  <summary>Details</summary>
Motivation: 边界值分析测试中，测试人员常常难以理解和证明特定输入-输出对为何代表有意义的行为边界。虽然大语言模型可以生成自然语言解释，但其在边界值分析中的实际价值尚未经过实证评估。

Method: 进行探索性研究：27名软件专业人员对GPT-4.1为20个边界对生成的解释进行评分（清晰度、正确性、完整性和感知有用性），其中6人参与后续访谈深入阐述观点。

Result: 总体63.5%的评分为正面（4-5分），17%为负面（1-2分）。参与者偏好具有清晰结构、引用权威来源、适应读者专业水平的解释，并强调需要支持调试和文档的可操作示例。

Conclusion: 基于研究结果提炼出七项需求清单，为未来基于LLM的边界解释工具定义具体设计标准。结果表明，经过进一步改进，基于LLM的工具可以通过使边界解释更具可操作性和可信度来支持测试工作流程。

Abstract: Boundary value analysis and testing (BVT) is fundamental in software quality assurance because faults tend to cluster at input extremes, yet testers often struggle to understand and justify why certain input-output pairs represent meaningful behavioral boundaries. Large Language Models (LLMs) could help by producing natural-language rationales, but their value for BVT has not been empirically assessed. We therefore conducted an exploratory study on LLM-generated boundary explanations: in a survey, twenty-seven software professionals rated GPT-4.1 explanations for twenty boundary pairs on clarity, correctness, completeness and perceived usefulness, and six of them elaborated in follow-up interviews. Overall, 63.5% of all ratings were positive (4-5 on a five-point Likert scale) compared to 17% negative (1-2), indicating general agreement but also variability in perceptions. Participants favored explanations that followed a clear structure, cited authoritative sources, and adapted their depth to the reader's expertise; they also stressed the need for actionable examples to support debugging and documentation. From these insights, we distilled a seven-item requirement checklist that defines concrete design criteria for future LLM-based boundary explanation tools. The results suggest that, with further refinement, LLM-based tools can support testing workflows by making boundary explanations more actionable and trustworthy.

</details>


### [42] [Just-in-Time Catching Test Generation at Meta](https://arxiv.org/abs/2601.22832)
*Matthew Becker,Yifei Chen,Nicholas Cochran,Pouyan Ghasemi,Abhishek Gulati,Mark Harman,Zachary Haluza,Mehrdad Honarkhah,Herve Robert,Jiacheng Liu,Weini Liu,Sreeja Thummala,Xiaoning Yang,Rui Xin,Sophie Zeng*

Main category: cs.SE

TL;DR: Meta开发了即时捕获测试生成系统，用于在大型后端系统中预防bug。与传统加固测试不同，捕获测试旨在失败，在代码落地前暴露问题。通过代码变更感知方法，候选捕获生成效率提升4倍；结合规则和LLM评估器，人工审查负载减少70%。系统报告了41个候选捕获，其中8个确认为真阳性，4个可导致严重故障。


<details>
  <summary>Details</summary>
Motivation: 在拥有数亿行代码的大规模后端系统中预防bug。传统加固测试在生成时通过，无法有效捕获潜在问题。需要一种能够在代码落地前主动发现bug的方法，同时减少误报带来的开发负担。

Method: 采用即时捕获测试生成方法：1) 代码变更感知方法生成候选捕获测试；2) 使用规则基和LLM基评估器筛选测试；3) 分析22,126个生成测试；4) 通过统计推断分析人类接受/拒绝的代码变更中的真/假阳性比例。

Result: 1) 代码变更感知方法使候选捕获生成比加固测试提升4倍，比偶然失败测试提升20倍；2) 评估器减少70%人工审查负载；3) 统计显示人类接受的变更有显著更多假阳性，被拒绝的变更有显著更多真阳性；4) 报告41个候选捕获，8个确认真阳性，其中4个可导致严重故障。

Conclusion: 即时捕获测试生成方法具有可扩展性和工业适用性，能够有效防止严重故障进入生产环境。通过减少误报和优化评估流程，该系统在大规模代码库中实现了高效的bug预防。

Abstract: We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The primary challenge is to reduce development drag from false positive test failures. Analyzing 22,126 generated tests, we show code-change-aware methods improve candidate catch generation 4x over hardening tests and 20x over coincidentally failing tests. To address false positives, we use rule-based and LLM-based assessors. These assessors reduce human review load by 70%. Inferential statistical analysis showed that human-accepted code changes are assessed to have significantly more false positives, while human-rejected changes have significantly more true positives. We reported 41 candidate catches to engineers; 8 were confirmed to be true positives, 4 of which would have led to serious failures had they remained uncaught. Overall, our results show that Just-in-Time catching is scalable, industrially applicable, and that it prevents serious failures from reaching production.

</details>


### [43] [AnoMod: A Dataset for Anomaly Detection and Root Cause Analysis in Microservice Systems](https://arxiv.org/abs/2601.22881)
*Ke Ping,Hamza Bin Mazhar,Yuqing Wang,Ying Song,Mika V. Mäntylä*

Main category: cs.SE

TL;DR: AnoMod是一个新的多模态微服务异常检测数据集，包含四种异常类型和五种监控模态，支持跨模态异常检测和根因分析研究。


<details>
  <summary>Details</summary>
Motivation: 当前微服务系统缺乏高质量、公开可用的异常检测和根因分析数据集，现有基准主要关注性能相关故障且模态单一，限制了更广泛故障模式和跨模态方法的研究。

Method: 基于两个开源微服务系统（SocialNetwork和TrainTicket），设计并注入四类异常（性能级、服务级、数据库级、代码级），收集五种模态数据（日志、指标、分布式追踪、API响应、代码覆盖率报告）。

Result: 创建了AnoMod数据集，提供了丰富的端到端系统状态视图和服务间交互信息，支持跨模态异常检测、融合/消融策略评估，以及跨服务和代码区域的细粒度根因分析。

Conclusion: AnoMod数据集填补了微服务异常检测领域的数据空白，为联合考虑检测和定位的端到端故障排除流程提供了支持，促进了跨模态异常检测和根因分析方法的研究。

Abstract: Microservice systems (MSS) have become a predominant architectural style for cloud services. Yet the community still lacks high-quality, publicly available datasets for anomaly detection (AD) and root cause analysis (RCA) in MSS. Most benchmarks emphasize performance-related faults and provide only one or two monitoring modalities, limiting research on broader failure modes and cross-modal methods. To address these gaps, we introduce a new multimodal anomaly dataset built on two open-source microservice systems: SocialNetwork and TrainTicket. We design and inject four categories of anomalies (Ano): performance-level, service-level, database-level, and code-level, to emulate realistic anomaly modes. For each scenario, we collect five modalities (Mod): logs, metrics, distributed traces, API responses, and code coverage reports, offering a richer, end-to-end view of system state and inter-service interactions. We name our dataset, reflecting its unique properties, as AnoMod. This dataset enables (1) evaluation of cross-modal anomaly detection and fusion/ablation strategies, and (2) fine-grained RCA studies across service and code regions, supporting end-to-end troubleshooting pipelines that jointly consider detection and localization.

</details>


### [44] [A Serverless Edge-Native Data Processing Architecture for Autonomous Driving Training](https://arxiv.org/abs/2601.22919)
*Fabian Bally,Michael Schötz,Thomas Limbrunner*

Main category: cs.SE

TL;DR: Lambda框架是一个边缘原生平台，通过用户定义函数实现车载数据过滤和处理，将FaaS原则适配到资源受限的汽车环境，支持实时数据处理。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶机器学习需要大量传感器数据，特别是罕见但安全关键的场景数据。捕获这些事件需要大量驾驶时间和高效选择，而现有方法在资源受限的汽车环境中存在瓶颈。

Method: 引入Lambda框架，这是一个边缘原生平台，通过用户定义函数实现车载数据过滤和处理。框架提供无服务器启发的抽象层，将应用逻辑与调度、部署、隔离等低级执行关注点分离，适配FaaS原则到汽车环境，保持与ROS 2和现有数据记录管道的兼容性。

Result: 在NVIDIA Jetson Orin Nano上评估框架，与原生ROS 2部署对比。结果显示具有竞争力的性能、降低的延迟和抖动，证实基于lambda的抽象可以支持嵌入式自动驾驶系统中的实时数据处理。

Conclusion: Lambda框架成功将FaaS原则适配到资源受限的汽车环境，支持模块化、事件驱动的过滤算法，为自动驾驶数据采集提供了高效解决方案，源代码已开源。

Abstract: Data is both the key enabler and a major bottleneck for machine learning in autonomous driving. Effective model training requires not only large quantities of sensor data but also balanced coverage that includes rare yet safety-critical scenarios. Capturing such events demands extensive driving time and efficient selection. This paper introduces the Lambda framework, an edge-native platform that enables on-vehicle data filtering and processing through user-defined functions. The framework provides a serverless-inspired abstraction layer that separates application logic from low-level execution concerns such as scheduling, deployment, and isolation. By adapting Function-as-a-Service (FaaS) principles to resource-constrained automotive environments, it allows developers to implement modular, event-driven filtering algorithms while maintaining compatibility with ROS 2 and existing data recording pipelines. We evaluate the framework on an NVIDIA Jetson Orin Nano and compare it against native ROS 2 deployments. Results show competitive performance, reduced latency and jitter, and confirm that lambda-based abstractions can support real-time data processing in embedded autonomous driving systems. The source code is available at https://github.com/LASFAS/jblambda.

</details>


### [45] [Sifting the Noise: A Comparative Study of LLM Agents in Vulnerability False Positive Filtering](https://arxiv.org/abs/2601.22952)
*Yunpeng Xiong,Ting Zhang*

Main category: cs.SE

TL;DR: 比较三种LLM智能体框架（Aider、OpenHands、SWE-agent）在SAST误报过滤中的效果，发现智能体能显著降低误报率但效果受模型能力和漏洞类型影响，存在计算成本差异和真漏洞抑制的权衡。


<details>
  <summary>Details</summary>
Motivation: SAST工具产生大量误报给开发者带来沉重的手动审查负担，LLM智能体通过迭代推理、工具使用和环境交互为误报过滤提供了新方向，但不同智能体架构的比较效果尚不清楚。

Method: 对三种最先进的LLM智能体框架（Aider、OpenHands、SWE-agent）进行对比研究，使用OWASP Benchmark漏洞和真实开源Java项目进行评估，分析不同配置下的误报过滤效果。

Result: LLM智能体能大幅减少SAST噪声，在OWASP Benchmark上将初始超过92%的误报率降至最低6.3%；在真实数据集中，最佳配置对CodeQL警报的误报识别率可达93.3%。但效果强烈依赖于骨干模型和CWE类型，且激进误报减少会抑制真漏洞，不同框架计算成本差异显著。

Conclusion: LLM智能体是SAST误报过滤的强大但非均匀解决方案，实际部署需要仔细考虑智能体设计、骨干模型选择、漏洞类别和运营成本之间的权衡。

Abstract: Static Application Security Testing (SAST) tools are essential for identifying software vulnerabilities, but they often produce a high volume of false positives (FPs), imposing a substantial manual triage burden on developers. Recent advances in Large Language Model (LLM) agents offer a promising direction by enabling iterative reasoning, tool use, and environment interaction to refine SAST alerts. However, the comparative effectiveness of different LLM-based agent architectures for FP filtering remains poorly understood. In this paper, we present a comparative study of three state-of-the-art LLM-based agent frameworks, i.e., Aider, OpenHands, and SWE-agent, for vulnerability FP filtering. We evaluate these frameworks using the vulnerabilities from the OWASP Benchmark and real-world open-source Java projects. The experimental results show that LLM-based agents can remove the majority of SAST noise, reducing an initial FP detection rate of over 92% on the OWASP Benchmark to as low as 6.3% in the best configuration. On real-world dataset, the best configuration of LLM-based agents can achieve an FP identification rate of up to 93.3% involving CodeQL alerts. However, the benefits of agents are strongly backbone- and CWE-dependent: agentic frameworks significantly outperform vanilla prompting for stronger models such as Claude Sonnet 4 and GPT-5, but yield limited or inconsistent gains for weaker backbones. Moreover, aggressive FP reduction can come at the cost of suppressing true vulnerabilities, highlighting important trade-offs. Finally, we observe large disparities in computational cost across agent frameworks. Overall, our study demonstrates that LLM-based agents are a powerful but non-uniform solution for SAST FP filtering, and that their practical deployment requires careful consideration of agent design, backbone model choice, vulnerability category, and operational cost.

</details>


### [46] [SWE-Manager: Selecting and Synthesizing Golden Proposals Before Coding](https://arxiv.org/abs/2601.22956)
*Boyin Tan,Haoning Deng,Junyuan Zhang,Junjielong Xu,Pinjia He,Youcheng Sun*

Main category: cs.SE

TL;DR: SWE-Manager：一个通过强化学习训练的8B模型，用于在软件工程问题解决中比较多个候选提案、选择最佳方案并合成黄金提案，模拟技术经理的决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM研究主要关注代码生成和错误修复，但在实际团队工作中，需要从多个候选提案中选择最佳方案进行实施。好的选择能提高问题解决的可靠性并降低风险，而差的选择会增加风险甚至导致不可预测的故障。

Method: 首先通过人工研究分析维护者在选择竞争提案时使用的理由。基于这些发现，提出SWE-Manager：一个8B模型，通过强化学习训练，能够比较提案、证明其选择合理性，并合成黄金提案用于实施。将提案选择视为推理任务，模拟技术经理在不执行代码或运行测试的情况下权衡问题上下文和每个提案解决方案的过程。

Result: 在SWE-Lancer Manager基准测试中，SWE-Manager达到53.21%的选择准确率和57.75%的收益率，赚取152,750美元，表现优于包括GPT-5在内的强基线模型。进一步通过P2A框架评估其在真实问题解决中的有效性。

Conclusion: SWE-Manager通过模拟技术经理的决策过程，在软件工程问题解决中有效选择最佳提案并合成黄金提案，为LLM在软件工程中的实际应用提供了新方向。

Abstract: Large language model (LLM) research in software engineering has largely focused on tasks such as code generation and bug repair. In practice, teams often draft multiple candidate proposals for fixing an issue and then deliberate on one golden proposal for implementation. This selection requires not only assessing the issue's scope, impact, and urgency, but also a clear understanding of each proposal's strengths and weaknesses. A good selection could make issue resolution more reliable while reducing regression and operational risk, whereas a poor choice can increase risk and even cause unpredictable failures.
  We first conduct a manual study of real-world issues to characterize the rationales maintainers use when selecting among competing proposals. Motivated by these findings, we introduce SWE-Manager, a joint selection and synthesis approach that selects the best proposal and synthesizes a golden proposal. SWE-Manager is an 8B model trained via reinforcement learning (RL) to compare proposals, justify its choice, and synthesize a golden proposal for implementation. We view proposal selection as a reasoning task, mirroring how technical managers review competing proposals by weighing issue context and each proposal's solution without executing code or running tests. On the SWE-Lancer Manager benchmark, SWE-Manager achieves 53.21 selection accuracy and 57.75 earn rate, earning 152,750 dollars and outperforming strong baselines including GPT-5. To further evaluate the effectiveness of SWE-Manager in real-world issue resolution, we design the P2A framework, which simulates a real-world workflow where multiple proposals are drafted, reviewed, and a golden proposal is selected for implementation ...

</details>


### [47] [Uncovering Hidden Inclusions of Vulnerable Dependencies in Real-World Java Projects](https://arxiv.org/abs/2601.23020)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: Unshade是一种混合依赖扫描方法，结合元数据扫描效率和代码中心方法检测修改依赖的能力，在Java项目中识别隐藏的易受攻击依赖。


<details>
  <summary>Details</summary>
Motivation: 开源软件依赖是现代软件代码库的主要组成部分，虽然能减少开发时间和成本，但也引入了重大安全风险。现有元数据扫描器轻量快速但无法检测修改的依赖，而代码中心扫描器能检测修改依赖但效率较低，需要一种结合两者优势的方法。

Method: Unshade采用混合方法：首先通过基于字节码的指纹识别机制识别修改和隐藏的依赖，增强Java项目的软件物料清单(SBOM)；然后将增强的SBOM传递给元数据漏洞扫描器，识别声明依赖和新发现依赖中的已知漏洞。

Result: 对GitHub上1,808个最流行的开源Java Maven项目进行大规模研究，结果显示：近50%的项目包含至少一个与已知漏洞相关的修改、隐藏依赖；平均每个受影响项目包含超过8个此类隐藏易受攻击依赖；共识别出7,712个元数据扫描器会漏掉的唯一CVE。

Conclusion: Unshade成功结合了元数据扫描的效率和代码中心方法检测修改依赖的能力，揭示了传统元数据扫描器会漏掉的大量隐藏易受攻击依赖，证明了混合方法在依赖安全扫描中的重要性。

Abstract: Open-source software (OSS) dependencies are a dominant component of modern software code bases. Using proven and well-tested OSS components lets developers reduce development time and cost while improving quality. However, heavy reliance on open-source software also introduces significant security risks, including the incorporation of known vulnerabilities into the codebase. To mitigate these risks, metadata-based dependency scanners, which are lightweight and fast, and code-centric scanners, which enable the detection of modified dependencies hidden from metadata-based approaches, have been developed. In this paper, we present Unshade, a hybrid approach towards dependency scanning in Java that combines the efficiency of metadata-based scanning with the ability to detect modified dependencies of code-centric approaches. Unshade first augments a Java project's software bill of materials (SBOM) by identifying modified and hidden dependencies via a bytecode-based fingerprinting mechanism. This augmented SBOM is then passed to a metadata-based vulnerability scanner to identify known vulnerabilities in both declared and newly revealed dependencies. Leveraging Unshade's high scalability, we conducted a large-scale study of the 1,808 most popular open-source Java Maven projects on GitHub. The results show that nearly 50% of these projects contain at least one modified, hidden dependency associated with a known vulnerability. On average, each affected project includes more than eight such hidden vulnerable dependencies, all missed by traditional metadata-based scanners. Overall, Unshade identified 7,712 unique CVEs in hidden dependencies that would remain undetected when relying on metadata-based scanning alone.

</details>


### [48] [On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study](https://arxiv.org/abs/2601.23059)
*Antonio Vitale,Emanuela Guglielmi,Simone Scalabrino,Rocco Oliveto*

Main category: cs.SE

TL;DR: 研究探讨了代码注释对LLM自动修复bug能力的影响，发现训练和推理阶段都包含注释时，修复准确率可提升三倍，且训练时包含注释不会降低无注释实例的性能。


<details>
  <summary>Details</summary>
Motivation: 自动bug修复（ABF）中通常会在训练前移除代码注释，但研究者假设注释可能通过提供设计和实现见解，在修复某些类型bug时发挥关键作用。本研究旨在验证注释在训练和推理阶段的存在与否如何影响LLM的bug修复能力。

Method: 采用实证评估方法，比较两个模型家族在所有训练和推理条件组合（有/无注释）下的表现。为解决现有数据集中注释有限的问题，使用LLM自动为缺乏注释的方法生成注释。同时进行可解释性分析，识别哪些类型的注释对bug修复最有效。

Result: 研究发现：1）当注释同时存在于训练和推理阶段时，ABF准确率最高可提升三倍；2）训练时包含注释不会降低无注释实例的性能；3）可解释性分析表明，详细描述方法实现的注释对帮助LLM准确修复bug特别有效。

Conclusion: 代码注释在自动bug修复中具有重要价值，特别是在训练和推理阶段都包含注释时能显著提升性能。研究挑战了当前ABF中移除注释的常见做法，建议重新考虑注释在代码表示中的作用。

Abstract: Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.

</details>


### [49] [Automated Testing of Prevalent 3D User Interactions in Virtual Reality Applications](https://arxiv.org/abs/2601.23139)
*Ruizhen Gu,José Miguel Rojas,Donghwan Shin*

Main category: cs.SE

TL;DR: 本文提出了一种自动化VR交互测试方法XRintTest，通过交互流图建模3D用户交互，在XRBench3D基准测试中实现了93%的交互覆盖率，比随机探索高效12倍。


<details>
  <summary>Details</summary>
Motivation: VR技术虽然提供沉浸式体验，但与传统软件相比存在独特的测试挑战。现有VR测试方法能够实现场景导航和交互激活，但缺乏自动合成真实3D用户输入（如通过手持控制器进行抓取和触发操作）的能力，且现有指标无法稳健捕捉多样化的交互覆盖率。

Method: 1. 经验性识别了9个开源VR项目中四种主要交互类型：fire、manipulate、socket和custom；2. 提出了交互流图，通过识别目标、动作和条件来系统建模3D用户交互；3. 构建了XRBench3D基准，包含10个VR场景和456个不同的用户交互；4. 开发了XRintTest自动化测试方法，利用交互流图进行动态场景探索和交互执行。

Result: 在XRBench3D上的评估显示，XRintTest在所有场景中实现了fire、manipulate和socket交互93%的覆盖率，比随机探索有效12倍、高效6倍。能够检测运行时异常和非异常交互问题，包括细微的配置缺陷。交互流图还能揭示可能损害预期功能并阻碍测试性能的交互设计异味。

Conclusion: 本文通过交互流图建模和XRintTest自动化测试方法，有效解决了VR交互测试中的关键挑战，显著提升了交互覆盖率和测试效率，为VR应用的质量保证提供了系统化解决方案。

Abstract: Virtual Reality (VR) technologies offer immersive user experiences across various domains, but present unique testing challenges compared to traditional software. Existing VR testing approaches enable scene navigation and interaction activation, but lack the ability to automatically synthesise realistic 3D user inputs (e.g, grab and trigger actions via hand-held controllers). Automated testing that generates and executes such input remains an unresolved challenge. Furthermore, existing metrics fail to robustly capture diverse interaction coverage. This paper addresses these gaps through four key contributions. First, we empirically identify four prevalent interaction types in nine open-source VR projects: fire, manipulate, socket, and custom. Second, we introduce the Interaction Flow Graph, a novel abstraction that systematically models 3D user interactions by identifying targets, actions, and conditions. Third, we construct XRBench3D, a benchmark comprising ten VR scenes that encompass 456 distinct user interactions for evaluating VR interaction testing. Finally, we present XRintTest, an automated testing approach that leverages this graph for dynamic scene exploration and interaction execution. Evaluation on XRBench3D shows that XRintTest achieves great effectiveness, reaching 93% coverage of fire, manipulate and socket interactions across all scenes, and performing 12x more effectively and 6x more efficiently than random exploration. Moreover, XRintTest can detect runtime exceptions and non-exception interaction issues, including subtle configuration defects. In addition, the Interaction Flow Graph can reveal potential interaction design smells that may compromise intended functionality and hinder testing performance for VR applications.

</details>


### [50] [Do Good, Stay Longer? Temporal Patterns and Predictors of Newcomer-to-Core Transitions in Conventional OSS and OSS4SG](https://arxiv.org/abs/2601.23142)
*Mohamed Ouf,Amr Mohamed,Mariam Guizani*

Main category: cs.SE

TL;DR: OSS4SG项目相比传统OSS项目，新人向核心贡献者转化的成功率更高（2.2倍留存率，19.6%更高概率），且提供多种转化路径而非单一主导路径。研究发现，先花时间了解项目再集中贡献（Late Spike模式）比一开始就高强度贡献（Early Spike模式）能更快成为核心贡献者。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决OSS可持续发展中新人向核心贡献者转化管道断裂的问题。传统OSS项目中大多数新人在初次贡献后变得不活跃，而OSS4SG项目以社会影响为主要使命，可能具有不同的新人转化结果，需要实证比较。

Method: 比较了375个项目（190个OSS4SG，185个传统OSS），分析了92,721名贡献者和350万次提交。通过量化分析贡献者留存率、核心地位达成概率、贡献模式（Early Spike vs Late Spike）和时间到核心的转化周期。

Result: OSS4SG项目保留贡献者的比率是传统项目的2.2倍，贡献者成为核心的概率高19.6%。早期广泛探索项目能预测核心地位达成（22.2%重要性）。传统OSS主要依赖单一主导路径（61.62%转化），而OSS4SG提供多种路径。Late Spike模式（先学习再集中贡献）比Early Spike模式（一开始高强度贡献）快2.4-2.9倍成为核心（21周 vs 51-60周）。

Conclusion: 项目使命与新人向核心转化环境显著相关。成功策略包括：选择与个人价值观一致的项目，先花时间理解代码库再进行主要贡献。OSS4SG支持多种有效的时间模式，而传统OSS只有Late Spike模式能最快达到核心地位。研究为贡献者和维护者提供了基于证据的指导。

Abstract: Open Source Software (OSS) sustainability relies on newcomers transitioning to core contributors, but this pipeline is broken, with most newcomers becoming inactive after initial contributions. Open Source Software for Social Good (OSS4SG) projects, which prioritize societal impact as their primary mission, may be associated with different newcomer-to-core transition outcomes than conventional OSS projects. We compared 375 projects (190 OSS4SG, 185 OSS), analyzing 92,721 contributors and 3.5 million commits. OSS4SG projects retain contributors at 2.2X higher rates and contributors have 19.6% higher probability of achieving core status. Early broad project exploration predicts core achievement (22.2% importance); conventional OSS concentrates on one dominant pathway (61.62% of transitions) while OSS4SG provides multiple pathways. Contrary to intuition, contributors who invest time learning the project before intensifying their contributions (Late Spike pattern) achieve core status 2.4-2.9X faster (21 weeks) than those who contribute intensively from day one (Early Spike pattern, 51-60 weeks). OSS4SG supports two effective temporal patterns while only Late Spike achieves fastest time-to-core in conventional OSS. Our findings suggest that finding a project aligned with personal values and taking time to understand the codebase before major contributions are key strategies for achieving core status. Our findings show that project mission is associated with measurably different environments for newcomer-to-core transitions and provide evidence-based guidance for newcomers and maintainers.

</details>


### [51] [GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion](https://arxiv.org/abs/2601.23254)
*Baoyi Wang,Xingliang Wang,Guochang Li,Chen Zhi,Junxiao Han,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: 论文提出GrepRAG框架，通过轻量级、无索引的词汇检索增强仓库级代码补全，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级代码补全方法依赖语义索引或图分析，计算开销大。受开发者常用轻量级搜索工具启发，探索简单词汇检索在复杂检索机制前的潜力。

Method: 1. 提出Naive GrepRAG基线框架：LLM自主生成ripgrep命令检索相关上下文；2. 提出改进版GrepRAG：增加标识符加权重排序和结构感知去重的后处理流程。

Result: Naive GrepRAG性能与复杂图基线相当；GrepRAG在CrossCodeEval和RepoEval-Updated上持续优于SOTA方法，在CrossCodeEval上代码精确匹配相对提升7.04-15.58%。

Conclusion: 轻量级无索引词汇检索能有效支持仓库级代码补全，GrepRAG通过简单后处理解决词汇检索的局限性，为代码补全提供高效实用方案。

Abstract: Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.

</details>


### [52] [Outcome-Conditioned Reasoning Distillation for Resolving Software Issues](https://arxiv.org/abs/2601.23257)
*Chenglin Li,Yisen Xu,Zehao Wang,Shin Hwei Tan,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: O-CRD框架通过从已验证的历史修复中反向重构修复轨迹，在推理时复用这些指导来引导定位和补丁生成，无需微调或在线搜索，显著提升软件问题修复成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM修复方法采用"重置-解决"模式，每次遇到新问题都重新推理，浪费了仓库中相似问题的历史修复经验。传统方法通过前向试错（如重复精炼或搜索）获取信号，成本高且易偏离正确补丁。

Method: 提出结果条件推理蒸馏(O-CRD)框架：1) 使用已验证的历史修复作为监督；2) 从历史修复反向重构阶段式修复轨迹；3) 在推理时复用蒸馏的指导来引导文件/函数定位和补丁合成，无需微调或在线搜索。

Result: 在SWE-Bench Lite基准测试中，该方法显著提升Pass@1：GPT-4o提升10.4%，DeepSeek-V3提升8.6%，GPT-5提升10.3%。表明结果条件的已验证修复复用可以替代成本高昂的前向探索。

Conclusion: O-CRD框架通过从历史修复中蒸馏指导并复用，有效解决了软件问题修复中的长程决策问题，避免了传统前向探索方法的高成本和发散风险，显著提升了修复成功率。

Abstract: Software issue resolution in large repositories is a long-range decision process: choices made during localization shape the space of viable edits, and missteps can compound into incorrect patches. Despite this, many LLM-based repair pipelines still operate in a reset-and-solve manner, producing fresh reasoning for every new issue instead of carrying forward what worked in past fixes. This is wasteful because repositories routinely contain earlier issues with overlapping structure, failure modes, or constraints, where prior repair experience could provide useful guidance. Existing approaches typically harvest this signal through forward-time trial procedures, such as repeated refinement or search, incurring high inference cost while still risking divergence from the eventual correct patch. We present an Outcome-Conditioned Reasoning Distillation(O-CRD) framework that uses resolved in-repository issues with verified patches as supervision. Starting from a historical fix, the method reconstructs a stage-wise repair trace backward from the verified outcome, then reuses the distilled guidance at inference time to steer file/function localization and patch synthesis, without fine-tuning or online search. On SWE-Bench Lite, this approach increases Pass@1 by 10.4% with GPT-4o, 8.6% with DeepSeek-V3, and 10.3% with GPT-5, indicating that outcome-conditioned reuse of verified repairs can replace costly forward exploration for software issue resolution.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [53] [ShellForge: Adversarial Co-Evolution of Webshell Generation and Multi-View Detection for Robust Webshell Defense](https://arxiv.org/abs/2601.22182)
*Yizhong Ding*

Main category: cs.CR

TL;DR: ShellForge：一种对抗性协同进化框架，通过自动生成Webshell与多视图检测相结合，持续强化PHP WebShell防御边界，显著提升检测鲁棒性并降低误报率。


<details>
  <summary>Details</summary>
Motivation: 现有PHP WebShell检测机制难以应对快速变种演进和复杂混淆技术，且在遇到用于知识产权保护的重度混淆良性管理脚本时误报率高。

Method: 采用对抗性协同进化框架，包含生成器和检测器的协同训练循环。生成器通过监督微调和基于偏好的强化学习合成功能性强、高度规避的变种；检测器集成多视图特征：长字符串压缩语义特征、修剪抽象语法树结构特征、香农熵等全局统计指标；使用LLM生成去恶意样本作为高质量硬负例。

Result: 在公开FWOID基准测试中，ShellForge显著增强防御鲁棒性。收敛后检测器保持0.981 F1分数，生成器在VirusTotal商业引擎上达到0.939的规避率。

Conclusion: ShellForge通过对抗性协同进化有效解决了PHP WebShell检测中的变种演进和误报问题，为WebShell防御提供了持续强化的解决方案。

Abstract: Webshells remain a primary foothold for attackers to compromise servers, particularly within PHP ecosystems. However, existing detection mechanisms often struggle to keep pace with rapid variant evolution and sophisticated obfuscation techniques that camouflage malicious intent. Furthermore, many current defenses suffer from high false-alarm rates when encountering benign administrative scripts that employ heavy obfuscation for intellectual property protection. To address these challenges, we present ShellForge, an adversarial co-evolution framework that couples automated webshell generation with multi-view detection to continuously harden defensive boundaries. The framework operates through an iterative co-training loop where a generator and a detector mutually reinforce each other via the exchange of hard samples. The generator is optimized through supervised fine-tuning and preference-based reinforcement learning to synthesize functional, highly evasive variants. Simultaneously, we develop a multi-view fusion detector that integrates semantic features from long-string compression, structural features from pruned abstract syntax trees, and global statistical indicators such as Shannon entropy. To minimize false positives, ShellForge utilizes a LLM-based transformation to create de-malicious samples--scripts that retain complex obfuscation patterns but lack harmful payloads--serving as high-quality hard negatives during training. Evaluations on the public FWOID benchmark demonstrate that ShellForge significantly enhances defensive robustness. Upon convergence, the detector maintains a 0.981 F1-score while the generator achieves a 0.939 evasion rate against commercial engines on VirusTotal.

</details>


### [54] [MemeChain: A Multimodal Cross-Chain Dataset for Meme Coin Forensics and Risk Analysis](https://arxiv.org/abs/2601.22185)
*Alberto Maria Mongardini,Alessandro Mei*

Main category: cs.CR

TL;DR: MemeChain是一个大规模开源跨链数据集，包含34,988个模因币，整合了链上数据和链下工件，用于模因币生态系统的多模态和法证研究。


<details>
  <summary>Details</summary>
Motivation: 模因币生态系统已成为加密货币市场中最活跃但最不可观察的领域，具有极高的流失率、最小的项目承诺和广泛的欺诈行为。现有数据集通常限于单链数据或缺乏多模态工件，无法进行全面的风险建模。

Method: 构建MemeChain数据集，涵盖以太坊、BNB智能链、Solana和Base四个区块链上的34,988个模因币。整合链上数据与链下工件，包括网站HTML源代码、代币标志和关联的社交媒体账户。

Result: 分析显示视觉品牌在低质量部署中经常被省略，许多项目缺乏功能性网站。量化了生态系统的极端波动性：1,801个代币（5.15%）在启动后24小时内停止所有交易活动。

Conclusion: MemeChain通过提供统一的跨链覆盖和丰富的链下上下文，成为模因币生态系统中金融法证、多模态异常检测和自动诈骗预防研究的基础资源。

Abstract: The meme coin ecosystem has grown into one of the most active yet least observable segments of the cryptocurrency market, characterized by extreme churn, minimal project commitment, and widespread fraudulent behavior. While countless meme coins are deployed across multiple blockchains, they rely heavily on off-chain web and social infrastructure to signal legitimacy. These very signals are largely absent from existing datasets, which are often limited to single-chain data or lack the multimodal artifacts required for comprehensive risk modeling.
  To address this gap, we introduce MemeChain, a large-scale, open-source, cross-chain dataset comprising 34,988 meme coins across Ethereum, BNB Smart Chain, Solana, and Base. MemeChain integrates on-chain data with off-chain artifacts, including website HTML source code, token logos, and linked social media accounts, enabling multimodal and forensic study of meme coin projects. Analysis of the dataset shows that visual branding is frequently omitted in low-effort deployments, and many projects lack a functional website. Moreover, we quantify the ecosystem's extreme volatility, identifying 1,801 tokens (5.15%) that cease all trading activity within just 24 hours of launch. By providing unified cross-chain coverage and rich off-chain context, MemeChain serves as a foundational resource for research in financial forensics, multimodal anomaly detection, and automated scam prevention in the meme coin ecosystem.

</details>


### [55] [A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy](https://arxiv.org/abs/2601.22240)
*Pedro H. Barcha Correia,Ryan W. Achjian,Diego E. G. Caetano de Oliveira,Ygor Acacio Maria,Victor Takashi Hayashi,Marcos Lopes,Charles Christian Miers,Marcos A. Simplicio*

Main category: cs.CR

TL;DR: 本文首次对提示注入缓解策略进行了系统性文献综述，涵盖88项研究，扩展了NIST对抗性机器学习分类法，提供了防御策略的全面目录和实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大型语言模型的快速发展，出现了新的安全漏洞和挑战，如越狱和其他提示注入攻击。这些恶意输入可能导致数据泄露、未授权操作或输出受损。由于攻防技术快速演进，需要系统性地理解缓解策略。

Method: 采用系统性文献综述方法，收集并分析了88项关于提示注入缓解策略的研究。基于NIST对抗性机器学习报告，扩展其分类法，引入新的防御类别，并建立标准化的术语体系。

Result: 提出了扩展的NIST分类法，创建了包含88项研究的全面防御策略目录，记录了各项防御在特定LLM和攻击数据集上的定量效果，并标注了开源和模型无关的解决方案。

Conclusion: 本研究为对抗性机器学习领域提供了首个系统性的提示注入缓解策略综述，通过标准化分类法和实用资源目录，为研究者和开发者提供了有效的防御实施指南，促进了该领域的进一步发展。

Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (GenAI) and large language models (LLMs) has been accompanied by the emergence of new security vulnerabilities and challenges, such as jailbreaking and other prompt injection attacks. These maliciously crafted inputs can exploit LLMs, causing data leaks, unauthorized actions, or compromised outputs, for instance. As both offensive and defensive prompt injection techniques evolve quickly, a structured understanding of mitigation strategies becomes increasingly important. To address that, this work presents the first systematic literature review on prompt injection mitigation strategies, comprehending 88 studies. Building upon NIST's report on adversarial machine learning, this work contributes to the field through several avenues. First, it identifies studies beyond those documented in NIST's report and other academic reviews and surveys. Second, we propose an extension to NIST taxonomy by introducing additional categories of defenses. Third, by adopting NIST's established terminology and taxonomy as a foundation, we promote consistency and enable future researchers to build upon the standardized taxonomy proposed in this work. Finally, we provide a comprehensive catalog of the reviewed prompt injection defenses, documenting their reported quantitative effectiveness across specific LLMs and attack datasets, while also indicating which solutions are open-source and model-agnostic. This catalog, together with the guidelines presented herein, aims to serve as a practical resource for researchers advancing the field of adversarial machine learning and for developers seeking to implement effective defenses in production systems.

</details>


### [56] [MirrorMark: A Distortion-Free Multi-Bit Watermark for Large Language Models](https://arxiv.org/abs/2601.22246)
*Ya Jiang,Massieh Kordi Boroujeny,Surender Suresh Kumar,Kai Zeng*

Main category: cs.CR

TL;DR: MirrorMark是一种用于大语言模型的多比特、无失真的水印方法，通过镜像采样随机性在保持概率分布不变的情况下嵌入多比特消息，同时保持文本质量并提高可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在问答和内容创作等应用中的普及，可靠的内容溯源变得日益重要。现有水印方法要么只提供二元信号，要么会扭曲采样分布从而降低文本质量；而无失真方法则往往存在可检测性弱或鲁棒性差的问题。

Method: MirrorMark采用保持测度的方式镜像采样随机性，在不改变标记概率分布的情况下嵌入多比特消息。为提高鲁棒性，引入了基于上下文的调度器，在消息位置间平衡标记分配，同时保持对插入和删除操作的弹性。还提供了等错误率的理论分析来解释经验性能。

Result: 实验表明，MirrorMark与非水印生成的文本质量相当，同时实现了显著更强的可检测性：在300个标记中嵌入54比特时，比特准确率提高了8-12%，在1%误报率下正确识别的水印文本增加了11%。

Conclusion: MirrorMark是一种有效的多比特、无失真水印方法，能够在不损害文本质量的前提下提供更强的可检测性和鲁棒性，为大语言模型的内容溯源提供了有前景的解决方案。

Abstract: As large language models (LLMs) become integral to applications such as question answering and content creation, reliable content attribution has become increasingly important. Watermarking is a promising approach, but existing methods either provide only binary signals or distort the sampling distribution, degrading text quality; distortion-free approaches, in turn, often suffer from weak detectability or robustness. We propose MirrorMark, a multi-bit and distortion-free watermark for LLMs. By mirroring sampling randomness in a measure-preserving manner, MirrorMark embeds multi-bit messages without altering the token probability distribution, preserving text quality by design. To improve robustness, we introduce a context-based scheduler that balances token assignments across message positions while remaining resilient to insertions and deletions. We further provide a theoretical analysis of the equal error rate to interpret empirical performance. Experiments show that MirrorMark matches the text quality of non-watermarked generation while achieving substantially stronger detectability: with 54 bits embedded in 300 tokens, it improves bit accuracy by 8-12% and correctly identifies up to 11% more watermarked texts at 1% false positive rate.

</details>


### [57] [Rethinking Anonymity Claims in Synthetic Data Generation: A Model-Centric Privacy Attack Perspective](https://arxiv.org/abs/2601.22434)
*Georgi Ganev,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 本文从模型中心视角重新思考合成数据的匿名性，认为有意义的评估必须考虑底层生成模型的能力和特性，并与最先进的隐私攻击相结合，指出合成数据技术本身不足以保证充分匿名化，并比较了差分隐私和基于相似性的隐私度量两种机制。


<details>
  <summary>Details</summary>
Motivation: 当前生成式机器学习模型生成合成表格数据已成为增强数据共享隐私的流行方法，但发布训练模型或生成的合成数据集仍存在隐私风险。现有研究、商业部署和GDPR等隐私法规主要在单个数据集层面评估匿名性，未能充分考虑生成模型的可访问性和特性。

Method: 从模型中心视角重新思考合成数据的匿名性主张，将GDPR的个人数据和匿名化定义在模型可访问假设下进行解释，识别必须缓解的可识别性风险类型，并将其映射到不同威胁设置下的隐私攻击。比较合成数据技术中常用的两种机制：差分隐私和基于相似性的隐私度量。

Result: 合成数据技术本身不足以保证充分的匿名化。差分隐私能够提供针对可识别性风险的强大保护，而基于相似性的隐私度量缺乏足够的保障措施。研究将监管层面的可识别性概念与模型中心的隐私攻击联系起来。

Conclusion: 有意义的匿名性评估必须考虑底层生成模型的能力和特性，并与最先进的隐私攻击相结合。差分隐私是更可靠的保护机制，而基于相似性的隐私度量不足。该工作为研究人员、从业者和政策制定者提供了更负责任和可信赖的合成数据系统监管评估框架。

Abstract: Training generative machine learning models to produce synthetic tabular data has become a popular approach for enhancing privacy in data sharing. As this typically involves processing sensitive personal information, releasing either the trained model or generated synthetic datasets can still pose privacy risks. Yet, recent research, commercial deployments, and privacy regulations like the General Data Protection Regulation (GDPR) largely assess anonymity at the level of an individual dataset.
  In this paper, we rethink anonymity claims about synthetic data from a model-centric perspective and argue that meaningful assessments must account for the capabilities and properties of the underlying generative model and be grounded in state-of-the-art privacy attacks. This perspective better reflects real-world products and deployments, where trained models are often readily accessible for interaction or querying. We interpret the GDPR's definitions of personal data and anonymization under such access assumptions to identify the types of identifiability risks that must be mitigated and map them to privacy attacks across different threat settings. We then argue that synthetic data techniques alone do not ensure sufficient anonymization. Finally, we compare the two mechanisms most commonly used alongside synthetic data -- Differential Privacy (DP) and Similarity-based Privacy Metrics (SBPMs) -- and argue that while DP can offer robust protections against identifiability risks, SBPMs lack adequate safeguards. Overall, our work connects regulatory notions of identifiability with model-centric privacy attacks, enabling more responsible and trustworthy regulatory assessment of synthetic data systems by researchers, practitioners, and policymakers.

</details>


### [58] [FraudShield: Knowledge Graph Empowered Defense for LLMs against Fraud Attacks](https://arxiv.org/abs/2601.22485)
*Naen Xu,Jinghuai Zhang,Ping He,Chunyi Zhou,Jun Wang,Zhihui Fu,Tianyu Du,Zhaoxiang Wang,Shouling Ji*

Main category: cs.CR

TL;DR: FraudShield是一个保护LLMs免受欺诈内容攻击的新框架，通过构建欺诈战术-关键词知识图谱来增强模型安全性


<details>
  <summary>Details</summary>
Motivation: LLMs已广泛应用于关键自动化工作流程，但容易受到欺诈信息操纵，导致有害后果。现有防御方法在有效性、可解释性和泛化性方面存在局限

Method: 构建并精炼欺诈战术-关键词知识图谱，捕捉可疑文本与欺诈技术之间的高置信度关联。结构化知识图谱通过突出关键词和提供支持证据来增强原始输入，引导LLM生成更安全的响应

Result: 在四个主流LLMs和五种代表性欺诈类型上的广泛实验表明，FraudShield始终优于最先进的防御方法，同时为模型生成提供可解释的线索

Conclusion: FraudShield通过结构化知识图谱方法有效保护LLMs免受欺诈内容攻击，在性能、可解释性和泛化性方面优于现有防御方法

Abstract: Large language models (LLMs) have been widely integrated into critical automated workflows, including contract review and job application processes. However, LLMs are susceptible to manipulation by fraudulent information, which can lead to harmful outcomes. Although advanced defense methods have been developed to address this issue, they often exhibit limitations in effectiveness, interpretability, and generalizability, particularly when applied to LLM-based applications. To address these challenges, we introduce FraudShield, a novel framework designed to protect LLMs from fraudulent content by leveraging a comprehensive analysis of fraud tactics. Specifically, FraudShield constructs and refines a fraud tactic-keyword knowledge graph to capture high-confidence associations between suspicious text and fraud techniques. The structured knowledge graph augments the original input by highlighting keywords and providing supporting evidence, guiding the LLM toward more secure responses. Extensive experiments show that FraudShield consistently outperforms state-of-the-art defenses across four mainstream LLMs and five representative fraud types, while also offering interpretable clues for the model's generations.

</details>


### [59] [VocBulwark: Towards Practical Generative Speech Watermarking via Additional-Parameter Injection](https://arxiv.org/abs/2601.22556)
*Weizhi Liu,Yue Li,Zhaoxia Yin*

Main category: cs.CR

TL;DR: VocBulwark是一种保护生成语音安全的水印框架，通过冻结生成模型参数保持音质，结合时间适配器和粗到细门控提取器实现高保真、高鲁棒性的水印嵌入。


<details>
  <summary>Details</summary>
Motivation: 生成语音已达到人类自然水平，但带来了滥用风险。现有水印方法无法平衡保真度和鲁棒性，要么在噪声空间简单叠加，要么需要侵入式修改模型权重。

Method: 提出VocBulwark框架：1)冻结生成模型参数以保持感知质量；2)设计时间适配器将水印与声学属性深度纠缠；3)结合粗到细门控提取器抵抗高级攻击；4)开发精度引导优化课程动态协调梯度流，解决保真度与鲁棒性之间的优化冲突。

Result: 综合实验表明，VocBulwark实现高容量、高保真水印，能有效防御复杂实际场景，对编解码器再生和变长操作具有强鲁棒性。

Conclusion: VocBulwark通过额外参数注入框架成功解决了生成语音水印中保真度与鲁棒性的平衡问题，为实际应用提供了有效的安全防护方案。

Abstract: Generated speech achieves human-level naturalness but escalates security risks of misuse. However, existing watermarking methods fail to reconcile fidelity with robustness, as they rely either on simple superposition in the noise space or on intrusive alterations to model weights. To bridge this gap, we propose VocBulwark, an additional-parameter injection framework that freezes generative model parameters to preserve perceptual quality. Specifically, we design a Temporal Adapter to deeply entangle watermarks with acoustic attributes, synergizing with a Coarse-to-Fine Gated Extractor to resist advanced attacks. Furthermore, we develop an Accuracy-Guided Optimization Curriculum that dynamically orchestrates gradient flow to resolve the optimization conflict between fidelity and robustness. Comprehensive experiments demonstrate that VocBulwark achieves high-capacity and high-fidelity watermarking, offering robust defense against complex practical scenarios, with resilience to Codec regenerations and variable-length manipulations.

</details>


### [60] [The Semantic Trap: Do Fine-tuned LLMs Learn Vulnerability Root Cause or Just Functional Pattern?](https://arxiv.org/abs/2601.22655)
*Feiyang Huang,Yuqiang Sun,Fan Zhang,Ziqi Yang,Han Liu,Yang Liu*

Main category: cs.CR

TL;DR: 研究发现微调后的大语言模型在漏洞检测中可能陷入"语义陷阱"，即依赖功能模式而非真正的安全语义理解，导致高指标分数具有欺骗性。


<details>
  <summary>Details</summary>
Motivation: 当前微调后的大语言模型在软件漏洞检测中表现出良好性能，但尚不清楚这种提升是源于对漏洞根本原因的真正理解，还是仅仅利用了功能模式。研究者希望探究模型是否真正理解了安全语义。

Method: 提出TrapEval评估框架，包含两个互补数据集：V2N（漏洞代码与无关良性代码配对）和V2P（漏洞代码与其修补版本配对）。使用该框架对五个代表性大语言模型进行微调和评估，采用跨数据集测试、语义保持扰动和基于CodeBLEU的语义差距度量。

Result: 尽管指标有所改善，但微调后的大语言模型在区分漏洞代码与其修补版本时持续困难；在轻微语义保持变换下表现出严重的鲁棒性退化；当语义差距较小时严重依赖功能上下文捷径。这些发现表明当前微调实践未能传授真正的漏洞推理能力。

Conclusion: 传统数据集上的高基准分数可能具有欺骗性，掩盖了模型无法理解漏洞真正因果逻辑的事实。研究结果警示需要开发能够真正理解安全语义的评估方法和模型训练策略。

Abstract: LLMs demonstrate promising performance in software vulnerability detection after fine-tuning. However, it remains unclear whether these gains reflect a genuine understanding of vulnerability root causes or merely an exploitation of functional patterns. In this paper, we identify a critical failure mode termed the "semantic trap," where fine-tuned LLMs achieve high detection scores by associating certain functional domains with vulnerability likelihood rather than reasoning about the underlying security semantics.To systematically evaluate this phenomenon, we propose TrapEval, a comprehensive evaluation framework designed to disentangle vulnerability root cause from functional pattern. TrapEval introduces two complementary datasets derived from real-world open-source projects: V2N, which pairs vulnerable code with unrelated benign code, and V2P, which pairs vulnerable code with its corresponding patched version, forcing models to distinguish near-identical code that differs only in subtle security-critical logic. Using TrapEval, we fine-tune five representative state-of-the-art LLMs across three model families and evaluate them under cross-dataset testing, semantic-preserving perturbations, and varying degrees of semantic gap measured by CodeBLEU.Our empirical results reveal that, despite improvements in metrics, fine-tuned LLMs consistently struggle to distinguish vulnerable code from its patched counterpart, exhibit severe robustness degradation under minor semantic-preserving transformations, and rely heavily on functional-context shortcuts when the semantic gap is small. These findings provide strong evidence that current fine-tuning practices often fail to impart true vulnerability reasoning. Our findings serve as a wake-up call: high benchmark scores on traditional datasets may be illusory, masking the model's inability to understand the true causal logic of vulnerabilities.

</details>


### [61] [RealSec-bench: A Benchmark for Evaluating Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2601.22706)
*Yanlin Wang,Ziyao Zhang,Chong Wang,Xinyi Xu,Mingwei Liu,Yong Wang,Jiachi Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: RealSec-bench：基于真实世界Java仓库构建的安全代码生成基准，包含105个实例，涵盖19种CWE类型，用于评估LLM生成安全代码的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估LLM代码生成安全性方面存在不足，主要依赖合成漏洞或孤立评估功能正确性，未能捕捉真实软件中功能与安全的复杂交互。

Method: 采用多阶段流水线构建基准：系统SAST扫描（CodeQL）+ LLM假阳性消除 + 人工专家验证。提出SecurePass@K复合指标同时评估功能正确性和安全性。

Result: 对5个流行LLM的评估显示：RAG技术能提升功能正确性但对安全性改善有限；显式安全提示常导致编译失败，损害功能正确性且不能可靠防止漏洞。

Conclusion: 当前LLM在功能代码生成与安全代码生成之间存在显著差距，需要专门的安全代码生成评估基准和方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their proficiency in producing secure code remains a critical, under-explored area. Existing benchmarks often fall short by relying on synthetic vulnerabilities or evaluating functional correctness in isolation, failing to capture the complex interplay between functionality and security found in real-world software. To address this gap, we introduce RealSec-bench, a new benchmark for secure code generation meticulously constructed from real-world, high-risk Java repositories. Our methodology employs a multi-stage pipeline that combines systematic SAST scanning with CodeQL, LLM-based false positive elimination, and rigorous human expert validation. The resulting benchmark contains 105 instances grounded in real-word repository contexts, spanning 19 Common Weakness Enumeration (CWE) types and exhibiting a wide diversity of data flow complexities, including vulnerabilities with up to 34-hop inter-procedural dependencies. Using RealSec-bench, we conduct an extensive empirical study on 5 popular LLMs. We introduce a novel composite metric, SecurePass@K, to assess both functional correctness and security simultaneously. We find that while Retrieval-Augmented Generation (RAG) techniques can improve functional correctness, they provide negligible benefits to security. Furthermore, explicitly prompting models with general security guidelines often leads to compilation failures, harming functional correctness without reliably preventing vulnerabilities. Our work highlights the gap between functional and secure code generation in current LLMs.

</details>


### [62] [AlienLM: Alienization of Language for API-Boundary Privacy in Black-Box LLMs](https://arxiv.org/abs/2601.22710)
*Jaehee Kim,Pilsung Kang*

Main category: cs.CR

TL;DR: AlienLM是一种API隐私保护层，通过词汇级双射将文本转换为"外星语言"，客户端可无损恢复，仅需标准微调API即可让目标模型直接在转换后的输入上运行。


<details>
  <summary>Details</summary>
Motivation: 现代LLM通过黑盒API访问时，用户需要将敏感提示、输出和微调数据传输给外部提供商，在API边界处产生严重的隐私风险。

Method: AlienLM通过词汇级双射将文本翻译为"外星语言"，客户端可无损恢复；使用Alien Adaptation Training（AAT）仅通过标准微调API即可让目标模型直接在转换后的输入上运行。

Result: 在4个LLM主干和7个基准测试中，AlienLM平均保持超过81%的明文性能，显著优于随机双射和字符级基线；在攻击者可访问模型权重、语料统计和学习型逆翻译的情况下，恢复攻击仅能重建少于0.22%的转换后标记。

Conclusion: AlienLM为API-only访问下的隐私保护LLM部署提供了实用途径，在保持任务性能的同时显著减少了明文暴露风险。

Abstract: Modern LLMs are increasingly accessed via black-box APIs, requiring users to transmit sensitive prompts, outputs, and fine-tuning data to external providers, creating a critical privacy risk at the API boundary. We introduce AlienLM, a deployable API-only privacy layer that protects text by translating it into an Alien Language via a vocabulary-scale bijection, enabling lossless recovery on the client side. Using only standard fine-tuning APIs, Alien Adaptation Training (AAT) adapts target models to operate directly on alienized inputs. Across four LLM backbones and seven benchmarks, AlienLM retains over 81\% of plaintext-oracle performance on average, substantially outperforming random-bijection and character-level baselines. Under adversaries with access to model weights, corpus statistics, and learning-based inverse translation, recovery attacks reconstruct fewer than 0.22\% of alienized tokens. Our results demonstrate a practical pathway for privacy-preserving LLM deployment under API-only access, substantially reducing plaintext exposure while maintaining task performance.

</details>


### [63] [Rust and Go directed fuzzing with LibAFL-DiFuzz](https://arxiv.org/abs/2601.22772)
*Timofey Mezhuev,Darya Parygina,Daniil Kuts*

Main category: cs.CR

TL;DR: 该论文提出了一种针对Rust和Go语言的定向灰盒模糊测试方法，通过编译器定制和高级预处理技术，相比现有工具在效率和准确性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着Rust和Go语言的流行，需要为这些语言提供精确高效的测试解决方案。传统定向模糊测试主要针对C/C++，缺乏对Rust和Go的有效支持，而覆盖引导模糊测试在验证静态分析报告或复现崩溃等特定任务中效果有限。

Method: 提出针对Rust和Go应用的定向灰盒模糊测试新方法，包括：1）高级预处理技术；2）rustc编译器定制；3）精细的图构建和插桩方法；4）基于LibAFL-DiFuzz后端实现模糊测试工具。

Result: Rust-LibAFL-DiFuzz在TTE（暴露时间）实验中表现最佳，优于afl.rs、cargo-fuzz等工具。Go-LibAFL-DiFuzz在多数情况下平均结果最佳，有两个案例显示出数量级差异。证明了该方法在效率和准确性上的优势。

Conclusion: 该研究成功将定向模糊测试扩展到C/C++以外的语言，为Rust和Go应用提供了有效的定向测试解决方案，在效率和准确性方面优于现有工具，稳定性问题可能与不同的变异策略有关。

Abstract: In modern SSDLC, program analysis and automated testing are essential for minimizing vulnerabilities before software release, with fuzzing being a fast and widely used dynamic testing method. However, traditional coverage-guided fuzzing may be less effective in specific tasks like verifying static analysis reports or reproducing crashes, while directed fuzzing, focusing on targeted program locations using proximity metrics, proves to be more effective. Some of the earliest directed fuzzers are, for example, AFLGo and BEACON, which use different proximity metric approaches. Although most automated testing tools focus on C/C++ code, the growing popularity of Rust and Go causes the need for precise and efficient testing solutions for these languages. This work expands the applicability of directed fuzzing beyond traditional analysis of C/C++ software. We present a novel approach to directed greybox fuzzing tailored specifically for Rust and Go applications. We introduce advanced preprocessing techniques, rustc compiler customizations, and elaborate graph construction and instrumentation methods to enable effective targeting of specific program locations. Our implemented fuzzing tools, based on LibAFL-DiFuzz backend, demonstrate competitive advantages compared to popular existing fuzzers like afl.rs, cargo-fuzz, and go-fuzz. According to TTE (Time to Exposure) experiments, Rust-LibAFL-DiFuzz outperforms other tools by the best TTE result. Some stability issues can be explained by different mutation approaches. Go-LibAFL-DiFuzz outperforms its opponent by the best and, in the majority of cases, by average result, having two cases with orders of magnitude difference. These results prove better efficiency and accuracy of our approach.

</details>


### [64] [Trojan-Resilient NTT: Protecting Against Control Flow and Timing Faults on Reconfigurable Platforms](https://arxiv.org/abs/2601.22804)
*Rourab Paul,Krishnendu Guha,Amlan Chakrabarti*

Main category: cs.CR

TL;DR: 提出一种安全的NTT架构，能够检测非常规延迟、控制流中断和软分析侧信道攻击，并提供自适应故障校正方法，用于后量子密码硬件安全防护。


<details>
  <summary>Details</summary>
Motivation: NTT是后量子密码算法（如Kyber、Dilithium、NTRU）多项式乘法的核心组件。侧信道攻击和硬件木马可能改变控制信号，破坏电路控制流并引入非常规延迟。硬件木马对控制信号的攻击成本低且影响大，因为单个被破坏的控制信号就能中断或绕过整个计算序列，而数据故障通常只导致局部错误。此外，攻击者可以利用插入的硬件木马对设计执行软分析侧信道攻击。

Method: 提出一种安全的NTT架构，包含故障检测和校正模块。该架构能够检测非常规延迟、控制流中断和SASCA攻击。采用自适应故障校正方法进行缓解。在Artix-7 FPGA上对不同的Kyber变体进行了广泛仿真和实现。

Result: 故障检测和校正模块能够高效检测和校正故障，无论是由硬件木马有意还是无意引起的故障，都具有较高的成功率。同时只引入了适度的面积和时间开销。

Conclusion: 提出的安全NTT架构能够有效防护控制流中断、非常规延迟和软分析侧信道攻击，为后量子密码硬件提供可靠的安全保障，且开销可控。

Abstract: Number Theoretic Transform (NTT) is the most essential component for polynomial multiplications used in lattice-based Post-Quantum Cryptography (PQC) algorithms such as Kyber, Dilithium, NTRU etc. However, side-channel attacks (SCA) and hardware vulnerabilities in the form of hardware Trojans may alter control signals to disrupt the circuit's control flow and introduce unconventional delays in the critical hardware of PQC. Hardware Trojans, especially on control signals, are more low cost and impactful than data signals because a single corrupted control signal can disrupt or bypass entire computation sequences, whereas data faults usually cause only localized errors. On the other hand, adversaries can perform Soft Analytical Side Channel Attacks (SASCA) on the design using the inserted hardware Trojan. In this paper, we present a secure NTT architecture capable of detecting unconventional delays, control-flow disruptions, and SASCA, while providing an adaptive fault-correction methodology for their mitigation. Extensive simulations and implementations of our Secure NTT on Artix-7 FPGA with different Kyber variants show that our fault detection and correction modules can efficiently detect and correct faults whether caused unintentionally or intentionally by hardware Trojans with a high success rate, while introducing only modest area and time overheads.

</details>


### [65] [Hide and Seek in Embedding Space: Geometry-based Steganography and Detection in Large Language Models](https://arxiv.org/abs/2601.22818)
*Charles Westphal,Keivan Navaie,Fernando E. Rosas*

Main category: cs.CR

TL;DR: 该论文研究LLM微调中的隐写术威胁，提出低可恢复性隐写方案，并开发基于机制可解释性的检测方法。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明微调后的LLM可以通过隐写通道将提示秘密编码到输出中，但这些方法依赖易于恢复的编码方式。需要研究更隐蔽的隐写方案及其检测方法。

Method: 1) 形式化有效载荷可恢复性概念，用分类器准确率衡量；2) 提出低可恢复性隐写术，用嵌入空间派生的映射替代任意映射；3) 提出基于机制可解释性的检测方法，在深层激活上训练线性探针。

Result: 低可恢复性方案显著提升秘密恢复率：Llama-8B从17%→30%(+78%)，Ministral-8B从24%→43%(+80%)，Llama-70B从9%→19%(+123%)，同时降低有效载荷可恢复性。线性探针检测在微调模型中比基础模型准确率高33%。

Conclusion: 恶意微调会在模型中留下可操作的内在特征，基于机制可解释性的检测方法比传统隐写分析更有效，为防御此类攻击提供了新方向。

Abstract: Fine-tuned LLMs can covertly encode prompt secrets into outputs via steganographic channels. Prior work demonstrated this threat but relied on trivially recoverable encodings. We formalize payload recoverability via classifier accuracy and show previous schemes achieve 100\% recoverability. In response, we introduce low-recoverability steganography, replacing arbitrary mappings with embedding-space-derived ones. For Llama-8B (LoRA) and Ministral-8B (LoRA) trained on TrojanStego prompts, exact secret recovery rises from 17$\rightarrow$30\% (+78\%) and 24$\rightarrow$43\% (+80\%) respectively, while on Llama-70B (LoRA) trained on Wiki prompts, it climbs from 9$\rightarrow$19\% (+123\%), all while reducing payload recoverability. We then discuss detection. We argue that detecting fine-tuning-based steganographic attacks requires approaches beyond traditional steganalysis. Standard approaches measure distributional shift, which is an expected side-effect of fine-tuning. Instead, we propose a mechanistic interpretability approach: linear probes trained on later-layer activations detect the secret with up to 33\% higher accuracy in fine-tuned models compared to base models, even for low-recoverability schemes. This suggests that malicious fine-tuning leaves actionable internal signatures amenable to interpretability-based defenses.

</details>


### [66] [Evaluating Large Language Models for Security Bug Report Prediction](https://arxiv.org/abs/2601.22921)
*Farnaz Soltaniani,Shoaib Razzaq,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 论文评估了基于提示工程和微调两种LLM方法在安全漏洞报告早期检测中的表现，发现两者存在明显的权衡：提示工程方法召回率高但精度低，微调方法精度高但召回率低。


<details>
  <summary>Details</summary>
Motivation: 安全漏洞报告（SBRs）的早期检测对于及时缓解漏洞至关重要，需要探索大型语言模型（LLMs）在SBR预测中的应用效果。

Method: 采用基于提示工程的专有模型和微调模型两种方法进行对比评估，在不同数据集上测试SBR预测性能。

Result: 提示工程专有模型平均G-measure为77%，召回率74%，但精度仅22%；微调模型平均G-measure为51%，精度75%，召回率36%。微调模型在最大数据集上的推理速度比专有模型快50倍。

Conclusion: 两种方法存在明显权衡：提示工程方法对SBR更敏感但误报率高，微调方法精度高但召回率低。需要进一步研究以充分利用LLMs进行SBR预测。

Abstract: Early detection of security bug reports (SBRs) is critical for timely vulnerability mitigation. We present an evaluation of prompt-based engineering and fine-tuning approaches for predicting SBRs using Large Language Models (LLMs). Our findings reveal a distinct trade-off between the two approaches. Prompted proprietary models demonstrate the highest sensitivity to SBRs, achieving a G-measure of 77% and a recall of 74% on average across all the datasets, albeit at the cost of a higher false-positive rate, resulting in an average precision of only 22%. Fine-tuned models, by contrast, exhibit the opposite behavior, attaining a lower overall G-measure of 51% but substantially higher precision of 75% at the cost of reduced recall of 36%. Though a one-time investment in building fine-tuned models is necessary, the inference on the largest dataset is up to 50 times faster than that of proprietary models. These findings suggest that further investigations to harness the power of LLMs for SBR prediction are necessary.

</details>


### [67] [Protecting Private Code in IDE Autocomplete using Differential Privacy](https://arxiv.org/abs/2601.22935)
*Evgeny Grigorenko,David Stanojević,David Ilić,Egor Bogomolov,Kostadin Cvejoski*

Main category: cs.CR

TL;DR: 该论文研究了在Kotlin代码补全LLM训练中使用差分隐私作为防御机制，证明DP能有效抵抗成员推理攻击且对模型性能影响很小。


<details>
  <summary>Details</summary>
Motivation: 现代IDE使用LLM进行代码补全，但训练这些模型时使用用户编写的代码会带来严重的隐私风险，使模型本身成为新的数据漏洞。恶意攻击者可以利用这些漏洞重构敏感训练数据或推断特定代码片段是否用于训练。

Method: 使用差分隐私(DP)作为防御机制，对Kotlin代码补全的LLM（Mellum模型）进行微调，并进行全面的隐私和效用评估。

Result: DP提供了强大的成员推理攻击(MIA)防御，将攻击成功率降低至接近随机猜测水平（AUC从0.901降至0.606）。DP训练模型在效用得分上与未使用DP的模型相当，即使训练数据量减少100倍。

Conclusion: 差分隐私是构建私密且可信赖的AI驱动IDE功能的实用有效解决方案，能在保证隐私的同时维持模型性能。

Abstract: Modern Integrated Development Environments (IDEs) increasingly leverage Large Language Models (LLMs) to provide advanced features like code autocomplete. While powerful, training these models on user-written code introduces significant privacy risks, making the models themselves a new type of data vulnerability. Malicious actors can exploit this by launching attacks to reconstruct sensitive training data or infer whether a specific code snippet was used for training. This paper investigates the use of Differential Privacy (DP) as a robust defense mechanism for training an LLM for Kotlin code completion. We fine-tune a \texttt{Mellum} model using DP and conduct a comprehensive evaluation of its privacy and utility. Our results demonstrate that DP provides a strong defense against Membership Inference Attacks (MIAs), reducing the attack's success rate close to a random guess (AUC from 0.901 to 0.606). Furthermore, we show that this privacy guarantee comes at a minimal cost to model performance, with the DP-trained model achieving utility scores comparable to its non-private counterpart, even when trained on 100x less data. Our findings suggest that DP is a practical and effective solution for building private and trustworthy AI-powered IDE features.

</details>


### [68] [A Real-Time Privacy-Preserving Behavior Recognition System via Edge-Cloud Collaboration](https://arxiv.org/abs/2601.22938)
*Huan Song,Shuyu Tian,Junyi Hao,Cheng Yuan,Zhenyu Jia,Jiawei Shao,Xuelong Li*

Main category: cs.CR

TL;DR: 提出基于AI Flow理论框架和边云协同架构的隐私保护感知技术，通过源端脱敏和不可逆特征映射，实现从视频监控到去身份行为感知的突破


<details>
  <summary>Details</summary>
Motivation: 智能感知扩展到高隐私环境（如卫生间、更衣室）时面临隐私-安全悖论：传统RGB监控存在视觉记录和存储的隐私担忧，现有隐私保护方法要么损害语义理解能力，要么无法保证数学不可逆性对抗重建攻击

Method: 基于AI Flow理论框架和边云协同架构，集成源端脱敏与不可逆特征映射。利用信息瓶颈理论，边缘设备通过非线性映射和随机噪声注入将原始图像转换为抽象特征向量，构建单向信息流；云端平台使用多模态家族模型仅基于这些抽象向量进行异常行为检测

Result: 该方法在架构层面从根本上切断了隐私泄露路径，实现了从视频监控到去身份行为感知的突破，为高敏感性公共场所的风险管理提供了鲁棒解决方案

Conclusion: 提出的隐私保护感知技术通过不可逆特征映射和边云协同架构，解决了高隐私环境下的隐私-安全悖论，在保证行为感知能力的同时确保了数学不可逆性，为智能感知在敏感场景的应用提供了可行方案

Abstract: As intelligent sensing expands into high-privacy environments such as restrooms and changing rooms, the field faces a critical privacy-security paradox. Traditional RGB surveillance raises significant concerns regarding visual recording and storage, while existing privacy-preserving methods-ranging from physical desensitization to traditional cryptographic or obfuscation techniques-often compromise semantic understanding capabilities or fail to guarantee mathematical irreversibility against reconstruction attacks. To address these challenges, this study presents a novel privacy-preserving perception technology based on the AI Flow theoretical framework and an edge-cloud collaborative architecture. The proposed methodology integrates source desensitization with irreversible feature mapping. Leveraging Information Bottleneck theory, the edge device performs millisecond-level processing to transform raw imagery into abstract feature vectors via non-linear mapping and stochastic noise injection. This process constructs a unidirectional information flow that strips identity-sensitive attributes, rendering the reconstruction of original images impossible. Subsequently, the cloud platform utilizes multimodal family models to perform joint inference solely on these abstract vectors to detect abnormal behaviors. This approach fundamentally severs the path to privacy leakage at the architectural level, achieving a breakthrough from video surveillance to de-identified behavior perception and offering a robust solution for risk management in high-sensitivity public spaces.

</details>


### [69] [From Data Leak to Secret Misses: The Impact of Data Leakage on Secret Detection Models](https://arxiv.org/abs/2601.22946)
*Farnaz Soltaniani,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究发现机器学习模型在软件安全任务中因数据集重复样本导致数据泄漏，显著夸大了基于AI的秘密检测器的性能评估


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在软件安全任务中广泛使用，但常用的大规模互联网数据集常包含重复或高度相似的样本。当这些样本分布在训练集和测试集中时，可能导致数据泄漏，使模型通过记忆而非泛化来获得虚假的高性能。

Method: 研究分析了广泛使用的硬编码秘密基准数据集中的重复情况，展示了数据泄漏如何影响模型性能评估。通过识别数据集中的重复样本及其在训练集和测试集之间的分布，揭示了数据泄漏对性能指标的夸大效应。

Result: 数据泄漏显著夸大了基于AI的秘密检测器的报告性能，导致对其真实世界有效性的误导性评估。研究发现重复样本在训练集和测试集之间的分布造成了模型性能的虚假提升。

Conclusion: 当前基于AI的秘密检测器性能评估存在严重的数据泄漏问题，重复样本导致模型通过记忆而非学习泛化模式，需要更严谨的数据集构建和评估方法来准确反映模型在真实场景中的有效性。

Abstract: Machine learning models are increasingly used for software security tasks. These models are commonly trained and evaluated on large Internet-derived datasets, which often contain duplicated or highly similar samples. When such samples are split across training and test sets, data leakage may occur, allowing models to memorize patterns instead of learning to generalize. We investigate duplication in a widely used benchmark dataset of hard coded secrets and show how data leakage can substantially inflate the reported performance of AI-based secret detectors, resulting in a misleading picture of their real-world effectiveness.

</details>


### [70] [From Similarity to Vulnerability: Key Collision Attack on LLM Semantic Caching](https://arxiv.org/abs/2601.23088)
*Zhixiang Zhang,Zesen Liu,Yuchong Xie,Quanfeng Huang,Dongdong She*

Main category: cs.CR

TL;DR: 论文将语义缓存键视为模糊哈希，揭示了性能（局部性）与安全（抗碰撞性）之间的根本冲突，提出了首个针对缓存碰撞完整性风险的系统研究，并开发了CacheAttack自动化攻击框架。


<details>
  <summary>Details</summary>
Motivation: 语义缓存已成为扩展LLM应用的关键技术，但现有研究主要关注侧信道和隐私风险，缺乏对缓存碰撞引发的完整性风险的系统研究。作者发现语义缓存键作为模糊哈希，其最大化缓存命中率所需的局部性与密码学雪崩效应所需的抗碰撞性存在根本冲突。

Method: 将语义缓存键概念化为模糊哈希，形式化分析性能与安全之间的权衡。开发CacheAttack自动化框架，用于发起黑盒碰撞攻击，并在安全关键任务和智能体工作流中评估攻击效果，包括LLM响应劫持和恶意行为诱导。

Result: CacheAttack在LLM响应劫持中达到86%的命中率，能够诱导LLM智能体产生恶意行为，且在不同嵌入模型间保持强可迁移性。金融智能体案例研究进一步展示了这些漏洞的实际影响。

Conclusion: 语义缓存天然易受密钥碰撞攻击，揭示了性能与安全之间的根本权衡。论文首次系统研究了缓存碰撞的完整性风险，提出了CacheAttack攻击框架，并讨论了缓解策略。

Abstract: Semantic caching has emerged as a pivotal technique for scaling LLM applications, widely adopted by major providers including AWS and Microsoft. By utilizing semantic embedding vectors as cache keys, this mechanism effectively minimizes latency and redundant computation for semantically similar queries. In this work, we conceptualize semantic cache keys as a form of fuzzy hashes. We demonstrate that the locality required to maximize cache hit rates fundamentally conflicts with the cryptographic avalanche effect necessary for collision resistance. Our conceptual analysis formalizes this inherent trade-off between performance (locality) and security (collision resilience), revealing that semantic caching is naturally vulnerable to key collision attacks.
  While prior research has focused on side-channel and privacy risks, we present the first systematic study of integrity risks arising from cache collisions. We introduce CacheAttack, an automated framework for launching black-box collision attacks. We evaluate CacheAttack in security-critical tasks and agentic workflows. It achieves a hit rate of 86\% in LLM response hijacking and can induce malicious behaviors in LLM agent, while preserving strong transferability across different embedding models. A case study on a financial agent further illustrates the real-world impact of these vulnerabilities. Finally, we discuss mitigation strategies.

</details>


### [71] [WiFiPenTester: Advancing Wireless Ethical Hacking with Governed GenAI](https://arxiv.org/abs/2601.23092)
*Haitham S. Al-Sinani,Chris J. Mitchell*

Main category: cs.CR

TL;DR: WiFiPenTester：一个实验性的、受管控的、可复现的GenAI辅助无线伦理黑客系统，通过集成大语言模型改进无线安全评估的目标选择准确性和效率，同时保持人工监督和伦理保障。


<details>
  <summary>Details</summary>
Motivation: 传统无线伦理黑客依赖人工解释侦察结果和执行复杂命令序列，存在劳动密集、难以扩展、主观判断和人为错误等问题。需要开发更智能、可扩展且受管控的自动化解决方案。

Method: 提出WiFiPenTester系统，将大语言模型集成到无线安全评估的侦察和决策支持阶段，实现智能目标排名、攻击可行性评估和策略推荐。系统包含架构设计、威胁模型、治理机制、提示工程方法，并在多个无线环境中进行实证实验。

Result: 实验结果表明，GenAI辅助提高了目标选择准确性和整体评估效率，同时保持了审计性和伦理保障。系统证明了在无线渗透测试中部署GenAI的可行性和价值。

Conclusion: WiFiPenTester是迈向实用、安全、可扩展的GenAI辅助无线渗透测试的有意义一步，同时强调了在伦理黑客中部署GenAI时必须保持有限自主性、人工监督和严格治理机制的重要性。

Abstract: Wireless ethical hacking relies heavily on skilled practitioners manually interpreting reconnaissance results and executing complex, time-sensitive sequences of commands to identify vulnerable targets, capture authentication handshakes, and assess password resilience; a process that is inherently labour-intensive, difficult to scale, and prone to subjective judgement and human error. To help address these limitations, we propose WiFiPenTester, an experimental, governed, and reproducible system for GenAI-enabled wireless ethical hacking. The system integrates large language models into the reconnaissance and decision-support phases of wireless security assessment, enabling intelligent target ranking, attack feasibility estimation, and strategy recommendation, while preserving strict human-in-the-loop control and budget-aware execution. We describe the system architecture, threat model, governance mechanisms, and prompt-engineering methodology, and empirical experiments conducted across multiple wireless environments. The results demonstrate that GenAI assistance improves target selection accuracy and overall assessment efficiency, while maintaining auditability and ethical safeguards. This indicates that WiFiPenTester is a meaningful step toward practical, safe, and scalable GenAI-assisted wireless penetration testing, while reinforcing the necessity of bounded autonomy, human oversight, and rigorous governance mechanisms when deploying GenAI in ethical hacking.

</details>


### [72] [No More, No Less: Least-Privilege Language Models](https://arxiv.org/abs/2601.23157)
*Paulius Rauba,Dominykas Seputis,Patrikas Vanagas,Mihaela van der Schaar*

Main category: cs.CR

TL;DR: 论文提出最小特权语言模型概念，通过前向传播中的可达内部计算定义特权，并设计Nested Least-Privilege Networks实现部署时的特权控制，挑战了语言模型只能在输出层控制的传统范式。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型部署普遍违反最小特权原则，通过单一API端点服务所有用户和请求，导致不必要的功能暴露。主要障碍在于定义和机制问题：如何在语言模型内部定义"访问"，以及如何在不重新训练或部署多个模型的情况下强制执行最小特权。

Method: 提出最小特权语言模型概念，将特权定义为前向传播中的可达内部计算。将部署时控制形式化为监控器-分配器-执行器三层架构，分离请求时信号、特权分配决策规则和推理时特权选择机制。设计Nested Least-Privilege Networks，这是一种保持形状、按秩索引的干预方法，提供平滑可逆的控制旋钮。

Result: 该控制旋钮能够产生策略可用的特权-效用前沿，并能够在各种策略下实现目标能力的选择性抑制，同时限制跨能力的附带性能下降。

Conclusion: 提出新的部署范式，挑战语言模型只能在输出层控制的前提，通过内部计算的可达性控制实现最小特权原则，为语言模型的安全部署提供了新途径。

Abstract: Least privilege is a core security principle: grant each request only the minimum access needed to achieve its goal. Deployed language models almost never follow it, instead being exposed through a single API endpoint that serves all users and requests. This gap exists not because least privilege would be unhelpful; deployments would benefit greatly from reducing unnecessary capability exposure. The real obstacle is definitional and mechanistic: what does "access" mean inside a language model, and how can we enforce it without retraining or deploying multiple models? We take inspiration from least privilege in computer systems and define a class of models called least-privilege language models, where privilege is reachable internal computation during the forward pass. In this view, lowering privilege literally shrinks the model's accessible function class, as opposed to denying access via learned policies. We formalize deployment-time control as a monitor-allocator-enforcer stack, separating (i) request-time signals, (ii) a decision rule that allocates privilege, and (iii) an inference-time mechanism that selects privilege. We then propose Nested Least-Privilege Networks, a shape-preserving, rank-indexed intervention that provides a smooth, reversible control knob. We show that this knob yields policy-usable privilege-utility frontiers and enables selective suppression of targeted capabilities with limited collateral degradation across various policies. Most importantly, we argue for a new deployment paradigm that challenges the premise that language models can only be controlled at the output level.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [73] [Investigating the Interplay of Parameterization and Optimizer in Gradient-Free Topology Optimization: A Cantilever Beam Case Study](https://arxiv.org/abs/2601.22241)
*Jelle Westra,Iván Olarte Rodríguez,Niki van Stein,Thomas Bäck,Elena Raponi*

Main category: cs.NE

TL;DR: 研究探讨了梯度自由黑盒优化中几何参数化与优化器选择的相互作用，发现在拓扑优化中参数化质量对性能的影响比优化器选择更重要。


<details>
  <summary>Details</summary>
Motivation: 梯度自由黑盒优化在工程设计中广泛应用，为拓扑优化提供了灵活框架，但其成功依赖于两个关键选择：定义搜索空间的几何参数化和探索该空间的优化器。本研究旨在探究这两者之间的相互作用。

Method: 通过悬臂梁的柔度最小化问题（带连通性约束），基准测试了三种几何参数化方法，每种分别与三种代表性BBO算法（差分进化、协方差矩阵自适应进化策略、异方差进化贝叶斯优化）结合，在10D、20D和50D设计空间中进行评估。

Result: 结果显示参数化质量对优化性能的影响比优化器选择更强：结构良好的参数化能够在不同算法中实现稳健且具有竞争力的性能，而较弱的表示则会增加对优化器的依赖性。

Conclusion: 几何参数化在实际基于BBO的拓扑优化中起主导作用，如果不考虑诱导的设计空间，无法公平评估算法性能和选择。

Abstract: Gradient-free black-box optimization (BBO) is widely used in engineering design and provides a flexible framework for topology optimization (TO), enabling the discovery of high-performing structural designs without requiring gradient information from simulations. Yet, its success depends on two key choices: the geometric parameterization defining the search space and the optimizer exploring it.
  This study investigates this interplay through a compliance minimization problem for a cantilever beam subject to a connectivity constraint. We benchmark three geometric parameterizations, each combined with three representative BBO algorithms: differential evolution, covariance matrix adaptation evolution strategy, and heteroscedastic evolutionary Bayesian optimization, across 10D, 20D, and 50D design spaces.
  Results reveal that parameterization quality has a stronger influence on optimization performance than optimizer choice: a well-structured parameterization enables robust and competitive performance across algorithms, whereas weaker representations increase optimizer dependency. Overall, this study highlights the dominant role of geometric parameterization in practical BBO-based TO and shows that algorithm performance and selection cannot be fairly assessed without accounting for the induced design space.

</details>


### [74] [Fairness-Aware Performance Evaluation for Multi-Party Multi-Objective Optimization](https://arxiv.org/abs/2601.22497)
*Zifan Zhao,Peilan Xu,Wenjian Luo*

Main category: cs.NE

TL;DR: 本文针对多参与者多目标优化问题，提出了基于公平性的性能评估框架，通过广义共识解概念和纳什积评估方法，解决传统均值评估不公平和严格共同帕累托最优解限制过严的问题。


<details>
  <summary>Details</summary>
Motivation: 传统多参与者多目标优化评估方法存在两个主要问题：1）基于均值的评估可能不公平，因为它假设对各参与者帕累托前沿的几何近似质量具有同等评估意义；2）现有MPMOP最优解概念局限于严格共同帕累托最优解，这种合作形式过于狭窄。这些限制使得无法判断解集是否反映了平衡的相对收益或有意义的共识。

Method: 从合作博弈论角度，形式化了MPMOP公平评估函数应满足的四个公理。引入让步率向量量化个体决策者可接受的妥协程度，推广了MPMOP最优解的经典定义。将经典性能指标嵌入基于纳什积的评估框架，理论证明该框架满足所有公理。为实证验证，构建了包含共识缺陷谈判结构的基准问题。

Result: 实验结果表明，所提出的评估框架能够以符合共识感知公平考虑的方式区分算法性能。具体而言：当存在严格共同解时，收敛于严格共同解的算法获得更高评估分数；当缺乏严格共同解时，有效覆盖共同可接受区域的算法获得更有利的评估。

Conclusion: 本文提出的公平感知性能评估框架通过广义共识解概念和纳什积评估方法，解决了MPMOP中传统评估方法的局限性，能够更公平地评估算法性能，反映参与者间的平衡相对收益和有意义共识。

Abstract: In multiparty multiobjective optimization problems, solution sets are usually evaluated using classical performance metrics, aggregated across DMs. However, such mean-based evaluations may be unfair by favoring certain parties, as they assume identical geometric approximation quality to each party's PF carries comparable evaluative significance. Moreover, prevailing notions of MPMOP optimal solutions are restricted to strictly common Pareto optimal solutions, representing a narrow form of cooperation in multiparty decision making scenarios. These limitations obscure whether a solution set reflects balanced relative gains or meaningful consensus among heterogeneous DMs. To address these issues, this paper develops a fairness-aware performance evaluation framework grounded in a generalized notion of consensus solutions. From a cooperative game-theoretic perspective, we formalize four axioms that a fairness-aware evaluation function for MPMOPs should satisfy. By introducing a concession rate vector to quantify acceptable compromises by individual DMs, we generalize the classical definition of MPMOP optimal solutions and embed classical performance metrics into a Nash-product-based evaluation framework, which is theoretically shown to satisfy all axioms. To support empirical validation, we further construct benchmark problems that extend existing MPMOP suites by incorporating consensus-deficient negotiation structures. Experimental results demonstrate that the proposed evaluation framework is able to distinguish algorithmic performance in a manner consistent with consensus-aware fairness considerations. Specifically, algorithms converging toward strictly common solutions are assigned higher evaluation scores when such solutions exist, whereas in the absence of strictly common solutions, algorithms that effectively cover the commonly acceptable region are more favorably evaluated.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [75] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: 该研究系统评估了多模态情感识别中复杂注意力机制在小数据集上的效果，发现简单领域适配方法优于复杂架构。


<details>
  <summary>Details</summary>
Motivation: 探究复杂注意力机制在小规模多模态情感识别数据集（EAV）上的实际效果，验证是否值得为小数据集设计复杂模型。

Method: 采用三类模型：基线Transformer（M1）、新型分解注意力机制（M2）、改进CNN基线（M3）。通过添加delta MFCCs、频域特征等简单领域适配方法进行对比。

Result: 复杂注意力机制（M2）在小数据集上表现不佳，比基线低5-13个百分点。简单领域适配方法效果显著：音频CNN添加delta MFCCs提升3.66pp至65.56%；EEG频域特征提升7.62pp至67.62%；视觉Transformer通过领域特定预训练达到75.30%，超过原论文ViViT结果。

Conclusion: 对于小规模情感识别任务，领域知识和适当实现比架构复杂性更重要，简单领域适配方法优于复杂注意力机制。

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [76] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 提出了一种结合多任务学习和量子卷积操作的混合模型，用于地球观测数据分类，探索量子机器学习在遥感领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 地球观测已进入大数据时代，传统深度学习方法处理海量复杂数据面临计算瓶颈。量子机器学习作为潜在解决方案，尽管当前量子设备存在限制，但仍值得探索其在地球观测数据分类中的优势。

Method: 提出混合模型：1) 结合多任务学习辅助高效数据编码；2) 采用位置权重模块配合量子卷积操作提取有效分类特征。模型在多个地球观测基准数据集上进行验证。

Result: 模型在多个地球观测基准数据集上验证有效。通过实验探索了模型的泛化能力，并分析了其优势来源，突出了量子机器学习在地球观测数据分析中的潜力。

Conclusion: 该研究展示了量子机器学习在地球观测数据分类中的应用潜力，提出的混合模型在现有量子设备限制下仍能有效工作，为未来量子计算在地球观测领域的应用提供了有益探索。

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [77] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 开发首个临床EEG到语言的基础模型CELM，用于从长时程EEG记录生成临床报告，在多项生成指标上实现显著提升


<details>
  <summary>Details</summary>
Motivation: 从长时程EEG记录生成总结异常模式、诊断发现和临床解释的报告目前仍然劳动密集，需要自动化解决方案

Method: 构建大规模临床EEG数据集（9,922份报告，约11,000小时EEG记录，9,048名患者），开发CELM模型整合预训练的EEG基础模型和语言模型，支持多尺度端到端报告生成

Result: 在患者病史监督下，标准生成指标（如ROUGE-1和METEOR）实现70%-95%的相对提升（从0.2-0.3到0.4-0.6）；零样本设置下生成分数达0.43-0.52，基线为0.17-0.26

Conclusion: CELM是首个能够总结长时程可变长度EEG记录并执行多尺度临床报告生成的临床EEG到语言基础模型，通过整合预训练EEG基础模型和语言模型实现可扩展多模态学习

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [78] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: FedAdaVR是一种解决联邦学习中客户端参与稀疏性问题的算法，通过自适应优化器和方差缩减技术，利用存储的客户端更新来模拟缺席客户端的参与，显著减少部分客户端参与误差。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临异构性挑战，特别是客户端参与稀疏性问题（部分客户端参与误差），这是当前文献中未充分解决的最普遍问题。现有方法在处理客户端参与不均衡时效果有限。

Method: 提出FedAdaVR算法，结合自适应优化器和方差缩减技术，存储客户端最近更新，即使客户端缺席当前训练轮次也能利用其历史信息。进一步提出FedAdaVR-Quant，将客户端更新量化为压缩形式，显著减少内存需求（50%、75%、87.5%）。

Result: 在非凸条件下分析了FedAdaVR的收敛性，证明该算法能消除部分客户端参与误差。在多个数据集（IID和非IID设置）上的实验表明，FedAdaVR始终优于最先进的基线方法。

Conclusion: FedAdaVR有效解决了联邦学习中客户端参与稀疏性问题，通过自适应优化和方差缩减技术显著提升性能，而FedAdaVR-Quant在保持同等模型性能的同时大幅降低内存需求。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [79] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 提出基于可穿戴惯性传感器和机器学习的低成本自动化人体活动识别框架，用于远程医疗和老年人辅助，其中支持张量机（STM）在活动分类中显著优于传统分类器


<details>
  <summary>Details</summary>
Motivation: 医疗基础设施有限导致老年人和弱势患者依赖家庭护理，但往往存在忽视和治疗性锻炼（如瑜伽或物理治疗）依从性差的问题，需要低成本自动化解决方案

Method: 使用加速度计和陀螺仪测量收集活动数据（行走、上楼梯、下楼梯、坐、站、躺），评估四种经典分类器（逻辑回归、随机森林、SVM、k-NN）并与提出的支持张量机（STM）进行比较，STM利用张量表示保留时空运动动态

Result: SVM准确率为93.33%，逻辑回归、随机森林和k-NN为91.11%，而STM显著优于这些模型，测试准确率达到96.67%，交叉验证准确率最高达98.50%

Conclusion: 提出的框架在远程医疗、老年人辅助、儿童活动监测、瑜伽反馈和智能家居健康方面具有强大潜力，为低资源和农村医疗环境提供了可扩展的解决方案

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [80] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DAJ：基于推理的LLM法官，通过双层数据重加权学习框架训练，使用可验证奖励，自动强调困难问题、分布内样本和轨迹对齐数据，在代码生成测试时缩放中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 代码生成中的测试时缩放通常依赖Best-of-N选择，但训练可靠的LLM法官面临挑战：分布偏移严重，包括简单与困难问题不平衡、训练任务与评估基准不匹配、以及由廉价模型生成的训练数据与推理时模型行为不匹配导致的轨迹失配。

Method: 提出DAJ，一种基于推理的LLM法官，在双层数据重加权学习框架下使用可验证奖励进行训练。该框架学习数据重要性权重（域级或实例级），以优化与目标基准对齐的保留元集上的泛化性能。自动强调困难问题、分布内样本和轨迹对齐数据，无需依赖手工启发式方法。

Result: 在LiveCodeBench和BigCodeBench上实现最先进性能，优于强大的测试时缩放基线以及领先的专有模型。

Conclusion: 这是数据重加权在LLM-as-a-Judge训练中的首次应用，通过自动数据重要性加权有效解决了分布偏移问题，为代码生成测试时缩放提供了更可靠的解决方案。

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [81] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: AgentScore使用LLM生成候选规则，通过数据验证和选择循环，在临床预测任务中生成可部署的单元加权检查表，性能优于现有评分生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型虽然性能强大，但常因与临床工作流程约束（如可记忆性、可审计性、床边执行）不匹配而无法转化为常规临床使用。可部署的指南通常采用单元加权临床检查表形式，但学习此类评分需要搜索指数级大的离散规则空间。

Method: AgentScore通过使用LLM提出候选规则，结合确定性的、基于数据的验证和选择循环，在离散规则空间中进行语义引导优化，确保统计有效性和可部署性约束。

Result: 在八个临床预测任务中，AgentScore优于现有的评分生成方法，并在更强的结构约束下实现了与更灵活的可解释模型相当的AUC。在两个外部验证任务中，AgentScore比已建立的基于指南的评分具有更高的区分度。

Conclusion: AgentScore通过将LLM的语义引导与数据驱动的验证相结合，成功生成了符合临床工作流程约束的可部署评分系统，弥合了机器学习模型与临床指南实施之间的差距。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [82] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: FunPRM是一种针对代码生成任务的测试时扩展方法，通过函数化分解和元学习奖励校正机制，显著提升大语言模型在复杂编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型（PRM）在代码生成任务中效果不佳，主要原因有两个：1）代码缺乏有意义的步骤分解；2）蒙特卡洛估计的部分解决方案正确性分数（奖励）存在噪声。需要一种专门针对代码生成特点的PRM方法。

Method: FunPRM采用两种关键技术：1）函数化分解：提示LLM生成模块化代码，将函数作为PRM推理步骤；2）元学习奖励校正：利用单元测试评估系统获得的干净最终解决方案奖励来净化噪声的部分解决方案奖励。

Result: 在LiveCodeBench和BigCodeBench上的实验表明，FunPRM在五个基础LLM上始终优于现有的测试时扩展方法，与O4-mini结合时在LiveCodeBench上实现了最先进的性能。此外，FunPRM生成的代码对开发者来说更具可读性和可重用性。

Conclusion: FunPRM通过将代码生成任务分解为函数化步骤并利用元学习校正奖励信号，有效解决了现有PRM在代码生成中的局限性，显著提升了LLM在复杂编程任务上的性能，同时改善了生成代码的质量。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [83] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 通过引入批处理采样的非学习查询和值偏置来打破注意力机制中的旋转对称性，改善简单优化器性能并增强注意力头的可解释性


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制中存在不影响模型激活或输出的冗余旋转自由度，这些自由度在计算中传递但未被有效利用

Method: 提出对称性打破协议：通过批处理采样的非学习查询和值偏置在旋转空间中插入首选方向，修改注意力机制

Result: 1) 显著改善简单内存高效优化器性能，缩小与复杂自适应方法的差距；2) 使冗余旋转自由度具有可解释性，能选择性放大语义有意义的token类别

Conclusion: 最小化、有原则的架构变化可以同时提高性能和可解释性，为注意力机制设计提供了新思路

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [84] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 将生存分析重新表述为一系列二分类问题，通过离散化事件时间，使表格基础模型能够通过上下文学习进行生存分析，无需显式训练。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在分类和回归任务上取得了显著成功，但由于存在右删失（数据观测可能在事件发生前结束），将其适应于建模生存分析中的时间到事件结果具有挑战性。

Method: 开发了一个基于分类的框架，通过离散化事件时间，将静态和动态生存分析重新表述为一系列二分类问题。删失观测被自然地处理为在某些时间点缺少标签的示例。这种分类表述使现有的表格基础模型能够通过上下文学习进行生存分析，无需显式训练。

Result: 在标准删失假设下，最小化二分类损失能够随着训练集规模的增加恢复真实的生存概率。在53个真实世界数据集上的评估表明，采用这种分类表述的现成表格基础模型在多个生存指标上平均优于经典和深度学习基线。

Conclusion: 该研究提出了一种有效的分类框架，使表格基础模型能够成功应用于生存分析任务，克服了右删失带来的挑战，并在多个真实数据集上展现了优越性能。

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [85] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: SPECIAL算法在联邦域增量学习(FDIL)中通过添加服务器端锚点实现无记忆的向后知识迁移和高效收敛


<details>
  <summary>Details</summary>
Motivation: 现实联邦系统中数据分布动态漂移且隐私限制禁止原始数据共享，现有方法缺乏向后知识迁移的理论保证和跨任务的部分参与收敛率分析

Method: SPECIAL算法在FedAvg基础上添加服务器端锚点：每轮中服务器通过轻量级近端项将采样参与客户端的更新推向先前全局模型，无需重放缓冲区或合成数据

Result: 理论证明SPECIAL具有向后知识迁移保证，能控制先前任务损失增加；获得首个FDIL部分参与的非凸收敛率O((E/NT)^(1/2))，匹配单任务FedAvg

Conclusion: SPECIAL是简单、无记忆的FDIL算法，通过服务器锚点控制累积漂移，提供理论保证并保持通信和模型大小不变，实验验证其有效性

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [86] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: SurrogateSHAP：一种免重训练的Shapley值近似框架，用于评估文本到图像扩散模型中数据贡献者的价值，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型在创意工作流中的广泛应用，需要公平评估数据贡献者价值的框架。传统Shapley值方法面临双重计算瓶颈：1）为每个数据子集重新训练模型的成本过高；2）需要评估组合数量的子集来计算边际贡献

Method: 提出SurrogateSHAP框架：1）使用预训练模型进行推理来近似昂贵的重新训练过程；2）采用梯度提升树近似效用函数，并从树模型中解析推导Shapley值

Result: 在三个多样化归因任务中评估：1）DDPM-CFG在CIFAR-20上的图像质量；2）Stable Diffusion在后印象派艺术品上的美学评估；3）FLUX.1在时尚产品数据上的产品多样性。SurrogateSHAP在所有设置中都优于先前方法，同时显著降低计算开销，能一致识别多个效用指标中的有影响力贡献者

Conclusion: SurrogateSHAP为公平补偿数据贡献者和可持续数据市场提供了可扩展解决方案，并能有效定位临床图像中虚假相关的数据源，为审计安全关键生成模型提供了可行路径

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [87] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: Riemannian Lyapunov Optimizers (RLOs) 是一种基于控制理论框架的优化算法家族，将经典优化器统一在几何框架下，通过识别不变流形将训练动态分为两个阶段，并提供系统化的优化器设计方法。


<details>
  <summary>Details</summary>
Motivation: 当前优化器设计多为启发式改进，缺乏统一的理论框架。本文旨在建立控制理论与机器学习优化之间的桥梁，提供系统化的优化器设计方法，而非零散的启发式改进。

Method: 将优化问题重新解释为黎曼参数流形上的扩展状态离散时间受控动力系统。核心是识别正常吸引不变流形(NAIM)，将训练动态组织为两个阶段：速度状态快速对齐到目标图，然后在其中受控演化。通过构建严格的Lyapunov函数来证明收敛性，并开发"优化器生成器"来系统设计RLOs。

Result: RLOs不仅能够恢复经典优化算法，还能通过控制理论原则设计新的优化器。在大型基准测试中表现出最先进的性能，并通过几何诊断验证了理论框架的有效性。

Conclusion: RLOs在控制理论与现代机器学习优化之间建立了桥梁，提供了统一的语言和系统化的工具包，用于设计稳定、有效的优化器，为优化器设计提供了理论基础而非启发式方法。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [88] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 论文提出一个架构无关的框架，揭示模型合并的成功因素不仅取决于模型本身，还取决于合并方法和任务特性，通过线性优化发现子空间重叠和梯度对齐是兼容性的基础前提。


<details>
  <summary>Details</summary>
Motivation: 当前对模型合并的研究通常将可合并性视为模型的固有属性，但缺乏对成功因素的系统理解。作者旨在揭示模型合并成功的关键因素，特别是合并方法和任务特性对可合并性的影响。

Method: 提出一个架构无关的框架，使用线性优化方法分析一组可解释的成对度量（如梯度L2距离），在四种不同的合并方法上评估这些度量与合并后性能的相关性。

Result: 发现不同合并方法的成功驱动因素存在显著差异（46.7%的度量重叠；55.3%的符号一致性），揭示了方法特定的"指纹"。但子空间重叠和梯度对齐度量始终作为兼容性的基础前提出现，具有方法无关的重要性。

Conclusion: 模型可合并性不仅取决于模型本身，还受合并方法和任务特性的影响。子空间重叠和梯度对齐是兼容性的基础前提，这为理解可合并性提供了诊断基础，并激励未来设计能显式促进这些属性的微调策略。

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [89] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: 提出CP4Gen方法，为条件生成模型提供系统化的共形预测框架，通过聚类密度估计构建更稳健、可解释且结构简单的预测集


<details>
  <summary>Details</summary>
Motivation: 条件生成模型缺乏校准的不确定性估计，这在高风险应用中会削弱对单个输出的信任，需要一种系统化的不确定性量化方法

Method: 提出CP4Gen方法，采用基于聚类的密度估计技术，利用模型生成的样本构建预测集，相比现有方法对异常值更不敏感、更可解释、结构复杂度更低

Result: 在合成数据集和真实世界应用（包括气候模拟任务）上的广泛实验表明，CP4Gen在预测集体积和结构简洁性方面始终优于现有方法

Conclusion: CP4Gen为条件生成模型的不确定性估计提供了强大工具，特别适用于需要严格且可解释预测集的场景

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [90] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL：一种结合DAG账本、侧链和零知识证明的安全去中心化联邦学习框架，通过隐私保护的模型验证实现快速收敛和高精度


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时实现协作模型训练，但现有集中式和去中心化方法在可扩展性、安全性和更新验证方面面临挑战，需要一种既能保护隐私又能高效验证的安全去中心化解决方案

Method: 提出ZK-HybridFL框架，集成有向无环图（DAG）账本、专用侧链和零知识证明（ZKPs），使用事件驱动智能合约和预言机辅助侧链验证本地模型更新而不暴露敏感数据，内置挑战机制检测对抗行为

Result: 在图像分类和语言建模任务实验中，相比Blade-FL和ChainFL，ZK-HybridFL实现更快收敛、更高准确率、更低困惑度和更低延迟；能抵抗大量对抗节点和空闲节点，支持亚秒级链上验证和高效gas使用，防止无效更新和孤儿式攻击

Conclusion: ZK-HybridFL为跨不同环境的去中心化联邦学习提供了一个可扩展且安全的解决方案，通过隐私保护的验证机制解决了现有联邦学习框架的关键挑战

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [91] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 提出贝叶斯工作流生成(BWG)框架，将工作流生成建模为贝叶斯推断问题，并实现为无需训练的BayesFlow算法，在六个基准数据集上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有自动工作流生成方法大多将其视为优化问题，缺乏理论基础。作者希望建立一个更理论化的框架，将工作流生成重新定义为贝叶斯推断问题

Method: 提出贝叶斯工作流生成(BWG)框架，通过并行前瞻展开进行重要性加权，结合序列内循环优化器进行池级改进。实现为无需训练的BayesFlow算法，逐步构建工作流

Result: 在六个基准数据集上，BayesFlow比最先进的工作流生成基线准确率提升高达9个百分点，比零样本提示提升高达65个百分点

Conclusion: BWG为基于搜索的工作流设计提供了理论化的升级方案，证明了将工作流生成建模为贝叶斯推断问题的有效性

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [92] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 提出针对回归模型的最优隐蔽攻击方法，能绕过现有防御；开发评估攻击效果与隐蔽性权衡的新方法；提出新防御BayesClean对抗隐蔽攻击


<details>
  <summary>Details</summary>
Motivation: 回归模型在工业、工程和科学领域广泛应用，但其对投毒攻击的鲁棒性研究不足。现有研究常采用不现实的威胁模型，导致实际应用价值有限。

Method: 1) 提出考虑不同可检测程度的最优隐蔽攻击公式；2) 开发基于目标归一化的新方法，评估攻击效果与隐蔽性之间的权衡；3) 设计新型防御BayesClean对抗隐蔽攻击。

Result: 提出的最优隐蔽攻击能绕过最先进的防御系统；BayesClean在攻击具有隐蔽性且投毒点数量显著时，优于之前的防御方法。

Conclusion: 该研究填补了回归模型对抗投毒攻击的空白，提出了更实用的攻击和防御方法，为实际应用中的模型安全提供了新思路。

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [93] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: SCALAR是一个用于评估材料基础模型在几何尺度泛化能力的基准测试，包含三个任务：晶体结构到性质预测、基于物理推理的思维链变体、以及根据目标性质反向检索晶体结构。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在材料科学推理中的应用日益增多，但它们在物理结构化分布偏移下的行为仍不清楚。需要评估几何尺度泛化能力及其与结构幻觉、一致性和推理能力的关系。

Method: 提出SCALAR基准测试，使用从DFT验证的晶胞通过超胞扩展和几何截断得到的纳米颗粒结构，涵盖从几个原子到超过18,000个原子的长度尺度。定义三个任务：CIF到性质预测、基于物理推理的思维链变体、以及反向检索任务。通过结构化指标评估模型输出。

Result: 实验显示，在不同基础模型中，显式推理会导致显著的模型依赖性偏移，通常减少幻觉和误差，但经常破坏一致性或有效性。几何尺度泛化能力不能仅从准确性推断。

Conclusion: SCALAR基准测试揭示了材料基础模型在几何尺度泛化方面的挑战，表明需要更全面的评估指标来捕捉模型在物理结构化分布偏移下的行为。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [94] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 论文揭示静态黑盒评估的局限性：即使通过所有标准对齐测试的大语言模型，在单次良性更新后也可能严重失准，且这种隐藏的对抗行为随模型规模增大而增强。


<details>
  <summary>Details</summary>
Motivation: 现有对齐研究通常假设初始模型基于静态黑盒评估是对齐的，但实践中大语言模型频繁更新，而静态对齐无法保证更新后的对齐性。需要揭示静态评估的根本局限性，并强调更新后鲁棒对齐评估的紧迫性。

Method: 1. 理论分析：形式化静态和更新后对齐，证明由于过参数化，静态对齐无法保证任何更新数据集后的对齐性；证明静态黑盒探测无法区分真正更新后鲁棒的模型与隐藏任意量对抗行为的模型。2. 实证验证：在三个核心对齐领域（隐私、越狱安全、行为诚实）的大语言模型中进行实验，展示通过所有标准黑盒测试的模型在单次良性更新后严重失准的现象。3. 规模分析：验证隐藏对抗行为的能力随模型规模增加而增强。

Result: 1. 理论证明静态黑盒评估存在根本局限性；2. 实证发现存在通过所有标准对齐测试但在单次良性更新后严重失准的大语言模型；3. 验证模型隐藏对抗行为的能力随参数数量增加而增强，支持理论预测；4. 在隐私、越狱安全和行为诚实三个领域均观察到这一现象。

Conclusion: 静态评估协议不足以评估大语言模型的对齐性，因为即使通过所有静态测试的模型也可能在更新后严重失准。研究结果强调了开发更新后鲁棒对齐评估方法的紧迫性，并指出模型规模越大，隐藏对抗行为的能力越强，更新后失准风险越高。

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [95] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: PA-GP-UCB是一种贝叶斯优化算法，利用昂贵真实评估和廉价预测模型，结合离线数据提高样本效率，通过控制变量估计器校正预测偏差，在合成和真实基准测试中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界优化问题通常涉及昂贵的真实评估（如人工评估、物理实验）和廉价的低保真度预测（如机器学习模型、仿真）。同时，大量离线数据（如历史实验和预测）可用于预训练预测模型并提供信息先验。需要开发能够同时利用两种评估源和离线数据来提高真实评估样本效率的算法。

Method: 提出预测增强高斯过程上置信界（PA-GP-UCB）算法，使用从联合高斯过程后验导出的控制变量估计器来校正预测偏差并减少不确定性。算法结合昂贵真实评估和廉价预测模型，利用离线数据预训练模型并建立信息先验。

Result: 理论证明PA-GP-UCB保持了GP-UCB的标准遗憾率，同时获得了严格更小的主导常数，该常数由预测质量和离线数据覆盖范围明确控制。实证表明，在合成基准测试和基于人类行为数据的真实世界假设评估任务中，PA-GP-UCB比Vanilla GP-UCB和朴素预测增强GP-UCB基线收敛更快。

Conclusion: PA-GP-UCB为昂贵反馈下的假设生成提供了一个通用且样本高效的框架，通过有效整合昂贵真实评估、廉价预测模型和离线数据，显著提高了优化过程的样本效率。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [96] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: PoSafeNet：一种可微分神经安全层，通过部分有序集建模异构安全约束，实现自适应安全执行选择，提升学习控制器的可行性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有安全学习方法通常统一或按固定优先级顺序强制执行多个安全约束，导致不可行性和脆弱行为。实践中安全需求具有异构性且仅存在部分优先级关系，需要更灵活的安全约束建模方法。

Method: 将安全约束形式化为部分有序集（poset），提出PoSafeNet可微分神经安全层，通过序列闭式投影在poset一致约束排序下强制执行安全，支持自适应选择或混合有效安全执行，同时通过构造保持优先级语义。

Result: 在多障碍物导航、受限机器人操作和基于视觉的自动驾驶实验中，相比非结构化和基于可微分二次规划的安全层，PoSafeNet在可行性、鲁棒性和可扩展性方面表现更优。

Conclusion: PoSafeNet通过部分有序集建模异构安全约束，提供了一种灵活且可扩展的安全学习方法，能够适应复杂安全需求，为安全关键机器人系统中的学习控制器部署提供了有效解决方案。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [97] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出了首个用于大语言模型路由的联邦学习框架，使客户端能够从本地离线查询-模型评估数据中学习共享路由策略，解决了数据碎片化和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为远程服务被边缘和企业客户端访问，不同模型在能力和价格上差异很大，需要路由查询以平衡质量和推理成本。现有路由方法假设可以访问集中化的查询-模型评估数据，但这些数据通常分散在客户端之间且具有隐私敏感性，无法集中化。同时，每个客户端单独训练路由策略效果有限，因为本地评估数据有限且覆盖范围受限。

Method: 提出了首个联邦学习框架用于LLM路由，支持参数化的多层感知机路由器和非参数的K-means路由器，能够处理异构客户端查询分布和非均匀模型覆盖。通过联邦协作使客户端能够从本地离线查询-模型评估数据中学习共享路由策略。

Result: 在两个基准测试中，联邦协作相比客户端本地路由器提高了准确率-成本前沿，既通过增加有效模型覆盖，也通过更好的查询泛化能力。理论结果也验证了联邦训练能够减少路由次优性。

Conclusion: 提出的联邦LLM路由框架有效解决了数据碎片化和隐私问题，通过联邦协作显著提升了路由性能，为分布式环境下的模型路由提供了可行的解决方案。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [98] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 该论文提出了一种用于离线强化学习的风格条件策略学习方法，通过子轨迹标注函数提供显式风格监督，解决了风格与任务性能之间的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，风格条件策略学习面临两个主要挑战：分布偏移以及风格与奖励之间的固有冲突。现有方法虽然引入了多种风格定义，但往往无法有效协调这两个目标。

Method: 首先提出了行为风格的统一定义，并基于此构建了Style-Conditioned Implicit Q-Learning (SCIQL)框架。该框架利用离线目标条件RL技术（如后见之明重标注和价值学习），并结合新的门控优势加权回归机制，以高效优化任务性能同时保持风格对齐。

Result: 实验表明，SCIQL在任务性能和风格对齐两个目标上都优于先前的离线方法。

Conclusion: 该研究通过统一的风格定义和SCIQL框架，成功解决了离线强化学习中风格条件策略学习的关键挑战，实现了风格与任务性能的有效平衡。

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [99] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 论文研究了在用户级差分隐私下的连续均值估计问题，提出了基于矩阵分解机制的新方法，相比纯差分隐私获得了更低的均方误差。


<details>
  <summary>Details</summary>
Motivation: 现有连续均值估计研究主要关注纯差分隐私，但这种方法会产生过度噪声，限制了实际应用。需要研究在近似差分隐私下的解决方案，以在保护用户隐私的同时获得更准确的估计。

Method: 采用矩阵分解机制的最新进展，提出了一种专门针对均值估计的新型分解方法。该方法既高效又准确，特别适用于用户级差分隐私下的连续均值估计场景。

Result: 新方法在用户级差分隐私下的连续均值估计中实现了渐近更低的均方误差界限，相比纯差分隐私方法显著提高了估计精度。

Conclusion: 通过采用近似差分隐私和专门设计的矩阵分解机制，可以显著改善连续均值估计的性能，为实际应用提供了更实用的隐私保护解决方案。

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [100] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: SCOPE是一个可扩展可控的路由框架，通过预测模型成本和性能实现智能路由，能够适应新模型和动态预算约束，在准确性和成本之间灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 现有路由方法通常将模型选择视为固定的小集合选择问题，难以适应新模型或变化的预算约束，需要更灵活、可扩展的路由解决方案。

Method: 提出SCOPE框架，使用强化学习训练，通过检索类似问题上的模型行为进行基于推理的预测，而非依赖固定模型名称，能够预测模型的准确性和成本，将路由转化为动态决策问题。

Result: SCOPE在性能优先时可提升准确性达25.7%，在效率优先时可降低成本达95.1%，既能节省成本又能提升性能，灵活适应用户需求。

Conclusion: SCOPE超越了传统路由方法，通过预测模型成本和性能实现智能路由决策，能够适应新模型和动态约束，在准确性和成本之间提供灵活权衡，是一个多功能的路由框架。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [101] [Label-Efficient Monitoring of Classification Models via Stratified Importance Sampling](https://arxiv.org/abs/2601.22326)
*Lupo Marsigli,Angel Lopez de Haro*

Main category: cs.LG

TL;DR: 提出基于分层重要性抽样(SIS)的通用框架，用于解决生产环境中分类模型监控面临的标签预算有限、批量标签获取和极低错误率等挑战。


<details>
  <summary>Details</summary>
Motivation: 生产环境中分类模型监控面临三大挑战：严格的标签预算限制、一次性批量标签获取需求、以及极低的错误率。现有方法难以同时应对这些约束条件。

Method: 提出基于分层重要性抽样(Stratified Importance Sampling, SIS)的通用监控框架。该方法结合重要性抽样和分层随机抽样的优势，即使使用非最优的提议分布和分层策略，也能在温和条件下获得无偏估计量。

Result: 理论分析表明，在温和条件下，SIS能产生无偏估计量，并在有限样本中相比重要性抽样(IS)和分层随机抽样(SRS)获得严格均方误差(MSE)改进。实验证明，在固定标签预算下，SIS在二元和多分类任务中均能实现一致的效率提升。

Conclusion: SIS为后部署模型监控提供了一种原则性、标签高效且操作轻量化的方法学，能够直接应对生产环境中的实际约束条件。

Abstract: Monitoring the performance of classification models in production is critical yet challenging due to strict labeling budgets, one-shot batch acquisition of labels and extremely low error rates. We propose a general framework based on Stratified Importance Sampling (SIS) that directly addresses these constraints in model monitoring. While SIS has previously been applied in specialized domains, our theoretical analysis establishes its broad applicability to the monitoring of classification models. Under mild conditions, SIS yields unbiased estimators with strict finite-sample mean squared error (MSE) improvements over both importance sampling (IS) and stratified random sampling (SRS). The framework does not rely on optimally defined proposal distributions or strata: even with noisy proxies and sub-optimal stratification, SIS can improve estimator efficiency compared to IS or SRS individually, though extreme proposal mismatch may limit these gains. Experiments across binary and multiclass tasks demonstrate consistent efficiency improvements under fixed label budgets, underscoring SIS as a principled, label-efficient, and operationally lightweight methodology for post-deployment model monitoring.

</details>


### [102] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: MAAT框架通过知识引导的核状态重构，从噪声、不完整观测数据中恢复物理系统控制方程，结合结构先验提升符号发现效果


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声、部分观测下失效，或依赖黑盒潜在动力学模型而缺乏可解释性，需要能处理实际观测限制的符号发现框架

Method: 基于再生核希尔伯特空间的状态重构，直接融入结构先验（非负性、守恒律、领域特定观测模型），处理异构采样和测量粒度，提供解析时间导数

Result: 在12个科学基准测试和多种噪声机制下，MAAT显著降低状态估计MSE，为下游符号回归提供更准确的轨迹和导数

Conclusion: MAAT为碎片化传感器数据与符号回归之间提供了原则性接口，通过知识引导的状态重构提升物理系统控制方程的发现能力

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [103] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS是一种用于Cell Painting数据批效应校正的可扩展方法，通过构建平滑的亲和矩阵实现跨批次样本对齐，具有近线性时间复杂度和理论保证。


<details>
  <summary>Details</summary>
Motivation: Cell Painting作为一种高通量成像技术，在规模化应用中受到实验室、仪器和协议差异导致的批效应严重影响，这些批效应会掩盖生物信号，需要可扩展的批校正方法。

Method: BALANS通过两个核心思想构建稀疏亲和矩阵：1) 使用批次感知的局部尺度计算高斯核亲和度；2) 采用自适应采样策略优先选择邻居覆盖度低的行，仅保留每行最强亲和度，实现稀疏但信息丰富的近似。

Result: BALANS在样本复杂度上达到最优阶，提供近似保证，运行时间接近线性。在真实Cell Painting数据集和合成基准测试中，BALANS能够扩展到大规模数据集，在保持校正质量的同时显著提升运行效率。

Conclusion: BALANS是一种高效、可扩展的批效应校正方法，特别适用于大规模Cell Painting数据分析，能够在保持校正质量的同时显著降低计算成本，为药物发现中的高通量形态学分析提供实用工具。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [104] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种新的差分隐私SGD噪声相关策略，仅将噪声与前一次迭代相关并部分抵消，使用伪随机噪声生成器消除存储需求，在无额外内存开销下提升精度


<details>
  <summary>Details</summary>
Motivation: 现有DP-SGD扩展方法（如矩阵分解机制）通过跨多个训练迭代引入相关噪声来提高精度，但需要存储先前添加的噪声向量，导致显著的内存开销。需要一种既能保持精度优势又无需额外存储开销的噪声相关策略。

Method: 提出新的噪声相关策略：1）仅将噪声与紧邻的前一次迭代相关；2）控制性地抵消部分噪声；3）使用伪随机噪声生成器重新生成噪声，无需存储历史噪声向量。该方法在标准DP-SGD基础上无需额外内存，计算开销最小。

Result: 方法在保持与标准DP-SGD相同内存需求的同时，实现了比DP-SGD更高的精度。计算开销极小，通过实验验证了精度改进。

Conclusion: 提出了一种内存高效的差分隐私SGD噪声相关方法，通过仅与前次迭代相关并部分抵消噪声，结合伪随机噪声生成，在无额外内存开销下显著提升模型训练精度，为实际部署提供了实用解决方案。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [105] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 本文提出了针对偏好贝叶斯优化的精确知识梯度方法，解决了传统知识梯度在偏好设置中计算非高斯后验的挑战。


<details>
  <summary>Details</summary>
Motivation: 知识梯度是贝叶斯优化中流行的采集函数，但许多实际场景只允许成对比较查询（偏好BO），而将知识梯度扩展到偏好BO面临计算挑战，核心问题在于前瞻步骤需要计算非高斯后验，之前被认为难以处理。

Method: 推导出偏好贝叶斯优化的精确解析知识梯度，解决了非高斯后验计算问题，实现了在偏好设置下的有效知识梯度计算。

Result: 精确知识梯度在一系列基准问题上表现优异，通常优于现有的采集函数，同时通过案例研究揭示了知识梯度在某些场景下的局限性。

Conclusion: 成功解决了偏好贝叶斯优化中知识梯度的计算难题，提供了精确解析解，显著提升了偏好BO的性能，但也指出了该方法在某些情况下的局限性。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [106] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 评估语言模型在有限交互预算下探索交互环境的能力，发现当前模型存在系统性探索不足和次优解问题，性能常低于简单启发式基线，且随预算增加提升有限。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在交互环境中的探索能力，特别是在有限交互预算下的表现。当前语言模型在需要主动探索的任务中表现如何尚不清楚，需要系统评估其探索效率和策略优化能力。

Method: 引入三个参数化任务，控制探索难度，涵盖连续和离散环境。评估最先进的语言模型在这些任务上的表现，并与简单探索-利用启发式基线比较。研究两种轻量级干预措施：将固定预算分割为并行执行，以及定期总结交互历史。

Result: 发现语言模型存在系统性探索不足和次优解问题，性能常显著低于简单启发式基线，且随预算增加提升较弱。令人惊讶的是，将预算分割为并行执行能改善性能（尽管理论上无增益），而定期总结交互历史能保留关键发现并进一步改善探索。

Conclusion: 当前语言模型在交互环境探索方面存在显著局限性，需要改进探索策略。轻量级干预措施如并行执行和定期总结能有效提升性能，为改善语言模型的探索能力提供了实用方向。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [107] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: MixQuant：一种基于块旋转感知的后训练量化框架，通过排列重新分配激活质量以优化异常值抑制，在保持推理效率的同时显著提升低精度量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法采用块旋转来扩散异常值，但块结构对异常值抑制的影响机制尚不明确。需要系统分析块Hadamard旋转的异常值抑制效果，并开发更有效的量化框架。

Method: 1. 首次对块Hadamard旋转进行系统性非渐近分析，发现异常值抑制受输入向量几何结构限制；2. 提出MixQuant框架，通过排列在旋转前重新分配激活质量；3. 设计贪心质量扩散算法校准排列，均衡块间ℓ₁范数期望；4. 识别Transformer架构中的排列等变区域，将排列合并到模型权重中以避免推理开销。

Result: MixQuant在所有块大小下均能持续提升精度，在Llama3 1B模型INT4量化（块大小16）中，可恢复高达90%的全向量旋转困惑度，相比无排列方法的46%有显著提升。

Conclusion: 块旋转的异常值抑制效果受输入向量几何结构限制，通过排列重新分配激活质量可显著优化量化性能。MixQuant框架在保持推理效率的同时，为低精度量化提供了有效的解决方案。

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [108] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一种基于集合架构的MDP策略表示学习方法，通过变分生成和对比学习构建平滑潜在空间，支持基于梯度的策略优化和行为合成


<details>
  <summary>Details</summary>
Motivation: 学习MDP中一系列策略的表示，以便在测试时进行行为引导。由于MDP策略由其占用测度唯一确定，需要开发能够统一近似策略范围并支持梯度优化的表示方法

Method: 将策略表示建模为状态-动作特征映射相对于占用测度的期望；使用集合架构编码状态-动作样本为潜在嵌入，从中解码策略及其对应多个奖励的价值函数；采用变分生成方法构建平滑潜在空间，并通过对比学习使潜在距离与价值函数差异对齐

Result: 实现了对策略范围的统一近似表示，构建了支持梯度优化的几何结构，能够解决新颖的行为合成任务，在未见过的价值函数约束下引导策略而无需额外训练

Conclusion: 提出的策略表示学习方法通过集合架构、变分生成和对比学习，成功构建了支持梯度优化的平滑潜在空间，实现了有效的策略引导和行为合成能力

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [109] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 该论文提出基于最优传输框架的两种几何量度——相对Wasserstein角和正交投影距离，用于量化经验分布与高斯分布之间的偏差，并证明在高维情况下矩匹配高斯并非W_2最近高斯分布。


<details>
  <summary>Details</summary>
Motivation: 现有方法在量化经验分布与高斯分布偏差时存在局限性，特别是在高维空间中，常用的矩匹配高斯方法并非最优传输意义下的最近高斯分布。需要建立严格的几何框架来度量非高斯性。

Method: 利用相对平移不变二次Wasserstein空间的锥几何结构，引入相对Wasserstein角和正交投影距离两个几何量。在一维情况下推导闭式解，在高维情况下开发基于半离散对偶公式的随机流形优化算法。

Result: 证明了该空间中任意两条射线生成的填充锥是平坦的，确保角度、投影和内积严格定义。实验表明相对Wasserstein角比Wasserstein距离更鲁棒，提出的最近高斯分布在FID评分评估中优于矩匹配方法。

Conclusion: 通过最优传输的几何视角，将高斯近似重构为高斯锥上的投影问题，揭示了矩匹配高斯的局限性，并提供了更鲁棒的非高斯性度量方法，在理论和应用上均有重要意义。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [110] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 系统级设计选择（数值精度、批处理策略、请求调度）对LLM推理能耗有数量级影响，低精度仅在计算受限时节能，批处理提升能效，结构化请求时序可降低单请求能耗100倍。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在生产环境中的部署增加，计算资源和能源需求从训练转向推理。虽然先前研究关注了每提示或每令牌的推理能耗，但系统级设计选择（如数值精度、批处理策略、请求调度）对能耗的影响尚未得到充分研究。

Method: 在NVIDIA H100 GPU上进行详细的LLM推理能耗和延迟实证研究，分析量化、批处理大小和服务配置（如使用Hugging Face的Text Generation Inference服务器）的影响。

Result: 1) 低精度格式仅在计算受限时产生节能效果；2) 批处理提高能效，特别是在解码等内存受限阶段；3) 结构化请求时序（到达整形）可将单请求能耗降低高达100倍。

Conclusion: 可持续的LLM部署不仅取决于模型内部设计，还依赖于服务栈的编排。研究结果支持基于阶段的能耗分析和系统级优化，以实现更绿色的AI服务。

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [111] [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)
*Rosen Ting-Ying Yu,Nicholas Sung,Faez Ahmed*

Main category: cs.LG

TL;DR: FIRE是一个免训练的多保真度回归框架，通过结合表格基础模型实现零样本上下文贝叶斯推理，利用高保真度校正模型对低保真度模型的后验预测分布进行条件化，在31个基准问题上优于7种现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程多保真度回归面临立方计算复杂度、对稀疏高保真度观测过拟合的问题，限制了实际应用的效率和泛化能力。需要一种更高效、更稳健的方法来处理极端数据不平衡的多保真度回归任务。

Method: FIRE框架通过表格基础模型实现零样本上下文贝叶斯推理，核心是使用高保真度校正模型对低保真度模型的后验预测分布进行条件化。这种跨保真度信息传递通过分布摘要捕获异方差误差，实现无需模型重新训练的稳健残差学习。

Result: 在31个基准问题（包括合成和真实世界任务如DrivAerNet、LCBench）上，FIRE在性能-时间权衡方面优于7种最先进的高斯过程或深度学习多保真度回归方法，在准确性和不确定性量化方面排名最高，并具有运行时优势。

Conclusion: FIRE提供了一种免训练的多保真度回归框架，通过表格基础模型和跨保真度分布传递实现了高效、稳健的回归性能。主要局限性包括上下文窗口约束和对预训练表格基础模型质量的依赖。

Abstract: Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.

</details>


### [112] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: G-Substrate：一种图表示学习框架，将图结构作为跨模态和任务的持久性结构基板，通过统一结构模式和角色交替训练策略实现知识积累


<details>
  <summary>Details</summary>
Motivation: 当前图结构学习通常在模态和任务隔离的环境中进行，导致跨模态和任务的结构规律需要重复重建，无法在中间图表示层面积累。这促使研究如何组织图结构，使其能够在异构模态和任务间持久存在并积累知识。

Method: 提出G-Substrate框架，包含两个互补机制：1）统一结构模式，确保跨异构模态和任务的图表示兼容性；2）角色交替训练策略，在训练过程中将相同的图结构暴露给多个功能角色

Result: 在多个领域、模态和任务上的实验表明，G-Substrate优于任务隔离方法和朴素的多任务学习方法

Conclusion: G-Substrate提供了一种以表示为中心的方法，将图结构作为跨学习环境的持久性结构基板，能够有效积累和重用跨模态和任务的结构知识，为图表示学习提供了新的框架

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [113] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR：基于LLM的在线强化学习控制器，用于多阶段ML推理管道的自动扩缩容，无需梯度更新，通过奖励塑造、经验检索和GPU速率控制优化延迟和成本


<details>
  <summary>Details</summary>
Motivation: 多阶段ML推理管道存在异构资源、跨阶段耦合和动态瓶颈迁移等问题，导致自动扩缩容困难，需要一种无需离线训练、能在线学习并适应动态变化的解决方案

Method: 使用LLM作为上下文强化学习控制器，结合帕累托优势奖励塑造与可证明分离边界、基于惊奇值的经验检索以提高上下文效率，以及通过用户空间CUDA拦截实现细粒度GPU速率控制

Result: 在四种ML服务管道和三种工作负载模式下，SAIR实现了最佳或并列最佳的P99延迟和有效资源成本，P99延迟提升高达50%，有效成本降低高达97%，瓶颈检测准确率达86%，且无需离线训练

Conclusion: SAIR框架通过LLM作为在线强化学习控制器，成功解决了多阶段ML推理管道的自动扩缩容问题，在延迟、成本和瓶颈检测方面均表现出色，为ML服务系统提供了有效的自适应资源管理方案

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [114] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: SIREN是一种通过估计数据似然分数函数来识别异常值根本原因的新方法，使用积分梯度沿异常值到正常数据分布的路径累积分数贡献，满足三个Shapley值公理和因果结构的不对称公理。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式或反事实推理的方法在不确定性和高维依赖下难以有效识别异常值的根本原因，需要一种可扩展且能处理非线性、高维和异方差因果模型的方法。

Method: SIREN通过估计数据似然的分数函数，使用积分梯度计算沿异常值到正常数据分布路径上的分数贡献累积，实现根本原因归因。该方法直接操作于分数函数，满足三个Shapley值公理（虚拟性、效率性、线性性）和因果结构的不对称公理。

Result: 在合成随机图和真实世界云服务及供应链数据集上的实验表明，SIREN在归因准确性和计算效率方面均优于现有最先进基线方法。

Conclusion: SIREN提供了一种可扩展、不确定性感知的异常值根本原因归因方法，能够有效处理非线性、高维和异方差因果模型，在理论和实证上都表现出优越性能。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [115] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: MM-OpenFGL是首个针对多模态联邦图学习的综合基准，包含19个数据集、8种模拟策略、6个下游任务和57种SOTA方法，系统评估了MMFGL的必要性、有效性、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多模态属性图通常分布在隔离平台上，由于隐私或商业限制无法共享，而现有联邦图学习研究主要关注单模态图，缺乏针对多模态联邦图学习的系统评估框架。

Method: 提出MM-OpenFGL基准，系统形式化MMFGL范式，包含19个多模态数据集覆盖7个应用领域，8种模拟策略捕捉模态和拓扑变化，6个下游任务，并通过模块化API实现57种SOTA方法。

Result: 通过大量实验从必要性、有效性、鲁棒性和效率四个维度全面研究MMFGL，为未来研究提供有价值的见解，证明MMFGL在分布式多模态图学习中的重要性。

Conclusion: MM-OpenFGL填补了多模态联邦图学习领域的空白，为系统评估和比较不同方法提供了标准化框架，推动了该领域的研究发展。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [116] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: MetaLead是一个完全人工标注的机器学习排行榜数据集，旨在解决现有自动化排行榜生成数据集的局限性，提供更透明和细致的评估资源。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域需要排行榜来基准测试和跟踪进展，但传统创建方式需要大量人工努力。现有的自动化排行榜生成数据集存在局限性：只捕获每篇论文的最佳结果，且元数据有限，无法支持更透明和细致的评估。

Method: 提出MetaLead数据集，采用完全人工标注的方式，捕获所有实验结果以实现结果透明性，并包含额外元数据：结果实验类型（基线、提出方法或变体方法）用于实验类型指导的比较，以及明确分离训练和测试数据集以支持跨领域评估。

Result: MetaLead通过其丰富的结构，包括所有实验结果、实验类型元数据和明确的数据集分离，成为一个强大的资源，支持更透明和细致的机器学习研究评估。

Conclusion: MetaLead解决了现有机器学习排行榜数据集的局限性，通过提供完全人工标注、包含所有实验结果和丰富元数据的结构，为更透明和细致的跨领域评估提供了有力工具。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [117] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 提出一种基于显式对比学习的替代方法，替代GRPO中通过优势估计软性区分好坏结果的方式，将K个结果明确分为正负集并最大化正结果似然


<details>
  <summary>Details</summary>
Motivation: GRPO方法虽然有效，但其依赖优势估计软性区分结果，且需要经验性调整如非对称裁剪和零方差数据过滤等技巧，这些技巧需要大量经验洞察且难以识别

Method: 提出显式对比学习方法，将K个结果明确分为正负集，然后最大化正结果的似然。该方法可视为LLM推理中（多标签）噪声对比估计的在线实例化

Result: 在具有挑战性的数学基准测试套件上，与DAPO和在线DPO等强基线相比，展示了具有竞争力的性能

Conclusion: 提出的显式对比学习方法为LLM推理提供了一种替代GRPO的有效方案，避免了经验性调整的复杂性，同时保持了竞争性性能

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [118] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 扩散模型在数据不匹配时仍能有效解决逆问题，研究揭示了弱先验何时有效及其理论依据


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型作为先验在逆问题中的应用，特别是当模型训练数据与目标信号不匹配（弱先验）时，为何仍能取得良好效果

Method: 通过大量实验分析弱先验在不同测量条件下的表现，并基于贝叶斯一致性理论建立理论框架，给出后验集中于真实信号的条件

Result: 发现弱先验在测量信息丰富时（如大量观测像素）表现良好，识别了其失效的机制，理论分析提供了弱扩散先验可靠使用的原则性依据

Conclusion: 扩散模型作为弱先验在逆问题中具有鲁棒性，特别是在测量信息充分时，这为实际应用中数据不匹配情况下的可靠使用提供了理论支持

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [119] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 提出基于权重的稀疏自编码器特征解释框架，通过直接权重交互测量功能效应，无需激活数据，揭示了特征在语言模型中的计算角色


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器特征解释方法仅关注激活模式，忽略了特征训练目的是重建在前向传播中发挥计算作用的激活，需要补充基于权重的功能分析

Method: 引入基于权重的解释框架，通过直接权重交互测量功能效应，无需激活数据；在Gemma-2和Llama-3.1模型上进行三个实验

Result: 1/4的特征直接预测输出词元；特征以深度依赖结构参与注意力机制；语义和非语义特征在注意力电路中呈现不同的分布特征

Conclusion: 该分析提供了稀疏自编码器特征解释中缺失的"上下文外"部分，揭示了特征在模型计算中的实际功能角色

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [120] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA是一种用于强化学习验证（RLVR）的高效提示采样和池增长方法，通过边界采样和在线策略查询增强来减少计算成本


<details>
  <summary>Details</summary>
Motivation: 当前RLVR训练中，提示池通常是静态的或与模型学习进度松散关联，均匀采样无法适应能力边界的变化，导致在已解决或无法解决的提示上浪费计算资源

Method: HeaPA维护有界演化池，使用基于堆的边界采样跟踪能力边界，通过轻量级异步验证的在线策略查询增强扩展池，并通过拓扑感知的统计重估计和控制性重新插入稳定相关查询

Result: 在两个训练语料库、两种训练方法和七个基准测试中，HeaPA持续提高准确性，以更少的计算达到目标性能，同时保持相当的挂钟时间，且模型规模越大收益越明显

Conclusion: HeaPA通过边界聚焦采样和在线策略池增长显著提高了RLVR训练效率，其优势随模型规模增加而放大，为大规模语言模型推理训练提供了有效的解决方案

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [121] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: 提出EvoEGF-Mol模型，通过信息几何方法在SBDD中建模分子为复合指数族分布，沿Fisher-Rao度量下的指数测地线定义生成流，避免传统方法在欧几里得和概率空间分离建模导致的统计流形不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 传统SBDD方法分别在欧几里得空间（连续原子坐标）和概率空间（离散化学类别）构建概率路径，导致与底层统计流形不匹配。需要从信息几何角度解决这一问题，实现更一致的分子建模。

Method: 将分子建模为复合指数族分布，在Fisher-Rao度量下沿指数测地线定义生成流。为避免直接以狄拉克分布为目标导致的瞬时轨迹崩溃，提出EvoEGF-Mol模型：用动态集中分布替代静态狄拉克目标，通过渐进参数精化架构确保训练稳定性。

Result: 在CrossDock上达到93.4%的PoseBusters通过率（参考水平），展现出卓越的几何精度和相互作用保真度。在MolGenBench真实任务中优于基线方法，能够恢复生物活性骨架并生成符合MedChem过滤器的候选分子。

Conclusion: EvoEGF-Mol通过信息几何框架解决了SBDD中的统计流形不匹配问题，实现了稳定高效的分子生成，在几何精度和化学可行性方面表现出色，为基于结构的药物设计提供了新方法。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [122] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: LLMs exhibit latent learning dynamics: unrewarded exploration阶段提升知识组织能力，随后引入奖励进一步强化性能，优于全程奖励学习


<details>
  <summary>Details</summary>
Motivation: 探索心理学中已确立的潜在学习现象是否能应用于LLMs训练，因为当前LLMs主要依赖奖励驱动的强化学习范式，限制了灵活性和泛化能力

Method: 采用两阶段训练范式：第一阶段无奖励探索，第二阶段引入奖励；在多个模型家族和多样化任务领域进行广泛实验，并提供理论分析解释机制

Result: LLMs在无奖励探索阶段表现出适度性能提升，引入奖励后性能进一步增强；采用两阶段探索机制训练的LLMs最终比全程奖励学习的模型获得更高能力

Conclusion: LLMs确实表现出潜在学习动态，无奖励探索有助于组织任务相关知识而不受奖励驱动偏见约束，为改进LLMs训练范式提供了新视角

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [123] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出教师-学生框架解耦持续强化学习：通过分布式RL训练单任务教师模型，持续蒸馏到中央通用模型，结合MoE架构和回放机制提升可塑性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习面临稳定性-可塑性困境和灾难性遗忘挑战。RL擅长解决单任务，而策略蒸馏作为相对稳定的监督学习过程，更适合大基础模型和多任务学习，需要将两者优势结合。

Method: 1) 分布式RL训练单任务教师模型；2) 持续蒸馏到中央通用学生模型；3) 采用混合专家(MoE)架构增强可塑性；4) 结合回放机制提升稳定性。

Result: 在Meta-World基准测试中，框架能恢复超过85%的教师模型性能，同时将任务级遗忘控制在10%以内，实现高效的持续强化学习。

Conclusion: 教师-学生框架有效解耦持续强化学习，结合单任务RL训练和多任务策略蒸馏，通过MoE和回放机制平衡稳定性与可塑性，为持续学习提供可扩展方案。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [124] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: TA-GRPO通过生成语义等效的问题变体并跨组池化奖励，解决了GRPO中的多样性崩溃和梯度消失问题，在数学推理基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型对表面表达变化敏感，而GRPO方法存在两个主要问题：多样性崩溃（训练放大单一解题策略）和梯度消失（大量问题产生零梯度）。这些问题限制了推理能力的提升。

Method: 提出TA-GRPO方法：1）通过转述、变量重命名和格式变化生成语义等效的问题变体；2）跨整个变体组池化奖励计算优势；3）确保即使原始问题过于简单或困难时也能获得混合奖励；4）在多样化表达上训练以促进多种解题策略。

Result: 在数学推理基准上获得一致的Pass@k提升：竞赛数学（AMC12, AIME24）上提升高达9.84分，分布外科学推理（GPQA-Diamond）上提升5.05分。理论分析表明TA-GRPO降低了零梯度概率并通过减少训练-测试分布偏移改善了泛化能力。

Conclusion: TA-GRPO通过语义变换增强和跨组奖励池化，有效解决了GRPO的多样性崩溃和梯度消失问题，显著提升了语言模型的推理能力和泛化性能。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [125] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: STARS框架通过监测隐藏状态中的L2距离尖峰来检测认知转折点，利用几何轨迹分析诊断推理结构问题，并注入状态感知的语言提示实时纠正大型推理模型的认知惯性问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在扩展测试时计算方面取得了显著性能，但经常遭受认知惯性的困扰，表现为过度思考（运动惯性）或推理僵化（方向惯性）。现有检测方法通常依赖表面的文本启发式方法（如自我纠正标记），往往无法捕捉模型未表达的内部冲突。

Method: 提出STARS（尖峰触发自适应推理引导）框架，通过监测隐藏状态中的L2距离尖峰来识别认知转折点（关键推理转换时刻），使用几何轨迹分析诊断转换的结构性质，并注入状态感知的语言提示实时引导模型。

Result: 在多样化基准测试中的实验证实，STARS能有效减少冗余循环，同时通过自适应纠正错误轨迹来提高准确性。该框架为大型推理模型提供了无需额外微调的推理过程优化机制。

Conclusion: STARS提供了一个强大、无监督的机制，可以在不需要额外微调的情况下优化大型推理模型的推理过程，有效解决认知惯性问题。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [126] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: 提出Gradual Fine-Tuning (GFT)框架，用于在目标分布样本可用时微调流匹配模型，通过温度控制平滑过渡，改善收敛稳定性并缩短概率路径


<details>
  <summary>Details</summary>
Motivation: 在数据有限、分布演化或效率要求严格的情况下，无约束微调会损害预训练获得的准确性和效率增益。现有奖励微调方法通常对漂移结构或训练技术施加限制

Method: 提出GFT框架，为随机流定义温度控制的中间目标序列，平滑插值预训练和目标漂移，温度趋近零时接近真实目标。证明边际和条件GFT目标的收敛性

Result: GFT提高收敛稳定性，缩短概率路径，实现更快推理，同时保持与标准微调相当的生成质量

Conclusion: GFT为流匹配模型在分布偏移下的可扩展适应提供了理论基础坚实且实际有效的替代方案

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [127] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 离线目标条件强化学习中，通过信息论框架提出动作充分性概念，证明价值充分性不足以保证最优控制，实验显示基于演员策略学习的表示优于基于价值估计的表示


<details>
  <summary>Details</summary>
Motivation: 现有分层策略在离线目标条件强化学习中通常通过价值函数学习目标表示，隐含假设保留价值估计所需信息就足以实现最优控制。作者发现这种假设可能失败，即使价值估计准确，这样的表示可能坍缩需要区分的状态，导致动作学习失败

Method: 引入信息论框架定义动作充分性条件，证明价值充分性不蕴含动作充分性。提出标准对数损失训练低层策略自然诱导动作充分表示。在流行基准上实验验证基于演员策略学习的表示优于基于价值估计的表示

Result: 理论证明价值充分性不足以保证动作充分性。实验验证动作充分性与控制成功更密切相关。基于演员策略学习的表示在基准测试中一致优于基于价值估计的表示

Conclusion: 在离线目标条件强化学习的分层架构中，目标表示的设计需要满足动作充分性条件，而不仅仅是价值充分性。基于演员策略学习的表示能更好地满足这一条件，从而提升长时域任务的控制性能

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [128] [Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually Drifting Tasks](https://arxiv.org/abs/2601.22509)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.LG

TL;DR: 提出DREE框架解决VRP神经求解器在持续漂移任务下的终身学习问题，通过双重回放和经验增强提高学习效率并缓解灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 现有VRP神经求解器要么在固定任务集上一次性训练，要么在顺序到达的多个任务上进行终身学习，但都假设每个任务有充分训练时间。这两种设置都忽略了现实世界问题模式可能随时间持续漂移的特性，导致任务大量顺序出现而每个任务只有有限训练资源

Method: 提出DREE（Dual Replay with Experience Enhancement）框架，包含双重回放机制和经验增强技术，在持续漂移的任务序列中提高学习效率并缓解灾难性遗忘

Result: 实验表明在持续漂移条件下，DREE能有效学习新任务、保留先验知识、提高对未见任务的泛化能力，并可应用于多种现有神经求解器

Conclusion: DREE为神经VRP求解器在持续漂移任务下的终身学习提供了有效解决方案，解决了现实世界中任务模式漂移而训练资源有限的挑战

Abstract: Existing neural solvers for vehicle routing problems (VRPs) are typically trained either in a one-off manner on a fixed set of pre-defined tasks or in a lifelong manner on several tasks arriving sequentially, assuming sufficient training on each task. Both settings overlook a common real-world property: problem patterns may drift continually over time, yielding massive tasks sequentially arising while offering only limited training resources per task. In this paper, we study a novel lifelong learning paradigm for neural VRP solvers under continually drifting tasks over learning time steps, where sufficient training for any given task at any time is not available. We propose Dual Replay with Experience Enhancement (DREE), a general framework to improve learning efficiency and mitigate catastrophic forgetting under such drift. Extensive experiments show that, under such continual drift, DREE effectively learns new tasks, preserves prior knowledge, improves generalization to unseen tasks, and can be applied to diverse existing neural solvers.

</details>


### [129] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 研究发现Transformer模型在技能组合学习上存在"破碎组合性"现象，即模型不按人类顺序学习技能，导致分布偏移时出现意外错误，且这种现象无法通过模型缩放或思维链缓解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常出现意外错误，现有研究揭示了LLM与人类在技能组合上的差异，但技能组合的学习动态及其非人类行为的根本原因仍不清楚。

Method: 通过在合成算术任务上训练Transformer模型，进行大量消融实验和细粒度诊断指标分析，探究学习动态机制。

Result: 发现Transformer不按人类顺序规则可靠构建技能组合，常以相反或并行方式获取技能，导致分布偏移时出现混合错误（破碎组合性）。证据表明是训练数据的相关性匹配而非因果或程序性组合塑造了学习动态。这种现象在现代LLM中持续存在，且无法通过模型缩放或思维链缓解。

Conclusion: 揭示了模型学习行为与期望技能组合之间的根本不匹配，对推理可靠性、分布外鲁棒性和对齐具有重要影响。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [130] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 该论文研究无人机辅助可见光通信系统中的三维轨迹规划，通过优化飞行高度和水平轨迹来最小化无人机飞行距离，从而提高数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 无人机与可见光通信技术的结合为提供灵活通信和高效照明提供了有前景的解决方案。然而，在无人机辅助的VLC系统中，如何规划三维轨迹以最小化飞行距离并最大化数据收集效率是一个具有挑战性的问题。

Method: 首先推导了在特定VLC信道增益阈值下的闭式最优飞行高度。然后，通过将新型信息素驱动奖励机制与双延迟深度确定性策略梯度算法相结合，优化无人机的水平轨迹，实现在复杂环境中的自适应运动策略。

Result: 仿真结果表明，推导的最优高度相比基线方法可减少高达35%的飞行距离。此外，提出的奖励机制显著缩短了约50%的收敛步数，在无人机辅助VLC数据收集场景中表现出显著的效率提升。

Conclusion: 该研究提出了一种有效的三维轨迹规划框架，通过优化飞行高度和水平轨迹，显著提高了无人机辅助VLC系统的数据收集效率，为解决混合整数非凸优化问题提供了创新解决方案。

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [131] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: 该研究提出了SCOPE-PD，一个基于可解释AI的帕金森病预测框架，通过整合主观和客观评估数据，使用随机森林算法达到98.66%的预测准确率，并通过SHAP分析识别出震颤、运动迟缓和面部表情为最重要的预测特征。


<details>
  <summary>Details</summary>
Motivation: 帕金森病是一种受遗传、临床和生活方式因素影响的复杂神经退行性疾病。早期预测具有挑战性，因为传统诊断方法存在主观性问题，常导致诊断延迟。虽然机器学习在支持PD诊断方面显示出潜力，但现有方法通常仅依赖主观报告，缺乏个体化风险评估的可解释性。

Method: 研究提出SCOPE-PD框架，整合主观和客观临床评估数据。数据来自帕金森病进展标志物倡议(PPMI)研究，构建多模态预测框架。应用多种机器学习技术，选择最佳模型解释结果。使用SHAP分析进行模型可解释性检验。

Result: 随机森林算法在使用主观和客观测试数据的组合特征时达到最高准确率98.66%。通过SHAP分析识别出MDS-UPDRS测试中震颤、运动迟缓和面部表情是预测帕金森病的三个最重要特征。

Conclusion: SCOPE-PD框架通过整合主观和客观评估，结合可解释AI技术，为帕金森病提供了准确且可解释的预测方法。该方法不仅提高了诊断准确性，还提供了个体化风险评估，有助于早期诊断和个性化医疗决策。

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [132] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 该论文通过构建简约基线方法，系统分析了强化学习微调中各种设计选择的作用和重要性，识别出关键因素并澄清了该领域的不一致结论。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调领域存在大量关于优化设计选择的研究，但性能提升的声称往往不一致，结论相互矛盾，使得进展显得虚幻。缺乏对两个基本问题的原则性答案：1）每个设计选择的作用是什么？2）哪些是关键的设计选择？

Method: 首先构建一个简约基线方法：每轮每个查询仅进行一次rollout，使用结果奖励作为训练信号（不使用优势函数技巧），批量大小为32。该基线连接到批量上下文赌博机学习，便于实验分析。围绕此基线设计实验流程，检查优势函数、rollout数量等因素的边际增益。在三个基础模型和两个数据集上进行实验。

Result: 实验不仅揭示了各种设计选择对学习和泛化动态的新理解，还识别出值得更多努力的关键设计选择。

Conclusion: 通过系统分析强化学习微调中的设计选择，该研究澄清了该领域的不一致结论，识别了关键因素，为未来的研究方向提供了指导。

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [133] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 提出L2D-SLDS模型用于非平稳时间序列的延迟学习，通过因子化切换线性高斯状态空间模型建模专家残差，支持专家动态注册，并提出基于信息导向采样的路由规则。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳时间序列中部分反馈和时变专家可用性的延迟学习问题，传统方法难以处理专家动态变化和跨专家信息共享。

Method: 提出L2D-SLDS模型：因子化切换线性高斯状态空间模型，包含上下文依赖的机制转换、共享全局因子（实现跨专家信息传递）和专家特定状态；支持专家动态注册；提出基于一步预测信念的IDS启发式路由规则，平衡预测成本与潜在机制和共享因子的信息获取。

Result: 实验表明该方法优于上下文多臂老虎机基准和无共享因子消融实验，验证了模型的有效性。

Conclusion: L2D-SLDS模型能有效处理非平稳时间序列的延迟学习问题，支持专家动态管理，通过信息导向的路由策略实现性能提升。

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [134] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 提出一种受人类多系统学习启发的贝叶斯采样算法，结合模型规划、无模型习惯和情景记忆三种机制，实现高效的后验分布探索


<details>
  <summary>Details</summary>
Motivation: 人类通过多种互补的神经控制系统（模型规划、无模型习惯、情景记忆）实现高效学习，作者希望将这种生物效率的计算原理转化为可扩展的贝叶斯推断采样算法

Method: 提出三模块算法：1) 模型规划模块使用目标分布进行引导但计算缓慢的采样；2) 无模型模块利用先前样本学习参数空间模式，实现快速反射式采样；3) 情景控制模块通过回忆特定过去事件（样本）支持快速采样

Result: 该方法推进了贝叶斯方法，促进了其在大规模统计机器学习问题中的应用，特别应用于贝叶斯深度学习，强调正确和原则性的不确定性量化

Conclusion: 受人类多系统学习启发的三模块采样算法为可扩展贝叶斯推断提供了有效解决方案，特别适用于需要不确定性量化的大规模机器学习问题

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [135] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 提出守恒量校正技术，通过融入物理守恒准则改进神经算子模型的长时稳定性，并分析现有架构在频谱域的性能局限


<details>
  <summary>Details</summary>
Motivation: 深度学习用于PDE数值解时存在长时预测性能不佳的问题，主要原因是自回归误差累积和模型无法保持物理守恒量

Method: 提出守恒量校正技术，这是一种模型无关的方法，将物理守恒准则融入深度学习模型，并分析神经算子在频谱域的性能

Result: 守恒量校正技术显著提高了自回归神经算子模型的长时稳定性，且与模型架构无关；频谱分析揭示了现有架构在高频分量处理上的显著局限

Conclusion: 需要设计能特别关注高频分量的架构，这对理解和建模湍流等复杂流动至关重要

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [136] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: FedDis：首个利用因果解耦的联邦时空预测框架，通过分离客户端特定因素和全局模式来解决非IID数据挑战


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护的交通预测中面临非IID数据挑战，现有方法难以处理数据异质性，通常将全局共享模式与客户端特定动态纠缠在单一表示中。作者认为这种异质性源于两个不同生成源的纠缠：客户端特定的局部动态和跨客户端的全局时空模式。

Method: 提出FedDis框架，采用双分支架构：个性化银行学习捕获客户端特定因素，全局模式银行提炼共同知识。通过互信息最小化目标强制两个分支之间的信息正交性，确保有效解耦。

Result: 在四个真实世界基准数据集上的综合实验表明，FedDis始终实现最先进的性能，具有高效性和优越的可扩展性。

Conclusion: FedDis通过因果解耦有效解决了联邦时空预测中的数据异质性挑战，实现了鲁棒的跨客户端知识转移同时保持对独特本地环境的高适应性。

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [137] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: MC-GRPO使用中位数基线替代均值基线来解决小样本训练中优势符号翻转问题，在保持G个梯度样本的同时提高低样本量训练的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的小样本训练场景中，基于群体相对策略优化的方法由于共享均值基线的噪声会导致优势符号翻转问题，即某些rollout接收到错误的正负优势符号，从而逆转更新方向，导致准确性下降。

Method: 提出中位数中心化群体相对策略优化(MC-GRPO)：用中位数基线替代均值基线，中位数对异常奖励值更不敏感。生成G+1个rollout用于中位数参考，使用群体中位数计算优势值。当群体大小为奇数时，恰好一个完成样本是中位数并获得零优势，将其从反向传播中排除，使每个提示的梯度贡献样本数保持为G，保持了标准G-rollout训练的核心更新成本。

Result: 在各种GRPO系列方法和广泛的模型规模上，中位数中心化训练在低样本量机制下持续提高了稳定性和最终准确性，将G=2和G=8之间的差距缩小到1%以内。

Conclusion: MC-GRPO通过简单而有效的中位数基线替换，解决了小样本训练中的优势符号翻转问题，在保持计算成本不变的情况下显著提高了低样本量训练的稳定性和性能。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [138] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: MGMT是一个统一的多图学习框架，通过图Transformer编码器将异构图映射到共享潜在空间，构建任务相关的超节点和元图，实现跨图推理和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多图学习面临异构图（不同拓扑、尺度、语义）整合的挑战，特别是在缺乏共享节点标识的情况下。需要开发能够有效整合跨图信息、可扩展且可解释的框架。

Method: MGMT首先使用图Transformer编码器将每个图的结构和属性映射到共享潜在空间；然后通过注意力机制选择任务相关的超节点；基于潜在空间相似性构建连接跨图功能对齐超节点的元图；在元图上应用额外的图Transformer层进行联合推理。

Result: 在合成数据集和真实世界神经科学应用中，MGMT在图像级预测任务中始终优于现有最先进模型，同时提供促进科学发现的可解释表示。

Conclusion: MGMT建立了结构化多图学习的统一框架，推进了图数据在核心领域的表示技术，为跨图学习提供了可扩展、可解释的解决方案。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [139] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: Lethe提出了一种新的联邦遗忘方法，通过解耦待遗忘知识与待保留知识，解决持续训练中知识重现问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘研究通常假设遗忘操作后协作结束，忽略了后续持续训练的情况。研究发现持续训练会重新激活已遗忘知识，导致知识重现问题

Method: Lethe采用Reshape-Rectify-Restore流程：1) 训练临时适配器对遗忘数据进行梯度上升获取放大更新；2) 使用这些更新作为校正信号，在两个流中对剩余更新进行层间校正；3) 移除适配器并在保留数据上进行短期恢复

Result: Lethe以统一方式支持联邦系统中所有级别的遗忘，在大多数情况下保持优异持久性（重现率<1%），即使在多轮后续训练后仍有效

Conclusion: Lethe解决了联邦遗忘中的知识重现问题，通过解耦知识确保持续训练中遗忘的持久性，为联邦遗忘提供了更实用的解决方案

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [140] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 共识机制作为注意力机制的替代方案，能提升Transformer在更广学习率范围内的训练稳定性，并通过混合共识-注意力框架在保持性能的同时改善稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准基于注意力的Transformer在训练过程中，特别是在高学习率下，存在学习率过指定时的不稳定性问题。虽然已有多种方法通过修改优化过程来提高对此类过指定的鲁棒性，但为此目的的基础架构创新仍未被充分探索。

Method: 提出共识机制作为注意力的即插即用替代方案，将其形式化为图模型，并进行广泛的实证分析。进一步提出混合共识-注意力框架，在保持性能的同时提高稳定性。

Result: 共识机制在文本、DNA和蛋白质模态的学习率扫描中表现出改进的稳定性。混合共识-注意力框架在保持性能的同时提高了稳定性。提供了理论分析来表征共识机制的特性。

Conclusion: 共识机制是Transformer架构的一种有前景的创新，能够显著提高训练稳定性，特别是在学习率过指定的情况下，为Transformer训练提供了更鲁棒的替代方案。

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [141] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 论文提出了一种形式逻辑验证引导的框架，将形式符号验证与自然语言生成过程动态交织，通过实时反馈检测和纠正错误，显著提升LLM的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然表现出色，但其随机的下一个token预测会产生逻辑不一致性和奖励黑客行为，而形式符号系统可以避免这些问题。现有神经符号方法局限于被动的事后验证，无法在推理过程中主动纠正错误。

Method: 提出形式逻辑验证引导框架，动态交织形式符号验证与自然语言生成过程，提供实时反馈检测和纠正错误。采用两阶段训练管道：形式逻辑验证引导的监督微调和策略优化，在推理链中主动惩罚中间谬误。

Result: 在六个涵盖数学、逻辑和一般推理的基准测试中，7B和14B模型分别以平均10.4%和14.2%的优势超越最先进的基线模型，验证了形式验证作为可扩展机制显著提升LLM推理性能边界的有效性。

Conclusion: 形式验证可以作为可扩展机制显著推动先进LLM推理的性能边界，通过主动的形式逻辑验证引导框架有效解决LLM的逻辑不一致问题，为神经符号推理提供了新的研究方向。

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [142] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: GUDA：基于群体遗忘的视觉生成模型训练数据归因方法，通过机器遗忘技术近似反事实模型，实现高效群体级训练数据影响评估


<details>
  <summary>Details</summary>
Motivation: 现有训练数据归因方法主要关注单个样本评分，但实际应用中常需要群体级答案（如艺术风格或对象类别）。传统的Leave-One-Group-Out重训练方法计算成本过高，需要更高效的群体级归因方法

Method: 提出GUDA方法：1）使用机器遗忘技术从共享全数据模型生成每个群体的反事实模型，避免从头训练；2）通过比较全模型与遗忘后模型在似然评分规则（ELBO）上的差异来量化群体影响

Result: 在CIFAR-10和Stable Diffusion艺术风格归因实验中，GUDA比语义相似性、基于梯度的归因和实例级遗忘方法更可靠地识别主要贡献群体，在CIFAR-10上相比LOGO重训练实现100倍加速

Conclusion: GUDA提供了一种计算高效的群体级训练数据归因方法，通过机器遗忘技术近似反事实模型，在保持准确性的同时显著降低计算成本，适用于扩散模型等视觉生成模型

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [143] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了一种无需验证数据的联邦学习早期停止框架，通过监控任务向量增长率来确定最优停止点，仅使用服务器端参数，在皮肤病变和血细胞分类任务上表现优于基于验证数据的早期停止方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习依赖固定的全局轮次或验证数据进行超参数调优，导致高计算成本和隐私风险，阻碍了实际部署。需要一种无需验证数据的早期停止方法来提高效率和保护隐私。

Method: 提出数据无关的早期停止框架，通过监控任务向量（task vector）的增长率来确定最优停止点。该方法仅使用服务器端参数，无需客户端数据或验证集，通过分析模型参数的变化模式来判断训练收敛状态。

Result: 在皮肤病变和血细胞分类任务上，该方法平均仅需47/20轮即可达到比基于验证数据的早期停止方法高12.5%/10.3%的性能。与多种最先进FL方法的实验表明，该框架性能与基于验证数据的早期停止相当。

Conclusion: 这是首个无需验证数据的联邦学习早期停止框架，通过监控任务向量增长率有效确定训练停止点，显著降低计算成本并减少隐私风险，为联邦学习的实际部署提供了实用解决方案。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [144] [Full-Graph vs. Mini-Batch Training: Comprehensive Analysis from a Batch Size and Fan-Out Size Perspective](https://arxiv.org/abs/2601.22678)
*Mengfan Liu,Da Zheng,Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 该论文系统比较了全图训练与mini-batch训练两种GNN训练方法，通过批大小和扇出大小的视角，揭示了它们在收敛性、泛化性和计算效率上的差异，发现全图训练并不总是优于适当调优的mini-batch训练。


<details>
  <summary>Details</summary>
Motivation: 全图训练和mini-batch训练在GNN中具有不同的系统设计需求，但缺乏系统比较这两种方法在模型性能（收敛性和泛化性）和计算效率方面的研究。批大小和扇出大小作为关键超参数对GNN训练的影响尚未充分探索。

Method: 通过实证和理论分析，从批大小和扇出大小的视角系统比较全图训练与mini-batch训练。使用Wasserstein距离进行泛化分析以研究图结构（特别是扇出大小）的影响，并分析批大小和扇出大小在GNN收敛和泛化中的非各向同性效应。

Result: 揭示了批大小和扇出大小对GNN收敛和泛化的非各向同性影响，为在资源约束下调整这些超参数提供了实用指导。发现全图训练并不总是比适当调优的小型mini-batch设置产生更好的模型性能或计算效率。

Conclusion: 全图训练并非总是最优选择，适当调优的mini-batch训练在特定条件下可能提供更好的性能效率平衡。批大小和扇出大小作为关键超参数需要综合考虑，研究为GNN训练方法选择提供了理论指导和实践建议。

Abstract: Full-graph and mini-batch Graph Neural Network (GNN) training approaches have distinct system design demands, making it crucial to choose the appropriate approach to develop. A core challenge in comparing these two GNN training approaches lies in characterizing their model performance (i.e., convergence and generalization) and computational efficiency. While a batch size has been an effective lens in analyzing such behaviors in deep neural networks (DNNs), GNNs extend this lens by introducing a fan-out size, as full-graph training can be viewed as mini-batch training with the largest possible batch size and fan-out size. However, the impact of the batch and fan-out size for GNNs remains insufficiently explored. To this end, this paper systematically compares full-graph vs. mini-batch training of GNNs through empirical and theoretical analyses from the view points of the batch size and fan-out size. Our key contributions include: 1) We provide a novel generalization analysis using the Wasserstein distance to study the impact of the graph structure, especially the fan-out size. 2) We uncover the non-isotropic effects of the batch size and the fan-out size in GNN convergence and generalization, providing practical guidance for tuning these hyperparameters under resource constraints. Finally, full-graph training does not always yield better model performance or computational efficiency than well-tuned smaller mini-batch settings. The implementation can be found in the github link: https://github.com/LIUMENGFAN-gif/GNN_fullgraph_minibatch_training.

</details>


### [145] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 该论文从流映射角度理论分析一致性模型，揭示训练不稳定性和收敛行为导致退化解的原因，提出改进的自蒸馏方法稳定优化，并扩展到扩散策略学习。


<details>
  <summary>Details</summary>
Motivation: 一致性模型虽然实现了快速生成建模，但在从头训练时表现出固有的不稳定性和有限的可复现性。现有研究对这些问题的解释较为零散，理论关系不明确，需要系统性的理论分析来阐明这些问题。

Method: 从流映射角度对一致性模型进行理论分析，阐明训练稳定性和收敛行为如何导致退化解。基于这些见解，重新审视自蒸馏作为次优收敛的实用补救措施，并重新表述以避免过大的梯度范数实现稳定优化。将策略扩展到扩散策略学习，无需预训练扩散模型初始化。

Result: 通过流映射视角的理论分析阐明了训练不稳定性和收敛行为的机制。改进的自蒸馏方法避免了过大的梯度范数，实现了更稳定的优化。该方法不仅适用于图像生成，还成功扩展到扩散策略学习，展示了更广泛的适用性。

Conclusion: 该研究为一致性模型提供了系统的理论分析框架，揭示了训练不稳定性的根本原因，并提出实用的改进方法。通过重新表述的自蒸馏策略，实现了更稳定的训练优化，且该方法具有扩展到其他领域的潜力，为快速生成建模提供了更可靠的理论基础。

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [146] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: Transformer模型在周期性OOD泛化方面存在局限，论文从抽象代数和推理角度分析周期性泛化问题，构建了Coper基准进行验证


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的大语言模型在分布外泛化方面与人类存在显著差距，需要研究周期性这一基本OOD场景下的泛化能力

Method: 从抽象代数和推理角度统一解释周期性（包括单周期和复合周期），构建Coper基准（包含Hollow和Extrapolation两种OOD设置），通过实验验证Transformer的周期性泛化能力

Result: 实验表明Transformer的周期性泛化能力有限，模型能够记忆训练数据中的周期性模式，但无法泛化到未见过的复合周期性场景

Conclusion: Transformer在周期性OOD泛化方面存在根本性局限，需要进一步研究改进模型的抽象推理和泛化能力

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [147] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 该论文提出了一个用于医学机器学习的数据质量评估框架和指标库，旨在支持可信AI在医疗领域的应用。


<details>
  <summary>Details</summary>
Motivation: 医学机器学习从研究转向实际应用，需要建立可信AI的证据基础。数据质量评估是可信AI发展的关键因素，但缺乏系统化的实践框架。

Method: 作者将之前提出的METRIC理论框架操作化，开发了一个数据质量指标库，为每个指标提供详细的指标卡片，包括定义、适用性、示例、陷阱和建议。还讨论了选择适当指标集的策略和决策树。

Result: 在PTB-XL心电图数据集上展示了该方法的影响，证明能够实际评估训练和测试数据的适用性。

Conclusion: 这是实现医学领域可信AI的重要第一步，为实践中评估训练和测试数据的适用性提供了基础。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [148] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: 提出基于深度学习的IR-drop早期估计方法，使用CNN架构将物理版图特征映射到IR-drop热力图，实现毫秒级快速预测，作为传统物理签核工具的补充


<details>
  <summary>Details</summary>
Motivation: 传统IR-drop分析依赖物理签核工具，精度高但计算成本大且需要接近最终版图信息，不适合早期快速设计探索。需要一种能在早期阶段快速估计IR-drop的方法

Method: 采用U-Net编码器-解码器架构，具有跳跃连接，将IR-drop估计建模为密集像素级回归问题。使用基于物理原理的合成数据集训练，包含电源网格结构、单元密度分布和开关活动等关键物理因素

Result: 模型能够准确预测IR-drop分布，推理时间达到毫秒级别，支持快速预签核筛选和迭代设计优化。使用MSE和PSNR等回归指标评估性能

Conclusion: 提出的深度学习框架可作为早期分析工具，在昂贵的签核分析之前为设计者提供快速IR-drop洞察。代码、数据集生成脚本和交互式推理应用已开源

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [149] [A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation](https://arxiv.org/abs/2601.22708)
*Haonan He,Jingqi Ye,Minglei Li,Zhengbo Wang,Tao Chen,Lei Bai,Peng Ye*

Main category: cs.LG

TL;DR: 该论文对LoRA变体进行了首次统一研究，提出了系统分类法、统一理论框架、结构化代码库和标准化评估，发现LoRA及其变体对学习率选择敏感，且适当配置下LoRA性能可匹配或超越大多数变体。


<details>
  <summary>Details</summary>
Motivation: LoRA作为参数高效微调的基本方法，其变体在方法、理论、代码和评估方面存在碎片化问题，缺乏系统性研究，需要统一框架来理解不同变体的关系、演变和实际效果。

Method: 1) 提出四轴分类法：秩、优化动态、初始化和与MoE集成；2) 在低秩更新动态框架下统一理论分析；3) 开发LoRAFactory模块化代码库；4) 在自然语言生成、理解和图像分类任务上进行大规模评估。

Result: 1) LoRA及其变体对学习率选择比其他超参数更敏感；2) 在适当的超参数配置下，LoRA能够匹配或超越大多数变体的性能；3) 通过系统评估揭示了不同变体的实际效果差异。

Conclusion: 该研究为LoRA变体提供了首个统一框架，通过系统分类、理论分析、代码实现和实证评估，揭示了LoRA方法的本质特性，为未来参数高效微调研究提供了基础。

Abstract: Low-Rank Adaptation (LoRA) is a fundamental parameter-efficient fine-tuning method that balances efficiency and performance in large-scale neural networks. However, the proliferation of LoRA variants has led to fragmentation in methodology, theory, code, and evaluation. To this end, this work presents the first unified study of LoRA variants, offering a systematic taxonomy, unified theoretical review, structured codebase, and standardized empirical assessment. First, we categorize LoRA variants along four principal axes: rank, optimization dynamics, initialization, and integration with Mixture-of-Experts. Then, we review their relationships and evolution within a common theoretical framework focused on low-rank update dynamics. Further, we introduce LoRAFactory, a modular codebase that implements variants through a unified interface, supporting plug-and-play experimentation and fine-grained analysis. Last, using this codebase, we conduct a large-scale evaluation across natural language generation, natural language understanding, and image classification tasks, systematically exploring key hyperparameters. Our results uncover several findings, notably: LoRA and its variants exhibit pronounced sensitivity to the choices of learning rate compared to other hyperparameters; moreover, with proper hyperparameter configurations, LoRA consistently matches or surpasses the performance of most of its variants.

</details>


### [150] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 该论文提出使用视觉语言模型（VLMs）的常识推理能力来提供可提示的表征，从而在无监督方式下分离可控变化与噪声，显著提升潜在动作模型（LAMs）在含干扰物场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前潜在动作模型（LAMs）在处理包含动作相关干扰物的观测时会失败，它们经常编码噪声而非有意义的潜在动作。相比之下，人类仅凭简短任务描述就能轻松区分视频中的任务相关运动与无关细节。因此需要利用视觉语言模型（VLMs）的常识推理能力来解决这一问题。

Method: 利用视觉语言模型（VLMs）提供可提示的表征，在无监督方式下分离可控变化与噪声。将这些表征作为LAM训练的目标，并对多种流行的VLMs进行基准测试，分析不同提示和超参数下的鲁棒性。研究发现，简单地要求VLMs忽略干扰物就能显著提升潜在动作质量。

Result: 研究揭示了不同VLMs在可提示表征质量上的显著差异，以及它们对不同提示和超参数的鲁棒性变化。有趣的是，较新的VLMs可能比旧版本表现更差。通过要求VLMs忽略干扰物，潜在动作质量得到显著提升，在Distracting MetaWorld下游任务中的成功率最高提升了六倍。

Conclusion: 利用视觉语言模型的常识推理能力提供可提示表征，可以有效解决潜在动作模型在处理动作相关干扰物时的问题。这种方法显著提升了潜在动作质量，为视觉-语言-动作模型的预训练流程提供了重要改进。

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [151] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: LoRDS提出一种基于低秩分解的元素级量化框架，通过连续低秩矩阵建模缩放流形，在保持效率的同时提供比块级量化更强的表达能力，实现量化、训练和微调的统一高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM量化方法主要依赖块级结构来保持效率，但这限制了表示灵活性。需要一种既能保持效率又能提供更强表达能力的量化方法。

Method: 提出低秩分解缩放（LoRDS）框架，将缩放流形建模为连续低秩矩阵S=BA，打破空间约束的块结构。该方法提供高保真PTQ初始化、权重和缩放因子的联合QAT训练，以及支持高秩乘法PEFT适配，无需额外推理开销。

Result: 在Llama3-8B上，3位量化比NormalFloat量化准确率提升27.0%，NVIDIA RTX 4090上推理速度提升1.5倍，下游任务PEFT性能比4位QLoRA提升9.6%。在各种模型家族中均优于现有基线方法。

Conclusion: LoRDS通过低秩分解实现了元素级量化的高效率和高表达能力，为LLM的统一压缩和适配提供了稳健的集成解决方案，在量化精度、推理速度和微调性能方面均有显著提升。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [152] [Decomposing Epistemic Uncertainty for Causal Decision Making](https://arxiv.org/abs/2601.22736)
*Md Musfiqur Rahman,Ziwei Jiang,Hilaf Hasson,Murat Kocaoglu*

Main category: cs.LG

TL;DR: 提出新框架区分因果效应边界中的样本不确定性和非可识别不确定性，通过置信集和神经因果模型获得更精确的因果效应估计


<details>
  <summary>Details</summary>
Motivation: 现有神经因果方法可能过拟合且无法区分因果效应边界宽度是由于根本不可识别性还是有限样本限制，需要系统方法指导实践者何时收集更多样本或变量

Method: 构建观测分布置信集，计算该置信集中所有分布的因果效应边界交集，通过神经因果模型求解min-max和max-min问题，区分样本不确定性和非可识别不确定性

Result: 在合成和真实数据集上的广泛实验表明，该方法能有效确定何时收集更多样本无助于确定最佳行动，指导实践者收集更多变量或转向随机研究

Conclusion: 提出的框架能系统区分因果效应边界中的两种不确定性类型，为因果推断实践提供重要指导，帮助决策何时收集更多数据或变量

Abstract: Causal inference from observational data provides strong evidence for the best action in decision-making without performing expensive randomized trials. The effect of an action is usually not identifiable under unobserved confounding, even with an infinite amount of data. Recent work uses neural networks to obtain practical bounds to such causal effects, which is often an intractable problem. However, these approaches may overfit to the dataset and be overconfident in their causal effect estimates. Moreover, there is currently no systematic approach to disentangle how much of the width of causal effect bounds is due to fundamental non-identifiability versus how much is due to finite-sample limitations. We propose a novel framework to address this problem by considering a confidence set around the empirical observational distribution and obtaining the intersection of causal effect bounds for all distributions in this confidence set. This allows us to distinguish the part of the interval that can be reduced by collecting more samples, which we call sample uncertainty, from the part that can only be reduced by observing more variables, such as latent confounders or instrumental variables, but not with more data, which we call non-ID uncertainty. The upper and lower bounds to this intersection are obtained by solving min-max and max-min problems with neural causal models by searching over all distributions that the dataset might have been sampled from, and all SCMs that entail the corresponding distribution. We demonstrate via extensive experiments on synthetic and real-world datasets that our algorithm can determine when collecting more samples will not help determine the best action. This can guide practitioners to collect more variables or lean towards a randomized study for best action identification.

</details>


### [153] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 论文系统研究了Softmax族损失函数，分析了不同替代损失的一致性、梯度动态和收敛行为，为大规模分类任务中的损失选择提供了理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: Softmax损失是分类和排序任务中最广泛使用的替代目标函数。虽然Fenchel-Young框架将其置于广泛的替代函数族中，但针对类别数量极大的情况，现有研究主要关注可扩展性近似方法。本文旨在结合这两个视角，对Softmax族损失进行系统性研究。

Method: 1) 分析不同替代损失与分类和排序指标的一致性；2) 研究梯度动态以揭示不同的收敛行为；3) 为近似方法引入系统的偏差-方差分解，提供收敛保证；4) 推导每轮复杂度分析，展示效果与效率之间的权衡。

Result: 实验表明，一致性、收敛性和实证性能之间存在强相关性。研究结果为大规模机器学习应用中的损失选择建立了理论基础，并提供了实用指导。

Conclusion: 本文系统研究了Softmax族损失的理论性质，分析了不同替代损失的一致性、收敛行为和复杂度权衡，为大规模分类任务中的损失函数选择提供了原则性框架和实践指导。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [154] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: OSNIP是一种轻量级客户端加密框架，通过将原始嵌入投影到"混淆语义零空间"来保护LLM推理隐私，无需后处理


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理中的隐私保护问题，需要在保持语义保真度的同时防止敏感信息泄露，现有方法往往需要在隐私和效用之间权衡

Method: 将线性核的几何直觉推广到LLM高维潜在空间，定义"混淆语义零空间"，注入扰动将原始嵌入投影到该空间，并使用密钥依赖的随机映射生成个性化扰动轨迹

Result: 在12个生成和分类基准测试中达到最先进性能，显著降低攻击成功率，同时在严格安全约束下保持强大的模型效用

Conclusion: OSNIP提供了一种有效的隐私保护LLM推理框架，通过几何方法在语义保真度和隐私保护之间取得良好平衡

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [155] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 该研究系统研究了分子语言模型在预训练和下游任务中的缩放规律，通过训练300个模型和超过10,000次实验，揭示了分子表示对性能的重要影响，并解释了先前观察到的分子生成缩放行为不一致性。


<details>
  <summary>Details</summary>
Motivation: 分子生成模型（通常基于GPT风格的语言模型）在大规模数据和模型尺寸下表现出潜力，但尚不清楚这些模型在固定计算预算下是否遵循可预测的缩放规律。理解这一规律对于在模型大小、数据量和分子表示之间优化资源分配至关重要。

Method: 研究训练了300个模型，进行了超过10,000次实验，在严格控制计算预算的同时，独立地改变模型大小、训练token数量和分子表示，系统研究了分子语言模型的缩放行为。

Result: 研究结果证明了分子模型在预训练和下游迁移中都存在清晰的缩放规律，揭示了分子表示对性能的显著影响，并解释了先前观察到的分子生成缩放行为不一致性。同时公开了迄今为止最大的分子语言模型库。

Conclusion: 分子语言模型确实遵循可预测的缩放规律，分子表示是影响性能的关键因素。研究为优化分子生成模型的资源分配提供了实证基础，并发布了大规模模型库以促进未来研究。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [156] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 该论文建立了稀疏注意力机制与紧支撑核之间的形式对应关系，揭示了归一化ReLU和sparsemax注意力分别对应固定和自适应归一化的Epanechnikov核回归，并将α-entmax注意力与经典非参数密度估计核联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有研究揭示了transformer中的自注意力机制与测试时核回归（通过Nadaraya-Watson估计器）之间的联系，标准softmax注意力对应高斯核。然而，目前缺乏对稀疏注意力机制的核理论理解。本文旨在填补这一空白，建立稀疏注意力与紧支撑核之间的形式对应关系。

Method: 通过理论分析建立稀疏注意力机制与紧支撑核之间的形式对应关系。具体展示了归一化ReLU和sparsemax注意力分别对应固定和自适应归一化的Epanechnikov核回归。更一般地，证明了非参数密度估计中广泛使用的核（包括Epanechnikov、biweight和triweight）对应α-entmax注意力，其中α=1+1/n（n∈ℕ），而softmax/高斯关系在n→∞时出现。使用基于核回归的transformer变体Memory Mosaics进行实验验证。

Result: 实验结果表明，基于核的稀疏注意力在语言建模、上下文学习和长度泛化任务上实现了有竞争力的性能。这一统一视角解释了稀疏性如何自然地从核设计中产生，并为启发式的top-k注意力和其他关联记忆机制提供了原则性替代方案。

Conclusion: 本文建立了稀疏注意力与紧支撑核之间的形式对应关系，为设计注意力机制提供了原则性框架。这一统一视角不仅解释了现有稀疏注意力机制的理论基础，还为开发新的注意力变体提供了指导，展示了基于核的稀疏注意力在实际任务中的有效性。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [157] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO用凸二次惩罚替代启发式裁剪，解决了强化学习后训练中的梯度消失、奖励攻击和训练不稳定问题


<details>
  <summary>Details</summary>
Motivation: 当前主流强化学习算法依赖裁剪机制，在大规模应用中存在零梯度区域、奖励攻击和训练不稳定等优化问题，需要更稳定的策略优化方法

Method: 提出无裁剪策略优化（CFPO），用基于总变差散度约束的凸二次惩罚替代启发式裁剪，得到处处可微的目标函数，无需硬边界即可实现稳定的策略更新

Result: 在推理任务中，CFPO与裁剪方法在下游基准测试中表现相当，同时扩展了稳定训练范围；在对齐任务中，CFPO缓解了冗长利用问题，减少了能力退化，同时保持了有竞争力的指令跟随性能

Conclusion: CFPO是LLM后训练中替代裁剪方法的有前景的即插即用方案，仅需一行代码修改且无需额外超参数

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [158] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: Sombrero是一种改进分层序列模型中边界学习的方法，通过边界富集度量B指导边界放置，使用置信对齐边界损失将边界导向预测困难位置，并通过置信加权平滑稳定边界学习。


<details>
  <summary>Details</summary>
Motivation: 分层序列模型通过学习的分段来压缩长字节序列以提高自回归建模效率，但现有方法难以定量评估和系统性地引导计算资源分配。边界放置的质量直接影响模型效率，需要更好的度量指标和引导机制。

Method: 1. 提出边界富集度量B，衡量块起始位置在具有高下一字节惊奇度位置上的集中程度；2. 提出Sombrero方法：使用置信对齐边界损失将边界放置引导至预测困难位置；3. 在输入级别应用置信加权平滑而非在已实现块上进行，以稳定边界学习。

Result: 在1B规模上，在涵盖英语和德语文本、代码以及数学内容的UTF-8语料库上，Sombrero改善了准确率-效率权衡，产生的边界更一致地将计算资源与难以预测的位置对齐。

Conclusion: 边界富集度量B为评估边界质量提供了有用的工具，Sombrero方法通过引导边界放置至预测困难位置，能够更有效地分配计算资源，提高分层序列模型的效率-准确率平衡。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [159] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出MS-EDEN量化方法和Quartet II方案，在NVFP4格式上实现端到端量化预训练，相比随机舍入降低2倍量化误差，在Blackwell GPU上获得4.2倍加速。


<details>
  <summary>Details</summary>
Motivation: NVFP4格式首次支持大规模模型端到端量化预训练，但现有量化方法为获得无偏梯度估计而牺牲表示能力，导致精度损失。需要改进NVFP4量化训练效果。

Method: 提出MS-EDEN无偏量化方法，针对微尺度格式降低量化误差；开发Quartet II全NVFP4量化方案，优化线性层的前向和反向传播梯度估计；与NVFP4特定训练改进协同。

Result: MS-EDEN比随机舍入降低2倍以上量化误差；Quartet II在所有主要矩阵乘法中实现更优梯度估计；在1.9B参数、38B tokens的LLM训练中验证；Blackwell GPU上获得4.2倍加速。

Conclusion: Quartet II方案显著提升NVFP4量化训练精度和效率，为大规模模型端到端量化预训练提供有效解决方案，在保持硬件加速优势的同时减少精度损失。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [160] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 提出一种级联扩散模型用于表格数据生成，通过低分辨率表示处理混合类型特征（离散状态与连续分布结合），显著提升生成质量


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在处理表格数据中混合类型特征（离散状态与连续分布结合）时面临挑战，需要更准确的方法来生成包含缺失值、膨胀值等离散结果的数值特征

Method: 采用级联方法：首先生成低分辨率版本（纯分类特征+数值特征的粗粒度分类表示），然后通过新颖的引导条件概率路径和数据依赖耦合，在流匹配模型中利用此信息生成高分辨率数据

Result: 模型生成更真实的样本，更准确地捕捉分布细节，检测分数提高40%；理论证明该方法收紧传输成本界限

Conclusion: 级联扩散模型通过低分辨率表示显式处理数值特征的离散结果，实现了对表格数据中混合类型特征的更忠实生成，推进了表格数据扩散模型的技术前沿

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [161] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: MetaDrug：针对电子健康记录中患者冷启动问题的多层级不确定性感知元学习框架，通过自适应和同伴自适应机制提升新患者的药物推荐效果


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐方法面临患者冷启动问题，即新患者因缺乏足够的处方历史而导致推荐不可靠。虽然已有研究利用医学知识图谱连接药物概念，但这些方法主要解决项目冷启动问题，无法提供适应个体患者特征的个性化推荐。元学习在处理稀疏交互的新用户方面表现出潜力，但在电子健康记录中的应用尚未充分探索。

Method: 提出MetaDrug框架，包含：1）两级元适应机制：自适应（利用患者自身医疗事件作为支持集捕捉时间依赖性）和同伴自适应（利用相似患者的相似就诊记录丰富新患者表示）；2）不确定性量化模块：对支持就诊进行排序并过滤无关信息以保证适应一致性

Result: 在MIMIC-III和急性肾损伤（AKI）数据集上的实验结果表明，MetaDrug在冷启动患者上持续优于最先进的药物推荐方法

Conclusion: MetaDrug通过创新的元学习框架有效解决了电子健康记录中的患者冷启动问题，结合自适应和同伴自适应机制以及不确定性量化，显著提升了新患者的药物推荐性能

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [162] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出了一种新颖的持续学习框架，通过将LoRA模块重构为可分解的Rank-1专家池，实现动态稀疏组合和正交化，显著减少参数更新并缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中的持续学习面临任务适应性和灾难性遗忘的挑战。现有方法通常具有沉重的推理负担或依赖外部知识，而LoRA在减少这些问题方面显示出潜力，但直接使用LoRA缓解灾难性遗忘问题并不简单。

Method: 1. 将单个LoRA模块重构为可分解的Rank-1专家池；2. 基于[CLS]令牌的语义动态选择专家池中的组件，形成稀疏的任务特定更新；3. 提出激活引导正交(AGO)损失，使关键LoRA权重在不同任务间正交化；4. 实现稀疏组合和正交化，减少参数更新，实现领域感知学习。

Result: 在多个设置下的广泛实验显示，在所有指标上都达到了最先进的结果，超越了零样本泛化的上界。与基线方法相比，可训练参数减少了96.7%，消除了对外部数据集或任务ID判别器的依赖。合并的LoRA保留较少权重且不增加推理延迟。

Conclusion: 该方法通过可分解的Rank-1专家池和AGO损失，实现了参数高效、计算轻量的持续学习，有效缓解了灾难性遗忘问题，同时保持了任务性能，为视觉语言模型的持续学习提供了新的解决方案。

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [163] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 提出一种用于时间序列生成的潜在流匹配框架，通过正则化预训练自编码器来显式鼓励等变性，从而改善生成质量并保持高效采样优势


<details>
  <summary>Details</summary>
Motivation: 现有基于流的时间序列生成模型通常在低维潜在空间中定义以实现高效采样，但如何设计具有理想等变性特性的潜在表示用于时间序列生成建模尚未得到充分探索

Method: 提出潜在流匹配框架，通过引入等变性损失来正则化预训练自编码器，该损失强制变换信号与其重构之间的一致性，用于微调基本时间序列变换（如平移和幅度缩放）相关的潜在空间

Result: 在多个真实世界数据集上的实验表明，该方法在标准时间序列生成指标上持续优于现有基于扩散的基线方法，同时实现数量级更快的采样速度

Conclusion: 将几何归纳偏置纳入时间序列的潜在生成模型中具有实际优势，等变性正则化的潜在空间在保持潜在流模型计算优势的同时提高了生成质量

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [164] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OptiMAG是一个基于不平衡最优传输的正则化框架，用于解决多模态属性图中显式图结构与隐式语义结构之间的不一致问题，通过融合Gromov-Wasserstein距离引导跨模态结构一致性，提升节点表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 多模态属性图(MAGs)在建模复杂系统时，不同模态嵌入诱导的隐式语义结构与显式图结构之间存在不一致性。例如，显式图中的邻居节点可能在某个模态中相似，但在另一个模态中却相距甚远。现有方法通常在固定的显式图结构上进行消息传递，这会导致聚合不相似的特征，引入模态特定噪声，阻碍有效的节点表示学习。

Method: 提出OptiMAG框架，采用基于不平衡最优传输的正则化方法。使用融合Gromov-Wasserstein距离在局部邻域内显式引导跨模态结构一致性，有效缓解结构-语义冲突。同时引入KL散度惩罚来自适应处理跨模态不一致性。该框架可作为即插即用的正则化器无缝集成到现有的多模态图模型中。

Result: 实验表明，OptiMAG在多个任务上一致优于基线方法，包括图中心任务（如节点分类、链接预测）和多模态中心生成任务（如图到文本、图到图像生成）。

Conclusion: OptiMAG通过最优传输理论有效解决了多模态属性图中结构-语义不一致问题，提供了一种通用的正则化框架，能够提升现有多模态图模型的性能，在多种图相关任务上表现出色。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [165] [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)
*Zhanglu Yan,Kaiwen Tang,Zixuan Zhu,Zhenyu Bai,Qianhui Liu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Matterhorn是一种新型脉冲变压器，通过掩码时间到首次脉冲编码和忆阻突触单元，显著降低SNN推理的能耗，在保持准确性的同时实现2.31倍的能效提升。


<details>
  <summary>Details</summary>
Motivation: 现有SNN能效评估主要关注计算操作，忽略了数据移动等实际硬件成本（占总能耗近80%）。需要开发能同时减少脉冲移动和权重访问开销的SNN架构。

Method: 提出掩码时间到首次脉冲编码方法，通过重新分配零能量静默状态到最频繁的膜电位来减少脉冲移动；采用"死区"策略最大化稀疏性；在硬件层面使用忆阻突触单元实现内存内计算，消除权重访问成本。

Result: 在GLUE基准测试中，Matterhorn建立了新的最先进水平，平均准确率比现有SNN提高1.42%，同时能效提升2.31倍。

Conclusion: Matterhorn通过创新的编码方案和硬件协同设计，有效解决了SNN推理中的能量瓶颈问题，为能效型大语言模型推理提供了有前景的解决方案。

Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.

</details>


### [166] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL：一种新颖的多任务强化学习方法，能够实现零样本泛化，不仅能在LTL公式结构上组合泛化，还能在命题上参数化泛化


<details>
  <summary>Details</summary>
Motivation: 现有LTL引导的多任务RL方法虽然能在LTL规范间成功泛化，但无法泛化到未见过的命题词汇表（描述LTL中高层事件的"符号"），这限制了策略的通用性

Method: 将命题视为参数化谓词的实例而非离散符号，通过新颖的架构嵌入和组合谓词来表示LTL规范，使策略能够学习相关命题间的共享结构

Result: 在具有挑战性的环境中成功实现了对新颖命题和任务的零样本泛化

Conclusion: PlatoLTL通过参数化谓词表示方法，解决了多任务RL中命题词汇表泛化的关键挑战，显著提升了策略的通用性和泛化能力

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [167] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出一种基于正则化的多变量校准方法，使用预秩函数在训练期间强制多变量校准，并引入基于PCA的新预秩函数来检测依赖结构误设。


<details>
  <summary>Details</summary>
Motivation: 尽管单变量概率预测已取得进展，但实现多变量校准仍然具有挑战性。现有预秩函数主要用于事后评估，缺乏在训练期间强制多变量校准的方法。

Method: 提出基于正则化的校准方法，在训练多变量分布回归模型时使用预秩函数强制多变量校准；引入基于PCA的新预秩函数，将预测投影到预测分布的主方向上。

Result: 在模拟研究和18个真实世界多输出回归数据集上的实验表明，该方法显著改善了多变量预秩校准，且不损害预测准确性；PCA预秩能够检测出现有预秩无法发现的依赖结构误设。

Conclusion: 该方法为多变量概率预测提供了一种有效的训练时校准方法，PCA预秩函数能够揭示预测分布依赖结构的缺陷，为模型诊断和改进提供了新工具。

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [168] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 提出一种结合贝叶斯决策树与高斯过程叶节点的混合模型，解决传统决策树在回归任务中无法可靠外推和不确定性校准的问题。


<details>
  <summary>Details</summary>
Motivation: 传统决策树在回归任务中存在局限性：1）分段常数叶预测受限于训练目标范围，无法可靠外推；2）在分布偏移下容易变得过度自信；3）缺乏良好的不确定性校准。

Method: 1）单树贝叶斯模型扩展VSPYCT，每个叶节点配备高斯过程预测器；2）贝叶斯倾斜分割提供输入空间的不确定性感知划分；3）GP叶节点建模局部函数行为，支持有原则的外推；4）高效推理预测方案结合分割参数的后验采样与GP后验预测；5）门控机制在输入超出叶节点训练支持时激活基于GP的外推。

Result: 在基准回归任务实验中，相比标准变分倾斜树，该模型在预测性能上有所提升，在外推场景中表现出显著的性能增益。

Conclusion: 提出的贝叶斯决策树-GP混合模型有效解决了传统决策树在回归外推和不确定性校准方面的不足，通过不确定性感知划分和局部GP建模实现了更好的预测性能和外推能力。

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [169] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: FlexLoRA：基于熵引导的灵活低秩适应框架，通过谱能量熵评估矩阵重要性，支持在全局预算下进行秩剪枝和扩展，使用零影响初始化确保稳定性，解决了现有方法的粒度、灵活性和稳定性限制。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型在多个领域取得显著成功，但完全微调的计算和内存成本过高。参数高效微调（PEFT）成为主流范式。其中LoRA引入可训练低秩矩阵并表现出色，但其固定秩设计限制了灵活性。动态秩分配方法通过剪枝冗余方向缓解此问题，但通常依赖启发式的元素级指标，缺乏矩阵级区分，且没有机制在需要额外适应的层中扩展容量。

Method: 提出FlexLoRA框架：1）通过谱能量熵评估矩阵重要性；2）在全局预算下支持秩剪枝和扩展；3）对新添加的奇异方向使用零影响初始化以确保稳定性。该方法解决了现有方法的粒度、灵活性和稳定性限制。

Result: 大量实验表明，FlexLoRA在多个基准测试中始终优于最先进的基线方法。

Conclusion: FlexLoRA通过熵引导的灵活低秩适应框架，为参数高效微调提供了更原则性的解决方案，解决了现有方法在粒度、灵活性和稳定性方面的限制。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [170] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 提出DC-LA算法用于采样目标分布π∝exp(-f-r)，其中r是非光滑的DC函数，通过Moreau包络平滑r并重新分配凹部分到数据保真项，在q-Wasserstein距离下建立收敛性


<details>
  <summary>Details</summary>
Motivation: 研究非光滑DC正则化项的采样问题，传统方法难以处理非光滑r=r1-r2结构，需要开发能利用DC结构的新算法

Method: 利用r的DC结构，对r1和r2分别应用Moreau包络进行平滑，将正则化项的凹部分重新分配到数据保真项，提出DC-LA（近端Langevin算法）

Result: 在V是距离耗散的假设下，DC-LA在q-Wasserstein距离（对所有q∈ℕ*）收敛到目标分布π（达到离散化和平滑误差），改进了非对数凹采样的现有结果

Conclusion: DC-LA算法能有效处理非光滑DC正则化项的采样问题，在合成和真实CT应用中均表现良好，为不确定性量化提供了可靠工具

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [171] [Scalable Topology-Preserving Graph Coarsening with Graph Collapse](https://arxiv.org/abs/2601.22943)
*Xiang Wu,Rong-Hua Li,Xunkai Li,Kangfei Zhao,Hongchao Qin,Guoren Wang*

Main category: cs.LG

TL;DR: 提出STPGC方法，通过图强坍缩和边坍缩概念实现可扩展的拓扑保持图粗化，在保持GNN性能的同时降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有图粗化方法主要保持谱或空间特征，而保持拓扑特征的方法虽然能维持GNN预测性能，但存在指数级时间复杂度的缺陷。

Method: 引入代数拓扑中的图强坍缩和图边坍缩概念，提出三种新算法：GStrongCollapse、GEdgeCollapse和NeighborhoodConing，消除支配节点和边的同时严格保持拓扑特征。

Result: STPGC能保持GNN感受野，通过近似算法加速GNN训练。节点分类实验证明了STPGC的效率和有效性。

Conclusion: STPGC解决了拓扑保持图粗化的可扩展性问题，在保持GNN性能的同时显著降低了计算复杂度，为大规模图处理提供了有效解决方案。

Abstract: Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.

</details>


### [172] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: ECTR提出环境条件尾部重加权方法，将TV不变风险最小化与环境内样本异质性处理相结合，提升混合分布偏移下的OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有IRM方法主要处理环境层面的虚假相关性，但忽略了环境内样本级异质性（如罕见或困难样本），这种异质性对OOD性能有重要影响。需要同时处理跨环境的相关性偏移和环境内的多样性偏移。

Method: 提出环境条件尾部重加权TV不变风险最小化(ECTR)框架：1) 基于总变差的不变学习处理环境级相关性偏移；2) 环境条件尾部重加权处理环境内样本异质性；3) 通过极小极大公式推断潜在环境，扩展至无显式环境标注场景。

Result: 在回归、表格数据、时间序列和图像分类基准测试中，ECTR在混合分布偏移下均取得一致改进，提升了最差环境和平均OOD性能。

Conclusion: ECTR通过统一框架同时处理环境级相关性偏移和样本级异质性，使两种机制在混合分布偏移下互补，显著提升OOD泛化能力，且可扩展至无显式环境标注场景。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [173] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR: 论文证明困惑度作为模型选择指标存在理论缺陷：如果紧凑型仅解码器Transformer模型能准确预测某些序列，则必然存在另一些困惑度很低但模型预测错误的序列，且困惑度不一定选择更准确的模型。


<details>
  <summary>Details</summary>
Motivation: 困惑度作为衡量模型质量的简单计算指标近年来获得广泛应用，但先前研究主要从经验角度指出其局限性。本文旨在从理论层面严格证明困惑度作为模型选择指标的不适性。

Method: 利用Transformer连续性理论结果进行严格数学证明：1）证明如果紧凑型仅解码器Transformer模型能准确且自信地预测某些序列，则必然存在另一些困惑度很低但模型预测错误的序列；2）通过分析等困惑度图，研究困惑度与模型选择的关系。

Result: 理论证明困惑度存在固有缺陷：1）存在"假阳性"序列（低困惑度但预测错误）；2）困惑度不一定选择更准确的模型，只有当模型置信度提升伴随相应准确率提升时，新模型才会被选择。

Conclusion: 困惑度作为模型选择指标在理论上存在严重问题，不能可靠地反映模型的实际预测能力，需要更谨慎地使用或寻找替代指标。

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [174] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 该论文解决了线性bandit中Nash regret的次优性问题，提出了新的分析工具实现最优Nash regret边界，并首次研究了p-means regret框架，提出了通用的FairLinBandit算法框架。


<details>
  <summary>Details</summary>
Motivation: 现有线性bandit中的Nash regret结果存在次优性，源于依赖限制性的集中不等式证明技术。需要新的分析工具来解决这一开放问题，并扩展研究更一般的p-means regret框架，统一公平性和效用目标。

Method: 提出了新的分析工具来解决Nash regret的次优性问题；引入了p-means regret框架，统一公平性和效用目标；提出了通用的FairLinBandit算法框架，可作为任何线性bandit策略的元算法；使用Phased Elimination和Upper Confidence Bound两种bandit算法实例化该框架。

Result: 实现了线性bandit中Nash regret的最优边界；证明了两种算法实例在整个p值范围内都能实现亚线性p-means regret；在真实世界数据集生成的线性bandit实例上的大量实验表明，该方法始终优于现有最先进基线。

Conclusion: 该工作解决了线性bandit中Nash regret的开放问题，提出了新的分析工具和通用的算法框架，扩展了公平性感知的bandit研究，为统一公平性和效用目标提供了理论基础和实用算法。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [175] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出一种端到端可学习的置换框架，通过可学习的置换成本矩阵、可微分的二分图匹配求解器和稀疏优化损失函数，优化Transformer模型的结构化稀疏化权重排列。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏化已成为流行的模型剪枝技术，权重置换能进一步改善剪枝后性能。然而，Transformer架构规模导致置换搜索空间指数增长，现有方法依赖贪心或启发式算法，限制了重新排序的有效性。

Method: 提出端到端可学习的置换框架：1) 引入可学习的置换成本矩阵量化权重矩阵任意两个输入通道交换的成本；2) 使用可微分的二分图匹配求解器根据成本矩阵获得最优二元置换矩阵；3) 设计稀疏优化损失函数直接优化置换算子。

Result: 在视觉和语言Transformer上广泛验证，方法在结构化稀疏化方面实现了最先进的置换结果。

Conclusion: 提出的端到端可学习置换框架能有效解决大规模Transformer架构中权重置换的搜索空间问题，为结构化稀疏化提供了更优的权重重新排序方案。

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [176] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: dgMARK是一种用于离散扩散语言模型（dLLMs）的解码引导水印方法，通过引导解掩码顺序实现水印嵌入，无需显式重新加权模型概率。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型（dLLMs）可以按任意顺序生成标记，而实际dLLMs对解掩码顺序表现出强烈敏感性，这为水印技术提供了新的通道。

Method: dgMARK引导解掩码顺序朝向满足二进制哈希诱导的简单奇偶约束的高奖励候选标记位置，无需显式重新加权模型学习概率。该方法可与常见解码策略（如置信度、熵和基于边界的排序）即插即用，并可通过一步前瞻变体增强。

Result: 水印通过提升的奇偶匹配统计量检测，滑动窗口检测器确保在插入、删除、替换和改写等后编辑操作下的鲁棒性。

Conclusion: dgMARK为离散扩散语言模型提供了一种有效的解码引导水印方法，利用模型对解掩码顺序的敏感性实现水印嵌入，具有鲁棒性和实用性。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [177] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: VaR-CPO算法：一种直接优化风险价值约束的样本高效保守方法，在可行环境中实现训练期间零约束违反，通过切比雪夫不等式处理VaR不可微性，扩展CPO信任域框架提供严格的最坏情况边界。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法在训练过程中保证零约束违反，特别是在需要安全探索的场景中。风险价值约束的直接优化面临非可微性挑战，需要开发既能保证安全性又具有理论保证的算法。

Method: 1. 使用单边切比雪夫不等式将非可微的VaR约束转化为基于成本回报一阶和二阶矩的可处理替代约束；2. 扩展约束策略优化的信任域框架，确保策略改进和约束违反的严格边界；3. 设计样本高效的保守算法，在可行环境中实现训练期间零约束违反。

Result: 1. 实证表明VaR-CPO能够进行安全探索，在可行环境中实现训练期间零约束违反，而基线方法无法保持这一关键特性；2. 算法提供了策略改进和约束违反的最坏情况理论边界；3. 通过可处理的替代约束克服了VaR约束的非可微性问题。

Conclusion: VaR-CPO是一种有效解决风险价值约束优化问题的方法，通过理论保证和实证验证，在安全强化学习领域提供了既能保证安全性又具有样本效率的解决方案，特别适用于需要严格安全约束的应用场景。

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [178] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: 提出Mano优化器，通过将动量投影到参数切空间并约束在旋转斜流形上，首次弥合了流形优化与现代优化器之间的性能差距，在LLaMA和Qwen3模型上优于AdamW和Muon，且内存和计算复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 当前主流优化器存在局限性：AdamW依赖对角曲率估计而忽略结构特性，Muon应用全局谱归一化但损失曲率信息。传统流形优化方法在大规模模型优化中表现不佳，需要开发能结合两者优势的新方法。

Method: 提出Mano优化器，创新性地将动量投影到模型参数的切空间，并将其约束在旋转斜流形上。这种方法结合了流形优化的结构特性和现代优化器的效率，是首个弥合流形优化与现代优化器性能差距的方法。

Result: 在LLaMA和Qwen3模型上的大量实验表明，Mano在性能上持续且显著优于AdamW和Muon，同时分别具有更低的内存消耗和计算复杂度，扩展了空间和时间效率的帕累托前沿。

Conclusion: Mano优化器成功解决了现有优化器的局限性，通过流形优化方法在大语言模型训练中实现了更好的性能与效率平衡，为大规模模型优化提供了新的有效途径。

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [179] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出Continuous Constraint Interpolation (CCI)统一框架，将离线RL中的三种约束方法（加权行为克隆、密度正则化、支持约束）统一为连续约束谱系，并开发自动约束策略优化(ACPO)算法，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法通常采用单一约束形式（加权行为克隆、密度正则化或支持约束），缺乏统一的原理来解释它们之间的联系和权衡，限制了方法的灵活性和性能。

Method: 提出Continuous Constraint Interpolation (CCI)统一优化框架，通过单个插值参数实现三种约束类型间的平滑过渡和组合。基于CCI开发Automatic Constraint Policy Optimization (ACPO)原始-对偶算法，通过拉格朗日对偶更新自适应调整插值参数。建立最大熵性能差异引理，推导闭式最优策略及其参数投影的性能下界。

Result: 在D4RL和NeoRL2基准测试中表现出稳健的性能提升，实现了整体最先进的性能，验证了CCI框架的有效性和ACPO算法的优越性。

Conclusion: CCI框架为离线强化学习中的约束方法提供了统一的理论基础，ACPO算法通过自适应约束插值实现了跨约束类型的灵活组合，为离线RL的约束设计提供了新的思路和方法。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [180] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 提出使用仅两个sEMG通道的深度学习框架，通过卷积稀疏自编码器提取特征，实现高精度手势识别，并采用少样本迁移学习和增量学习解决个体差异和功能扩展问题。


<details>
  <summary>Details</summary>
Motivation: 传统肌电假肢控制面临高个体间变异性和高密度传感器阵列临床不实用的问题，需要开发仅使用少量传感器通道的高精度、可扩展解决方案。

Method: 使用卷积稀疏自编码器直接从原始sEMG信号提取时域特征，避免启发式特征工程；采用少样本迁移学习协议处理个体差异；通过增量学习策略支持功能扩展。

Result: 6类手势识别达到94.3%±0.3%的F1分数；少样本迁移学习将未见受试者性能从35.1%±3.1%提升至92.3%±0.9%；增量学习扩展到10类手势仍保持90.0%±0.2%的F1分数。

Conclusion: 该框架结合高精度、低计算和传感器开销，为下一代经济、自适应的假肢系统提供了可扩展且高效的解决方案。

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [181] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 提出一个因果模型，将异常分为测量误差和机制偏移两类，通过潜在干预建模实现根因定位和异常类型分类


<details>
  <summary>Details</summary>
Motivation: 现有异常根因分析方法忽略了一个关键区别：异常可能源于两种根本不同的过程——测量误差（数据生成正常但记录错误）和机制偏移（数据生成过程本身发生变化）。测量误差通常可以安全修正，而机制异常需要仔细考虑。

Method: 定义了一个因果模型，通过将异常视为对潜在变量（"真实"变量）和观测变量（"测量"变量）的潜在干预来显式捕捉两种异常类型。证明了这两种异常的可识别性，并提出最大似然估计方法将其应用于实践。

Result: 实验表明，该方法在根因定位方面达到最先进性能，同时还能准确分类异常类型，即使在因果DAG未知的情况下也能保持鲁棒性。

Conclusion: 通过显式区分测量误差和机制偏移，提出的因果模型不仅改进了异常根因分析，还提供了对异常本质的深入理解，增强了异常分析的实用性和可靠性。

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [182] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出DC-CoT方法，通过并行推理减少长思维链的延迟，在保持准确率的同时将最长路径长度降低35-40%


<details>
  <summary>Details</summary>
Motivation: 长思维链推理（Long CoT）是现代LLMs的核心能力，尤其在数学推理中至关重要。然而，LLM生成是高度顺序化的，长CoT会导致高延迟。需要一种方法来降低延迟同时保持高准确率。

Method: 提出Divide-and-Conquer CoT（DC-CoT）方法：模型作为导演识别可并行执行的子任务，然后生成工作线程执行这些子任务。采用多阶段强化学习算法：1）使用小规模精选演示集进行SFT初始化；2）设计多阶段RL算法配合数据过滤策略，在降低最长路径长度的同时恢复准确率。

Result: 在AIME 2024和HMMT 2025等多个基准测试中，DC-CoT在保持与DeepScaleR-1.5B-Preview相似准确率的同时，将最长路径长度降低了35-40%。

Conclusion: DC-CoT通过并行推理有效降低了长思维链的延迟，在保持准确率的同时显著减少了推理时间，为低延迟CoT推理提供了实用解决方案。

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [183] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 论文分析了可微分匹配层中离散排列恢复不稳定的根本原因——过早模式崩溃，提出了基于热力学速度限制的自适应调度算法Efficient PH-ASC，将计算开销从O(N³)降低到摊销O(1)。


<details>
  <summary>Details</summary>
Motivation: 可微分匹配层（通常通过熵正则化最优传输实现）在结构化预测中作为关键近似推理机制，但通过退火ε→0恢复离散排列的过程极不稳定。需要识别这种失败的根本机制并设计解决方案。

Method: 通过分析Sinkhorn固定点映射的非正规动力学，揭示了热力学速度限制理论。提出Efficient PH-ASC自适应调度算法，监控推理过程的稳定性，通过强制线性稳定性定律，将昂贵的谱诊断与训练循环解耦。

Result: 识别了过早模式崩溃的根本机制：在标准指数冷却下，目标后验的偏移(O(1))超过了推理算子的收缩率(O(1/ε))，导致推理轨迹陷入虚假局部盆地。提出的算法将计算开销从O(N³)降低到摊销O(1)。

Conclusion: 论文揭示了可微分匹配层中离散排列恢复不稳定的热力学机制，提出了高效的自适应调度解决方案，为结构化预测中的稳定推理提供了理论基础和实用工具。

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [184] [Adaptive Edge Learning for Density-Aware Graph Generation](https://arxiv.org/abs/2601.23052)
*Seyedeh Ava Razi Razavi,James Sargant,Sheridan Houghten,Renata Dividino*

Main category: cs.LG

TL;DR: 提出基于Wasserstein GAN的密度感知条件图生成框架，用可学习的基于距离的边预测器替代随机采样，生成具有类一致连接模式的真实图结构数据。


<details>
  <summary>Details</summary>
Motivation: 传统图生成方法难以处理离散结构、可变大小和类特定连接模式，现有基于GAN的方法通常依赖固定概率的随机边采样，限制了捕捉复杂节点间结构依赖的能力。

Method: 使用Wasserstein GAN框架，将节点嵌入到潜在空间（接近度与边概率相关），通过可微边预测器从节点嵌入直接确定成对关系，采用密度感知选择机制自适应控制边密度以匹配真实图的类特定稀疏分布，使用基于GCN的判别器确保生成图具有真实拓扑结构。

Result: 在基准数据集上的实验表明，该方法生成的图在结构连贯性和类一致连接性方面优于现有基线，学习的边预测器能捕捉超越简单启发式的复杂关系模式，生成的图在密度和拓扑结构上更接近真实分布，训练稳定性更好且合成可控。

Conclusion: 提出的密度感知条件图生成框架能有效生成真实图结构数据，适用于数据增强等应用，通过可学习的边预测器和自适应密度控制机制显著提升了图生成的质量和可控性。

Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.

</details>


### [185] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 提出RLRR框架，将强化学习奖励从绝对评分转向相对排名，解决群体方法中绝对奖励的稀疏性和不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的强化学习方法（如GRPO）依赖绝对数值奖励存在固有局限：在可验证任务中，相同群体评估导致监督稀疏；在开放场景中，奖励模型分数范围不稳定影响基于群体均值的优势估计

Method: 提出RLRR框架，将奖励塑造从绝对评分转向相对排名；引入Ranking Reward Model，这是一个专门为群体优化设计的列表式偏好模型，可直接生成相对排名

Result: 实验结果表明，RLRR在推理基准测试和开放生成任务中，相比标准基于群体的基线方法，能够带来一致的性能提升

Conclusion: 通过将原始评估转化为稳健的相对信号，RLRR有效缓解了信号稀疏性和奖励不稳定性问题，为基于群体的强化学习提供了更有效的优化框架

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [186] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: SplineFlow提出了一种基于B样条插值的流匹配算法，用于更好地建模动态系统，特别是从非规则采样观测中学习高阶动态。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法不适合建模动态系统，因为它们使用线性插值构建条件路径，无法准确捕捉底层状态演化，特别是在从非规则采样观测中学习高阶动态时。构建满足多边际约束的统一路径具有挑战性，因为简单的高阶多项式往往不稳定且振荡。

Method: SplineFlow利用B样条插值的平滑性和稳定性，通过B样条基函数联合建模跨观测的条件路径，以结构化方式学习复杂底层动态，同时确保满足多边际要求。

Result: 在各种确定性和随机动态系统以及细胞轨迹推断任务上的综合实验表明，SplineFlow相比现有基线方法有显著改进。

Conclusion: SplineFlow是一种理论基础的流匹配算法，通过B样条插值有效解决了动态系统建模中的路径构建问题，特别适用于从非规则采样观测中学习高阶动态的场景。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [187] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: 提出CATTO校准感知训练目标，解决LLM置信度校准问题，在保持任务准确性的同时显著降低校准误差，并引入Confidence@k测试时缩放机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能做出准确的下一词预测，但其置信度校准存在问题：高置信度预测经常错误，低置信度预测反而可能正确。基于偏好的对齐方法进一步破坏了预测概率与正确性之间的联系。

Method: 提出校准感知的token级训练目标CATTO，将预测置信度与经验预测正确性对齐，可与原始偏好优化目标结合使用。同时引入Confidence@k测试时缩放机制，利用校准后的token概率进行贝叶斯最优输出token选择。

Result: 相比直接偏好优化，CATTO在分布内将预期校准误差降低2.22%-7.61%，在分布外降低1.46%-10.44%；相比最强DPO基线，在分布内降低0.22%-1.24%，在分布外降低1.23%-5.07%。置信度改进不损失任务准确性，在五个数据集上保持或略微提高多项选择题回答准确率。

Conclusion: CATTO能有效解决LLM置信度校准问题，在保持任务性能的同时显著改善校准质量，为更可靠的LLM置信度估计提供了实用解决方案。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [188] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 论文提出进化预测（EF）范式，解决直接预测（DF）在长期时间序列预测中的优化异常问题，通过短时域训练结合EF显著超越直接长时域训练，实现单一模型适应多预测时域。


<details>
  <summary>Details</summary>
Motivation: 直接预测范式在长期时间序列预测中占主导地位，但其将输出与评估时域刚性耦合，导致每次改变目标时域都需要重新训练，计算成本高昂。作者发现了一个反直觉的优化异常：在短时域上训练的模型结合EF范式能显著优于直接在长时域上训练的模型。

Method: 提出进化预测（EF）作为统一的生成框架，证明DF只是EF的一个退化特例。EF通过缓解DF中存在的根本优化病理——来自遥远未来的冲突梯度会破坏局部动态的学习，使模型能够从短时域训练中有效扩展到长时域预测。

Result: 大量实验表明，单一的EF模型在标准基准测试中超越了任务特定的DF集成模型，并在极端外推中表现出鲁棒的渐近稳定性。EF实现了从被动静态映射到自主进化推理的范式转变。

Conclusion: 这项工作推动了长期时间序列预测的范式转变：从被动的静态映射转向自主的进化推理。EF框架不仅解决了DF的计算效率问题，还通过缓解优化病理实现了更好的预测性能。

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [189] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: DCR提出了一种基于分布信息的保形排序方法，通过精确推导非一致性分数的分布来生成更高效的预测集，相比现有方法平均减少36%的预测集大小，同时保持有效覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于保形预测的排序方法依赖非一致性分数的上界，导致预测集过于保守且规模过大，需要更高效的方法来量化排序模型的不确定性。

Method: 提出分布信息保形排序(DCR)，推导校准项目绝对排名的负超几何分布，基于相对排名条件推导非一致性分数分布，从而确定保形阈值。

Result: DCR在理论上保证比基线方法更高的效率，同时满足温和假设下的有效覆盖率。实验表明平均预测集大小减少高达36%，同时保持有效覆盖率。

Conclusion: DCR通过精确建模非一致性分数分布，显著提高了保形排序的效率，为排序模型在现实应用中的安全部署提供了更实用的不确定性量化方法。

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [190] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 提出一种约束感知的数据扰动方法，解决生成模型在等式约束分布建模中的数学局限性问题


<details>
  <summary>Details</summary>
Motivation: 生成模型在科学领域等式中约束分布建模时存在固有数学局限性，需要一种计算廉价、数学合理且灵活的分布修正方法

Method: 提出约束感知的数据扰动方法，通过扰动数据分布使其支撑集匹配环境空间维度，同时隐式包含底层流形几何结构

Result: 该方法在多个代表性任务中能够一致实现数据分布恢复和稳定采样，适用于扩散模型和标准化流模型

Conclusion: 提出的约束感知扰动方法有效解决了等式约束生成模型的已知缺陷，具有计算廉价、数学合理和高度灵活的优点

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [191] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 该研究使用深度强化学习从ICU数据中学习镇痛镇静药物剂量策略，发现仅优化短期疼痛控制目标的策略与死亡率正相关，而同时优化疼痛和死亡率的策略与死亡率负相关，表明考虑长期结局对安全治疗至关重要。


<details>
  <summary>Details</summary>
Motivation: ICU疼痛管理需要在治疗效果和患者安全之间进行复杂权衡，现有研究通常优化不重视患者生存的目标，且算法不适合不完全信息环境。本研究旨在探讨这些设计选择的风险，开发更安全的药物剂量策略。

Method: 使用深度强化学习框架，在部分可观测环境下制定每小时药物剂量建议。基于MIMIC-IV数据库中47,144例ICU住院数据，训练两种策略：1)仅减少疼痛；2)联合减少疼痛和死亡率。药物包括阿片类、丙泊酚、苯二氮䓬类和右美托咪定。

Result: 两种策略均与较低的疼痛相关，但仅优化疼痛的策略与死亡率呈正相关，而联合优化疼痛和死亡率的策略与死亡率呈负相关。这表明重视长期结局对制定更安全的治疗策略至关重要。

Conclusion: 即使短期目标是主要关注点，在强化学习策略中考虑长期结局（如死亡率）对于开发更安全的ICU疼痛管理方案至关重要。仅优化短期疼痛控制可能导致不良的长期后果。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [192] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 提出一种基于语法的无监督技能分割和层次结构发现方法，可在像素环境中自动发现可重用技能及其层次关系


<details>
  <summary>Details</summary>
Motivation: 现有技能分割方法大多依赖动作标签、奖励或人工标注，限制了其应用范围，需要一种完全无监督的方法来发现轨迹中的技能和层次结构

Method: 使用基于语法的方法对无标签轨迹进行技能分割，并诱导出层次结构，该方法能捕捉低层行为及其组合成高层技能的过程

Result: 在Craftax和完整版Minecraft等高维像素环境中，该方法在技能分割、重用和层次质量指标上均优于现有基线，产生更具结构性和语义意义的层次

Conclusion: 发现的层次结构能加速和稳定下游强化学习任务的学习，证明了该方法的实用价值

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [193] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: 该研究提出了一种系统探测大语言模型推理轨迹的协议，通过截断推理轨迹并重新注入模型来测量答案选择分布，发现准确率和决策确定性随推理token比例增加而提升，且主要由相关内容而非长度或风格效应驱动。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型通过生成"推理轨迹"来解决复杂问题，但尚不清楚准确率和决策确定性如何沿推理轨迹演变，以及中间轨迹片段是否提供超出通用长度或风格效应的答案相关信息。

Method: 提出系统探测推理轨迹的协议：1)生成模型的推理轨迹；2)在固定token百分位数处截断；3)将每个部分轨迹重新注入模型（或不同模型），通过下一个token概率测量诱导的答案选择分布。应用于Qwen3-4B/-8B/-14B和gpt-oss-20b/-120b模型，在GPQA Diamond和MMLU-Pro基准测试上进行评估。

Result: 准确率和决策确定性随着提供的推理token比例增加而一致提升。这些增益主要由模型生成中的相关内容驱动，而非上下文长度或通用"推理风格"效应。更强的模型通常能从错误的部分轨迹成功回溯，但即时答案往往仍锚定在较弱模型的错误响应中。

Conclusion: 轨迹探测为推理模型的高效和安全部署提供了诊断工具，测量结果可以指导实用的轨迹处理和监控策略，在不假设中间token是固有忠实解释的情况下提高可靠性。

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [194] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 该论文研究了带参数噪声的随机线性老虎机问题，提出了紧致的遗憾上下界，并展示了在特定动作集上简单探索-利用算法能达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究参数噪声模型下的随机线性老虎机问题，该模型中奖励函数为a⊤θ，其中θ是从某个分布中独立同分布采样的。与经典加性噪声模型不同，参数噪声模型中的噪声与动作相关，这导致了不同的遗憾界限和算法设计挑战。

Method: 1. 针对一般动作集（大小为K，维度为d），提出了遗憾上界分析；2. 提供了匹配的下界证明；3. 对于特定动作集（ℓp单位球，p≤2，对偶范数为q），推导了极小极大遗憾界限；4. 展示了简单的探索-利用算法能达到最优遗憾界限。

Result: 1. 一般动作集的遗憾上界为Õ(√(dT log(K/δ)σ²_max))，下界为Ω̃(d√(Tσ²_max))，当log(K)≈d时上下界紧致；2. ℓp单位球动作集的极小极大遗憾为Θ̃(√(dTσ²_q))，其中σ²_q≤4；3. 简单探索-利用算法能达到最优遗憾界限，这与经典加性噪声模型需要d√T遗憾形成对比。

Conclusion: 参数噪声模型下的随机线性老虎机具有与经典加性噪声模型不同的理论性质：1）在特定动作集上能达到更好的遗憾界限；2）简单算法即可达到最优性能；3）遗憾界限依赖于方差相关的量σ²_q而非维度d。这揭示了参数噪声模型的独特理论特征和算法设计优势。

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [195] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: DyCAST是一种动态字符对齐的语音分词器，通过软字符级对齐和显式时长建模实现可变帧率分词，相比固定帧率编解码器显著减少token数量。


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器通常以固定帧率运行，在时间上均匀分配token，产生不必要的长序列，这限制了其在LLM处理中的效率。

Method: DyCAST通过软字符级对齐和显式时长建模实现可变帧率分词，学习将token与字符级语言单元关联，并引入检索增强解码机制以提高低帧率下的语音重合成质量。

Result: 实验表明，DyCAST在保持竞争性语音重合成质量和下游性能的同时，使用的token数量显著少于固定帧率编解码器。

Conclusion: DyCAST通过动态字符对齐和可变帧率分词，为对话语音技术提供了更高效的离散表示方法，同时通过检索增强解码保持了低比特率下的重建保真度。

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [196] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: MeshGraphNet-Transformer (MGN-T) 结合Transformer的全局建模能力和MeshGraphNets的几何归纳偏置，解决了标准MGN在大规模高分辨率网格上长程信息传播效率低的问题，实现了工业级网格的高效物理模拟。


<details>
  <summary>Details</summary>
Motivation: 标准MeshGraphNets (MGN) 在大规模高分辨率网格上存在关键限制：基于迭代消息传递的长程信息传播效率低下，这限制了其在工业级物理模拟中的应用。

Method: 提出MeshGraphNet-Transformer (MGN-T) 架构，结合物理注意力Transformer作为全局处理器，同时更新所有节点状态并显式保留节点和边属性。该方法直接捕获长程物理相互作用，无需深度消息传递堆栈或分层粗化网格。

Result: MGN-T成功处理工业级冲击动力学网格，准确模拟自接触、塑性和多变量输出（包括内部现象学塑性变量）。在经典基准测试中优于最先进方法，精度更高且保持实际效率，仅需竞争基线参数的一小部分。

Conclusion: MGN-T通过结合Transformer的全局建模能力和MeshGraphNets的几何归纳偏置，解决了标准MGN的长程信息传播限制，实现了工业级高分辨率网格的高效物理模拟，为复杂物理系统建模提供了有效解决方案。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [197] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 该研究提出了一种利用精细交通数据预测超本地化空气质量的创新方法，通过将彩色交通图转换为同心环描述符，结合偏最小二乘回归模型，在墨西哥城实现了动态空气质量预测。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染是全球性问题，交通是主要污染源。现有空气质量监测和预报在时空上较为粗糙，而实时交通数据通常更精细且公开可用。研究旨在利用精细交通数据提供超本地化、动态的空气质量预测。

Method: 开发创新方法将简单的彩色编码交通图转换为基于同心环的描述符，以改进交通状况表征。使用偏最小二乘回归（PLSR）基于新定义的交通强度预测污染水平。通过不同训练样本优化模型以获得最佳预测性能。

Result: 模型成功预测了基于交通强度的污染水平，通过优化训练样本获得了最佳预测性能，并深入理解了污染物与交通之间的关系。工作流程简单且可适应其他城市环境。

Conclusion: 研究表明利用精细交通数据可以有效预测超本地化空气质量，提出的方法具有创新性和可扩展性，为其他城市提供了可借鉴的空气质量预测框架。

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [198] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 该论文研究众包标注聚合中的公平性问题，提出了在ε-公平性框架下分析多数投票和最优贝叶斯聚合的公平性差距，并开发了强制执行严格人口统计均等约束的后处理算法。


<details>
  <summary>Details</summary>
Motivation: 获取可靠真实标注成本高昂或不可行，通常采用众包和噪声人工标注聚合。但聚合主观标注可能放大个体偏见，特别是在敏感特征方面，引发公平性担忧。然而众包聚合的公平性研究仍很缺乏，现有方法缺乏收敛保证，且仅有有限的后处理方法在人口统计均等下强制执行ε-公平性。

Method: 1. 在ε-公平性框架下分析多数投票和最优贝叶斯聚合的公平性差距；2. 在小众包规模下推导多数投票公平性差距的上界；3. 证明聚合共识的公平性差距在可解释条件下以指数速度收敛到真实标注的公平性差距；4. 将最先进的多类公平性后处理算法从连续设置推广到离散设置，强制执行严格人口统计均等约束。

Result: 1. 推导了多数投票公平性差距的上界；2. 证明了聚合共识公平性差距的指数收敛性；3. 开发了适用于离散设置的公平性后处理算法；4. 在合成和真实数据集上的实验验证了方法的有效性并证实了理论见解。

Conclusion: 该研究填补了众包聚合公平性分析的空白，提供了理论保证和实用算法，为解决众包标注中的公平性问题提供了系统框架，特别是在敏感特征可能引入偏见的情况下。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [199] [YuriiFormer: A Suite of Nesterov-Accelerated Transformers](https://arxiv.org/abs/2601.23236)
*Aleksandr Zimin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 该论文提出将Transformer层解释为对token嵌入进行优化的迭代算法，将自注意力视为交互能量的梯度步，MLP层视为势能的梯度更新，标准GPT风格Transformer对应复合目标的梯度下降。


<details>
  <summary>Details</summary>
Motivation: 为Transformer架构提供基于优化理论的统一解释框架，从而能够利用经典优化思想进行有原则的架构设计，改进现有模型性能。

Method: 提出变分框架，将Transformer层解释为优化算法迭代：自注意力对应交互能量的梯度步，MLP层对应势能的梯度更新。标准GPT Transformer被视为复合目标的梯度下降，通过Lie-Trotter分裂实现。基于此框架设计Nesterov风格加速Transformer。

Result: 在TinyStories和OpenWebText数据集上，Nesterov风格加速Transformer consistently优于nanoGPT基线，证明优化理论见解能够转化为实际性能提升。

Conclusion: Transformer层可以统一解释为优化算法迭代，这一优化理论视角为架构设计提供了原则性指导，并能实际改进模型性能，展示了理论洞察与工程实践的有效结合。

Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.

</details>


### [200] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: 该论文研究语言识别和生成任务，在完全放松"可实现性"假设的情况下，建立了这些任务的统计速率界限。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别和生成研究通常基于强可实现性假设：输入数据来自某个未知分布，且该分布必然支持给定语言集合中的某种语言。本文旨在放松这一假设，研究在更一般的"不可知"设置下的语言识别和生成问题。

Method: 提出在完全放松可实现性假设（不对输入数据分布施加任何限制）的情况下，研究语言识别和生成的目标函数。在不可知设置下分析这两个问题，获得新的特征描述。

Result: 在不可知设置下获得了语言识别和生成问题的新颖特征描述，并得到了近乎紧致的统计速率界限。

Conclusion: 通过完全放松可实现性假设，在更一般的不可知设置下建立了语言识别和生成的理论框架，获得了重要的理论结果和近乎最优的统计速率。

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [201] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: FOCUS是一个针对扩散大语言模型的推理系统，通过动态聚焦计算资源于可解码token并实时剔除不可解码token，显著提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）相比自回归模型具有优势，但其部署受到高解码成本的限制。研究发现DLLM解码存在关键低效问题：虽然计算在token块上并行化，但每个扩散步骤中只有一小部分token可解码，导致大部分计算浪费在不可解码token上。

Method: 提出FOCUS推理系统，基于注意力机制导出的token重要性与token解码概率之间的强相关性，动态聚焦计算资源于可解码token，实时剔除不可解码token，从而增加有效批处理大小，缓解计算限制。

Result: 实证评估表明，FOCUS相比生产级引擎LMDeploy实现了高达3.52倍的吞吐量提升，同时在多个基准测试中保持或改善了生成质量。

Conclusion: FOCUS系统通过解决DLLM解码中的计算浪费问题，显著提升了扩散大语言模型的推理效率，为DLLM的实际部署提供了可行的解决方案。

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [202] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出DDIS框架：解耦设计的扩散模型，用于PDE逆问题求解，具有数据高效性和物理感知能力


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散后验采样的方法通过联合建模系数-解来隐式表示物理，需要大量配对监督数据。当训练数据稀缺时，这些方法会出现指导衰减问题，限制了在数据有限场景下的应用。

Method: DDIS采用解耦设计：1）无条件扩散模型学习系数先验分布；2）神经算子显式建模前向PDE物理过程用于指导。同时提出DAPS采样策略避免DPS中的过平滑问题。

Result: 理论证明DDIS在训练数据稀缺时能避免联合模型的指导衰减问题。实验表明：在稀疏观测下，DDIS达到SOTA性能，平均提升l2误差11%、谱误差54%；数据限制到1%时，DDIS保持准确度，相比联合模型在l2误差上有40%优势。

Conclusion: DDIS通过解耦设计实现了数据高效的物理感知生成框架，在PDE逆问题求解中显著优于现有方法，特别是在数据稀缺场景下表现出色。

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [203] [Advanced techniques and applications of LiDAR Place Recognition in Agricultural Environments: A Comprehensive Survey](https://arxiv.org/abs/2601.22198)
*Judith Vilella-Cantos,Mónica Ballesta,David Valiente,María Flores,Luis Payá*

Main category: cs.RO

TL;DR: 本文是关于农业环境中基于LiDAR的定位与地点识别技术的综述，重点分析了深度学习在该领域的应用、面临的挑战以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 农业环境缺乏显著特征且结构非结构化，使得基于LiDAR的地点识别面临挑战。虽然LiDAR地点识别技术近年来在城市场景广泛应用，但在农业环境中的应用研究相对不足，需要系统性的综述来推动该领域发展。

Method: 采用文献综述方法，系统分析农业环境中基于LiDAR的定位技术，包括深度学习应用、现有方法、数据集和评估指标，特别关注农业环境特有的挑战。

Result: 这是首个专注于农业环境中基于LiDAR定位的综述，全面梳理了该领域的最新进展，识别了现有方法的局限性，并提出了未来研究方向。

Conclusion: 农业环境中的LiDAR定位技术具有重要应用价值但面临独特挑战，需要针对性的解决方案。该综述为该领域提供了系统性的理解框架，有助于推动精准农业中自主机器人系统的定位技术发展。

Abstract: An optimal solution to the localization problem is essential for developing autonomous robotic systems. Apart from autonomous vehicles, precision agriculture is one of the elds that can bene t most from these systems. Although LiDAR place recognition is a widely used technique in recent years to achieve accurate localization, it is mostly used in urban settings. However, the lack of distinctive features and the unstructured nature of agricultural environments make place recognition challenging. This work presents a comprehensive review of state-of-the-art the latest deep learning applications for agricultural environments and LPR techniques. We focus on the challenges that arise in these environments. We analyze the existing approaches, datasets, and metrics used to evaluate LPR system performance and discuss the limitations and future directions of research in this eld. This is the rst survey that focuses on LiDAR based localization in agricultural settings, with the aim of providing a thorough understanding and fostering further research in this specialized domain.

</details>


### [204] [ReloPush-BOSS: Optimization-guided Nonmonotone Rearrangement Planning for a Car-like Robot Pusher](https://arxiv.org/abs/2601.22289)
*Jeeho Ahn,Christoforos Mavrogiannis*

Main category: cs.RO

TL;DR: 提出ReloPush-BOSS框架，用于在密集杂乱环境中使用类车机器人推手进行多物体重排规划，通过优化预重定位和构建物体可穿越图来解决约束满足问题。


<details>
  <summary>Details</summary>
Motivation: 在密集杂乱环境中使用类车机器人推手进行多物体重排面临运动学、几何和物理约束的挑战，导致非单调问题实例需要将操作分解为多个部分。现有方法通过预重定位解决约束问题，但预重定位位置选择困难，容易陷入局部最优导致不可行或高成本路径。

Method: 提出ReloPush-BOSS框架：1) 基于Dubins路径分类引导预重定位优化，避免局部最优；2) 构建编码运动学、几何和推动约束的物体可穿越图；3) 采用深度优先搜索在图中寻找高效可行的重排序列。

Result: 在最多13个物体的密集杂乱场景测试中，ReloPush-BOSS相比现有方法具有最高的成功率和最短的推动路径。硬件实验在1/10比例类车推手上验证了方法的鲁棒性。

Conclusion: 通过优化预重定位和构建约束感知的物体可穿越图，ReloPush-BOSS能够有效解决密集杂乱环境中的多物体重排问题，在仿真和硬件实验中均表现出优越性能。

Abstract: We focus on multi-object rearrangement planning in densely cluttered environments using a car-like robot pusher. The combination of kinematic, geometric and physics constraints underlying this domain results in challenging nonmonotone problem instances which demand breaking each manipulation action into multiple parts to achieve a desired object rearrangement. Prior work tackles such instances by planning prerelocations, temporary object displacements that enable constraint satisfaction, but deciding where to prerelocate remains difficult due to local minima leading to infeasible or high-cost paths. Our key insight is that these minima can be avoided by steering a prerelocation optimization toward low-cost regions informed by Dubins path classification. These optimized prerelocations are integrated into an object traversability graph that encodes kinematic, geometric, and pushing constraints. Searching this graph in a depth-first fashion results in efficient, feasible rearrangement sequences. Across a series of densely cluttered scenarios with up to 13 objects, our framework, ReloPush-BOSS, exhibits consistently highest success rates and shortest pushing paths compared to state-of-the-art baselines. Hardware experiments on a 1/10 car-like pusher demonstrate the robustness of our approach. Code and footage from our experiments can be found at: https://fluentrobotics.com/relopushboss.

</details>


### [205] [Toward Fully Autonomous Driving: AI, Challenges, Opportunities, and Needs](https://arxiv.org/abs/2601.22927)
*Lars Ullrich,Michael Buchholz,Klaus Dietmayer,Knut Graichen*

Main category: cs.RO

TL;DR: 本文重新审视了完全自动驾驶，分析了AI在自动驾驶领域带来的挑战与机遇，识别了当前局限性和未来技术可能性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶向完全自主驾驶的过渡面临开放世界的复杂性和变化性挑战，而AI在自动驾驶领域显示出超越传统方法、处理更高复杂性和实现新自主水平的潜力，但同时也引发了安全性和可迁移性等新问题。

Method: 通过分析自动驾驶当前状态、概述局限性、识别可预见的技术可能性，并在前瞻性发展背景下考察各种挑战。

Result: 识别了AI在自动驾驶功能方面带来的挑战和机遇，重新考虑了完全自动驾驶在AI进展背景下的可行性，并明确了相应的需求和由此产生的研究问题。

Conclusion: AI为自动驾驶带来了突破传统方法限制的可能性，但同时也引入了新的安全性和可迁移性挑战；需要系统性地分析这些因素，以推动完全自动驾驶的实现。

Abstract: Automated driving (AD) is promising, but the transition to fully autonomous driving is, among other things, subject to the real, ever-changing open world and the resulting challenges. However, research in the field of AD demonstrates the ability of artificial intelligence (AI) to outperform classical approaches, handle higher complexities, and reach a new level of autonomy. At the same time, the use of AI raises further questions of safety and transferability. To identify the challenges and opportunities arising from AI concerning autonomous driving functionalities, we have analyzed the current state of AD, outlined limitations, and identified foreseeable technological possibilities. Thereby, various further challenges are examined in the context of prospective developments. In this way, this article reconsiders fully autonomous driving with respect to advancements in the field of AI and carves out the respective needs and resulting research questions.

</details>


### [206] [Lantern: A Minimalist Robotic Object Platform](https://arxiv.org/abs/2601.22381)
*Victor Nikhil Antony,Zhili Gong,Guanchen Li,Clara Jeon,Chien-Ming Huang*

Main category: cs.RO

TL;DR: Lantern是一个低成本、简约的机器人对象平台，旨在通过简单形式激发人类社交互动，降低人机交互研究门槛


<details>
  <summary>Details</summary>
Motivation: 设计一个低成本、简约的机器人对象平台，利用人类倾向于为简单形式赋予社交意义的特性，降低人机交互研究的门槛，促进该领域的探索

Method: 通过深入的机电架构设计和工程迭代，开发了约40美元的低成本Lantern平台；通过五个探索性研究评估其潜力：协同设计工作坊、感官室案例研究、外部HRI实验室分发、研究生HRI课程整合、以及面向老年人和儿童的公共展览

Result: Lantern能有效激发参与度，支持从情绪调节到专注工作等多种应用场景，并作为降低HRI领域门槛的可行平台

Conclusion: Lantern作为一个低成本、开源、可扩展的简约机器人对象平台，成功展示了其在促进人机交互研究方面的潜力，能够激发人类参与并支持多样化的应用场景

Abstract: Robotic objects are simple actuated systems that subtly blend into human environments. We design and introduce Lantern, a minimalist robotic object platform to enable building simple robotic artifacts. We conducted in-depth design and engineering iterations of Lantern's mechatronic architecture to meet specific design goals while maintaining a low build cost (~40 USD). As an extendable, open-source platform, Lantern aims to enable exploration of a range of HRI scenarios by leveraging human tendency to assign social meaning to simple forms. To evaluate Lantern's potential for HRI, we conducted a series of explorations: 1) a co-design workshop, 2) a sensory room case study, 3) distribution to external HRI labs, 4) integration into a graduate-level HRI course, and 5) public exhibitions with older adults and children. Our findings show that Lantern effectively evokes engagement, can support versatile applications ranging from emotion regulation to focused work, and serves as a viable platform for lowering barriers to HRI as a field.

</details>


### [207] [Accurate Pedestrian Tracking in Urban Canyons: A Multi-Modal Fusion Approach](https://arxiv.org/abs/2601.22406)
*Shahar Dubiner,Peng Ren,Roberto Manduchi*

Main category: cs.RO

TL;DR: 提出一种融合GNSS和惯性数据的粒子滤波行人导航方法，利用地图空间先验改善城市环境中GNSS性能下降时的定位精度，特别针对盲人或低视力用户需求。


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中GNSS性能下降导致的定位精度问题，这对依赖精确导航（如识别街道正确侧）的盲人或低视力用户尤为关键。相机视觉定位不实用，需要替代方案。

Method: 采用粒子滤波融合GNSS和惯性数据，结合地图空间先验（如不可通行建筑区域和不可能行走区域）作为概率形式的地图匹配。惯性定位使用RoNIN机器学习方法，通过粒子权重调整实现与GNSS估计和不确定性的融合。

Result: 在旧金山市中心6条具有挑战性的步行路线上评估，使用三个与行人道正确性和定位误差相关的指标。结果显示融合方法（GNSS+RoNIN+PF）在大多数指标上显著优于仅使用GNSS定位，而仅使用惯性定位加粒子滤波也在关键指标（如行人道分配和跨街道误差）上超越GNSS单独定位。

Conclusion: 提出的融合方法能有效改善城市环境中GNSS受限时的行人导航精度，特别是对需要精确街道侧识别的视觉障碍用户具有重要意义。结合地图先验的粒子滤波为城市行人导航提供了实用解决方案。

Abstract: The contribution describes a pedestrian navigation approach designed to improve localization accuracy in urban environments where GNSS performance is degraded, a problem that is especially critical for blind or low-vision users who depend on precise guidance such as identifying the correct side of a street. To address GNSS limitations and the impracticality of camera-based visual positioning, the work proposes a particle filter based fusion of GNSS and inertial data that incorporates spatial priors from maps, such as impassable buildings and unlikely walking areas, functioning as a probabilistic form of map matching. Inertial localization is provided by the RoNIN machine learning method, and fusion with GNSS is achieved by weighting particles based on their consistency with GNSS estimates and uncertainty. The system was evaluated on six challenging walking routes in downtown San Francisco using three metrics related to sidewalk correctness and localization error. Results show that the fused approach (GNSS+RoNIN+PF) significantly outperforms GNSS only localization on most metrics, while inertial-only localization with particle filtering also surpasses GNSS alone for critical measures such as sidewalk assignment and across street error.

</details>


### [208] [High-Definition 5MP Stereo Vision Sensing for Robotics](https://arxiv.org/abs/2601.22445)
*Leaf Jiang,Matthew Holzel,Bernhard Kaplan,Hsiou-Yuan Liu,Sabyasachi Paul,Karen Rankin,Piotr Swierczynski*

Main category: cs.RO

TL;DR: 该研究提出了一种用于5MP+高分辨率立体视觉系统的新型校准和立体匹配方法，旨在实现高精度和快速处理，并引入了实时性能评估方法。


<details>
  <summary>Details</summary>
Motivation: 高分辨率（5MP+）立体视觉系统对于提升机器人能力至关重要，但传统方法无法满足高分辨率传感器所需的高精度校准和快速处理要求。

Method: 采用新颖的帧间校准和立体匹配方法处理5MP相机图像，并引入通过比较实时视差图与计算密集型算法生成的基准视差图来评估实时性能的新方法。

Result: 研究表明，高像素相机只有通过实施高精度校准才能生成高质量点云，新方法在精度和速度方面均优于传统方法。

Conclusion: 高分辨率立体视觉系统的潜力实现依赖于高精度校准和高效处理方法的结合，所提出的方法为5MP+系统提供了有效的解决方案。

Abstract: High-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds. However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing -- requirements often unmet by conventional methods. This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed. Furthermore, we introduce a new approach to evaluate real-time performance by comparing real-time disparity maps with ground-truth disparity maps derived from more computationally intensive stereo matching algorithms. Crucially, the research demonstrates that high-pixel-count cameras yield high-quality point clouds only through the implementation of high-accuracy calibration.

</details>


### [209] [CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control](https://arxiv.org/abs/2601.22467)
*Jiaqi Shi,Xulong Zhang,Xiaoyang Qu,Jianzong Wang*

Main category: cs.RO

TL;DR: CARE框架通过仅使用视频-文本对训练VLA模型进行机器人控制，无需动作标注，通过多任务预训练学习连续潜在动作表示，少量标注数据微调即可实现控制。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型依赖动作监督，限制了可扩展性和泛化能力。需要开发不依赖动作标注的弱监督方法，利用更易获取的视频-文本数据来训练机器人控制模型。

Method: CARE框架仅使用视频-文本对进行预训练，无需动作标注。通过新设计的多任务预训练目标学习连续潜在动作表示。在微调阶段，使用少量标注数据训练动作头进行控制。

Result: 在多个仿真任务中，CARE表现出更高的成功率、更好的语义可解释性，并能避免捷径学习。证明了其在弱监督机器人控制中的可扩展性、可解释性和有效性。

Conclusion: CARE通过消除对动作标注的依赖，仅使用视频-文本对训练VLA模型，实现了可扩展且有效的机器人控制，为弱监督机器人学习提供了有前景的解决方案。

Abstract: Recent advances in Vision-Language-Action (VLA) models have shown promise for robot control, but their dependence on action supervision limits scalability and generalization. To address this challenge, we introduce CARE, a novel framework designed to train VLA models for robotic task execution. Unlike existing methods that depend on action annotations during pretraining, CARE eliminates the need for explicit action labels by leveraging only video-text pairs. These weakly aligned data sources enable the model to learn continuous latent action representations through a newly designed multi-task pretraining objective. During fine-tuning, a small set of labeled data is used to train the action head for control. Experimental results across various simulation tasks demonstrate CARE's superior success rate, semantic interpretability, and ability to avoid shortcut learning. These results underscore CARE's scalability, interpretability, and effectiveness in robotic control with weak supervision.

</details>


### [210] [RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing](https://arxiv.org/abs/2601.22517)
*Kangning Yin,Zhe Cao,Wentao Dong,Weishuai Zeng,Tianyi Zhang,Qiang Zhang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Weinan Zhang*

Main category: cs.RO

TL;DR: RoboStriker：一个三阶段分层框架，通过解耦高层战略推理与低层物理执行，实现全自主人形机器人拳击。该方法结合人类运动捕捉数据学习拳击技能，在潜在空间中稳定多智能体训练，并实现仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在接触丰富且高度动态的任务（如拳击）中实现人类水平的竞争智能和身体敏捷性仍面临重大挑战。多智能体强化学习（MARL）虽然为战略交互提供了原则性框架，但直接应用于人形控制受到高维接触动力学和缺乏强物理运动先验的限制。

Method: 提出RoboStriker三阶段分层框架：1）在人类运动捕捉数据上训练单智能体运动跟踪器，学习全面的拳击技能库；2）将技能蒸馏到结构化潜在流形，通过将高斯参数化分布投影到单位超球面上进行正则化；3）引入潜在空间神经虚拟自博弈（LS-NFSP），让竞争智能体在潜在动作空间而非原始电机空间中进行交互，显著稳定多智能体训练。

Result: 实验结果表明，RoboStriker在仿真中实现了优越的竞争性能，并展现出仿真到现实的迁移能力。

Conclusion: RoboStriker通过解耦战略推理与物理执行、在潜在空间中稳定多智能体训练的方法，成功实现了全自主人形机器人拳击，为解决接触丰富动态任务中的人形机器人控制问题提供了有效框架。

Abstract: Achieving human-level competitive intelligence and physical agility in humanoid robots remains a major challenge, particularly in contact-rich and highly dynamic tasks such as boxing. While Multi-Agent Reinforcement Learning (MARL) offers a principled framework for strategic interaction, its direct application to humanoid control is hindered by high-dimensional contact dynamics and the absence of strong physical motion priors. We propose RoboStriker, a hierarchical three-stage framework that enables fully autonomous humanoid boxing by decoupling high-level strategic reasoning from low-level physical execution. The framework first learns a comprehensive repertoire of boxing skills by training a single-agent motion tracker on human motion capture data. These skills are subsequently distilled into a structured latent manifold, regularized by projecting the Gaussian-parameterized distribution onto a unit hypersphere. This topological constraint effectively confines exploration to the subspace of physically plausible motions. In the final stage, we introduce Latent-Space Neural Fictitious Self-Play (LS-NFSP), where competing agents learn competitive tactics by interacting within the latent action space rather than the raw motor space, significantly stabilizing multi-agent training. Experimental results demonstrate that RoboStriker achieves superior competitive performance in simulation and exhibits sim-to-real transfer. Our website is available at RoboStriker.

</details>


### [211] [Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios](https://arxiv.org/abs/2601.22545)
*Feng Tao,Luca Paparusso,Chenyi Gu,Robin Koehler,Chenxu Wu,Xinyu Huang,Christian Juette,David Paz,Ren Liu*

Main category: cs.RO

TL;DR: 提出基于深度强化学习的实时路径规划框架，专门针对狭窄停车场景，通过单次前向传播生成动作，实现实时部署，超越传统规划器性能。


<details>
  <summary>Details</summary>
Motivation: 传统经典规划器在完美感知假设下有效，但对实际感知约束敏感且计算成本高，难以在复杂环境中实时部署。需要解决狭窄停车场景中需要多次倒车调整的挑战性路径规划问题。

Method: 采用深度强化学习框架，将路径规划建模为基于自行车模型动力学的序列决策问题，使智能体在闭环设置中直接学习符合车辆运动学和环境约束的导航策略。开发新基准支持训练和评估。

Result: 方法在成功率和效率方面达到最先进水平，超越经典规划器基线：成功率提高96%，效率提高52%。开发的开源基准包含多样化的挑战性场景。

Conclusion: 提出的DRL框架为约束环境中的实时路径规划提供了实用解决方案，无需理想化感知和额外模块，计算轻量适合实时部署。开源基准将促进自主系统领域的未来研究。

Abstract: Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive. To overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by +96% in success rate and +52% in efficiency. Furthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems. The benchmark and accompanying tools are available at https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios.

</details>


### [212] [Exo-Plore: Exploring Exoskeleton Control Space through Human-aligned Simulation](https://arxiv.org/abs/2601.22550)
*Geonho Leem,Jaedong Lee,Jehee Lee,Seungmoon Song,Jungdam Won*

Main category: cs.RO

TL;DR: Exo-plore：结合神经力学模拟与深度强化学习的仿真框架，无需真人实验即可优化髋关节外骨骼辅助


<details>
  <summary>Details</summary>
Motivation: 当前外骨骼控制器优化方法需要大量真人步行实验，这对行动不便者参与造成障碍，形成"受益者无法参与"的悖论

Method: 结合神经力学模拟与深度强化学习，通过仿真框架优化髋关节外骨骼辅助策略

Result: 能够生成真实步态数据、在随机步态中产生可靠优化结果，并能泛化到病理步态，显示病理严重程度与最优辅助间的线性关系

Conclusion: Exo-plore框架为外骨骼辅助优化提供了无需真人实验的有效途径，特别适用于行动不便人群

Abstract: Exoskeletons show great promise for enhancing mobility, but providing appropriate assistance remains challenging due to the complexity of human adaptation to external forces. Current state-of-the-art approaches for optimizing exoskeleton controllers require extensive human experiments in which participants must walk for hours, creating a paradox: those who could benefit most from exoskeleton assistance, such as individuals with mobility impairments, are rarely able to participate in such demanding procedures. We present Exo-plore, a simulation framework that combines neuromechanical simulation with deep reinforcement learning to optimize hip exoskeleton assistance without requiring real human experiments. Exo-plore can (1) generate realistic gait data that captures human adaptation to assistive forces, (2) produce reliable optimization results despite the stochastic nature of human gait, and (3) generalize to pathological gaits, showing strong linear relationships between pathology severity and optimal assistance.

</details>


### [213] [Postural Virtual Fixtures for Ergonomic Physical Interactions with Supernumerary Robotic Bodies](https://arxiv.org/abs/2601.22672)
*Theodora Kastritsi,Marta Lagomarsino,Arash Ajoudani*

Main category: cs.RO

TL;DR: 提出一种为超数机器人身体用户提供动觉反馈的控制框架，通过虚拟夹具和在线姿态评估来促进人体工程学姿势学习


<details>
  <summary>Details</summary>
Motivation: 超数机器人身体能增强人类负载能力，但用户在与机器人物理交互时仍可能采用非人体工程学姿势，导致不适或伤害，需要引导用户养成良好姿势习惯

Method: 开发了虚拟夹具方法，结合连续在线人体工程学姿态评估框架，当检测到非人体工程学姿势时提供动觉反馈阻力；为改善操作者与浮动基座机器人的协调性，根据需要调整浮动基座位置

Result: 实验验证了人体工程学驱动控制框架的功能和有效性，包括两个涉及14名受试者的实际移动操作任务用户研究，与不考虑人体工程学的基线控制框架相比表现出优势

Conclusion: 提出的控制框架能有效引导用户采用人体工程学姿势，促进长期姿势习惯养成，改善人机协作安全性

Abstract: Conjoined collaborative robots, functioning as supernumerary robotic bodies (SRBs), can enhance human load tolerance abilities. However, in tasks involving physical interaction with humans, users may still adopt awkward, non-ergonomic postures, which can lead to discomfort or injury over time. In this paper, we propose a novel control framework that provides kinesthetic feedback to SRB users when a non-ergonomic posture is detected, offering resistance to discourage such behaviors. This approach aims to foster long-term learning of ergonomic habits and promote proper posture during physical interactions. To achieve this, a virtual fixture method is developed, integrated with a continuous, online ergonomic posture assessment framework. Additionally, to improve coordination between the operator and the SRB, which consists of a robotic arm mounted on a floating base, the position of the floating base is adjusted as needed. Experimental results demonstrate the functionality and efficacy of the ergonomics-driven control framework, including two user studies involving practical loco-manipulation tasks with 14 subjects, comparing the proposed framework with a baseline control framework that does not account for human ergonomics.

</details>


### [214] [FlyAware: Inertia-Aware Aerial Manipulation via Vision-Based Estimation and Post-Grasp Adaptation](https://arxiv.org/abs/2601.22686)
*Biyu Ye,Na Fan,Zhengping Fan,Weiliang Deng,Hongming Chen,Qifeng Chen,Ximin Lyu*

Main category: cs.RO

TL;DR: 提出一种用于空中机械臂的机载框架，通过视觉预抓取惯性估计和抓取后自适应机制，实现实时惯性动力学估计与适应，解决负载变化和机械臂配置带来的时变惯性参数挑战。


<details>
  <summary>Details</summary>
Motivation: 空中机械臂在自动运输和应急服务中具有优势，但实际部署面临时变惯性参数的复杂性挑战，这些参数对负载变化和机械臂配置高度敏感，需要解决实时估计和适应问题。

Method: 提出集成视觉预抓取惯性估计模块和抓取后自适应机制的机载框架；开发基于增益调度的惯性感知自适应控制策略；通过频域系统辨识评估鲁棒性。

Result: 研究为空中机械臂的抓取后控制提供了新见解；真实世界实验验证了所提框架的有效性和可行性。

Conclusion: 该框架通过实时惯性估计和自适应控制，解决了空中机械臂在实际部署中的关键挑战，为稳健的空中操作提供了有效解决方案。

Abstract: Aerial manipulators (AMs) are gaining increasing attention in automated transportation and emergency services due to their superior dexterity compared to conventional multirotor drones. However, their practical deployment is challenged by the complexity of time-varying inertial parameters, which are highly sensitive to payload variations and manipulator configurations. Inspired by human strategies for interacting with unknown objects, this letter presents a novel onboard framework for robust aerial manipulation. The proposed system integrates a vision-based pre-grasp inertia estimation module with a post-grasp adaptation mechanism, enabling real-time estimation and adaptation of inertial dynamics. For control, we develop an inertia-aware adaptive control strategy based on gain scheduling, and assess its robustness via frequency-domain system identification. Our study provides new insights into post-grasp control for AMs, and real-world experiments validate the effectiveness and feasibility of the proposed framework.

</details>


### [215] [Robust Rigid Body Assembly via Contact-Implicit Optimal Control with Exact Second-Order Derivatives](https://arxiv.org/abs/2601.22849)
*Christian Dietz,Sebastian Albrecht,Gianluca Frison,Moritz Diehl,Armin Nurkanović*

Main category: cs.RO

TL;DR: 提出一种基于可微分物理仿真的高效鲁棒最优控制方法，用于机器人装配运动规划，相比传统强化学习和采样方法显著减少仿真步数


<details>
  <summary>Details</summary>
Motivation: 机器人装配运动规划长期依赖强化学习和基于采样的方法，需要大量物理仿真步骤，计算成本高。需要更高效的规划方法，减少仿真需求

Method: 构建可微分物理仿真器，提供二阶解析导数给数值求解器；使用基于内点法的平滑技术使碰撞检测和接触解析可微分；提出改进的基于优化的碰撞检测线性规划公式；设计多场景轨迹优化确保对仿真-现实差异的鲁棒性

Result: 在真实世界实验中实现超过99%的成功执行率；验证了接触动力学平滑近似和鲁棒建模对成功率的影响；在模拟中测试不同孔轴配合问题，显示使用精确Hessian矩阵相比常用近似的优势

Conclusion: 提出的可微分物理仿真和鲁棒最优控制方法为机器人装配运动规划提供了样本高效的解决方案，显著减少仿真需求，在真实环境中实现高成功率，证明了精确导数信息的重要性

Abstract: Efficient planning of assembly motions is a long standing challenge in the field of robotics that has been primarily tackled with reinforcement learning and sampling-based methods by using extensive physics simulations. This paper proposes a sample-efficient robust optimal control approach for the determination of assembly motions, which requires significantly less physics simulation steps during planning through the efficient use of derivative information. To this end, a differentiable physics simulation is constructed that provides second-order analytic derivatives to the numerical solver and allows one to traverse seamlessly from informative derivatives to accurate contact simulation. The solution of the physics simulation problem is made differentiable by using smoothing inspired by interior-point methods applied to both the collision detection as well as the contact resolution problem. We propose a modified variant of an optimization-based formulation of collision detection formulated as a linear program and present an efficient implementation for the nominal evaluation and corresponding first- and second-order derivatives. Moreover, a multi-scenario-based trajectory optimization problem that ensures robustness with respect to sim-to-real mismatches is derived. The capability of the considered formulation is illustrated by results where over 99\% successful executions are achieved in real-world experiments. Thereby, we carefully investigate the effect of smooth approximations of the contact dynamics and robust modeling on the success rates. Furthermore, the method's capability is tested on different peg-in-hole problems in simulation to show the benefit of using exact Hessians over commonly used Hessian approximations.

</details>


### [216] [MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2601.22930)
*Xidong Li,Mingyu Guo,Chenchao Xu,Bailin Li,Wenjing Zhu,Yangang Zou,Rui Chen,Zehuan Wang*

Main category: cs.RO

TL;DR: MTDrive：基于多模态大语言模型的多轮轨迹规划框架，通过迭代优化和相对策略优化提升自动驾驶在复杂场景下的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM和强化学习的轨迹规划方法仅限于单轮推理，无法处理需要迭代优化的复杂任务，限制了在长尾场景中的表现

Method: 提出MTDrive多轮框架，引入多轮组相对策略优化(mtGRPO)缓解奖励稀疏问题，构建交互式轨迹理解数据集，并进行系统级优化提升训练效率

Result: 在NAVSIM基准测试中表现优于现有方法，验证了多轮推理范式的有效性，系统优化实现了2.5倍训练吞吐量提升

Conclusion: 多轮推理框架能够显著提升自动驾驶轨迹规划在复杂场景下的性能，通过迭代优化和相对优势计算有效解决了奖励稀疏问题

Abstract: Trajectory planning is a core task in autonomous driving, requiring the prediction of safe and comfortable paths across diverse scenarios. Integrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL) has shown promise in addressing "long-tail" scenarios. However, existing methods are constrained to single-turn reasoning, limiting their ability to handle complex tasks requiring iterative refinement. To overcome this limitation, we present MTDrive, a multi-turn framework that enables MLLMs to iteratively refine trajectories based on environmental feedback. MTDrive introduces Multi-Turn Group Relative Policy Optimization (mtGRPO), which mitigates reward sparsity by computing relative advantages across turns. We further construct an interactive trajectory understanding dataset from closed-loop simulation to support multi-turn training. Experiments on the NAVSIM benchmark demonstrate superior performance compared to existing methods, validating the effectiveness of our multi-turn reasoning paradigm. Additionally, we implement system-level optimizations to reduce data transfer overhead caused by high-resolution images and multi-turn sequences, achieving 2.5x training throughput. Our data, models, and code will be made available soon.

</details>


### [217] [Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation](https://arxiv.org/abs/2601.22965)
*Runhua Zhang,Junyi Hou,Changxu Cheng,Qiyi Chen,Tao Wang,Wuyue Zhao*

Main category: cs.RO

TL;DR: SIDP提出了一种自模仿扩散策略框架，通过选择性模仿自身采样轨迹来学习改进规划，减少对专家演示的依赖和推理时的后处理需求，实现更高效的实时视觉导航。


<details>
  <summary>Details</summary>
Motivation: 标准扩散策略依赖模仿学习训练，继承了专家演示的次优性和冗余性，需要计算密集的"生成-过滤"流水线和辅助选择器，导致推理效率低下。

Method: 提出自模仿扩散策略(SIDP)，引入奖励引导的自模仿机制，让策略选择性模仿自身采样的高质量轨迹；采用奖励驱动的课程学习缓解数据利用效率问题，以及目标无关的探索进行轨迹增强。

Result: 在综合仿真基准上显著超越先前方法，真实世界实验验证了在多机器人平台上的有效性；在Jetson Orin Nano上推理速度比基线NavDP快2.5倍(110ms vs 273ms)。

Conclusion: SIDP通过自模仿机制有效提升了扩散策略的规划质量和推理效率，减少了对专家演示的依赖和后处理需求，实现了高效的实时视觉导航部署。

Abstract: Diffusion policies (DP) have demonstrated significant potential in visual navigation by capturing diverse multi-modal trajectory distributions. However, standard imitation learning (IL), which most DP methods rely on for training, often inherits sub-optimality and redundancy from expert demonstrations, thereby necessitating a computationally intensive "generate-then-filter" pipeline that relies on auxiliary selectors during inference. To address these challenges, we propose Self-Imitated Diffusion Policy (SIDP), a novel framework that learns improved planning by selectively imitating a set of trajectories sampled from itself. Specifically, SIDP introduces a reward-guided self-imitation mechanism that encourages the policy to consistently produce high-quality trajectories efficiently, rather than outputs of inconsistent quality, thereby reducing reliance on extensive sampling and post-filtering. During training, we employ a reward-driven curriculum learning paradigm to mitigate inefficient data utility, and goal-agnostic exploration for trajectory augmentation to improve planning robustness. Extensive evaluations on a comprehensive simulation benchmark show that SIDP significantly outperforms previous methods, with real-world experiments confirming its effectiveness across multiple robotic platforms. On Jetson Orin Nano, SIDP delivers a 2.5$\times$ faster inference than the baseline NavDP, i.e., 110ms VS 273ms, enabling efficient real-time deployment.

</details>


### [218] [Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation](https://arxiv.org/abs/2601.22988)
*Di Zhang,Weicheng Duan,Dasen Gu,Hongye Lu,Hai Zhang,Hang Yu,Junqiao Zhao,Guang Chen*

Main category: cs.RO

TL;DR: MethodName：一种用于机器人操作的统一表示-策略学习框架，通过单视图3D预训练和多步蒸馏实现强大的视角泛化能力


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人操作需要具备强大空间场景理解和跨不同相机视角泛化能力的视觉运动策略。现有3D感知视觉表示方法存在三个关键限制：1）推理时依赖多视角观察，这在单视图受限场景中不实用；2）不完整的场景建模，无法捕捉精确操作所需的全方位和细粒度几何结构；3）缺乏有效的策略训练策略来保留和利用获得的3D知识。

Method: MethodName提出统一表示-策略学习框架，包含：1）单视图3D预训练范式，利用点云重建和前馈高斯溅射在多视角监督下学习全方位几何表示；2）策略学习阶段进行多步蒸馏，保留预训练的几何理解并有效转移到操作技能中。

Result: 在12个RLBench任务上，MethodName比先前最先进方法的平均成功率高出12.7%。在6个代表性任务的零样本视角泛化评估中，在中等和大幅视角偏移下成功率仅下降22.0%和29.7%，而最先进方法下降幅度更大，分别为41.6%和51.5%。

Conclusion: MethodName通过单视图3D预训练和多步蒸馏策略，有效解决了现有3D感知表示方法的局限性，实现了强大的视角泛化机器人操作能力，在多个基准测试中显著优于现有方法。

Abstract: Real-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints. While recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations, including reliance on multi-view observations during inference which is impractical in single-view restricted scenarios, incomplete scene modeling that fails to capture holistic and fine-grained geometric structures essential for precise manipulation, and lack of effective policy training strategies to retain and exploit the acquired 3D knowledge. To address these challenges, we present MethodName, a unified representation-policy learning framework for view-generalizable robotic manipulation. MethodName introduces a single-view 3D pretraining paradigm that leverages point cloud reconstruction and feed-forward gaussian splatting under multi-view supervision to learn holistic geometric representations. During policy learning, MethodName performs multi-step distillation to preserve the pretrained geometric understanding and effectively transfer it to manipulation skills. We conduct experiments on 12 RLBench tasks, where our approach outperforms the previous state-of-the-art method by 12.7% in average success rate. Further evaluation on six representative tasks demonstrates strong zero-shot view generalization, with success rate drops of only 22.0% and 29.7% under moderate and large viewpoint shifts respectively, whereas the state-of-the-art method suffers larger decreases of 41.6% and 51.5%.

</details>


### [219] [Robust and Generalized Humanoid Motion Tracking](https://arxiv.org/abs/2601.23080)
*Yubiao Ma,Han Yu,Jiayin Xie,Changtai Lv,Qiang Luo,Chi Zhang,Yunpeng Yin,Boyang Xing,Xuemei Ren,Dongdong Zheng*

Main category: cs.RO

TL;DR: 提出一种基于动力学条件命令聚合框架的人形机器人全身控制器，通过因果时序编码器和多头交叉注意力机制处理噪声参考运动，结合跌倒恢复课程提升鲁棒性，仅需3.5小时运动数据即可实现端到端训练和零样本迁移


<details>
  <summary>Details</summary>
Motivation: 学习通用人形机器人全身控制器面临挑战：参考运动转移到机器人域后可能存在噪声和不一致性，闭环执行会放大局部缺陷，导致高动态和接触丰富行为中的漂移或失败

Method: 提出动力学条件命令聚合框架：1) 因果时序编码器总结近期本体感知；2) 多头交叉注意力命令编码器基于当前动力学选择性聚合上下文窗口；3) 集成跌倒恢复课程，包含随机不稳定初始化和退火向上辅助力以提升鲁棒性和抗干扰能力

Result: 该方法仅需约3.5小时运动数据，支持单阶段端到端训练无需蒸馏。在多样化参考输入和挑战性运动机制下评估，展示了零样本迁移到未见运动的能力，以及在物理人形机器人上的鲁棒仿真到现实迁移

Conclusion: 提出的动力学条件命令聚合框架有效解决了参考运动噪声和不一致性问题，通过选择性上下文聚合和鲁棒性训练策略，实现了高效、鲁棒的人形机器人全身控制，具备良好的泛化能力和现实部署可行性

Abstract: Learning a general humanoid whole-body controller is challenging because practical reference motions can exhibit noise and inconsistencies after being transferred to the robot domain, and local defects may be amplified by closed-loop execution, causing drift or failure in highly dynamic and contact-rich behaviors. We propose a dynamics-conditioned command aggregation framework that uses a causal temporal encoder to summarize recent proprioception and a multi-head cross-attention command encoder to selectively aggregate a context window based on the current dynamics. We further integrate a fall recovery curriculum with random unstable initialization and an annealed upward assistance force to improve robustness and disturbance rejection. The resulting policy requires only about 3.5 hours of motion data and supports single-stage end-to-end training without distillation. The proposed method is evaluated under diverse reference inputs and challenging motion regimes, demonstrating zero-shot transfer to unseen motions as well as robust sim-to-real transfer on a physical humanoid robot.

</details>


### [220] [Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation](https://arxiv.org/abs/2601.23087)
*Wu Songwei,Jiang Zhiduo,Xie Guanghu,Liu Yang,Liu Hong*

Main category: cs.RO

TL;DR: LG-Flow Policy：一种在连续潜在动作空间中执行流匹配的轨迹级模仿学习框架，通过编码动作序列到时间正则化潜在轨迹，实现平滑可靠的长时程执行，同时保持接近单步推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有生成策略在长时程机器人操作中存在表达性行为建模、实时推理和稳定执行之间的权衡问题。基于扩散的方法建模能力强但推理延迟高，而流匹配方法虽然能实现快速单步生成，但在原始动作空间中直接应用常导致执行不稳定。

Method: 提出LG-Flow Policy框架：1）将动作序列编码为时间正则化的潜在轨迹；2）在连续潜在动作空间中学习显式的潜在空间流；3）将全局运动结构与低级控制噪声解耦；4）结合几何感知点云条件化和执行时多模态调制。

Result: 在仿真和物理机器人平台上的实验表明：LG-Flow Policy实现接近单步推理速度，相比在原始动作空间中操作的流匹配基线显著提高轨迹平滑度和任务成功率，且比基于扩散的策略更高效。

Conclusion: LG-Flow Policy通过潜在空间流匹配成功解决了长时程机器人操作中建模能力、推理速度和执行稳定性之间的权衡问题，为实际机器人应用提供了高效可靠的解决方案。

Abstract: Learning long-horizon robotic manipulation requires jointly achieving expressive behavior modeling, real-time inference, and stable execution, which remains challenging for existing generative policies. Diffusion-based approaches provide strong modeling capacity but typically incur high inference latency, while flow matching enables fast one-step generation yet often leads to unstable execution when applied directly in the raw action space.
  We propose LG-Flow Policy, a trajectory-level imitation learning framework that performs flow matching in a continuous latent action space. By encoding action sequences into temporally regularized latent trajectories and learning an explicit latent-space flow, the proposed approach decouples global motion structure from low-level control noise, resulting in smooth and reliable long-horizon execution.
  LG-Flow Policy further incorporates geometry-aware point cloud conditioning and execution-time multimodal modulation, with visual cues evaluated as a representative modality in real-world settings. Experimental results in simulation and on physical robot platforms demonstrate that LG-Flow Policy achieves near single-step inference, substantially improves trajectory smoothness and task success over flow-based baselines operating in the raw action space, and remains significantly more efficient than diffusion-based policies.

</details>


### [221] [End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms](https://arxiv.org/abs/2601.23285)
*MH Farhadi,Ali Rabiee,Sima Ghafoori,Anna Cetera,Andrew Fisher,Reza Abiri*

Main category: cs.RO

TL;DR: BRACE框架通过贝叶斯意图推断与上下文自适应辅助的端到端联合优化，在共享自主系统中实现了优于现有方法的性能提升。


<details>
  <summary>Details</summary>
Motivation: 共享自主系统需要从用户意图推断到辅助水平决策的完整优化框架。现有方法要么使用静态混合比例，要么将目标推断与辅助仲裁分离，导致在非结构化环境中性能不佳。

Method: 提出BRACE框架，通过允许意图推断和辅助仲裁之间端到端梯度流的架构，对贝叶斯意图推断和上下文自适应辅助进行联合微调。该框架将协作控制策略基于环境上下文和完整目标概率分布。

Result: 在三个逐步隔离末端执行器控制挑战的评估中：1) 2D人机交互任务，2) 机器人臂非线性动力学，3) 目标模糊和环境约束下的集成操作。相比SOTA方法，成功率提高6.3%，路径效率提高41%；相比无辅助控制，成功率提高36.3%，路径效率提高87%。

Conclusion: 集成优化在复杂、目标模糊的场景中最为有益，可推广到需要目标导向辅助的机器人领域，推进了自适应共享自主系统的技术前沿。

Abstract: Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.

</details>
