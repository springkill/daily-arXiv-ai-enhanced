{"id": "2601.17322", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17322", "abs": "https://arxiv.org/abs/2601.17322", "authors": ["Yong Wang"], "title": "Structural Operational Semantics for True Concurrency", "comment": null, "summary": "It is natural that we can extend Structural Operational Semantics (SOS) to SOS for true concurrency. From SOS to SOS for true concurrency, it is in nature to give the related concepts in SOS a truly concurrent semantics foundation, i.e., a transition occurs by executing a Partially Ordered Multi Set (pomset) of actions replacing just one single action. Under the framework of SOS, for the extension to the truly concurrent one, something are changing: Labelled Transition System (LTS) is generalized to Pomset LTS (PLTS), Transition System Specification (TSS) to Pomset TSS (PTSS), interleaving behavioural equivalences to truly concurrent ones, congruence formats of TSSs to those of PTSSs; something are remained, such as the concept of conservative extension, the meanings of TSSs and PTSSs, higher-order languages and denotational semantics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u7ed3\u6784\u64cd\u4f5c\u8bed\u4e49(SOS)\u6269\u5c55\u5230\u771f\u5e76\u53d1SOS\uff0c\u901a\u8fc7\u7528\u504f\u5e8f\u591a\u96c6(pomset)\u66ff\u4ee3\u5355\u4e2a\u52a8\u4f5c\u4f5c\u4e3a\u8f6c\u6362\u57fa\u7840\uff0c\u5efa\u7acb\u771f\u5e76\u53d1\u8bed\u4e49\u6846\u67b6", "motivation": "\u5c06\u4f20\u7edf\u7684\u7ed3\u6784\u64cd\u4f5c\u8bed\u4e49\u6269\u5c55\u5230\u771f\u5e76\u53d1\u8bed\u4e49\uff0c\u4e3aSOS\u76f8\u5173\u6982\u5ff5\u63d0\u4f9b\u771f\u6b63\u7684\u5e76\u53d1\u8bed\u4e49\u57fa\u7840\uff0c\u89e3\u51b3\u4f20\u7edfSOS\u53ea\u80fd\u5904\u7406\u4ea4\u9519\u5e76\u53d1\u7684\u95ee\u9898", "method": "\u5c06SOS\u6846\u67b6\u6269\u5c55\u5230\u771f\u5e76\u53d1\uff1a\u7528pomset\u52a8\u4f5c\u66ff\u4ee3\u5355\u4e2a\u52a8\u4f5c\uff0c\u5c06LTS\u63a8\u5e7f\u4e3aPLTS\uff0cTSS\u63a8\u5e7f\u4e3aPTSS\uff0c\u4ea4\u9519\u884c\u4e3a\u7b49\u4ef7\u63a8\u5e7f\u4e3a\u771f\u5e76\u53d1\u884c\u4e3a\u7b49\u4ef7\uff0c\u5e76\u4fdd\u6301\u4fdd\u5b88\u6269\u5c55\u7b49\u6982\u5ff5\u4e0d\u53d8", "result": "\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u771f\u5e76\u53d1SOS\u7406\u8bba\u6846\u67b6\uff0c\u5305\u62ecPLTS\u3001PTSS\u3001\u771f\u5e76\u53d1\u884c\u4e3a\u7b49\u4ef7\u548c\u4e00\u81f4\u6027\u683c\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f20\u7edfSOS\u7684\u4fdd\u5b88\u6269\u5c55\u3001\u9ad8\u9636\u8bed\u8a00\u548c\u6307\u79f0\u8bed\u4e49\u7b49\u6838\u5fc3\u7279\u6027", "conclusion": "\u6210\u529f\u5c06SOS\u6269\u5c55\u5230\u771f\u5e76\u53d1\u8bed\u4e49\uff0c\u4e3a\u5e76\u53d1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bed\u4e49\u63cf\u8ff0\u6846\u67b6\uff0c\u65e2\u4fdd\u6301\u4e86SOS\u7684\u7406\u8bba\u4f18\u52bf\uff0c\u53c8\u514b\u670d\u4e86\u4f20\u7edf\u4ea4\u9519\u5e76\u53d1\u7684\u5c40\u9650\u6027"}}
{"id": "2601.18000", "categories": ["cs.LO", "cs.FL", "math.CT"], "pdf": "https://arxiv.org/pdf/2601.18000", "abs": "https://arxiv.org/abs/2601.18000", "authors": ["Paul-Andr\u00e9 Melli\u00e8s", "Vincent Moreau"], "title": "A cartesian closed fibration of higher-order regular languages", "comment": null, "summary": "We explain how to construct in two different ways a cartesian closed fibration of higher-order regular languages in the sense of Salvati. In the first construction, we use fibrational techniques to derive the cartesian closed fibration from the various categories of regular languages of $\u03bb$-terms associated to finite sets of ground states. In the second construction, we take advantage of the recent notion of profinite $\u03bb$-calculus to define the cartesian closed fibration by a change-of-base from the fibration of clopen subsets over the category of Stone spaces, using an elegant idea coming from Hermida. We illustrate the expressive power of the cartesian closed fibration by generalizing the notion of Brzozowski derivative to higher-order regular languages, using an Isbell-like adjunction in the sense of Melli\u00e8s and Zeilberger.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u6784\u9020\u9ad8\u9636\u6b63\u5219\u8bed\u8a00\u7684\u7b1b\u5361\u5c14\u95ed\u7ea4\u7ef4\u5316\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63a8\u5e7fBrzozowski\u5bfc\u6570\u6982\u5ff5\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u4e3aSalvati\u610f\u4e49\u4e0b\u7684\u9ad8\u9636\u6b63\u5219\u8bed\u8a00\u6784\u5efa\u7b1b\u5361\u5c14\u95ed\u7ea4\u7ef4\u5316\u7ed3\u6784\uff0c\u8fd9\u6709\u52a9\u4e8e\u5f62\u5f0f\u5316\u5730\u5904\u7406\u9ad8\u9636\u6b63\u5219\u8bed\u8a00\u7684\u8303\u7574\u8bba\u6027\u8d28\uff0c\u5e76\u4e3a\u63a8\u5e7f\u7ecf\u5178\u6b63\u5219\u8bed\u8a00\u7406\u8bba\u4e2d\u7684\u6982\u5ff5\uff08\u5982Brzozowski\u5bfc\u6570\uff09\u63d0\u4f9b\u6570\u5b66\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6784\u9020\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u7ea4\u7ef4\u5316\u6280\u672f\u4ece\u4e0e\u6709\u9650\u57fa\u6001\u96c6\u5408\u76f8\u5173\u7684\u03bb\u9879\u6b63\u5219\u8bed\u8a00\u8303\u7574\u63a8\u5bfc\u51fa\u7b1b\u5361\u5c14\u95ed\u7ea4\u7ef4\u5316\uff1b2\uff09\u5229\u7528\u6700\u8fd1\u63d0\u51fa\u7684profinite \u03bb-\u6f14\u7b97\u6982\u5ff5\uff0c\u901a\u8fc7\u4eceStone\u7a7a\u95f4\u8303\u7574\u4e0a\u7684clopen\u5b50\u96c6\u7ea4\u7ef4\u5316\u8fdb\u884c\u57fa\u53d8\u6362\u6765\u5b9a\u4e49\u7b1b\u5361\u5c14\u95ed\u7ea4\u7ef4\u5316\uff0c\u8be5\u65b9\u6cd5\u501f\u9274\u4e86Hermida\u7684\u4f18\u96c5\u601d\u60f3\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u9ad8\u9636\u6b63\u5219\u8bed\u8a00\u7684\u7b1b\u5361\u5c14\u95ed\u7ea4\u7ef4\u5316\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u8be5\u7ed3\u6784\u5c06Brzozowski\u5bfc\u6570\u7684\u6982\u5ff5\u63a8\u5e7f\u5230\u9ad8\u9636\u6b63\u5219\u8bed\u8a00\uff0c\u8fd9\u662f\u901a\u8fc7Melli\u00e8s\u548cZeilberger\u610f\u4e49\u4e0a\u7684Isbell\u578b\u4f34\u968f\u5173\u7cfb\u5b9e\u73b0\u7684\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u9ad8\u9636\u6b63\u5219\u8bed\u8a00\u7684\u7b1b\u5361\u5c14\u95ed\u7ea4\u7ef4\u5316\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63a8\u5e7f\u7ecf\u5178\u6b63\u5219\u8bed\u8a00\u7406\u8bba\u6982\u5ff5\u65b9\u9762\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u9ad8\u9636\u6b63\u5219\u8bed\u8a00\u7684\u8303\u7574\u8bba\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u89c6\u89d2\u3002"}}
{"id": "2601.18429", "categories": ["cs.LO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2601.18429", "abs": "https://arxiv.org/abs/2601.18429", "authors": ["Corentin Barloy", "Felix Tschirbs", "Nils Vortmeier", "Thomas Zeume"], "title": "Algebraic Characterizations of Classes of Regular Languages in DynFO", "comment": "Full version of the homonymous paper to appear in the proceedings of STACS'26", "summary": "This paper explores the fine-grained structure of classes of regular languages maintainable in fragments of first-order logic within the dynamic descriptive complexity framework of Patnaik and Immerman. A result by Hesse states that the class of regular languages is maintainable by first-order formulas even if only unary auxiliary relations can be used. Another result by Gelade, Marquardt,and Schwentick states that the class of regular languages coincides with the class of languages maintainable by quantifier-free formulas with binary auxiliary relations. We refine Hesse's result and show that with unary auxiliary data formulas with one quantifier alternation can maintain all regular languages. We then obtain precise algebraic characterizations of the classes of languages maintainable with quantifier-free formulas and positive existential formulas in the presence of unary auxiliary relations.", "AI": {"tldr": "\u672c\u6587\u5728\u52a8\u6001\u63cf\u8ff0\u590d\u6742\u6027\u6846\u67b6\u4e0b\uff0c\u7cbe\u5316\u4e86\u5bf9\u4e00\u9636\u903b\u8f91\u7247\u6bb5\u4e2d\u53ef\u7ef4\u62a4\u6b63\u5219\u8bed\u8a00\u7c7b\u7684\u7406\u89e3\uff0c\u8bc1\u660e\u4e86\u5e26\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\u7684\u4e00\u91cf\u8bcd\u4ea4\u66ff\u516c\u5f0f\u53ef\u7ef4\u62a4\u6240\u6709\u6b63\u5219\u8bed\u8a00\uff0c\u5e76\u83b7\u5f97\u4e86\u5e26\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\u7684\u65e0\u91cf\u8bcd\u516c\u5f0f\u548c\u6b63\u5b58\u5728\u516c\u5f0f\u53ef\u7ef4\u62a4\u8bed\u8a00\u7c7b\u7684\u7cbe\u786e\u4ee3\u6570\u523b\u753b\u3002", "motivation": "\u5728Patnaik\u548cImmerman\u7684\u52a8\u6001\u63cf\u8ff0\u590d\u6742\u6027\u6846\u67b6\u4e0b\uff0c\u63a2\u7d22\u6b63\u5219\u8bed\u8a00\u5728\u4e00\u9636\u903b\u8f91\u7247\u6bb5\u4e2d\u7684\u7cbe\u7ec6\u7ed3\u6784\u3002\u5df2\u6709\u7814\u7a76\u8868\u660e\uff1aHesse\u8bc1\u660e\u5373\u4f7f\u53ea\u4f7f\u7528\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\uff0c\u4e00\u9636\u516c\u5f0f\u4e5f\u80fd\u7ef4\u62a4\u6240\u6709\u6b63\u5219\u8bed\u8a00\uff1bGelade\u7b49\u4eba\u8bc1\u660e\u5e26\u4e8c\u5143\u8f85\u52a9\u5173\u7cfb\u7684\u65e0\u91cf\u8bcd\u516c\u5f0f\u53ef\u7ef4\u62a4\u7684\u8bed\u8a00\u7c7b\u6070\u597d\u662f\u6b63\u5219\u8bed\u8a00\u3002\u672c\u6587\u65e8\u5728\u7cbe\u5316\u8fd9\u4e9b\u7ed3\u679c\uff0c\u7814\u7a76\u5728\u66f4\u53d7\u9650\u6761\u4ef6\u4e0b\uff08\u5982\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\uff09\u4e0d\u540c\u903b\u8f91\u7247\u6bb5\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u52a8\u6001\u63cf\u8ff0\u590d\u6742\u6027\u6846\u67b6\uff0c\u5206\u6790\u4e0d\u540c\u903b\u8f91\u7247\u6bb5\uff08\u5e26\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\u7684\u516c\u5f0f\uff09\u7ef4\u62a4\u6b63\u5219\u8bed\u8a00\u7684\u80fd\u529b\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u7cbe\u5316Hesse\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u5e26\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\u7684\u4e00\u91cf\u8bcd\u4ea4\u66ff\u516c\u5f0f\u53ef\u7ef4\u62a4\u6240\u6709\u6b63\u5219\u8bed\u8a00\uff1b2\uff09\u5bf9\u5e26\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\u7684\u65e0\u91cf\u8bcd\u516c\u5f0f\u548c\u6b63\u5b58\u5728\u516c\u5f0f\u53ef\u7ef4\u62a4\u7684\u8bed\u8a00\u7c7b\uff0c\u63d0\u4f9b\u7cbe\u786e\u7684\u4ee3\u6570\u523b\u753b\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\uff1a1\uff09\u8bc1\u660e\u4e86\u5e26\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\u7684\u4e00\u91cf\u8bcd\u4ea4\u66ff\u516c\u5f0f\uff08\u5373\u516c\u5f0f\u5f62\u5f0f\u4e3a\u2203\u2200\u6216\u2200\u2203\uff09\u53ef\u4ee5\u7ef4\u62a4\u6240\u6709\u6b63\u5219\u8bed\u8a00\uff0c\u8fd9\u7cbe\u5316\u4e86Hesse\u7684\u539f\u59cb\u7ed3\u679c\uff1b2\uff09\u83b7\u5f97\u4e86\u5e26\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\u7684\u65e0\u91cf\u8bcd\u516c\u5f0f\u548c\u6b63\u5b58\u5728\u516c\u5f0f\u53ef\u7ef4\u62a4\u8bed\u8a00\u7c7b\u7684\u7cbe\u786e\u4ee3\u6570\u523b\u753b\uff0c\u8fd9\u4e9b\u523b\u753b\u57fa\u4e8e\u8bed\u8a00\u7684\u4ee3\u6570\u6027\u8d28\u3002", "conclusion": "\u672c\u6587\u5728\u52a8\u6001\u63cf\u8ff0\u590d\u6742\u6027\u6846\u67b6\u4e0b\uff0c\u5bf9\u6b63\u5219\u8bed\u8a00\u5728\u4e00\u9636\u903b\u8f91\u7247\u6bb5\u4e2d\u7684\u53ef\u7ef4\u62a4\u6027\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u7406\u89e3\u3002\u901a\u8fc7\u8bc1\u660e\u5e26\u4e00\u5143\u8f85\u52a9\u5173\u7cfb\u7684\u4e00\u91cf\u8bcd\u4ea4\u66ff\u516c\u5f0f\u8db3\u4ee5\u7ef4\u62a4\u6240\u6709\u6b63\u5219\u8bed\u8a00\uff0c\u5e76\u7cbe\u786e\u523b\u753b\u4e86\u66f4\u5f31\u903b\u8f91\u7247\u6bb5\u7684\u80fd\u529b\uff0c\u6df1\u5316\u4e86\u5bf9\u52a8\u6001\u7ef4\u62a4\u673a\u5236\u4e0e\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u5173\u7cfb\u7684\u8ba4\u8bc6\u3002"}}
{"id": "2601.18745", "categories": ["cs.LO", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18745", "abs": "https://arxiv.org/abs/2601.18745", "authors": ["Ruotong Cheng", "Azadeh Farzan"], "title": "Symmetric Proofs of Parameterized Programs", "comment": null, "summary": "We investigate the problem of safety verification of infinite-state parameterized programs that are formed based on a rich class of topologies. We introduce a new proof system, called parametric proof spaces, which exploits the underlying symmetry in such programs. This is a local notion of symmetry which enables the proof system to reuse proof arguments for isomorphic neighbourhoods in program topologies. We prove a sophisticated relative completeness result for the proof system with respect to a class of universally quantified invariants. We also investigate the problem of algorithmic construction of these proofs. We present a construction, inspired by classic results in model theory, where an infinitary limit program can be soundly and completely verified in place of the parameterized family, under some conditions. Furthermore, we demonstrate how these proofs can be constructed and checked against these programs without the need for axiomatization of the underlying topology for proofs or the programs. Finally, we present conditions under which our algorithm becomes a decision procedure.", "AI": {"tldr": "\u63d0\u51fa\u53c2\u6570\u5316\u8bc1\u660e\u7a7a\u95f4\u7cfb\u7edf\uff0c\u7528\u4e8e\u9a8c\u8bc1\u57fa\u4e8e\u4e30\u5bcc\u62d3\u6251\u7ed3\u6784\u7684\u65e0\u9650\u72b6\u6001\u53c2\u6570\u5316\u7a0b\u5e8f\u7684\u5b89\u5168\u6027\uff0c\u5229\u7528\u5c40\u90e8\u5bf9\u79f0\u6027\u91cd\u7528\u8bc1\u660e\uff0c\u5efa\u7acb\u76f8\u5bf9\u5b8c\u5907\u6027\u7406\u8bba\uff0c\u5e76\u7ed9\u51fa\u7b97\u6cd5\u6784\u9020\u548c\u51b3\u7b56\u6761\u4ef6\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u590d\u6742\u62d3\u6251\u7ed3\u6784\u7684\u65e0\u9650\u72b6\u6001\u53c2\u6570\u5316\u7a0b\u5e8f\u7684\u5b89\u5168\u9a8c\u8bc1\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u7c7b\u7a0b\u5e8f\u7684\u5bf9\u79f0\u6027\u548c\u53c2\u6570\u5316\u7279\u6027\uff0c\u9700\u8981\u65b0\u7684\u9a8c\u8bc1\u6846\u67b6\u3002", "method": "\u5f15\u5165\u53c2\u6570\u5316\u8bc1\u660e\u7a7a\u95f4\u7cfb\u7edf\uff0c\u5229\u7528\u7a0b\u5e8f\u7684\u5c40\u90e8\u5bf9\u79f0\u6027\u91cd\u7528\u8bc1\u660e\u8bba\u8bc1\uff1b\u5efa\u7acb\u76f8\u5bf9\u5b8c\u5907\u6027\u7406\u8bba\uff1b\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u8bba\u601d\u60f3\u7684\u7b97\u6cd5\u6784\u9020\uff0c\u901a\u8fc7\u65e0\u7a77\u6781\u9650\u7a0b\u5e8f\u66ff\u4ee3\u53c2\u6570\u5316\u5bb6\u65cf\u8fdb\u884c\u9a8c\u8bc1\uff1b\u65e0\u9700\u5bf9\u62d3\u6251\u7ed3\u6784\u8fdb\u884c\u516c\u7406\u5316\u3002", "result": "\u8bc1\u660e\u4e86\u53c2\u6570\u5316\u8bc1\u660e\u7a7a\u95f4\u7cfb\u7edf\u76f8\u5bf9\u4e8e\u4e00\u7c7b\u5168\u79f0\u91cf\u5316\u4e0d\u53d8\u91cf\u7684\u76f8\u5bf9\u5b8c\u5907\u6027\uff1b\u63d0\u51fa\u4e86\u7b97\u6cd5\u6784\u9020\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u516c\u7406\u5316\u62d3\u6251\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u6784\u9020\u548c\u68c0\u67e5\u8bc1\u660e\uff1b\u7ed9\u51fa\u4e86\u7b97\u6cd5\u6210\u4e3a\u51b3\u7b56\u7a0b\u5e8f\u7684\u6761\u4ef6\u3002", "conclusion": "\u53c2\u6570\u5316\u8bc1\u660e\u7a7a\u95f4\u7cfb\u7edf\u4e3a\u57fa\u4e8e\u62d3\u6251\u7ed3\u6784\u7684\u53c2\u6570\u5316\u7a0b\u5e8f\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7406\u8bba\u6846\u67b6\u548c\u7b97\u6cd5\u5de5\u5177\uff0c\u80fd\u591f\u5229\u7528\u5c40\u90e8\u5bf9\u79f0\u6027\u7b80\u5316\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0\u81ea\u52a8\u5316\u51b3\u7b56\u3002"}}
{"id": "2601.17454", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17454", "abs": "https://arxiv.org/abs/2601.17454", "authors": ["Muhammad Ahmed Atif", "Nehal Naeem Haji", "Mohammad Shahid Shaikh", "Muhammad Ebad Atif"], "title": "Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning", "comment": null, "summary": "Centralized value learning is often assumed to improve coordination and stability in multi-agent reinforcement learning, yet this assumption is rarely tested under controlled conditions. We directly evaluate it in a fully tabular predator-prey gridworld by comparing independent and centralized Q-learning under explicit embodiment constraints on agent speed and stamina. Across multiple kinematic regimes and asymmetric agent roles, centralized learning fails to provide a consistent advantage and is frequently outperformed by fully independent learning, even under full observability and exact value estimation. Moreover, asymmetric centralized-independent configurations induce persistent coordination breakdowns rather than transient learning instability. By eliminating confounding effects from function approximation and representation learning, our tabular analysis isolates coordination structure as the primary driver of these effects. The results show that increased coordination can become a liability under embodiment constraints, and that the effectiveness of centralized learning is fundamentally regime and role dependent rather than universal.", "AI": {"tldr": "\u96c6\u4e2d\u5f0f\u4ef7\u503c\u5b66\u4e60\u5728\u5177\u8eab\u7ea6\u675f\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u72ec\u7acb\u5b66\u4e60\uff0c\u5176\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u5177\u4f53\u8fd0\u52a8\u5b66\u673a\u5236\u548c\u667a\u80fd\u4f53\u89d2\u8272\u3002", "motivation": "\u9a8c\u8bc1\u96c6\u4e2d\u5f0f\u4ef7\u503c\u5b66\u4e60\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u6539\u5584\u534f\u8c03\u6027\u548c\u7a33\u5b9a\u6027\u7684\u5047\u8bbe\u662f\u5426\u6210\u7acb\uff0c\u7279\u522b\u662f\u5728\u5177\u8eab\u7ea6\u675f\u6761\u4ef6\u4e0b\u3002", "method": "\u5728\u5b8c\u5168\u8868\u683c\u5316\u7684\u6355\u98df\u8005-\u730e\u7269\u7f51\u683c\u4e16\u754c\u4e2d\uff0c\u6bd4\u8f83\u72ec\u7acbQ\u5b66\u4e60\u548c\u96c6\u4e2d\u5f0fQ\u5b66\u4e60\uff0c\u8003\u8651\u667a\u80fd\u4f53\u901f\u5ea6\u548c\u8010\u529b\u7684\u663e\u5f0f\u5177\u8eab\u7ea6\u675f\uff0c\u5206\u6790\u591a\u79cd\u8fd0\u52a8\u5b66\u673a\u5236\u548c\u975e\u5bf9\u79f0\u667a\u80fd\u4f53\u89d2\u8272\u3002", "result": "\u96c6\u4e2d\u5f0f\u5b66\u4e60\u672a\u80fd\u63d0\u4f9b\u4e00\u81f4\u4f18\u52bf\uff0c\u7ecf\u5e38\u88ab\u5b8c\u5168\u72ec\u7acb\u5b66\u4e60\u8d85\u8d8a\uff0c\u5373\u4f7f\u5728\u5b8c\u5168\u53ef\u89c2\u6d4b\u6027\u548c\u7cbe\u786e\u4ef7\u503c\u4f30\u8ba1\u6761\u4ef6\u4e0b\u3002\u975e\u5bf9\u79f0\u7684\u96c6\u4e2d-\u72ec\u7acb\u914d\u7f6e\u5bfc\u81f4\u6301\u7eed\u534f\u8c03\u5d29\u6e83\u800c\u975e\u77ed\u6682\u5b66\u4e60\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u6d88\u9664\u51fd\u6570\u903c\u8fd1\u548c\u8868\u793a\u5b66\u4e60\u7684\u6df7\u6742\u6548\u5e94\u540e\uff0c\u534f\u8c03\u7ed3\u6784\u662f\u8fd9\u4e9b\u6548\u5e94\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002\u589e\u52a0\u534f\u8c03\u6027\u5728\u5177\u8eab\u7ea6\u675f\u4e0b\u53ef\u80fd\u6210\u4e3a\u8d1f\u62c5\uff0c\u96c6\u4e2d\u5f0f\u5b66\u4e60\u7684\u6709\u6548\u6027\u672c\u8d28\u4e0a\u53d6\u51b3\u4e8e\u5177\u4f53\u673a\u5236\u548c\u89d2\u8272\u800c\u975e\u666e\u904d\u9002\u7528\u3002"}}
{"id": "2601.17670", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17670", "abs": "https://arxiv.org/abs/2601.17670", "authors": ["Roberto Rossi", "Steven D. Prestwich"], "title": "Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop", "comment": "18 pages, 10 figures", "summary": "This work investigates generative mathematical programming through the lens of Algebraic Modelling Languages (AMLs) and compiler-guided model synthesis. By leveraging PyOPL, an OPL-like AML compiler that provides detailed syntax diagnostics, we introduce SyntAGM, an end-to-end system that translates natural language problem descriptions into PyOPL models via a generate--compile--assess--revise loop. SyntAGM is grammar-aware thanks to in-context exposure to the PyOPL BNF grammar, and benefits from few-shot retrieval of literate PyOPL model exemplars. To obtain a valid PyOPL model that matches the problem description, SyntAGM mobilises compiler feedback and an LLM-based alignment judge. In a comparative study against established prompting baselines SyntAGM achieves competitive accuracy with superior token, cost, and latency profiles.", "AI": {"tldr": "SyntAGM\uff1a\u4e00\u4e2a\u901a\u8fc7\u751f\u6210-\u7f16\u8bd1-\u8bc4\u4f30-\u4fee\u8ba2\u5faa\u73af\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u8f6c\u6362\u4e3aPyOPL\u6570\u5b66\u89c4\u5212\u6a21\u578b\u7684\u7aef\u5230\u7aef\u7cfb\u7edf", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4ee3\u6570\u5efa\u6a21\u8bed\u8a00\u548c\u7f16\u8bd1\u5668\u5f15\u5bfc\u7684\u6a21\u578b\u5408\u6210\u6765\u5b9e\u73b0\u751f\u6210\u5f0f\u6570\u5b66\u89c4\u5212\uff0c\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u5230\u6570\u5b66\u89c4\u5212\u6a21\u578b\u7684\u81ea\u52a8\u8f6c\u6362\u95ee\u9898", "method": "\u57fa\u4e8ePyOPL\u7f16\u8bd1\u5668\u6784\u5efaSyntAGM\u7cfb\u7edf\uff0c\u91c7\u7528\u751f\u6210-\u7f16\u8bd1-\u8bc4\u4f30-\u4fee\u8ba2\u5faa\u73af\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587BNF\u8bed\u6cd5\u5b66\u4e60\u548c\u5c11\u6837\u672c\u68c0\u7d22\uff0c\u5229\u7528\u7f16\u8bd1\u5668\u53cd\u9988\u548cLLM\u5bf9\u9f50\u5224\u65ad\u5668", "result": "\u4e0e\u73b0\u6709\u63d0\u793a\u57fa\u51c6\u76f8\u6bd4\uff0cSyntAGM\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728token\u6570\u91cf\u3001\u6210\u672c\u548c\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u66f4\u4f18", "conclusion": "SyntAGM\u5c55\u793a\u4e86\u7f16\u8bd1\u5668\u5f15\u5bfc\u7684\u6a21\u578b\u5408\u6210\u5728\u751f\u6210\u6570\u5b66\u89c4\u5212\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5230\u4ee3\u6570\u5efa\u6a21\u8bed\u8a00\u7684\u8f6c\u6362\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17292", "abs": "https://arxiv.org/abs/2601.17292", "authors": ["Zhiyin Zhou"], "title": "Risk-based test framework for LLM features in regulated software", "comment": null, "summary": "Large language models are increasingly embedded in regulated and safety-critical software, including clinical research platforms and healthcare information systems. While these features enable natural language search, summarization, and configuration assistance, they introduce risks such as hallucinations, harmful or out-of-scope advice, privacy and security issues, bias, instability under change, and adversarial misuse. Prior work on machine learning testing and AI assurance offers useful concepts but limited guidance for interactive, product-embedded assistants. This paper proposes a risk-based testing framework for LLM features in regulated software: a six-category risk taxonomy, a layered test strategy mapping risks to concrete tests across guardrail, orchestration, and system layers, and a case study applying the approach to a Knowledgebase assistant in a clinical research platform.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u98ce\u9669\u7684LLM\u529f\u80fd\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b\u98ce\u9669\u5206\u7c7b\u3001\u5206\u5c42\u6d4b\u8bd5\u7b56\u7565\uff0c\u5e76\u5728\u4e34\u5e8a\u7814\u7a76\u5e73\u53f0\u4e2d\u5e94\u7528\u9a8c\u8bc1", "motivation": "LLM\u5728\u53d7\u76d1\u7ba1\u548c\u5b89\u5168\u5173\u952e\u8f6f\u4ef6\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u3001\u6709\u5bb3\u5efa\u8bae\u3001\u9690\u79c1\u5b89\u5168\u3001\u504f\u89c1\u3001\u4e0d\u7a33\u5b9a\u6027\u548c\u5bf9\u6297\u6027\u6ee5\u7528\u7b49\u98ce\u9669\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6d4b\u8bd5\u548cAI\u4fdd\u8bc1\u65b9\u6cd5\u5bf9\u4ea4\u4e92\u5f0f\u4ea7\u54c1\u5d4c\u5165\u5f0f\u52a9\u624b\u6307\u5bfc\u6709\u9650\uff0c\u9700\u8981\u4e13\u95e8\u6d4b\u8bd5\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u98ce\u9669\u7684\u6d4b\u8bd5\u6846\u67b6\uff1a1) \u516d\u7c7b\u98ce\u9669\u5206\u7c7b\u6cd5\uff1b2) \u5206\u5c42\u6d4b\u8bd5\u7b56\u7565\uff0c\u5c06\u98ce\u9669\u6620\u5c04\u5230\u62a4\u680f\u5c42\u3001\u7f16\u6392\u5c42\u548c\u7cfb\u7edf\u5c42\u7684\u5177\u4f53\u6d4b\u8bd5\uff1b3) \u5728\u4e34\u5e8a\u7814\u7a76\u5e73\u53f0\u7684\u77e5\u8bc6\u5e93\u52a9\u624b\u4e2d\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u6846\u67b6\u5728\u4e34\u5e8a\u7814\u7a76\u5e73\u53f0\u7684\u77e5\u8bc6\u5e93\u52a9\u624b\u4e2d\u6210\u529f\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u98ce\u9669\u5206\u7c7b\u6620\u5c04\u5230\u5177\u4f53\u6d4b\u8bd5\uff0c\u4e3a\u53d7\u76d1\u7ba1\u8f6f\u4ef6\u4e2d\u7684LLM\u529f\u80fd\u63d0\u4f9b\u7cfb\u7edf\u5316\u6d4b\u8bd5\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53d7\u76d1\u7ba1\u8f6f\u4ef6\u4e2d\u7684LLM\u529f\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u98ce\u9669\u5bfc\u5411\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u548c\u5177\u4f53\u6848\u4f8b\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\uff0c\u6709\u52a9\u4e8e\u964d\u4f4eLLM\u96c6\u6210\u98ce\u9669\u5e76\u786e\u4fdd\u5408\u89c4\u6027\u3002"}}
{"id": "2601.16985", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16985", "abs": "https://arxiv.org/abs/2601.16985", "authors": ["Pierrick Lorang"], "title": "Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics", "comment": "IEEE ICRA 2025 Doctoral Consortium", "summary": "Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u62bd\u8c61\u3001\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5feb\u901f\u9002\u5e94\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u610f\u5916\u53d8\u5316", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u9002\u5e94\u610f\u5916\u53d8\u5316\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u6df7\u5408\u89c4\u5212\u4e0e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u9002\u5e94\u901f\u5ea6\u6162\u548c\u707e\u96be\u6027\u9057\u5fd8\u7b49\u95ee\u9898", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u6574\u5408\u5206\u5c42\u62bd\u8c61\u3001\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212(TAMP)\u548c\u5f3a\u5316\u5b66\u4e60\u3002\u7ed3\u5408\u7b26\u53f7\u5316\u76ee\u6807\u5bfc\u5411\u5b66\u4e60\u548c\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u63a2\u7d22\uff0c\u4fc3\u8fdb\u5bf9\u73af\u5883\u53d8\u5316\u7684\u5feb\u901f\u9002\u5e94", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u6539\u8fdb\u7684\u6837\u672c\u6548\u7387\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u6574\u5408\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u4e16\u754c\u9002\u5e94\u6027\u95ee\u9898"}}
{"id": "2601.17256", "categories": ["cs.ET", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.17256", "abs": "https://arxiv.org/abs/2601.17256", "authors": ["Gabriel Geffen", "Jun Zhao", "Mingfeng Shang", "Shian Wang", "Yao-Jan Wu"], "title": "Safety, Mobility, and Environmental Impacts of Driver-Assistance-Enabled Electric Vehicles: An Empirical Study", "comment": null, "summary": "The advancement of vehicle automation and the growing adoption of electric vehicles (EVs) are reshaping transportation systems. While fully automated vehicles are expected to improve traffic stability, efficiency, and sustainability, recent studies suggest that partially automated vehicles, such as those equipped with adaptive cruise control (ACC), may adversely affect traffic flow. These drawbacks may not extend to ACC-enabled EVs due to their distinct mechanical characteristics, including regenerative braking and smoother torque delivery. As a result, the impacts of EVs operating under ACC remain insufficiently understood.\n  To address this gap, this study develops an empirical framework using the OpenACC dataset to compare ACC-enabled EVs and internal combustion engine vehicles. Dynamic time warping aligns comparable lead-vehicle trajectories. Results show that EVs exhibit smoother speed profiles, lower speed variability, and shorter spacing, leading to higher efficiency. EVs reduce critical safety events by over 85% and lower platoon-level emissions by up to 26.2%.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u6bd4\u8f83\u4e86\u914d\u5907\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\uff08ACC\uff09\u7684\u7535\u52a8\u6c7d\u8f66\u548c\u5185\u71c3\u673a\u8f66\u8f86\u5bf9\u4ea4\u901a\u6d41\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7535\u52a8\u6c7d\u8f66\u5728ACC\u6a21\u5f0f\u4e0b\u5177\u6709\u66f4\u5e73\u6ed1\u7684\u901f\u5ea6\u66f2\u7ebf\u3001\u66f4\u4f4e\u7684\u4ea4\u901a\u6ce2\u52a8\u548c\u66f4\u9ad8\u7684\u6548\u7387\u3002", "motivation": "\u867d\u7136\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6709\u671b\u6539\u5584\u4ea4\u901a\u7a33\u5b9a\u6027\u3001\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\uff0c\u4f46\u90e8\u5206\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08\u5982\u914d\u5907ACC\u7684\u8f66\u8f86\uff09\u53ef\u80fd\u5bf9\u4ea4\u901a\u6d41\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u7531\u4e8e\u7535\u52a8\u6c7d\u8f66\u5177\u6709\u518d\u751f\u5236\u52a8\u548c\u5e73\u6ed1\u626d\u77e9\u8f93\u51fa\u7b49\u72ec\u7279\u673a\u68b0\u7279\u6027\uff0cACC\u5bf9\u7535\u52a8\u6c7d\u8f66\u7684\u5f71\u54cd\u53ef\u80fd\u4e0d\u540c\uff0c\u4f46\u76ee\u524d\u5bf9\u6b64\u4e86\u89e3\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u4f7f\u7528OpenACC\u6570\u636e\u96c6\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u8bc1\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u6280\u672f\u5bf9\u9f50\u53ef\u6bd4\u8f83\u7684\u524d\u8f66\u8f68\u8ff9\uff0c\u6bd4\u8f83ACC-enabled\u7535\u52a8\u6c7d\u8f66\u548c\u5185\u71c3\u673a\u8f66\u8f86\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u7535\u52a8\u6c7d\u8f66\u8868\u73b0\u51fa\u66f4\u5e73\u6ed1\u7684\u901f\u5ea6\u66f2\u7ebf\u3001\u66f4\u4f4e\u7684\u901f\u5ea6\u53d8\u5f02\u6027\u3001\u66f4\u77ed\u7684\u8f66\u95f4\u8ddd\uff0c\u4ece\u800c\u5e26\u6765\u66f4\u9ad8\u7684\u6548\u7387\u3002\u7535\u52a8\u6c7d\u8f66\u5c06\u5173\u952e\u5b89\u5168\u4e8b\u4ef6\u51cf\u5c11\u4e8685%\u4ee5\u4e0a\uff0c\u5e76\u5c06\u8f66\u961f\u7ea7\u6392\u653e\u964d\u4f4e\u4e86\u9ad8\u8fbe26.2%\u3002", "conclusion": "\u914d\u5907ACC\u7684\u7535\u52a8\u6c7d\u8f66\u76f8\u6bd4\u4f20\u7edf\u5185\u71c3\u673a\u8f66\u8f86\u5728\u4ea4\u901a\u6d41\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5305\u62ec\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u66f4\u4f4e\u7684\u6392\u653e\uff0c\u8fd9\u5bf9\u672a\u6765\u4ea4\u901a\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.17608", "categories": ["cs.HC", "cs.SD", "eess.AS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.17608", "abs": "https://arxiv.org/abs/2601.17608", "authors": ["Dong Yoon Lee", "Alyssa Weakley", "Hui Wei", "Daniel Cardona", "Shijia Pan"], "title": "Home Health System Deployment Experience for Geriatric Care Remote Monitoring", "comment": null, "summary": "To support aging-in-place, adult children often provide care to their aging parents from a distance. These informal caregivers desire plug-and-play remote care solutions for privacy-preserving continuous monitoring that enabling real-time activity monitoring and intuitive, actionable information. This short paper presents insights from three iterations of deployment experience for remote monitoring system and the iterative improvement in hardware, modeling, and user interface guided by the Geriatric 4Ms framework (matters most, mentation, mobility, and medication). An LLM-assisted solution is developed to balance user experience (privacy-preserving, plug-and-play) and system performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8eGeriatric 4Ms\u6846\u67b6\u7684\u8fdc\u7a0b\u76d1\u62a4\u7cfb\u7edf\uff0c\u901a\u8fc7LLM\u8f85\u52a9\u65b9\u6848\u5728\u7528\u6237\u4f53\u9a8c\uff08\u9690\u79c1\u4fdd\u62a4\u3001\u5373\u63d2\u5373\u7528\uff09\u548c\u7cfb\u7edf\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u652f\u6301\u5b50\u5973\u8fdc\u7a0b\u7167\u62a4\u8001\u4eba\u3002", "motivation": "\u652f\u6301\"\u5728\u5730\u8001\u5316\"\uff0c\u89e3\u51b3\u6210\u5e74\u5b50\u5973\u8fdc\u7a0b\u7167\u62a4\u7236\u6bcd\u7684\u9700\u6c42\uff0c\u9700\u8981\u5373\u63d2\u5373\u7528\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u6301\u7eed\u76d1\u62a4\u65b9\u6848\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6d3b\u52a8\u76d1\u6d4b\u548c\u76f4\u89c2\u53ef\u64cd\u4f5c\u7684\u4fe1\u606f\u53cd\u9988\u3002", "method": "\u57fa\u4e8eGeriatric 4Ms\u6846\u67b6\uff08\u6700\u91cd\u8981\u4e8b\u9879\u3001\u7cbe\u795e\u72b6\u6001\u3001\u884c\u52a8\u80fd\u529b\u3001\u7528\u836f\u7ba1\u7406\uff09\u8fdb\u884c\u4e09\u8f6e\u90e8\u7f72\u8fed\u4ee3\uff0c\u6539\u8fdb\u786c\u4ef6\u3001\u5efa\u6a21\u548c\u7528\u6237\u754c\u9762\uff0c\u5f00\u53d1LLM\u8f85\u52a9\u65b9\u6848\u5e73\u8861\u7528\u6237\u4f53\u9a8c\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u4e09\u6b21\u90e8\u7f72\u7ecf\u9a8c\u83b7\u5f97\u7cfb\u7edf\u6539\u8fdb\u7684\u6df1\u5165\u89c1\u89e3\uff0cLLM\u8f85\u52a9\u65b9\u6848\u6210\u529f\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u5373\u63d2\u5373\u7528\u7684\u7528\u6237\u4f53\u9a8c\u4e0e\u7cfb\u7edf\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u57fa\u4e8eGeriatric 4Ms\u6846\u67b6\u7684\u8fed\u4ee3\u5f00\u53d1\u548cLLM\u8f85\u52a9\u65b9\u6848\u80fd\u591f\u6709\u6548\u652f\u6301\u8fdc\u7a0b\u7167\u62a4\uff0c\u5728\u7528\u6237\u4f53\u9a8c\u548c\u7cfb\u7edf\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u826f\u597d\u5e73\u8861\uff0c\u4e3a\"\u5728\u5730\u8001\u5316\"\u63d0\u4f9b\u53ef\u884c\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17513", "categories": ["cs.NE", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.17513", "abs": "https://arxiv.org/abs/2601.17513", "authors": ["Moahmed Hamza Boulaich", "Said Ohamouddou", "Mohammed Ali Ennasar", "Abdelatif El Afia"], "title": "Constrained Multi-Objective Genetic Algorithm Variants for Design and Optimization of Tri-Band Microstrip Patch Antenna loaded CSRR for IoT Applications: A Comparative Case Study", "comment": null, "summary": "This paper presents an automated antenna design and optimization framework employing multi-objective genetic algorithms (MOGAs) to investigate various evolutionary optimization approaches, with a primary emphasis on multi-band frequency optimization. Five MOGA variants were implemented and compared: the Pareto genetic algorithm (PGA), non-dominated sorting genetic algorithm with niching (NSGA-I), non-dominated sorting genetic algorithm with elitism (NSGA-II), non-dominated sorting genetic algorithm using reference points (NSGA-III), and strength Pareto evolutionary algorithm (SPEA). These algorithms are employed to design and optimize microstrip patch antennas loaded with complementary split-ring resonators (CSRRs). A weighted-sum scalarization approach was adopted within a single-objective genetic algorithm framework enhanced with domain-specific constraint handling mechanisms. The optimization addresses the conflicting objectives of minimizing the return loss ($S_{11} < -10$~dB) and achieving multi-band resonance at 2.4~GHz, 3.6~GHz, and 5.2~GHz. The proposed method delivers a superior overall performance by aggregating these objectives into a unified fitness function encompassing $S_{11}$(2.4~GHz), $S_{11}$(3.6~GHz), and $S_{11}$(5.2~GHz). This approach effectively balances all three frequency bands simultaneously, rather than exploring trade-off solutions typical of traditional multi-objective approaches. The antenna was printed on a Rogers RT5880 substrate with a dielectric constant of 2.2 , loss tangent of 0.0009 , and thickness of 1.57~mm . Scalarization approach achieved return loss values of $-21.56$~dB, $-16.60$~dB, and $-27.69$~dB, with corresponding gains of 1.96~dBi, 2.6~dB, and 3.99~dBi at 2.4~GHz, 3.6~GHz, and 5.2~GHz, respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\u7684\u81ea\u52a8\u5316\u5929\u7ebf\u8bbe\u8ba1\u4e0e\u4f18\u5316\u6846\u67b6\uff0c\u91cd\u70b9\u7814\u7a76\u591a\u9891\u6bb5\u9891\u7387\u4f18\u5316\uff0c\u6bd4\u8f83\u4e86\u4e94\u79cdMOGA\u53d8\u4f53\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u548c\u6807\u91cf\u5316\u65b9\u6cd5\u8bbe\u8ba1CSRR\u52a0\u8f7d\u7684\u5fae\u5e26\u8d34\u7247\u5929\u7ebf\uff0c\u57282.4GHz\u30013.6GHz\u548c5.2GHz\u4e09\u4e2a\u9891\u6bb5\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u56de\u6ce2\u635f\u8017\u548c\u589e\u76ca\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\u5728\u5929\u7ebf\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u9488\u5bf9\u591a\u9891\u6bb5\u9891\u7387\u4f18\u5316\u7684\u9700\u6c42\u3002\u4f20\u7edf\u591a\u76ee\u6807\u65b9\u6cd5\u901a\u5e38\u4ea7\u751f\u6743\u8861\u89e3\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5e73\u8861\u591a\u4e2a\u9891\u6bb5\u6027\u80fd\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e94\u79cd\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\u53d8\u4f53\uff08PGA\u3001NSGA-I\u3001NSGA-II\u3001NSGA-III\u3001SPEA\uff09\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002\u4f7f\u7528\u52a0\u6743\u548c\u6807\u91cf\u5316\u65b9\u6cd5\u5c06\u591a\u76ee\u6807\u95ee\u9898\u8f6c\u5316\u4e3a\u5355\u76ee\u6807\u4f18\u5316\uff0c\u7ed3\u5408\u7279\u5b9a\u9886\u57df\u7684\u7ea6\u675f\u5904\u7406\u673a\u5236\u3002\u8bbe\u8ba1\u5bf9\u8c61\u4e3a\u52a0\u8f7d\u4e92\u8865\u5f00\u53e3\u73af\u8c10\u632f\u5668\u7684\u5fae\u5e26\u8d34\u7247\u5929\u7ebf\uff0c\u4f18\u5316\u76ee\u6807\u662f\u6700\u5c0f\u5316\u56de\u6ce2\u635f\u8017\u5e76\u5b9e\u73b02.4GHz\u30013.6GHz\u548c5.2GHz\u4e09\u4e2a\u9891\u6bb5\u7684\u8c10\u632f\u3002", "result": "\u6807\u91cf\u5316\u65b9\u6cd5\u5728\u4e09\u4e2a\u76ee\u6807\u9891\u6bb5\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u56de\u6ce2\u635f\u8017\u6027\u80fd\uff1a2.4GHz\u4e3a-21.56dB\u30013.6GHz\u4e3a-16.60dB\u30015.2GHz\u4e3a-27.69dB\u3002\u5bf9\u5e94\u7684\u589e\u76ca\u5206\u522b\u4e3a1.96dBi\u30012.6dB\u548c3.99dBi\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u5e73\u8861\u4e09\u4e2a\u9891\u6bb5\u7684\u6027\u80fd\uff0c\u800c\u975e\u4ea7\u751f\u4f20\u7edf\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u6743\u8861\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a0\u6743\u548c\u6807\u91cf\u5316\u65b9\u6cd5\u5728\u591a\u9891\u6bb5\u5929\u7ebf\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u591a\u4e2a\u9891\u6bb5\u7684\u6027\u80fd\u8981\u6c42\u3002\u8be5\u65b9\u6cd5\u4e3a\u5929\u7ebf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u4f18\u5316\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u591a\u4e2a\u9891\u6bb5\u6027\u80fd\u6307\u6807\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.17957", "categories": ["cs.PL", "cs.DC", "cs.FL", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17957", "abs": "https://arxiv.org/abs/2601.17957", "authors": ["Ehud Shapiro"], "title": "Types for Grassroots Logic Programs", "comment": null, "summary": "Grassroots Logic Programs (GLP) is a concurrent logic programming language in which logic variables are partitioned into paired readers and writers. An assignment is produced at most once via a writer and consumed at most once via its paired reader, and may contain additional readers and/or writers. This enables the concise expression of rich multidirectional communication modalities.\n  ``Logic Programs as Types for Logic Programs'' (LICS'91) defined types as regular sets of paths over derivable ground atoms. Here, we define types to be regular sets of moded paths, where a mode captures directionality of communication -- whether a subterm is consumed from or produced to the environment -- enabling the typing of interactive partial computations including those that eventually deadlock or fail, or never terminate. We provide a syntactic definition of well-typing and prove that a program is well-typed iff the path abstraction of its moded-atom semantics satisfies covariance and contravariance conditions with respect to its type.\n  The GLP type system was implemented in Dart by AI, starting from a mathematical specification of Typed GLP (this paper), deriving from it an English spec (written by AI), and from the spec deriving Dart code (by AI). While GLP is naturally untyped, the motivation for Typed GLP comes from programming with AI: Asking AI to program complex communication modalities in GLP (and in general) and hoping for the best is a tenuous strategy. The emerging discipline we advocate and employ is for the human designer and AI to jointly develop and agree upon (1)~GLP types; (2)~GLP procedure type declarations; (3)~informal (English) descriptions of the procedures; and only then let AI attempt to write (4)~GLP code based on those.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Typed GLP\uff08\u7c7b\u578b\u5316Grassroots Logic Programs\uff09\uff0c\u4e3a\u5e76\u53d1\u903b\u8f91\u7f16\u7a0b\u8bed\u8a00GLP\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7c7b\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u6001\u8def\u5f84\u548c\u901a\u4fe1\u65b9\u5411\u6027\u6765\u7c7b\u578b\u5316\u4ea4\u4e92\u5f0f\u90e8\u5206\u8ba1\u7b97\uff0c\u5e76\u5b9e\u73b0\u4e86\u4ece\u6570\u5b66\u89c4\u8303\u5230Dart\u4ee3\u7801\u7684AI\u9a71\u52a8\u5f00\u53d1\u6d41\u7a0b\u3002", "motivation": "GLP\u4f5c\u4e3a\u65e0\u7c7b\u578b\u5e76\u53d1\u903b\u8f91\u7f16\u7a0b\u8bed\u8a00\uff0c\u5728\u8868\u8fbe\u590d\u6742\u901a\u4fe1\u6a21\u5f0f\u65f6\u5b58\u5728\u6311\u6218\u3002\u7279\u522b\u662f\u5f53\u4e0eAI\u534f\u4f5c\u7f16\u7a0b\u65f6\uff0c\u7f3a\u4e4f\u7c7b\u578b\u7cfb\u7edf\u4f7f\u5f97\u8ba9AI\u7f16\u5199\u590d\u6742\u7684GLP\u4ee3\u7801\u53d8\u5f97\u4e0d\u53ef\u9760\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u7c7b\u578b\u7cfb\u7edf\u6765\u652f\u6301AI\u8f85\u52a9\u7f16\u7a0b\uff0c\u4f7f\u4eba\u7c7b\u8bbe\u8ba1\u5e08\u548cAI\u80fd\u591f\u57fa\u4e8e\u7c7b\u578b\u89c4\u8303\u548c\u58f0\u660e\u534f\u540c\u5f00\u53d1\u53ef\u9760\u7684GLP\u7a0b\u5e8f\u3002", "method": "1. \u6269\u5c55LICS'91\u7684\u7c7b\u578b\u5b9a\u4e49\uff0c\u5c06\u7c7b\u578b\u5b9a\u4e49\u4e3a\u6a21\u6001\u8def\u5f84\u7684\u6b63\u5219\u96c6\u5408\uff0c\u5176\u4e2d\u6a21\u5f0f\u6355\u83b7\u901a\u4fe1\u65b9\u5411\u6027\uff08\u6d88\u8d39\u6216\u751f\u4ea7\uff09\u30022. \u63d0\u4f9b\u8bed\u6cd5\u4e0a\u7684\u826f\u7c7b\u578b\u5b9a\u4e49\uff0c\u5e76\u8bc1\u660e\u7a0b\u5e8f\u826f\u7c7b\u578b\u5f53\u4e14\u4ec5\u5f53\u5176\u6a21\u6001\u539f\u5b50\u8bed\u4e49\u7684\u8def\u5f84\u62bd\u8c61\u6ee1\u8db3\u76f8\u5bf9\u4e8e\u5176\u7c7b\u578b\u7684\u534f\u53d8\u548c\u9006\u53d8\u6761\u4ef6\u30023. \u91c7\u7528AI\u9a71\u52a8\u5b9e\u73b0\uff1a\u4ece\u6570\u5b66\u89c4\u8303\u63a8\u5bfc\u51fa\u82f1\u6587\u89c4\u8303\uff0c\u518d\u4ece\u89c4\u8303\u751f\u6210Dart\u4ee3\u7801\u3002", "result": "1. \u5efa\u7acb\u4e86Typed GLP\u7c7b\u578b\u7cfb\u7edf\u7684\u7406\u8bba\u57fa\u7840\uff0c\u80fd\u591f\u7c7b\u578b\u5316\u5305\u62ec\u53ef\u80fd\u6b7b\u9501\u3001\u5931\u8d25\u6216\u6c38\u4e0d\u7ec8\u6b62\u7684\u4ea4\u4e92\u5f0f\u90e8\u5206\u8ba1\u7b97\u30022. \u5b9e\u73b0\u4e86GLP\u7c7b\u578b\u7cfb\u7edf\u7684Dart\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u4ece\u6570\u5b66\u89c4\u8303\u5230\u4ee3\u7801\u7684AI\u9a71\u52a8\u5f00\u53d1\u6d41\u7a0b\u30023. \u63d0\u51fa\u4e86\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u7684\u7f16\u7a0b\u65b9\u6cd5\u8bba\uff1a\u5148\u5171\u540c\u5f00\u53d1\u7c7b\u578b\u3001\u8fc7\u7a0b\u7c7b\u578b\u58f0\u660e\u548c\u82f1\u6587\u63cf\u8ff0\uff0c\u518d\u8ba9AI\u57fa\u4e8e\u8fd9\u4e9b\u7f16\u5199\u4ee3\u7801\u3002", "conclusion": "Typed GLP\u4e3a\u5e76\u53d1\u903b\u8f91\u7f16\u7a0b\u63d0\u4f9b\u4e86\u7c7b\u578b\u5b89\u5168\u7684\u57fa\u7840\uff0c\u7279\u522b\u9002\u5408AI\u8f85\u52a9\u7f16\u7a0b\u573a\u666f\u3002\u901a\u8fc7\u6a21\u6001\u8def\u5f84\u548c\u65b9\u5411\u6027\u7c7b\u578b\uff0c\u80fd\u591f\u7cbe\u786e\u63cf\u8ff0\u590d\u6742\u7684\u591a\u5411\u901a\u4fe1\u6a21\u5f0f\u3002\u8bba\u6587\u5c55\u793a\u7684AI\u9a71\u52a8\u5f00\u53d1\u6d41\u7a0b\u4e3a\u5f62\u5f0f\u5316\u89c4\u8303\u5230\u5b9e\u73b0\u4ee3\u7801\u7684\u81ea\u52a8\u5316\u8f6c\u6362\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u4fc3\u8fdb\u4e86\u4eba\u7c7b\u4e0eAI\u5728\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u534f\u4f5c\u3002"}}
{"id": "2601.18284", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18284", "abs": "https://arxiv.org/abs/2601.18284", "authors": ["Hsiao-Chuan Chang", "Sheng-You Huang", "Yen-Chi Chen", "I-Chen Wu"], "title": "VissimRL: A Multi-Agent Reinforcement Learning Framework for Traffic Signal Control Based on Vissim", "comment": null, "summary": "Traffic congestion remains a major challenge for urban transportation, leading to significant economic and environmental impacts. Traffic Signal Control (TSC) is one of the key measures to mitigate congestion, and recent studies have increasingly applied Reinforcement Learning (RL) for its adaptive capabilities. With respect to SUMO and CityFlow, the simulator Vissim offers high-fidelity driver behavior modeling and wide industrial adoption but remains underutilized in RL research due to its complex interface and lack of standardized frameworks. To address this gap, this paper proposes VissimRL, a modular RL framework for TSC that encapsulates Vissim's COM interface through a high-level Python API, offering standardized environments for both single- and multi-agent training. Experiments show that VissimRL significantly reduces development effort while maintaining runtime efficiency, and supports consistent improvements in traffic performance during training, as well as emergent coordination in multi-agent control. Overall, VissimRL demonstrates the feasibility of applying RL in high-fidelity simulations and serves as a bridge between academic research and practical applications in intelligent traffic signal control.", "AI": {"tldr": "\u63d0\u51faVissimRL\u6846\u67b6\uff0c\u5c06\u9ad8\u4fdd\u771f\u4ea4\u901a\u4eff\u771f\u5668Vissim\u5c01\u88c5\u4e3a\u6a21\u5757\u5316RL\u73af\u5883\uff0c\u89e3\u51b3\u5176\u5728RL\u7814\u7a76\u4e2d\u5e94\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u652f\u6301\u5355/\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8bad\u7ec3\u3002", "motivation": "Vissim\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u4ea4\u901a\u4eff\u771f\u5668\u5728\u5de5\u4e1a\u754c\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u63a5\u53e3\u590d\u6742\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u6846\u67b6\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9700\u8981\u642d\u5efa\u6865\u6881\u8fde\u63a5\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f00\u53d1VissimRL\u6a21\u5757\u5316RL\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u5c42Python API\u5c01\u88c5Vissim\u7684COM\u63a5\u53e3\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u73af\u5883\uff0c\u964d\u4f4e\u5f00\u53d1\u590d\u6742\u5ea6\u3002", "result": "VissimRL\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u5de5\u4f5c\u91cf\u540c\u65f6\u4fdd\u6301\u8fd0\u884c\u6548\u7387\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6539\u5584\u4ea4\u901a\u6027\u80fd\uff0c\u5e76\u5728\u591a\u667a\u80fd\u4f53\u63a7\u5236\u4e2d\u5c55\u73b0\u51fa\u6d8c\u73b0\u7684\u534f\u8c03\u884c\u4e3a\u3002", "conclusion": "VissimRL\u8bc1\u660e\u4e86\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u4e2d\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7684\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u642d\u5efa\u4e86\u6865\u6881\u3002"}}
{"id": "2601.17390", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17390", "abs": "https://arxiv.org/abs/2601.17390", "authors": ["Yayi Wang", "Shenao Wang", "Jian Zhao", "Shaosen Shi", "Ting Li", "Yan Cheng", "Lizhong Bian", "Kan Yu", "Yanjie Zhao", "Haoyu Wang"], "title": "YASA: Scalable Multi-Language Taint Analysis on the Unified AST at Ant Group", "comment": null, "summary": "Modern enterprises increasingly adopt diverse technology stacks with various programming languages, posing significant challenges for static application security testing (SAST). Existing taint analysis tools are predominantly designed for single languages, requiring substantial engineering effort that scales with language diversity. While multi-language tools like CodeQL, Joern, and WALA attempt to address these challenges, they face limitations in intermediate representation design, analysis precision, and extensibility, which make them difficult to scale effectively for large-scale industrial applications at Ant Group. To bridge this gap, we present YASA (Yet Another Static Analyzer), a unified multi-language static taint analysis framework designed for industrial-scale deployment. Specifically, YASA introduces the Unified Abstract Syntax Tree (UAST) that provides a unified abstraction for compatibility across diverse programming languages. Building on the UAST, YASA performs point-to analysis and taint propagation, leveraging a unified semantic model to manage language-agnostic constructs, while incorporating language-specific semantic models to handle other unique language features. When compared to 6 single- and 2 multi-language static analyzers on an industry-standard benchmark, YASA consistently outperformed all baselines across Java, JavaScript, Python, and Go. In real-world deployment within Ant Group, YASA analyzed over 100 million lines of code across 7.3K internal applications. It identified 314 previously unknown taint paths, with 92 of them confirmed as 0-day vulnerabilities. All vulnerabilities were responsibly reported, with 76 already patched by internal development teams, demonstrating YASA's practical effectiveness for securing large-scale industrial software systems.", "AI": {"tldr": "YASA\u662f\u4e00\u4e2a\u4e3a\u5de5\u4e1a\u89c4\u6a21\u90e8\u7f72\u8bbe\u8ba1\u7684\u7edf\u4e00\u591a\u8bed\u8a00\u9759\u6001\u6c61\u70b9\u5206\u6790\u6846\u67b6\uff0c\u5728Ant Group\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u6210\u529f\u5206\u6790\u4e86\u8d85\u8fc71\u4ebf\u884c\u4ee3\u7801\uff0c\u53d1\u73b0\u4e86314\u6761\u5148\u524d\u672a\u77e5\u7684\u6c61\u70b9\u8def\u5f84\uff0c\u5176\u4e2d92\u6761\u88ab\u786e\u8ba4\u4e3a0-day\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u4ee3\u4f01\u4e1a\u91c7\u7528\u591a\u6837\u5316\u7684\u6280\u672f\u6808\u548c\u7f16\u7a0b\u8bed\u8a00\uff0c\u7ed9\u9759\u6001\u5e94\u7528\u5b89\u5168\u6d4b\u8bd5\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u6c61\u70b9\u5206\u6790\u5de5\u5177\u4e3b\u8981\u9488\u5bf9\u5355\u4e00\u8bed\u8a00\u8bbe\u8ba1\uff0c\u591a\u8bed\u8a00\u5de5\u5177\u5982CodeQL\u3001Joern\u548cWALA\u5728\u4e2d\u95f4\u8868\u793a\u8bbe\u8ba1\u3001\u5206\u6790\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u96be\u4ee5\u6709\u6548\u6269\u5c55\u5230Ant Group\u7684\u5927\u89c4\u6a21\u5de5\u4e1a\u5e94\u7528\u3002", "method": "YASA\u5f15\u5165\u4e86\u7edf\u4e00\u62bd\u8c61\u8bed\u6cd5\u6811\uff08UAST\uff09\uff0c\u4e3a\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u63d0\u4f9b\u7edf\u4e00\u62bd\u8c61\u3002\u57fa\u4e8eUAST\uff0cYASA\u6267\u884c\u6307\u9488\u5206\u6790\u548c\u6c61\u70b9\u4f20\u64ad\uff0c\u5229\u7528\u7edf\u4e00\u8bed\u4e49\u6a21\u578b\u7ba1\u7406\u8bed\u8a00\u65e0\u5173\u7684\u6784\u9020\uff0c\u540c\u65f6\u7ed3\u5408\u8bed\u8a00\u7279\u5b9a\u7684\u8bed\u4e49\u6a21\u578b\u5904\u7406\u5176\u4ed6\u72ec\u7279\u7684\u8bed\u8a00\u7279\u6027\u3002", "result": "\u5728\u884c\u4e1a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cYASA\u5728Java\u3001JavaScript\u3001Python\u548cGo\u4e0a\u59cb\u7ec8\u4f18\u4e8e6\u4e2a\u5355\u8bed\u8a00\u548c2\u4e2a\u591a\u8bed\u8a00\u9759\u6001\u5206\u6790\u5668\u3002\u5728Ant Group\u7684\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0cYASA\u5206\u6790\u4e86\u8d85\u8fc71\u4ebf\u884c\u4ee3\u7801\uff0c\u8986\u76d67.3K\u4e2a\u5185\u90e8\u5e94\u7528\uff0c\u53d1\u73b0\u4e86314\u6761\u5148\u524d\u672a\u77e5\u7684\u6c61\u70b9\u8def\u5f84\uff0c\u5176\u4e2d92\u6761\u88ab\u786e\u8ba4\u4e3a0-day\u6f0f\u6d1e\uff0c76\u4e2a\u6f0f\u6d1e\u5df2\u88ab\u5185\u90e8\u5f00\u53d1\u56e2\u961f\u4fee\u590d\u3002", "conclusion": "YASA\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u8bed\u8a00\u9759\u6001\u6c61\u70b9\u5206\u6790\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5de5\u4e1a\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u591a\u8bed\u8a00\u9759\u6001\u5206\u6790\u7684\u53ef\u6269\u5c55\u6027\u548c\u7cbe\u5ea6\u95ee\u9898\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u53d1\u73b0\u548c\u4fee\u590d\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\u3002"}}
{"id": "2601.17219", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17219", "abs": "https://arxiv.org/abs/2601.17219", "authors": ["David Wireko Atibila", "Vineet R. Kamat", "Carol C. Menassa"], "title": "Advancing Improvisation in Human-Robot Construction Collaboration: Taxonomy and Research Roadmap", "comment": "73 pages, 8 figures", "summary": "The construction industry faces productivity stagnation, skilled labor shortages, and safety concerns. While robotic automation offers solutions, construction robots struggle to adapt to unstructured, dynamic sites. Central to this is improvisation, adapting to unexpected situations through creative problem-solving, which remains predominantly human. In construction's unpredictable environments, collaborative human-robot improvisation is essential for workflow continuity. This research develops a six-level taxonomy classifying human-robot collaboration (HRC) based on improvisation capabilities. Through systematic review of 214 articles (2010-2025), we categorize construction robotics across: Manual Work (Level 0), Human-Controlled Execution (Level 1), Adaptive Manipulation (Level 2), Imitation Learning (Level 3), Human-in-Loop BIM Workflow (Level 4), Cloud-Based Knowledge Integration (Level 5), and True Collaborative Improvisation (Level 6). Analysis reveals current research concentrates at lower levels, with critical gaps in experiential learning and limited progression toward collaborative improvisation. A five-dimensional radar framework illustrates progressive evolution of Planning, Cognitive Role, Physical Execution, Learning Capability, and Improvisation, demonstrating how complementary human-robot capabilities create team performance exceeding individual contributions. The research identifies three fundamental barriers: technical limitations in grounding and dialogic reasoning, conceptual gaps between human improvisation and robotics research, and methodological challenges. We recommend future research emphasizing improved human-robot communication via Augmented/Virtual Reality interfaces, large language model integration, and cloud-based knowledge systems to advance toward true collaborative improvisation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5373\u5174\u80fd\u529b\u7684\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u4f5c\u516d\u5c42\u7ea7\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e86\u5efa\u7b51\u673a\u5668\u4eba\u7814\u7a76\u73b0\u72b6\uff0c\u53d1\u73b0\u5f53\u524d\u4e3b\u8981\u96c6\u4e2d\u5728\u8f83\u4f4e\u5c42\u7ea7\uff0c\u5e76\u8bc6\u522b\u4e86\u5b9e\u73b0\u771f\u6b63\u534f\u4f5c\u5373\u5174\u7684\u4e09\u4e2a\u4e3b\u8981\u969c\u788d\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u9762\u4e34\u751f\u4ea7\u529b\u505c\u6ede\u3001\u719f\u7ec3\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u5b89\u5168\u95ee\u9898\uff0c\u673a\u5668\u4eba\u81ea\u52a8\u5316\u867d\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5efa\u7b51\u673a\u5668\u4eba\u96be\u4ee5\u9002\u5e94\u975e\u7ed3\u6784\u5316\u3001\u52a8\u6001\u7684\u65bd\u5de5\u73b0\u573a\u3002\u5373\u5174\u80fd\u529b\uff08\u9002\u5e94\u610f\u5916\u60c5\u51b5\u7684\u521b\u9020\u6027\u95ee\u9898\u89e3\u51b3\uff09\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u4eba\u7c7b\uff0c\u5728\u4e0d\u53ef\u9884\u6d4b\u7684\u5efa\u7b51\u73af\u5883\u4e2d\uff0c\u4eba\u673a\u534f\u4f5c\u5373\u5174\u5bf9\u4e8e\u5de5\u4f5c\u6d41\u7a0b\u8fde\u7eed\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff02010-2025\u5e74\u7684214\u7bc7\u6587\u732e\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5373\u5174\u80fd\u529b\u7684\u4eba\u673a\u534f\u4f5c\u516d\u5c42\u7ea7\u5206\u7c7b\u6cd5\uff1a\u624b\u52a8\u5de5\u4f5c\uff080\u7ea7\uff09\u3001\u4eba\u7c7b\u63a7\u5236\u6267\u884c\uff081\u7ea7\uff09\u3001\u81ea\u9002\u5e94\u64cd\u4f5c\uff082\u7ea7\uff09\u3001\u6a21\u4eff\u5b66\u4e60\uff083\u7ea7\uff09\u3001\u4eba\u5728\u73afBIM\u5de5\u4f5c\u6d41\uff084\u7ea7\uff09\u3001\u57fa\u4e8e\u4e91\u7684\u77e5\u8bc6\u96c6\u6210\uff085\u7ea7\uff09\u548c\u771f\u6b63\u534f\u4f5c\u5373\u5174\uff086\u7ea7\uff09\u3002\u4f7f\u7528\u4e94\u7ef4\u96f7\u8fbe\u6846\u67b6\uff08\u89c4\u5212\u3001\u8ba4\u77e5\u89d2\u8272\u3001\u7269\u7406\u6267\u884c\u3001\u5b66\u4e60\u80fd\u529b\u3001\u5373\u5174\uff09\u5206\u6790\u6e10\u8fdb\u6f14\u5316\u3002", "result": "\u5206\u6790\u663e\u793a\u5f53\u524d\u7814\u7a76\u96c6\u4e2d\u5728\u8f83\u4f4e\u5c42\u7ea7\uff080-3\u7ea7\uff09\uff0c\u5728\u7ecf\u9a8c\u5b66\u4e60\u548c\u5411\u534f\u4f5c\u5373\u5174\u8fdb\u5c55\u65b9\u9762\u5b58\u5728\u5173\u952e\u5dee\u8ddd\u3002\u8bc6\u522b\u4e86\u4e09\u4e2a\u57fa\u672c\u969c\u788d\uff1a1\uff09\u5728\u57fa\u7840\u548c\u5bf9\u8bdd\u63a8\u7406\u65b9\u9762\u7684\u6280\u672f\u9650\u5236\uff1b2\uff09\u4eba\u7c7b\u5373\u5174\u4e0e\u673a\u5668\u4eba\u7814\u7a76\u4e4b\u95f4\u7684\u6982\u5ff5\u5dee\u8ddd\uff1b3\uff09\u65b9\u6cd5\u8bba\u6311\u6218\u3002\u4e94\u7ef4\u96f7\u8fbe\u6846\u67b6\u5c55\u793a\u4e86\u4e92\u8865\u7684\u4eba\u673a\u80fd\u529b\u5982\u4f55\u521b\u9020\u8d85\u8d8a\u4e2a\u4f53\u8d21\u732e\u7684\u56e2\u961f\u7ee9\u6548\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u7814\u7a76\u91cd\u70b9\u901a\u8fc7\u589e\u5f3a/\u865a\u62df\u73b0\u5b9e\u754c\u9762\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u548c\u57fa\u4e8e\u4e91\u7684\u77e5\u8bc6\u7cfb\u7edf\u6539\u8fdb\u4eba\u673a\u901a\u4fe1\uff0c\u4ee5\u63a8\u8fdb\u771f\u6b63\u7684\u534f\u4f5c\u5373\u5174\u3002\u9700\u8981\u89e3\u51b3\u6280\u672f\u3001\u6982\u5ff5\u548c\u65b9\u6cd5\u8bba\u969c\u788d\uff0c\u5b9e\u73b0\u5efa\u7b51\u73af\u5883\u4e2d\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u5373\u5174\u3002"}}
{"id": "2601.18015", "categories": ["cs.ET", "cs.CC", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18015", "abs": "https://arxiv.org/abs/2601.18015", "authors": ["Hyun-Gee Jei", "Mustafa Demir", "Farzan Sasangohar"], "title": "Eyes on the Mission: Mixed Methods Assessment of Eye-Tracker-Enabled Interactive Decision Support in a Simulated Unmanned Aerial Vehicle System", "comment": "27 pages, 6 figures, 4 tables, under review", "summary": "Supervisors in military command and control (C2) environments face dynamic conditions. Dynamically changing information continuously flows to the supervisors through multiple displays. In this environment, important pieces of information can be overlooked due to the complexity of tasks and environments. This study examined the efficacy of an eye-tracker-based adaptive attention-guided decision support tool (DST) for supervisors in a simulated C2 environment. The DST monitors supervisors' visual attention allocation in real time and displays visually salient cues if critical changes or events are missed. Twenty-five military students participated in a simulated intelligence task. Results indicated significant performance enhancement when the adaptive DST was present. Eye-tracking analysis also showed that longer, more frequent fixations on critical areas of interest were negatively correlated with performance. Additionally, post-experiment interviews revealed that the adaptive DST was unobtrusive and positively received. These findings underscore the potential of real-time gaze-based interventions to optimize supervisory decision-making. Future research could incorporate AI-driven approaches to better support supervisors in complex task environments.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u5f15\u5bfc\u51b3\u7b56\u652f\u6301\u5de5\u5177\u5728\u519b\u4e8b\u6307\u6325\u63a7\u5236\u73af\u5883\u4e2d\u7684\u6548\u679c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u5de5\u5177\u80fd\u663e\u8457\u63d0\u5347\u76d1\u7763\u8005\u7ee9\u6548", "motivation": "\u519b\u4e8b\u6307\u6325\u63a7\u5236\u73af\u5883\u4e2d\uff0c\u76d1\u7763\u8005\u9762\u4e34\u52a8\u6001\u590d\u6742\u7684\u4fe1\u606f\u6d41\uff0c\u91cd\u8981\u4fe1\u606f\u53ef\u80fd\u56e0\u4efb\u52a1\u548c\u73af\u5883\u590d\u6742\u6027\u800c\u88ab\u5ffd\u7565\uff0c\u9700\u8981\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u6765\u8f85\u52a9\u6ce8\u610f\u529b\u7ba1\u7406", "method": "\u5f00\u53d1\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u5f15\u5bfc\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u5b9e\u65f6\u76d1\u6d4b\u76d1\u7763\u8005\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u914d\uff0c\u5f53\u5173\u952e\u53d8\u5316\u6216\u4e8b\u4ef6\u88ab\u9519\u8fc7\u65f6\u663e\u793a\u89c6\u89c9\u663e\u8457\u63d0\u793a\uff1b\u5728\u6a21\u62dfC2\u73af\u5883\u4e2d\uff0c25\u540d\u519b\u4e8b\u5b66\u5458\u53c2\u4e0e\u6a21\u62df\u60c5\u62a5\u4efb\u52a1\uff0c\u8fdb\u884c\u773c\u52a8\u8ffd\u8e2a\u5206\u6790\u548c\u7ee9\u6548\u8bc4\u4f30", "result": "\u81ea\u9002\u5e94DST\u5b58\u5728\u65f6\u7ee9\u6548\u663e\u8457\u63d0\u5347\uff1b\u773c\u52a8\u5206\u6790\u663e\u793a\u5bf9\u5173\u952e\u5174\u8da3\u533a\u66f4\u957f\u3001\u66f4\u9891\u7e41\u7684\u6ce8\u89c6\u4e0e\u7ee9\u6548\u8d1f\u76f8\u5173\uff1b\u540e\u5b9e\u9a8c\u8bbf\u8c08\u8868\u660e\u81ea\u9002\u5e94DST\u4e0d\u5177\u4fb5\u5165\u6027\u4e14\u53d7\u5230\u79ef\u6781\u8bc4\u4ef7", "conclusion": "\u5b9e\u65f6\u57fa\u4e8e\u6ce8\u89c6\u7684\u5e72\u9884\u5177\u6709\u4f18\u5316\u76d1\u7763\u51b3\u7b56\u7684\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u53ef\u6574\u5408AI\u9a71\u52a8\u65b9\u6cd5\u4ee5\u66f4\u597d\u5730\u652f\u6301\u590d\u6742\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u76d1\u7763\u8005"}}
{"id": "2601.17123", "categories": ["cs.HC", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17123", "abs": "https://arxiv.org/abs/2601.17123", "authors": ["Daehwa Kim", "Chris Harrison"], "title": "Acoustic Field Video for Multimodal Scene Understanding", "comment": null, "summary": "We introduce and explore a new multimodal input representation for vision-language models: acoustic field video. Unlike conventional video (RGB with stereo/mono audio), our video stream provides a spatially grounded visualization of sound intensity across a scene, offering a new and powerful dimension of perceptual understanding. Our real-time pipeline uses low-cost beamforming microphone arrays that are already common in smart speakers and increasingly present in robotics and XR headsets, yet this sensing capability remains unutilized for scene understanding. To assess the value of spatial acoustic information, we constructed an evaluation set of 402 question-answer scenes, comparing a state-of-the-art VLM given conventional video with and without paired acoustic field video. Results show a clear and consistent improvement when incorporating spatial acoustic data; the VLM we test improves from 38.3% correct to 67.4%. Our findings highlight that many everyday scene understanding tasks remain underconstrained when relying solely on visual and audio input, and that acoustic field data provides a promising and practical direction for multimodal reasoning. A video demo is available at https://daehwakim.com/seeingsound", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u591a\u6a21\u6001\u8f93\u5165\u8868\u793a\uff1a\u58f0\u573a\u89c6\u9891\u3002\u4e0e\u4f20\u7edf\u89c6\u9891\uff08RGB\u52a0\u7acb\u4f53\u58f0/\u5355\u58f0\u9053\u97f3\u9891\uff09\u4e0d\u540c\uff0c\u58f0\u573a\u89c6\u9891\u63d0\u4f9b\u4e86\u58f0\u97f3\u5f3a\u5ea6\u5728\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u53ef\u89c6\u5316\uff0c\u4e3a\u611f\u77e5\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u5f3a\u5927\u7ef4\u5ea6\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u97f3\u7bb1\u3001\u673a\u5668\u4eba\u548cXR\u5934\u663e\u4e2d\u666e\u904d\u914d\u5907\u7684\u4f4e\u6210\u672c\u6ce2\u675f\u6210\u5f62\u9ea6\u514b\u98ce\u9635\u5217\u7684\u4f20\u611f\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u5229\u7528\u4e8e\u573a\u666f\u7406\u89e3\u3002\u8bb8\u591a\u65e5\u5e38\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4ec5\u4f9d\u8d56\u89c6\u89c9\u548c\u97f3\u9891\u8f93\u5165\u65f6\u4ecd\u7136\u5b58\u5728\u7ea6\u675f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u591a\u6a21\u6001\u4fe1\u606f\u6765\u63d0\u5347\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u65f6\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u4f4e\u6210\u672c\u6ce2\u675f\u6210\u5f62\u9ea6\u514b\u98ce\u9635\u5217\u751f\u6210\u58f0\u573a\u89c6\u9891\u3002\u6784\u5efa\u4e86\u5305\u542b402\u4e2a\u95ee\u7b54\u573a\u666f\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u7edf\u89c6\u9891\u8f93\u5165\u4e0e\u7ed3\u5408\u58f0\u573a\u89c6\u9891\u8f93\u5165\u65f6\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ed3\u5408\u7a7a\u95f4\u58f0\u5b66\u6570\u636e\u540e\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u4ece38.3%\u7684\u6b63\u786e\u7387\u663e\u8457\u63d0\u5347\u81f367.4%\u3002\u8fd9\u8868\u660e\u58f0\u573a\u4fe1\u606f\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "conclusion": "\u58f0\u573a\u89c6\u9891\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u8f93\u5165\u8868\u793a\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002\u7a7a\u95f4\u58f0\u5b66\u6570\u636e\u4e3a\u89e3\u51b3\u4ec5\u4f9d\u8d56\u89c6\u89c9\u548c\u97f3\u9891\u8f93\u5165\u65f6\u5b58\u5728\u7684\u7ea6\u675f\u4e0d\u8db3\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u4f4e\u6210\u672c\u73b0\u6709\u786c\u4ef6\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u672a\u5f00\u53d1\u6f5c\u529b\u3002"}}
{"id": "2601.17058", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17058", "abs": "https://arxiv.org/abs/2601.17058", "authors": ["Wei Zhou", "Jun Zhou", "Haoyu Wang", "Zhenghao Li", "Qikang He", "Shaokun Han", "Guoliang Li", "Xuanhe Zhou", "Yeye He", "Chunwei Liu", "Zirui Tang", "Bin Wang", "Shen Tang", "Kai Zuo", "Yuyu Luo", "Zhenzhe Zheng", "Conghui He", "Jingren Zhou", "Fan Wu"], "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "comment": "Please refer to our repository for more details: https://github.com/weAIDB/awesome-data-llm", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86LLM\u589e\u5f3a\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\uff0c\u4ece\u4f20\u7edf\u89c4\u5219\u9a71\u52a8\u8303\u5f0f\u8f6c\u5411\u63d0\u793a\u9a71\u52a8\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u6db5\u76d6\u6570\u636e\u6e05\u6d17\u3001\u96c6\u6210\u548c\u4e30\u5bcc\u4e09\u5927\u4efb\u52a1\uff0c\u5206\u6790\u6280\u672f\u4f18\u52bf\u4e0e\u5c40\u9650\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6570\u636e\u51c6\u5907\u5bf9\u4e8e\u6570\u636e\u9a71\u52a8\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002\u968f\u7740(1)\u5bf9\u5e94\u7528\u5c31\u7eea\u6570\u636e\u7684\u9700\u6c42\u589e\u957f\uff0c(2)LLM\u6280\u672f\u7684\u5f3a\u5927\u53d1\u5c55\uff0c(3)\u667a\u80fd\u4f53\u57fa\u7840\u8bbe\u65bd\u7684\u6210\u719f\uff0cLLM\u589e\u5f3a\u65b9\u6cd5\u6b63\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u53d8\u9769\u6027\u8303\u5f0f\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u3002", "method": "\u57fa\u4e8e\u5bf9\u6570\u767e\u7bc7\u8fd1\u671f\u6587\u732e\u7684\u7cfb\u7edf\u8c03\u7814\uff0c\u9996\u5148\u5206\u6790\u8303\u5f0f\u8f6c\u53d8\uff1a\u4ece\u57fa\u4e8e\u89c4\u5219\u7684\u7279\u5b9a\u6a21\u578b\u7ba1\u9053\u8f6c\u5411\u63d0\u793a\u9a71\u52a8\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002\u63d0\u51fa\u4efb\u52a1\u4e2d\u5fc3\u5206\u7c7b\u6cd5\uff0c\u5c06\u9886\u57df\u5206\u4e3a\u4e09\u5927\u4efb\u52a1\uff1a\u6570\u636e\u6e05\u6d17\uff08\u6807\u51c6\u5316\u3001\u9519\u8bef\u5904\u7406\u3001\u586b\u8865\uff09\u3001\u6570\u636e\u96c6\u6210\uff08\u5b9e\u4f53\u5339\u914d\u3001\u6a21\u5f0f\u5339\u914d\uff09\u548c\u6570\u636e\u4e30\u5bcc\uff08\u6570\u636e\u6807\u6ce8\u3001\u5206\u6790\uff09\u3002\u5bf9\u6bcf\u7c7b\u4efb\u52a1\u8c03\u67e5\u4ee3\u8868\u6027\u6280\u672f\uff0c\u5206\u6790\u5176\u4f18\u52bf\uff08\u5982\u6539\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\u3001\u8bed\u4e49\u7406\u89e3\uff09\u548c\u5c40\u9650\uff08\u5982LLM\u6269\u5c55\u6210\u672c\u3001\u5e7b\u89c9\u95ee\u9898\u3001\u65b9\u6cd5\u8bc4\u4f30\u4e0d\u5339\u914d\uff09\u3002\u5206\u6790\u5e38\u7528\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u6700\u540e\u8ba8\u8bba\u5f00\u653e\u7814\u7a76\u6311\u6218\u3002", "result": "LLM\u589e\u5f3a\u65b9\u6cd5\u5728\u6570\u636e\u51c6\u5907\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5305\u62ec\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3001\u8bed\u4e49\u7406\u89e3\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u3002\u7136\u800c\u5b58\u5728\u660e\u663e\u5c40\u9650\uff1aLLM\u6269\u5c55\u6210\u672c\u9ad8\u6602\u3001\u667a\u80fd\u4f53\u6301\u7eed\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u3001\u5148\u8fdb\u65b9\u6cd5\u4e0e\u8584\u5f31\u8bc4\u4f30\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002\u5f53\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "LLM\u589e\u5f3a\u7684\u6570\u636e\u51c6\u5907\u4ee3\u8868\u4e86\u8303\u5f0f\u8f6c\u53d8\uff0c\u4f46\u9762\u4e34\u53ef\u6269\u5c55\u6027\u3001\u53ef\u9760\u6027\u548c\u8bc4\u4f30\u65b9\u9762\u7684\u6311\u6218\u3002\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\uff1a\u53ef\u6269\u5c55\u7684LLM-\u6570\u636e\u7cfb\u7edf\u3001\u53ef\u9760\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u539f\u5219\u8bbe\u8ba1\u3001\u7a33\u5065\u8bc4\u4f30\u534f\u8bae\u3001\u6210\u672c\u6548\u76ca\u4f18\u5316\uff0c\u4ee5\u53caLLM\u6280\u672f\u4e0e\u4f20\u7edf\u6570\u636e\u7ba1\u7406\u65b9\u6cd5\u7684\u66f4\u597d\u96c6\u6210\u3002"}}
{"id": "2601.17065", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17065", "abs": "https://arxiv.org/abs/2601.17065", "authors": ["Haoxuan Li", "He Chang", "Yunshan Ma", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting", "comment": null, "summary": "Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.", "code_url": "https://github.com/LuminosityX/ThinkTank-ME", "code_stars": 2, "code_last_update": "2025-11-18", "AI": {"tldr": "\u63d0\u51faThinkTank-ME\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u667a\u5e93\u7684\u4e13\u5bb6\u534f\u4f5c\u5206\u6790\u6765\u6539\u8fdb\u4e2d\u4e1c\u4e8b\u4ef6\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u5355\u6a21\u578b\u67b6\u6784\u5728\u6355\u6349\u590d\u6742\u5730\u533a\u5730\u7f18\u653f\u6cbb\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e8b\u4ef6\u9884\u6d4b\u53d7\u5230\u591a\u65b9\u9762\u56e0\u7d20\u5f71\u54cd\uff0c\u5305\u62ec\u56fd\u9645\u5173\u7cfb\u3001\u5730\u533a\u5386\u53f2\u52a8\u6001\u548c\u6587\u5316\u80cc\u666f\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u91c7\u7528\u5355\u6a21\u578b\u67b6\u6784\uff0c\u4ec5\u6cbf\u5355\u4e00\u663e\u5f0f\u8f68\u8ff9\u751f\u6210\u9884\u6d4b\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u5730\u533a\u80cc\u666f\u4e0b\u6355\u6349\u591a\u6837\u5316\u5730\u7f18\u653f\u6cbb\u7ec6\u5fae\u5dee\u522b\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165ThinkTank-ME\u6846\u67b6\uff0c\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u6218\u7565\u51b3\u7b56\u4e2d\u7684\u534f\u4f5c\u4e13\u5bb6\u5206\u6790\u3002\u6784\u5efaPOLECAT-FOR-ME\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u4e2d\u4e1c\u4e8b\u4ef6\u9884\u6d4b\uff0c\u4ee5\u4fc3\u8fdb\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u591a\u4e13\u5bb6\u534f\u4f5c\u5728\u5904\u7406\u590d\u6742\u65f6\u95f4\u5730\u7f18\u653f\u6cbb\u9884\u6d4b\u4efb\u52a1\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ThinkTank-ME\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u667a\u5e93\u534f\u4f5c\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e2d\u4e1c\u4e8b\u4ef6\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5bf9\u590d\u6742\u5730\u7f18\u653f\u6cbb\u7ec6\u5fae\u5dee\u522b\u7684\u6355\u6349\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u5730\u533a\u4e8b\u4ef6\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18793", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18793", "abs": "https://arxiv.org/abs/2601.18793", "authors": ["Michael Lee", "Ningning Xie", "Oleg Kiselyov", "Jeremy Yallop"], "title": "Handling Scope Checks (Extended Version)", "comment": "Extended version of Handling Scope Checks (POPL'26): includes appendices, fixes minor typos, and tweaks phrasing for readability", "summary": "Metaprogramming and effect handlers interact in unexpected, and sometimes undesirable, ways. One example is scope extrusion: the generation of ill-scoped code. Scope extrusion can either be preemptively prevented, via static type systems, or retroactively detected, via dynamic checks. Static type systems exist in theory, but struggle with a range of implementation and usability problems in practice. In contrast, dynamic checks exist in practice (e.g. in MetaOCaml), but are understudied in theory. Designers of metalanguages are thus given little guidance regarding the design and implementation of checks. We present the first formal study of dynamic scope extrusion checks, introducing a calculus ($\u03bb_{\\langle\\langle\\text{op}\\rangle\\rangle}$) for describing and evaluating checks. Further, we introduce a novel dynamic check $\\unicode{x2014}$ the \"Cause-for-Concern\" check $\\unicode{x2014}$ which we prove correct, characterise without reference to its implementation, and argue combines the advantages of existing dynamic checks. Finally, we extend our framework with refined environment classifiers, which statically prevent scope extrusion, and compare their expressivity with the dynamic checks.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5f62\u5f0f\u5316\u7814\u7a76\u52a8\u6001\u4f5c\u7528\u57df\u5916\u6cc4\u68c0\u67e5\uff0c\u63d0\u51fa\u03bb\u27e8\u27e8op\u27e9\u27e9\u6f14\u7b97\u63cf\u8ff0\u548c\u8bc4\u4f30\u68c0\u67e5\uff0c\u5f15\u5165\u65b0\u9896\u7684\"Cause-for-Concern\"\u52a8\u6001\u68c0\u67e5\u5e76\u8bc1\u660e\u5176\u6b63\u786e\u6027\uff0c\u540c\u65f6\u6269\u5c55\u6846\u67b6\u5305\u542b\u9759\u6001\u73af\u5883\u5206\u7c7b\u5668\uff0c\u6bd4\u8f83\u52a8\u6001\u4e0e\u9759\u6001\u65b9\u6cd5\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u5143\u7f16\u7a0b\u4e0e\u6548\u5e94\u5904\u7406\u5668\u4ee5\u610f\u5916\u4e14\u6709\u65f6\u4e0d\u826f\u7684\u65b9\u5f0f\u4ea4\u4e92\uff0c\u4f8b\u5982\u4f5c\u7528\u57df\u5916\u6cc4\u95ee\u9898\u3002\u9759\u6001\u7c7b\u578b\u7cfb\u7edf\u5b58\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u95ee\u9898\uff0c\u800c\u52a8\u6001\u68c0\u67e5\uff08\u5982MetaOCaml\u4e2d\uff09\u5728\u7406\u8bba\u4e0a\u7814\u7a76\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5bf9\u5143\u8bed\u8a00\u8bbe\u8ba1\u8005\u7684\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u03bb\u27e8\u27e8op\u27e9\u27e9\u6f14\u7b97\u5f62\u5f0f\u5316\u6846\u67b6\u63cf\u8ff0\u548c\u8bc4\u4f30\u52a8\u6001\u4f5c\u7528\u57df\u5916\u6cc4\u68c0\u67e5\uff1b\u5f15\u5165\u65b0\u9896\u7684\"Cause-for-Concern\"\u52a8\u6001\u68c0\u67e5\uff0c\u8bc1\u660e\u5176\u6b63\u786e\u6027\u5e76\u901a\u8fc7\u5b9e\u73b0\u65e0\u5173\u65b9\u5f0f\u63cf\u8ff0\uff1b\u6269\u5c55\u6846\u67b6\u5305\u542b\u7cbe\u5316\u73af\u5883\u5206\u7c7b\u5668\u5b9e\u73b0\u9759\u6001\u9884\u9632\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u52a8\u6001\u4f5c\u7528\u57df\u5916\u6cc4\u68c0\u67e5\u7684\u5f62\u5f0f\u5316\u7814\u7a76\u6846\u67b6\uff1b\u63d0\u51fa\u7684\"Cause-for-Concern\"\u68c0\u67e5\u88ab\u8bc1\u660e\u6b63\u786e\u4e14\u7ed3\u5408\u4e86\u73b0\u6709\u52a8\u6001\u68c0\u67e5\u7684\u4f18\u70b9\uff1b\u6bd4\u8f83\u4e86\u52a8\u6001\u68c0\u67e5\u4e0e\u9759\u6001\u73af\u5883\u5206\u7c7b\u5668\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u52a8\u6001\u4f5c\u7528\u57df\u5916\u6cc4\u68c0\u67e5\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u4e3a\u5143\u8bed\u8a00\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u6307\u5bfc\uff0c\u5c55\u793a\u4e86\u52a8\u6001\u68c0\u67e5\u4e0e\u9759\u6001\u9884\u9632\u65b9\u6cd5\u7684\u4e92\u8865\u5173\u7cfb\uff0c\u4e3a\u5143\u7f16\u7a0b\u4e0e\u6548\u5e94\u5904\u7406\u5668\u7684\u5b89\u5168\u4ea4\u4e92\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.17227", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17227", "abs": "https://arxiv.org/abs/2601.17227", "authors": ["Avraiem Iskandar", "Shamak Dutta", "Kevin Murrant", "Yash Vardhan Pant", "Stephen L. Smith"], "title": "Hierarchical Informative Path Planning via Graph Guidance and Trajectory Optimization", "comment": null, "summary": "We study informative path planning (IPP) with travel budgets in cluttered environments, where an agent collects measurements of a latent field modeled as a Gaussian process (GP) to reduce uncertainty at target locations. Graph-based solvers provide global guarantees but assume pre-selected measurement locations, while continuous trajectory optimization supports path-based sensing but is computationally intensive and sensitive to initialization in obstacle-dense settings. We propose a hierarchical framework with three stages: (i) graph-based global planning, (ii) segment-wise budget allocation using geometric and kernel bounds, and (iii) spline-based refinement of each segment with hard constraints and obstacle pruning. By combining global guidance with local refinement, our method achieves lower posterior uncertainty than graph-only and continuous baselines, while running faster than continuous-space solvers (up to 9x faster than gradient-based methods and 20x faster than black-box optimizers) across synthetic cluttered environments and Arctic datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6742\u4e71\u73af\u5883\u4e2d\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u7684\u4e09\u9636\u6bb5\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u56fe\u641c\u7d22\u7684\u5168\u5c40\u4fdd\u8bc1\u548c\u8fde\u7eed\u8f68\u8ff9\u4f18\u5316\u7684\u8def\u5f84\u611f\u77e5\u4f18\u52bf\uff0c\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u964d\u4f4e\u76ee\u6807\u4f4d\u7f6e\u7684\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u56fe\u7684\u6c42\u89e3\u5668\u5047\u8bbe\u9884\u9009\u6d4b\u91cf\u4f4d\u7f6e\uff0c\u7f3a\u4e4f\u8def\u5f84\u611f\u77e5\uff1b\u8fde\u7eed\u8f68\u8ff9\u4f18\u5316\u8ba1\u7b97\u5bc6\u96c6\u4e14\u5bf9\u521d\u59cb\u5316\u654f\u611f\uff0c\u5728\u969c\u788d\u5bc6\u96c6\u73af\u5883\u4e2d\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u4f9b\u5168\u5c40\u4fdd\u8bc1\u53c8\u80fd\u8fdb\u884c\u8def\u5f84\u611f\u77e5\u4f18\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u4e09\u9636\u6bb5\u5206\u5c42\u6846\u67b6\uff1a1) \u57fa\u4e8e\u56fe\u7684\u5168\u5c40\u89c4\u5212\uff1b2) \u4f7f\u7528\u51e0\u4f55\u548c\u6838\u8fb9\u754c\u8fdb\u884c\u5206\u6bb5\u9884\u7b97\u5206\u914d\uff1b3) \u5e26\u786c\u7ea6\u675f\u548c\u969c\u788d\u4fee\u526a\u7684\u6837\u6761\u57fa\u5206\u6bb5\u7ec6\u5316\u3002\u7ed3\u5408\u5168\u5c40\u6307\u5bfc\u4e0e\u5c40\u90e8\u4f18\u5316\u3002", "result": "\u5728\u5408\u6210\u6742\u4e71\u73af\u5883\u548c\u5317\u6781\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u4ec5\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u548c\u8fde\u7eed\u57fa\u7ebf\uff0c\u83b7\u5f97\u4e86\u66f4\u4f4e\u7684\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\u3002\u8ba1\u7b97\u901f\u5ea6\u663e\u8457\u63d0\u5347\uff1a\u6bd4\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u5feb9\u500d\uff0c\u6bd4\u9ed1\u76d2\u4f18\u5316\u5668\u5feb20\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6742\u4e71\u73af\u5883\u4e2d\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u641c\u7d22\u7684\u5168\u5c40\u4fdd\u8bc1\u548c\u8fde\u7eed\u4f18\u5316\u7684\u8def\u5f84\u611f\u77e5\u80fd\u529b\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.18435", "categories": ["cs.ET", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2601.18435", "abs": "https://arxiv.org/abs/2601.18435", "authors": ["Biraja Ghoshal"], "title": "The Quantum Cliff: A Critical Proton Tunneling Threshold Determines Clinical Severity in RPE65-Mediated Retinal Disease", "comment": null, "summary": "Predicting clinical severity from genotype remains a fundamental challenge in molecular medicine, particularly for enzymes whose function depends on sub-atomic-scale geometry. Mutations in the \\textit{RPE65} isomerohydrolase cause Leber Congenital Amaurosis (LCA) and related retinal diseases; however, the kinetic mechanisms connecting sub-atomic-scale perturbations to blindness remain unclear. In this study, we demonstrate that mutations in the human visual isomerase RPE65 are governed by a quantum-mechanical threshold effect arising from proton tunneling in the active site. We established a hybrid quantum-classical structure-to-phenotype pipeline combining AlphaFold structure prediction with \\textit{ab initio} quantum simulation using the Variational Quantum Eigensolver (VQE) to analyze minimal proton-coupled electron transfer in the visual cycle. Our analysis reveals that many pathogenic mutations do not merely occlude the active site, but rather strongly reduce the quantum probability of proton tunneling. We observed a sharp non-linear effect, termed the \"Quantum Cliff,\" where minute structural changes (below 0.1 \u00c5) reduce the reaction rate by multiple orders of magnitude. Based on these findings, we introduce a dimensionless Relative Quantum Activity Score (RQAS) that isolates the geometry-controlled exponential sensitivity of the reaction rate and successfully distinguishes between mild and severe patient phenotypes. These results suggest that RPE65 operates near a quantum-critical point, where sub-Angstrom structural perturbations induce a catastrophic loss of function. Furthermore, our findings establish quantum tunneling as a predictive mechanistic link between atomic structure and clinical phenotype, proposing a general framework for quantum-structural disease modeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86RPE65\u9176\u7a81\u53d8\u5bfc\u81f4\u5931\u660e\u7684\u91cf\u5b50\u673a\u5236\uff1a\u8d28\u5b50\u96a7\u7a7f\u6982\u7387\u7684\u5fae\u5c0f\u53d8\u5316\uff08<0.1\u00c5\uff09\u5f15\u8d77\u53cd\u5e94\u901f\u7387\u6570\u91cf\u7ea7\u4e0b\u964d\uff08\"\u91cf\u5b50\u60ac\u5d16\"\u6548\u5e94\uff09\uff0c\u5efa\u7acb\u4e86\u91cf\u5b50\u96a7\u7a7f\u4f5c\u4e3a\u539f\u5b50\u7ed3\u6784\u4e0e\u4e34\u5e8a\u8868\u578b\u95f4\u7684\u9884\u6d4b\u6027\u673a\u5236\u8054\u7cfb\u3002", "motivation": "\u4ece\u57fa\u56e0\u578b\u9884\u6d4b\u4e34\u5e8a\u4e25\u91cd\u7a0b\u5ea6\u662f\u5206\u5b50\u533b\u5b66\u7684\u57fa\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u529f\u80fd\u4f9d\u8d56\u4e8e\u4e9a\u539f\u5b50\u5c3a\u5ea6\u51e0\u4f55\u7ed3\u6784\u7684\u9176\u3002RPE65\u5f02\u6784\u6c34\u89e3\u9176\u7a81\u53d8\u5bfc\u81f4Leber\u5148\u5929\u6027\u9ed1\u8499\u75c7\u53ca\u76f8\u5173\u89c6\u7f51\u819c\u75be\u75c5\uff0c\u4f46\u8fde\u63a5\u4e9a\u539f\u5b50\u5c3a\u5ea6\u6270\u52a8\u4e0e\u5931\u660e\u7684\u52a8\u529b\u5b66\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5efa\u7acb\u4e86\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u7ed3\u6784\u5230\u8868\u578b\u5206\u6790\u6d41\u7a0b\uff0c\u7ed3\u5408AlphaFold\u7ed3\u6784\u9884\u6d4b\u4e0e\u4f7f\u7528\u53d8\u5206\u91cf\u5b50\u672c\u5f81\u6c42\u89e3\u5668\uff08VQE\uff09\u7684\u4ece\u5934\u7b97\u91cf\u5b50\u6a21\u62df\uff0c\u5206\u6790\u89c6\u89c9\u5faa\u73af\u4e2d\u7684\u6700\u5c0f\u8d28\u5b50\u8026\u5408\u7535\u5b50\u8f6c\u79fb\u3002", "result": "\u53d1\u73b0\u8bb8\u591a\u81f4\u75c5\u7a81\u53d8\u5e76\u975e\u4ec5\u4ec5\u963b\u585e\u6d3b\u6027\u4f4d\u70b9\uff0c\u800c\u662f\u5f3a\u70c8\u964d\u4f4e\u8d28\u5b50\u96a7\u7a7f\u7684\u91cf\u5b50\u6982\u7387\u3002\u89c2\u5bdf\u5230\u79f0\u4e3a\"\u91cf\u5b50\u60ac\u5d16\"\u7684\u5c16\u9510\u975e\u7ebf\u6027\u6548\u5e94\uff0c\u5fae\u5c0f\u7ed3\u6784\u53d8\u5316\uff08<0.1\u00c5\uff09\u4f7f\u53cd\u5e94\u901f\u7387\u964d\u4f4e\u591a\u4e2a\u6570\u91cf\u7ea7\u3002\u5efa\u7acb\u4e86\u76f8\u5bf9\u91cf\u5b50\u6d3b\u6027\u8bc4\u5206\uff08RQAS\uff09\uff0c\u6210\u529f\u533a\u5206\u8f7b\u5ea6\u548c\u91cd\u5ea6\u60a3\u8005\u8868\u578b\u3002", "conclusion": "RPE65\u5728\u91cf\u5b50\u4e34\u754c\u70b9\u9644\u8fd1\u8fd0\u4f5c\uff0c\u4e9a\u57c3\u7ea7\u7ed3\u6784\u6270\u52a8\u5bfc\u81f4\u529f\u80fd\u707e\u96be\u6027\u4e27\u5931\u3002\u91cf\u5b50\u96a7\u7a7f\u5efa\u7acb\u4e86\u539f\u5b50\u7ed3\u6784\u4e0e\u4e34\u5e8a\u8868\u578b\u95f4\u7684\u9884\u6d4b\u6027\u673a\u5236\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u91cf\u5b50\u7ed3\u6784\u75be\u75c5\u5efa\u6a21\u7684\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2601.17134", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17134", "abs": "https://arxiv.org/abs/2601.17134", "authors": ["Matthew K. Hong", "Joey Li", "Alexandre Filipowicz", "Monica Van", "Kalani Murakami", "Yan-Ying Chen", "Shiwali Mohan", "Shabnam Hakimi", "Matthew Klenk"], "title": "Deconstructing Taste: Toward a Human-Centered AI Framework for Modeling Consumer Aesthetic Perceptions", "comment": null, "summary": "Understanding and modeling consumers' stylistic taste such as \"sporty\" is crucial for creating designs that truly connect with target audiences. However, capturing taste during the design process remains challenging because taste is abstract and subjective, and preference data alone provides limited guidance for concrete design decisions. This paper proposes an integrated human-centered computational framework that links subjective evaluations (e.g., perceived luxury of car wheels) with domain-specific features (e.g., spoke configuration) and computer vision-based measures (e.g., texture). By jointly modeling human-derived (consumer and designer) and machine-extracted features, our framework advances aesthetic assessment by explicitly linking model outcomes to interpretable design features. In particular, it demonstrates how perceptual features, domain-specific design patterns, and consumers' own interpretations of style contribute to aesthetic evaluations. This framework will enable product teams to better understand, communicate, and critique aesthetic decisions, supporting improved anticipation of consumer taste and more informed exploration of design alternatives at design time.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210\u7684\u4eba\u4e3a\u4e2d\u5fc3\u8ba1\u7b97\u6846\u67b6\uff0c\u5c06\u4e3b\u89c2\u5ba1\u7f8e\u8bc4\u4ef7\u4e0e\u9886\u57df\u7279\u5b9a\u8bbe\u8ba1\u7279\u5f81\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7279\u5f81\u76f8\u8fde\u63a5\uff0c\u4ee5\u6539\u8fdb\u4ea7\u54c1\u8bbe\u8ba1\u4e2d\u7684\u6d88\u8d39\u8005\u54c1\u5473\u9884\u6d4b", "motivation": "\u6d88\u8d39\u8005\u98ce\u683c\u54c1\u5473\uff08\u5982\"\u8fd0\u52a8\u611f\"\uff09\u7684\u5efa\u6a21\u5bf9\u4ea7\u54c1\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u54c1\u5473\u62bd\u8c61\u4e14\u4e3b\u89c2\uff0c\u4ec5\u51ed\u504f\u597d\u6570\u636e\u96be\u4ee5\u6307\u5bfc\u5177\u4f53\u8bbe\u8ba1\u51b3\u7b56\u3002\u9700\u8981\u5efa\u7acb\u4e3b\u89c2\u8bc4\u4ef7\u4e0e\u53ef\u64cd\u4f5c\u8bbe\u8ba1\u7279\u5f81\u4e4b\u95f4\u7684\u660e\u786e\u8054\u7cfb\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u7684\u4eba\u4e3a\u4e2d\u5fc3\u8ba1\u7b97\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u4eba\u7c7b\u884d\u751f\u7279\u5f81\uff08\u6d88\u8d39\u8005\u548c\u8bbe\u8ba1\u5e08\u8bc4\u4ef7\uff09\u4e0e\u673a\u5668\u63d0\u53d6\u7279\u5f81\uff08\u8ba1\u7b97\u673a\u89c6\u89c9\u7279\u5f81\u5982\u7eb9\u7406\uff09\uff0c\u5c06\u4e3b\u89c2\u8bc4\u4ef7\uff08\u5982\u611f\u77e5\u8c6a\u534e\u5ea6\uff09\u4e0e\u9886\u57df\u7279\u5b9a\u7279\u5f81\uff08\u5982\u8f6e\u8f90\u914d\u7f6e\uff09\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6d4b\u91cf\u76f8\u8fde\u63a5\u3002", "result": "\u6846\u67b6\u80fd\u591f\u660e\u786e\u5c06\u6a21\u578b\u7ed3\u679c\u4e0e\u53ef\u89e3\u91ca\u7684\u8bbe\u8ba1\u7279\u5f81\u8054\u7cfb\u8d77\u6765\uff0c\u5c55\u793a\u611f\u77e5\u7279\u5f81\u3001\u9886\u57df\u7279\u5b9a\u8bbe\u8ba1\u6a21\u5f0f\u548c\u6d88\u8d39\u8005\u81ea\u8eab\u98ce\u683c\u89e3\u91ca\u5982\u4f55\u5171\u540c\u5f71\u54cd\u5ba1\u7f8e\u8bc4\u4ef7\uff0c\u4e3a\u4ea7\u54c1\u56e2\u961f\u63d0\u4f9b\u66f4\u597d\u7684\u8bbe\u8ba1\u51b3\u7b56\u652f\u6301\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u4ea7\u54c1\u56e2\u961f\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u3001\u6c9f\u901a\u548c\u6279\u5224\u5ba1\u7f8e\u51b3\u7b56\uff0c\u652f\u6301\u5728\u8bbe\u8ba1\u9636\u6bb5\u6539\u8fdb\u6d88\u8d39\u8005\u54c1\u5473\u9884\u6d4b\uff0c\u5e76\u8fdb\u884c\u66f4\u660e\u667a\u7684\u8bbe\u8ba1\u66ff\u4ee3\u65b9\u6848\u63a2\u7d22\u3002"}}
{"id": "2601.18069", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18069", "abs": "https://arxiv.org/abs/2601.18069", "authors": ["Haoyuan Pan", "Sizhao Chen", "Zhaorui Wang", "Tse-Tin Chan"], "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control", "comment": "16 pages, 11 figures", "summary": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7528\u4e8e\u591a\u7528\u6237\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u7248\u672c\u4fe1\u606f\u5e74\u9f84\u8c03\u5ea6\uff1aD2SAC\u7528\u4e8e\u6700\u5c0f\u5316\u5e73\u5747VAoI\uff0cRS-D3SAC\u7528\u4e8e\u5c3e\u90e8\u98ce\u9669\u654f\u611f\u4f18\u5316\uff0c\u5728\u6ee1\u8db3\u957f\u671f\u4f20\u8f93\u6210\u672c\u7ea6\u675f\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eCVaR\u98ce\u9669\u3002", "motivation": "\u73b0\u6709VAoI\u8c03\u5ea6\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6700\u5c0f\u5316\u5e73\u5747\u7248\u672c\u4fe1\u606f\u5e74\u9f84\uff0c\u5ffd\u7565\u4e86\u5728\u968f\u673a\u5305\u5230\u8fbe\u548c\u4e0d\u53ef\u9760\u4fe1\u9053\u4e0b\u53ef\u80fd\u53d1\u751f\u7684\u7f55\u89c1\u4f46\u4e25\u91cd\u7684\u9648\u65e7\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u4f1a\u635f\u5bb3\u7cfb\u7edf\u53ef\u9760\u6027\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4f18\u5316\u5e73\u5747\u6027\u80fd\u53c8\u80fd\u63a7\u5236\u5c3e\u90e8\u98ce\u9669\u7684\u8c03\u5ea6\u7b97\u6cd5\u3002", "method": "\u9996\u5148\u5c06\u5e73\u5747VAoI\u6700\u5c0f\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u6269\u6563\u7684Soft Actor-Critic\u7b97\u6cd5(D2SAC)\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u98ce\u9669\u654f\u611f\u7684\u6df1\u5ea6\u5206\u5e03\u6269\u6563Soft Actor-Critic\u7b97\u6cd5(RS-D3SAC)\uff0c\u7ed3\u5408\u6269\u6563actor\u548c\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u5206\u5e03critic\uff0c\u901a\u8fc7\u6761\u4ef6\u98ce\u9669\u4ef7\u503c(CVaR)\u8fdb\u884c\u5c3e\u90e8\u98ce\u9669\u4f18\u5316\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0cD2SAC\u80fd\u6709\u6548\u964d\u4f4e\u5e73\u5747VAoI\uff0c\u800cRS-D3SAC\u5728\u4fdd\u6301\u5e73\u5747\u6027\u80fd\u7684\u540c\u65f6\uff0c\u80fd\u663e\u8457\u964d\u4f4eCVaR\u3002\u5c3e\u90e8\u98ce\u9669\u964d\u4f4e\u7684\u4e3b\u8981\u589e\u76ca\u6765\u81ea\u5206\u5e03critic\uff0c\u6269\u6563actor\u5219\u63d0\u4f9b\u8865\u5145\u4f18\u5316\u4ee5\u7a33\u5b9a\u548c\u4e30\u5bcc\u7b56\u7565\u51b3\u7b56\u3002", "conclusion": "RS-D3SAC\u7b97\u6cd5\u901a\u8fc7\u6574\u5408\u6269\u6563actor\u548c\u5206\u5e03critic\uff0c\u4e3a\u591a\u7528\u6237\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u98ce\u9669\u611f\u77e5\u7684VAoI\u8c03\u5ea6\u65b9\u6848\uff0c\u65e2\u80fd\u4f18\u5316\u5e73\u5747\u6027\u80fd\u53c8\u80fd\u6709\u6548\u63a7\u5236\u5c3e\u90e8\u98ce\u9669\uff0c\u6ee1\u8db3\u957f\u671f\u4f20\u8f93\u6210\u672c\u7ea6\u675f\u3002"}}
{"id": "2601.18681", "categories": ["cs.LG", "cs.AI", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.18681", "abs": "https://arxiv.org/abs/2601.18681", "authors": ["Yilie Huang", "Wenpin Tang", "Xunyu Zhou"], "title": "ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule", "comment": "17 pages, 7 figures", "summary": "We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fr\u00e9chet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u91cd\u53c2\u6570\u5316\u65f6\u95f4\uff08ART\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u91cd\u53c2\u6570\u5316\u65f6\u95f4\u53d8\u91cf\u7684\u65f6\u949f\u901f\u5ea6\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u6b65\u8c03\u5ea6\uff0c\u51cf\u5c11\u79bb\u6563\u5316\u8bef\u5dee\uff0c\u5e76\u5f00\u53d1\u4e86ART-RL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u5b66\u4e60\u6700\u4f18\u65f6\u95f4\u8868\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4f7f\u7528\u5747\u5300\u6216\u624b\u5de5\u8bbe\u8ba1\u7684\u65f6\u95f4\u7f51\u683c\u5728\u6709\u9650\u65f6\u95f4\u6b65\u9884\u7b97\u4e0b\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u65f6\u95f4\u8c03\u5ea6\u65b9\u6cd5\u6765\u6700\u5c0f\u5316\u79bb\u6563\u5316\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u91cd\u53c2\u6570\u5316\u65f6\u95f4\uff08ART\uff09\u65b9\u6cd5\uff0c\u63a7\u5236\u91cd\u53c2\u6570\u5316\u65f6\u95f4\u53d8\u91cf\u7684\u65f6\u949f\u901f\u5ea6\uff0c\u5b9e\u73b0\u4e0d\u5747\u5300\u65f6\u95f4\u6b65\u8c03\u5ea6\u3002\u8fdb\u4e00\u6b65\u5f00\u53d1ART-RL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u65f6\u95f4\u53d8\u5316\u5efa\u6a21\u4e3a\u8fde\u7eed\u65f6\u95f4RL\u95ee\u9898\uff0c\u4f7f\u7528\u9ad8\u65af\u7b56\u7565\u5e76\u901a\u8fc7actor-critic\u7b97\u6cd5\u5b66\u4e60\u6700\u4f18\u65f6\u95f4\u8868\u3002", "result": "\u57fa\u4e8eEDM\u5b98\u65b9\u6d41\u7a0b\uff0cART-RL\u5728CIFAR-10\u4e0a\u663e\u8457\u6539\u5584\u4e86Fr\u00e9chet Inception Distance\uff08FID\uff09\uff0c\u5e76\u5728AFHQv2\u3001FFHQ\u548cImageNet\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "ART\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u65f6\u95f4\u8c03\u5ea6\u6709\u6548\u51cf\u5c11\u4e86\u6269\u6563\u6a21\u578b\u7684\u79bb\u6563\u5316\u8bef\u5dee\uff0cART-RL\u6846\u67b6\u80fd\u591f\u6570\u636e\u9a71\u52a8\u5730\u5b66\u4e60\u6700\u4f18\u65f6\u95f4\u8868\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.17808", "categories": ["cs.NE", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2601.17808", "abs": "https://arxiv.org/abs/2601.17808", "authors": ["Alejandro Medina", "Mary Lauren Benton"], "title": "Motif Diversity in Human Liver ChIP-seq Data Using MAP-Elites", "comment": "Under review for a conference companion paper", "summary": "Motif discovery is a core problem in computational biology, traditionally formulated as a likelihood optimization task that returns a single dominant motif from a DNA sequence dataset. However, regulatory sequence data admit multiple plausible motif explanations, reflecting underlying biological heterogeneity. In this work, we frame motif discovery as a quality-diversity problem and apply the MAP-Elites algorithm to evolve position weight matrix motifs under a likelihood-based fitness objective while explicitly preserving diversity across biologically meaningful dimensions. We evaluate MAP-Elites using three complementary behavioral characterizations that capture trade-offs between motif specificity, compositional structure, coverage, and robustness. Experiments on human CTCF liver ChIP-seq data aligned to the human reference genome compare MAP-Elites against a standard motif discovery tool, MEME, under matched evaluation criteria across stratified dataset subsets. Results show that MAP-Elites recovers multiple high-quality motif variants with fitness comparable to MEME's strongest solutions while revealing structured diversity obscured by single-solution approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u6a21\u4f53\u53d1\u73b0\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8d28\u91cf-\u591a\u6837\u6027\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528MAP-Elites\u7b97\u6cd5\u5728\u4eba\u7c7bCTCF ChIP-seq\u6570\u636e\u4e0a\u53d1\u73b0\u591a\u4e2a\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u6a21\u4f53\u53d8\u4f53\u3002", "motivation": "\u4f20\u7edf\u6a21\u4f53\u53d1\u73b0\u65b9\u6cd5\u901a\u5e38\u53ea\u8fd4\u56de\u5355\u4e00\u4e3b\u5bfc\u6a21\u4f53\uff0c\u4f46\u8c03\u63a7\u5e8f\u5217\u6570\u636e\u5b9e\u9645\u4e0a\u5305\u542b\u591a\u79cd\u53ef\u80fd\u7684\u6a21\u4f53\u89e3\u91ca\uff0c\u53cd\u6620\u4e86\u6f5c\u5728\u7684\u751f\u7269\u5b66\u5f02\u8d28\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u53d1\u73b0\u591a\u4e2a\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u6a21\u4f53\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u6a21\u4f53\u53d1\u73b0\u6846\u67b6\u5316\u4e3a\u8d28\u91cf-\u591a\u6837\u6027\u4f18\u5316\u95ee\u9898\uff0c\u5e94\u7528MAP-Elites\u7b97\u6cd5\u5728\u4f3c\u7136\u5ea6\u9002\u5e94\u5ea6\u76ee\u6807\u4e0b\u6f14\u5316\u4f4d\u7f6e\u6743\u91cd\u77e9\u9635\u6a21\u4f53\uff0c\u540c\u65f6\u5728\u751f\u7269\u5b66\u610f\u4e49\u7ef4\u5ea6\u4e0a\u4fdd\u6301\u591a\u6837\u6027\u3002\u4f7f\u7528\u4e09\u79cd\u4e92\u8865\u7684\u884c\u4e3a\u7279\u5f81\u6765\u6355\u6349\u6a21\u4f53\u7279\u5f02\u6027\u3001\u7ec4\u6210\u7ed3\u6784\u3001\u8986\u76d6\u5ea6\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5728\u4eba\u7c7bCTCF\u809d\u810fChIP-seq\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAP-Elites\u80fd\u591f\u6062\u590d\u591a\u4e2a\u9ad8\u8d28\u91cf\u6a21\u4f53\u53d8\u4f53\uff0c\u5176\u9002\u5e94\u5ea6\u4e0eMEME\u7684\u6700\u5f3a\u89e3\u51b3\u65b9\u6848\u76f8\u5f53\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u88ab\u5355\u89e3\u51b3\u65b9\u6848\u65b9\u6cd5\u6240\u63a9\u76d6\u7684\u7ed3\u6784\u5316\u591a\u6837\u6027\u3002", "conclusion": "\u8d28\u91cf-\u591a\u6837\u6027\u4f18\u5316\u4e3a\u6a21\u4f53\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u80fd\u591f\u540c\u65f6\u53d1\u73b0\u591a\u4e2a\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u6a21\u4f53\uff0c\u66f4\u597d\u5730\u53cd\u6620\u4e86\u8c03\u63a7\u5e8f\u5217\u6570\u636e\u7684\u751f\u7269\u5b66\u5f02\u8d28\u6027\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u5355\u89e3\u51b3\u65b9\u6848\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.17221", "categories": ["cs.DB", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.17221", "abs": "https://arxiv.org/abs/2601.17221", "authors": ["Dominik Winecki", "Arnab Nandi"], "title": "Vidformer: Drop-in Declarative Optimization for Rendering Video-Native Query Results", "comment": null, "summary": "When interactively exploring video data, video-native querying involves consuming query results as videos, including steps such as compilation of extracted video clips or data overlays. These video-native queries are bottlenecked by rendering, not the execution of the underlying queries. This rendering is currently performed using post-processing scripts that are often slow. This step poses a critical point of friction in interactive video data workloads: even short clips contain thousands of high-definition frames; conventional OpenCV/Python scripts must decode -> transform -> encode the entire data stream before a single pixel appears, leaving users waiting for many seconds, minutes, or hours.\n  To address these issues, we present Vidformer, a drop-in rendering accelerator for video-native querying which, (i) transparently lifts existing visualization code into a declarative representation, (ii) transparently optimizes and parallelizes rendering, and (iii) instantly serves videos through a Video on Demand protocol with just-in-time segment rendering. We demonstrate that Vidformer cuts full-render time by 2-3x across diverse annotation workloads, and, more critically, drops time-to-playback to 0.25-0.5s. This represents a 400x improvement that decouples clip length from first-frame playback latency, and unlocks the ability to perform interactive video-native querying with sub-second latencies. Furthermore, we show how our approach enables interactive video-native LLM-based conversational querying as well.", "AI": {"tldr": "Vidformer\u662f\u4e00\u4e2a\u89c6\u9891\u539f\u751f\u67e5\u8be2\u6e32\u67d3\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u900f\u660e\u63d0\u5347\u53ef\u89c6\u5316\u4ee3\u7801\u4e3a\u58f0\u660e\u5f0f\u8868\u793a\u3001\u4f18\u5316\u5e76\u884c\u5316\u6e32\u67d3\u3001\u5373\u65f6\u89c6\u9891\u670d\u52a1\uff0c\u5c06\u64ad\u653e\u5ef6\u8fdf\u964d\u81f30.25-0.5\u79d2\uff0c\u5b9e\u73b0400\u500d\u6539\u8fdb\u3002", "motivation": "\u89c6\u9891\u539f\u751f\u67e5\u8be2\u7684\u74f6\u9888\u5728\u4e8e\u6e32\u67d3\u800c\u975e\u67e5\u8be2\u6267\u884c\uff0c\u73b0\u6709\u540e\u5904\u7406\u811a\u672c\u7f13\u6162\uff0c\u5bfc\u81f4\u4ea4\u4e92\u5f0f\u89c6\u9891\u6570\u636e\u5206\u6790\u5de5\u4f5c\u6d41\u4e2d\u7528\u6237\u9700\u8981\u7b49\u5f85\u6570\u79d2\u3001\u5206\u949f\u751a\u81f3\u5c0f\u65f6\u624d\u80fd\u770b\u5230\u7ed3\u679c\u3002", "method": "1) \u900f\u660e\u5730\u5c06\u73b0\u6709\u53ef\u89c6\u5316\u4ee3\u7801\u63d0\u5347\u4e3a\u58f0\u660e\u5f0f\u8868\u793a\uff1b2) \u900f\u660e\u5730\u4f18\u5316\u548c\u5e76\u884c\u5316\u6e32\u67d3\uff1b3) \u901a\u8fc7\u89c6\u9891\u70b9\u64ad\u534f\u8bae\u5373\u65f6\u670d\u52a1\u89c6\u9891\uff0c\u5b9e\u73b0\u5b9e\u65f6\u7247\u6bb5\u6e32\u67d3\u3002", "result": "Vidformer\u5c06\u5b8c\u6574\u6e32\u67d3\u65f6\u95f4\u51cf\u5c112-3\u500d\uff0c\u66f4\u91cd\u8981\u7684\u662f\u5c06\u64ad\u653e\u65f6\u95f4\u964d\u81f30.25-0.5\u79d2\uff0c\u5b9e\u73b0400\u500d\u6539\u8fdb\uff0c\u89e3\u8026\u7247\u6bb5\u957f\u5ea6\u4e0e\u9996\u5e27\u64ad\u653e\u5ef6\u8fdf\uff0c\u652f\u6301\u4e9a\u79d2\u7ea7\u5ef6\u8fdf\u7684\u4ea4\u4e92\u5f0f\u89c6\u9891\u539f\u751f\u67e5\u8be2\u3002", "conclusion": "Vidformer\u89e3\u51b3\u4e86\u89c6\u9891\u539f\u751f\u67e5\u8be2\u7684\u6e32\u67d3\u74f6\u9888\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u64ad\u653e\u5ef6\u8fdf\uff0c\u4f7f\u4ea4\u4e92\u5f0f\u89c6\u9891\u6570\u636e\u5206\u6790\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u652f\u6301\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u5f0f\u89c6\u9891\u67e5\u8be2\u3002"}}
{"id": "2601.17133", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17133", "abs": "https://arxiv.org/abs/2601.17133", "authors": ["Inderjeet Singh", "Eleonore Vissol-Gaudin", "Andikan Otung", "Motoyoshi Sekiya"], "title": "Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation", "comment": "Accepted to AAAI 2026. 13 pages, 3 figures, 10 tables. Code available at: https://github.com/FujitsuResearch/knexa-fl", "summary": "Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.", "AI": {"tldr": "KNEXA-FL\uff1a\u4e00\u79cd\u9762\u5411\u5f02\u6784LLM\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e2d\u592e\u5339\u914d\u5668\u5b66\u4e60\u6700\u4f18\u7684P2P\u534f\u4f5c\u7b56\u7565\uff0c\u907f\u514d\u8d1f\u8fc1\u79fb\u5e76\u4fdd\u62a4\u6570\u636e\u9690\u79c1", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u4e13\u4e1a\u5316LLM\u5fae\u8c03\u4e2d\u5b58\u5728\u77db\u76fe\uff1a\u9700\u8981\u8de8\u7ec4\u7ec7\u6570\u636e\u4f46\u53d7\u9650\u4e8e\u9690\u79c1\u4fdd\u62a4\u3002\u96c6\u4e2d\u5f0fFL\u5b58\u5728\u5355\u70b9\u6545\u969c\u548c\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u98ce\u9669\uff0c\u800c\u968f\u673aP2P\u534f\u4f5c\u7684DFL\u5219\u5ffd\u7565\u667a\u80fd\u4f53\u5f02\u6784\u6027\u5e76\u53ef\u80fd\u5bfc\u81f4\u8d1f\u8fc1\u79fb", "method": "\u63d0\u51faKNEXA-FL\u6846\u67b6\uff0c\u91c7\u7528\u975e\u805a\u5408\u7684\u4e2d\u592e\u5206\u6790\u5668/\u5339\u914d\u5668(CPM)\uff0c\u5c06P2P\u534f\u4f5c\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u4f7f\u7528LinUCB\u7b97\u6cd5\u5728\u62bd\u8c61\u667a\u80fd\u4f53\u914d\u7f6e\u6587\u4ef6\u4e0a\u5b66\u4e60\u6700\u4f18\u5339\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u5b89\u5168\u84b8\u998f\u5b9e\u73b0\u5f02\u6784PEFT-based LLM\u667a\u80fd\u4f53\u95f4\u7684\u76f4\u63a5\u77e5\u8bc6\u4ea4\u6362", "result": "\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKNEXA-FL\u76f8\u6bd4\u968f\u673aP2P\u534f\u4f5c\u5c06Pass@1\u63d0\u9ad8\u4e86\u7ea650%\uff0c\u4e14\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u6536\u655b\u6027\uff0c\u800c\u5f3a\u5927\u7684\u96c6\u4e2d\u5f0f\u84b8\u998f\u57fa\u7ebf\u5219\u51fa\u73b0\u707e\u96be\u6027\u6027\u80fd\u5d29\u6e83", "conclusion": "\u81ea\u9002\u5e94\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u7f16\u6392\u662f\u6784\u5efa\u9c81\u68d2\u6709\u6548\u7684\u53bb\u4e2d\u5fc3\u5316AI\u751f\u6001\u7cfb\u7edf\u7684\u57fa\u672c\u539f\u5219\uff0cKNEXA-FL\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u534f\u4f5c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898"}}
{"id": "2601.17231", "categories": ["cs.RO", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.17231", "abs": "https://arxiv.org/abs/2601.17231", "authors": ["Tanmay Desai", "Brian Plancher", "R. Iris Bahar"], "title": "Real-Time, Energy-Efficient, Sampling-Based Optimal Control via FPGA Acceleration", "comment": "8 pages, 5 figures", "summary": "Autonomous mobile robots (AMRs), used for search-and-rescue and remote exploration, require fast and robust planning and control schemes. Sampling-based approaches for Model Predictive Control, especially approaches based on the Model Predictive Path Integral Control (MPPI) algorithm, have recently proven both to be highly effective for such applications and to map naturally to GPUs for hardware acceleration. However, both GPU and CPU implementations of such algorithms can struggle to meet tight energy and latency budgets on battery-constrained AMR platforms that leverage embedded compute. To address this issue, we present an FPGA-optimized MPPI design that exposes fine-grained parallelism and eliminates synchronization bottlenecks via deep pipelining and parallelism across algorithmic stages. This results in an average 3.1x to 7.5x speedup over optimized implementations on an embedded GPU and CPU, respectively, while simultaneously achieving a 2.5x to 5.4x reduction in energy usage. These results demonstrate that FPGA architectures are a promising direction for energy-efficient and high-performance edge robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9FPGA\u4f18\u5316\u7684MPPI\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5e76\u884c\u5316\u548c\u6d88\u9664\u540c\u6b65\u74f6\u9888\uff0c\u5728\u5d4c\u5165\u5f0f\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u6bd4GPU\u548cCPU\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\uff08AMR\uff09\u5728\u641c\u7d22\u6551\u63f4\u548c\u8fdc\u7a0b\u63a2\u7d22\u5e94\u7528\u4e2d\u9700\u8981\u5feb\u901f\u9c81\u68d2\u7684\u89c4\u5212\u63a7\u5236\u65b9\u6848\u3002\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff08\u5982MPPI\uff09\u867d\u7136\u6709\u6548\u4e14\u9002\u5408GPU\u52a0\u901f\uff0c\u4f46\u5728\u5d4c\u5165\u5f0f\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u96be\u4ee5\u6ee1\u8db3\u4e25\u683c\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\u9884\u7b97\u3002", "method": "\u8bbe\u8ba1\u4e86\u9488\u5bf9FPGA\u4f18\u5316\u7684MPPI\u67b6\u6784\uff0c\u901a\u8fc7\u6df1\u5ea6\u6d41\u6c34\u7ebf\u548c\u8de8\u7b97\u6cd5\u9636\u6bb5\u7684\u5e76\u884c\u5316\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u5e76\u884c\u5e76\u6d88\u9664\u4e86\u540c\u6b65\u74f6\u9888\u3002", "result": "\u76f8\u6bd4\u4f18\u5316\u7684\u5d4c\u5165\u5f0fGPU\u548cCPU\u5b9e\u73b0\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u5e73\u57473.1\u500d\u548c7.5\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u80fd\u8017\u964d\u4f4e\u4e862.5\u500d\u52305.4\u500d\u3002", "conclusion": "FPGA\u67b6\u6784\u662f\u5b9e\u73b0\u9ad8\u80fd\u6548\u3001\u9ad8\u6027\u80fd\u8fb9\u7f18\u673a\u5668\u4eba\u8ba1\u7b97\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2601.18710", "categories": ["cs.ET", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18710", "abs": "https://arxiv.org/abs/2601.18710", "authors": ["A. Bano", "L. Liebovitch"], "title": "Analyzing Images of Blood Cells with Quantum Machine Learning Methods: Equilibrium Propagation and Variational Quantum Circuits to Detect Acute Myeloid Leukemia", "comment": "5 pages, 1 figure, 2 tables", "summary": "This paper presents a feasibility study demonstrating that quantum machine learning (QML) algorithms achieve competitive performance on real-world medical imaging despite operating under severe constraints. We evaluate Equilibrium Propagation (EP), an energy-based learning method that does not use backpropagation (incompatible with quantum systems due to state-collapsing measurements) and Variational Quantum Circuits (VQCs) for automated detection of Acute Myeloid Leukemia (AML) from blood cell microscopy images using binary classification (2 classes: AML vs. Healthy).\n  Key Result: Using limited subsets (50-250 samples per class) of the AML-Cytomorphology dataset (18,365 expert-annotated images), quantum methods achieve performance only 12-15% below classical CNNs despite reduced image resolution (64x64 pixels), engineered features (20D), and classical simulation via Qiskit. EP reaches 86.4% accuracy (only 12% below CNN) without backpropagation, while the 4-qubit VQC attains 83.0% accuracy with consistent data efficiency: VQC maintains stable 83% performance with only 50 samples per class, whereas CNN requires 250 samples (5x more data) to reach 98%. These results establish reproducible baselines for QML in healthcare, validating NISQ-era feasibility.", "AI": {"tldr": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u53ef\u884c\u6027\u7814\u7a76\uff1a\u4f7f\u7528\u5e73\u8861\u4f20\u64ad\u548c\u53d8\u5206\u91cf\u5b50\u7535\u8def\u5728\u6025\u6027\u9ad3\u7cfb\u767d\u8840\u75c5\u68c0\u6d4b\u4e2d\u8fbe\u5230\u63a5\u8fd1\u7ecf\u5178CNN\u7684\u6027\u80fd\uff0c\u6570\u636e\u6548\u7387\u66f4\u9ad8", "motivation": "\u63a2\u7d22\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u73b0\u5b9e\u4e16\u754c\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u662f\u5728NISQ\u65f6\u4ee3\u786c\u4ef6\u9650\u5236\u4e0b\uff0c\u9a8c\u8bc1\u91cf\u5b50\u65b9\u6cd5\u80fd\u5426\u5728\u533b\u5b66\u8bca\u65ad\u4efb\u52a1\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd", "method": "\u4f7f\u7528\u5e73\u8861\u4f20\u64ad\uff08\u4e00\u79cd\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u80fd\u91cf\u57fa\u5b66\u4e60\u65b9\u6cd5\uff09\u548c\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff0c\u5bf9AML-Cytomorphology\u6570\u636e\u96c6\u4e2d\u7684\u8840\u6db2\u7ec6\u80de\u663e\u5fae\u955c\u56fe\u50cf\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\uff08AML vs. \u5065\u5eb7\uff09\uff0c\u56fe\u50cf\u964d\u91c7\u6837\u81f364x64\u50cf\u7d20\uff0c\u63d0\u53d620\u7ef4\u5de5\u7a0b\u7279\u5f81\uff0c\u901a\u8fc7Qiskit\u8fdb\u884c\u7ecf\u5178\u6a21\u62df", "result": "\u5e73\u8861\u4f20\u64ad\u8fbe\u523086.4%\u51c6\u786e\u7387\uff08\u4ec5\u6bd4CNN\u4f4e12%\uff09\uff0c4\u91cf\u5b50\u6bd4\u7279VQC\u8fbe\u523083.0%\u51c6\u786e\u7387\uff1bVQC\u5728\u6bcf\u7c7b\u4ec550\u4e2a\u6837\u672c\u65f6\u4fdd\u6301\u7a33\u5b9a\u768483%\u6027\u80fd\uff0c\u800cCNN\u9700\u8981\u6bcf\u7c7b250\u4e2a\u6837\u672c\uff085\u500d\u6570\u636e\u91cf\uff09\u624d\u80fd\u8fbe\u523098%\u51c6\u786e\u7387", "conclusion": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aNISQ\u65f6\u4ee3\u91cf\u5b50\u8ba1\u7b97\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6"}}
{"id": "2601.17149", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17149", "abs": "https://arxiv.org/abs/2601.17149", "authors": ["Jathushan Kaetheeswaran", "Jenny Wei"], "title": "Exploring EEG-driven brain-heart coupling across sleep stages in individuals with sleep disorders", "comment": null, "summary": "The interactions between the brain and heart during sleep are responsible for regulating autonomic function. While brain-heart coupling has been studied in healthy populations, the relationships between neural and cardiac activity across sleep stages in the presence of sleep disorders are not clear. This study examines the influence of brain-driven cardiac activity across sleep stages for individuals with sleep disorders. Overnight recordings of C3 and C4 electroencephalogram (EEG) channels and electrocardiogram (ECG) signals from 146 individuals were preprocessed and analyzed in the frequency domain through a linear mixed-effect model. Our results show that parasympathetic activity is sensitive to changes in delta and beta powers during later stages of non-rapid eye movement (NREM) sleep, as both band powers exhibited strong negative effects on high-frequency heart rate variability (HF-HRV) power. These findings show that neural activity can drive vagal tone across sleep stages, suggesting that treatments on key EEG bands during NREM and REM stages may help restore regular cardiac behaviour.", "AI": {"tldr": "\u7814\u7a76\u7761\u7720\u969c\u788d\u60a3\u8005\u5728\u4e0d\u540c\u7761\u7720\u9636\u6bb5\u8111\u5fc3\u8026\u5408\u5173\u7cfb\uff0c\u53d1\u73b0\u526f\u4ea4\u611f\u795e\u7ecf\u6d3b\u52a8\u5bf9NREM\u540e\u671f\u7761\u7720\u4e2ddelta\u548cbeta\u9891\u6bb5\u529f\u7387\u53d8\u5316\u654f\u611f\uff0c\u795e\u7ecf\u6d3b\u52a8\u53ef\u9a71\u52a8\u8ff7\u8d70\u795e\u7ecf\u5f20\u529b", "motivation": "\u867d\u7136\u5065\u5eb7\u4eba\u7fa4\u7684\u8111\u5fc3\u8026\u5408\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u7761\u7720\u969c\u788d\u60a3\u8005\u5728\u4e0d\u540c\u7761\u7720\u9636\u6bb5\u795e\u7ecf\u4e0e\u5fc3\u810f\u6d3b\u52a8\u7684\u5173\u7cfb\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u63a2\u7a76\u8111\u9a71\u52a8\u7684\u5fc3\u810f\u6d3b\u52a8\u5728\u7761\u7720\u969c\u788d\u60a3\u8005\u4e2d\u7684\u8868\u73b0", "method": "\u6536\u96c6146\u540d\u4e2a\u4f53\u7684C3\u3001C4\u8111\u7535\u56fe\u548c\u5fc3\u7535\u56fe\u6574\u591c\u8bb0\u5f55\uff0c\u9884\u5904\u7406\u540e\u5728\u9891\u57df\u901a\u8fc7\u7ebf\u6027\u6df7\u5408\u6548\u5e94\u6a21\u578b\u5206\u6790\u8111\u5fc3\u8026\u5408\u5173\u7cfb", "result": "\u526f\u4ea4\u611f\u795e\u7ecf\u6d3b\u52a8\u5728\u975e\u5feb\u901f\u773c\u52a8\u7761\u7720\u540e\u671f\u5bf9delta\u548cbeta\u9891\u6bb5\u529f\u7387\u53d8\u5316\u654f\u611f\uff0c\u8fd9\u4e24\u4e2a\u9891\u6bb5\u529f\u7387\u5bf9\u9ad8\u9891\u5fc3\u7387\u53d8\u5f02\u6027\u529f\u7387\u6709\u5f3a\u70c8\u8d1f\u5411\u5f71\u54cd", "conclusion": "\u795e\u7ecf\u6d3b\u52a8\u53ef\u9a71\u52a8\u4e0d\u540c\u7761\u7720\u9636\u6bb5\u7684\u8ff7\u8d70\u795e\u7ecf\u5f20\u529b\uff0c\u9488\u5bf9NREM\u548cREM\u9636\u6bb5\u5173\u952e\u8111\u7535\u56fe\u9891\u6bb5\u7684\u6cbb\u7597\u53ef\u80fd\u6709\u52a9\u4e8e\u6062\u590d\u89c4\u5f8b\u7684\u5fc3\u810f\u884c\u4e3a"}}
{"id": "2601.18783", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18783", "abs": "https://arxiv.org/abs/2601.18783", "authors": ["Deepthi Pathare", "Leo Laine", "Morteza Haghir Chehreghani"], "title": "Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic", "comment": null, "summary": "Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePPO\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e3a\u91cd\u578b\u8f66\u8f86\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u5b66\u4e60\u8fde\u7eed\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u96c6\uff0c\u5e73\u8861\u5b89\u5168\u3001\u80fd\u6548\u548c\u65f6\u95f4\u6548\u7387\u4e09\u4e2a\u51b2\u7a81\u76ee\u6807\u3002", "motivation": "\u4f20\u7edf\u6807\u91cf\u5956\u52b1\u51fd\u6570\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u7ade\u4e89\u76ee\u6807\uff0c\u5f80\u5f80\u6a21\u7cca\u4e86\u76ee\u6807\u95f4\u7684\u6743\u8861\u7ed3\u6784\u3002\u91cd\u578b\u8f66\u8f86\u5728\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u4e2d\u9700\u8981\u5e73\u8861\u5b89\u5168\u3001\u6548\u7387\u548c\u8fd0\u8425\u6210\u672c\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u5361\u8f66\u6218\u672f\u51b3\u7b56\u7684\u53ef\u6269\u5c55\u4eff\u771f\u5e73\u53f0\u4e0a\u5b66\u4e60\u8fde\u7eed\u7b56\u7565\u96c6\uff0c\u660e\u786e\u8868\u793a\u5b89\u5168\u3001\u80fd\u6548\u548c\u65f6\u95f4\u6548\u7387\u4e09\u4e2a\u51b2\u7a81\u76ee\u6807\u95f4\u7684\u6743\u8861\u3002", "result": "\u5b66\u4e60\u5230\u5e73\u6ed1\u4e14\u53ef\u89e3\u91ca\u7684\u8fde\u7eed\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u96c6\uff0c\u6355\u83b7\u4e86\u5b89\u5168\uff08\u78b0\u649e\u548c\u6210\u529f\u5b8c\u6210\uff09\u3001\u80fd\u6548\uff08\u80fd\u6e90\u6210\u672c\uff09\u548c\u65f6\u95f4\u6548\u7387\uff08\u9a7e\u9a76\u5458\u6210\u672c\uff09\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5e15\u7d2f\u6258\u524d\u6cbf\u5e73\u6ed1\uff0c\u53ef\u5728\u4e0d\u540c\u51b2\u7a81\u76ee\u6807\u95f4\u7075\u6d3b\u9009\u62e9\u9a7e\u9a76\u884c\u4e3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u5141\u8bb8\u5728\u4e0d\u540c\u9a7e\u9a76\u7b56\u7565\u95f4\u65e0\u7f1d\u5207\u6362\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u81ea\u9002\u5e94\u7684\u51b3\u7b56\u7b56\u7565\u3002"}}
{"id": "2601.17899", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2601.17899", "abs": "https://arxiv.org/abs/2601.17899", "authors": ["Junhao Qiu", "Xin Chen", "Liang Ge", "Liyong Lin", "Zhichao Lu", "Qingfu Zhang"], "title": "Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization", "comment": null, "summary": "Neighborhood search operators are critical to the performance of Multi-Objective Evolutionary Algorithms (MOEAs) and rely heavily on expert design. Although recent LLM-based Automated Heuristic Design (AHD) methods have made notable progress, they primarily optimize individual heuristics or components independently, lacking explicit exploration and exploitation of dynamic coupling relationships between multiple operators. In this paper, multi-operator optimization in MOEAs is formulated as a Markov decision process, enabling the improvement of interdependent operators through sequential decision-making. To address this, we propose the Evolution of Operator Combination (E2OC) framework for MOEAs, which achieves the co-evolution of design strategies and executable codes. E2OC employs Monte Carlo Tree Search to progressively search combinations of operator design strategies and adopts an operator rotation mechanism to identify effective operator configurations while supporting the integration of mainstream AHD methods as the underlying designer. Experimental results across AHD tasks with varying objectives and problem scales show that E2OC consistently outperforms state-of-the-art AHD and other multi-heuristic co-design frameworks, demonstrating strong generalization and sustained optimization capability.", "AI": {"tldr": "\u63d0\u51faE2OC\u6846\u67b6\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u548c\u591a\u7b97\u5b50\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u73b0MOEA\u4e2d\u7b97\u5b50\u8bbe\u8ba1\u7b56\u7565\u4e0e\u53ef\u6267\u884c\u4ee3\u7801\u7684\u534f\u540c\u8fdb\u5316\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u8bbe\u8ba1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u4e3b\u8981\u72ec\u7acb\u4f18\u5316\u5355\u4e2a\u542f\u53d1\u5f0f\u6216\u7ec4\u4ef6\uff0c\u7f3a\u4e4f\u5bf9\u591a\u4e2a\u7b97\u5b50\u95f4\u52a8\u6001\u8026\u5408\u5173\u7cfb\u7684\u663e\u5f0f\u63a2\u7d22\u548c\u5229\u7528\uff0c\u9650\u5236\u4e86\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u5c06MOEA\u4e2d\u7684\u591a\u7b97\u5b50\u4f18\u5316\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51faE2OC\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6e10\u8fdb\u641c\u7d22\u7b97\u5b50\u8bbe\u8ba1\u7b56\u7565\u7ec4\u5408\uff1b2\uff09\u91c7\u7528\u7b97\u5b50\u8f6e\u6362\u673a\u5236\u8bc6\u522b\u6709\u6548\u7b97\u5b50\u914d\u7f6e\uff1b3\uff09\u652f\u6301\u4e3b\u6d41AHD\u65b9\u6cd5\u4f5c\u4e3a\u5e95\u5c42\u8bbe\u8ba1\u5668\u3002", "result": "\u5728\u4e0d\u540c\u76ee\u6807\u548c\u95ee\u9898\u89c4\u6a21\u7684AHD\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cE2OC\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684AHD\u65b9\u6cd5\u548c\u5176\u4ed6\u591a\u542f\u53d1\u5f0f\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6301\u7eed\u4f18\u5316\u80fd\u529b\u3002", "conclusion": "E2OC\u6846\u67b6\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u7b97\u5b50\u8bbe\u8ba1\u7b56\u7565\u548c\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7b97\u5b50\u534f\u540c\u4f18\u5316\u95ee\u9898\uff0c\u4e3aMOEA\u7684\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17006", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17006", "abs": "https://arxiv.org/abs/2601.17006", "authors": ["Xuchen Li", "Jing Chen", "Xuzhao Li", "Hao Liang", "Xiaohuan Zhou", "Taifeng Wang", "Wentao Zhang"], "title": "MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning", "comment": "Preprint, Under review", "summary": "In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.", "AI": {"tldr": "MathMixup\uff1a\u4e00\u79cd\u901a\u8fc7\u6df7\u5408\u548c\u5206\u89e3\u7b56\u7565\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u96be\u5ea6\u53ef\u63a7\u6570\u5b66\u63a8\u7406\u95ee\u9898\u7684\u6570\u636e\u5408\u6210\u8303\u5f0f\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u663e\u8457\u63d0\u5347LLMs\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u6709\u9650\u548c\u96be\u5ea6\u63a7\u5236\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301\u8bfe\u7a0b\u5b66\u4e60\u7b49\u9ad8\u6548\u8bad\u7ec3\u8303\u5f0f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u96be\u5ea6\u53ef\u63a7\u6570\u636e\u7684\u65b9\u6cd5", "method": "\u63d0\u51faMathMixup\u6570\u636e\u5408\u6210\u8303\u5f0f\uff0c\u91c7\u7528\u6df7\u5408\u548c\u5206\u89e3\u7b56\u7565\u7cfb\u7edf\u751f\u6210\u96be\u5ea6\u53ef\u63a7\u7684\u6570\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u7ed3\u5408\u81ea\u52a8\u81ea\u68c0\u548c\u4eba\u5de5\u7b5b\u9009\u786e\u4fdd\u8bed\u4e49\u6e05\u6670\u548c\u96be\u5ea6\u68af\u5ea6\u7ed3\u6784\uff0c\u6784\u5efaMathMixupQA\u6570\u636e\u96c6\u5e76\u8bbe\u8ba1\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565", "result": "Qwen2.5-7B\u5728\u4e03\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5f97\u520652.6%\uff0c\u8d85\u8d8a\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86MathMixup\u5728\u63d0\u5347LLMs\u6570\u5b66\u63a8\u7406\u80fd\u529b\u548c\u63a8\u8fdb\u6570\u636e\u4e2d\u5fc3\u8bfe\u7a0b\u5b66\u4e60\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "MathMixup\u901a\u8fc7\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u96be\u5ea6\u53ef\u63a7\u7684\u6570\u5b66\u63a8\u7406\u6570\u636e\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u663e\u8457\u63d0\u5347LLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7684\u8bfe\u7a0b\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17285", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17285", "abs": "https://arxiv.org/abs/2601.17285", "authors": ["Lantian Xu", "Junhua Zhang", "Dong Wen", "Lu Qin", "Ying Zhang", "Xuemin Lin"], "title": "Constant-time Connectivity and 2-Edge Connectivity Querying in Dynamic Graphs", "comment": null, "summary": "Connectivity query processing is a fundamental problem in graph processing. Given an undirected graph and two query vertices, the problem aims to identify whether they are connected via a path. Given frequent edge updates in real graph applications, in this paper, we study connectivity query processing in fully dynamic graphs, where edges are frequently inserted or deleted. A recent solution, called D-tree, maintains a spanning tree for each connected component and applies several heuristics to reduce the depth of the tree. To improve efficiency, we propose a new spanning-tree-based solution by maintaining a disjoint-set tree simultaneously. By combining the advantages of two trees, we achieve the constant query time complexity and also significantly improve the theoretical running time in both edge insertion and edge deletion. In addition, we extend our connectivity maintenance algorithms to maintain 2-edge connectivity. Our performance studies on real large datasets show considerable improvement of our algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u52a8\u6001\u56fe\u8fde\u901a\u6027\u67e5\u8be2\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u7ef4\u62a4\u751f\u6210\u6811\u548c\u4e0d\u76f8\u4ea4\u96c6\u6811\uff0c\u5b9e\u73b0\u5e38\u6570\u67e5\u8be2\u65f6\u95f4\u5e76\u663e\u8457\u63d0\u9ad8\u8fb9\u63d2\u5165\u548c\u5220\u9664\u7684\u7406\u8bba\u8fd0\u884c\u65f6\u95f4", "motivation": "\u73b0\u5b9e\u56fe\u5e94\u7528\u4e2d\u8fb9\u9891\u7e41\u66f4\u65b0\uff0c\u9700\u8981\u9ad8\u6548\u5904\u7406\u52a8\u6001\u56fe\u4e2d\u7684\u8fde\u901a\u6027\u67e5\u8be2\u3002\u73b0\u6709D-tree\u65b9\u6cd5\u901a\u8fc7\u7ef4\u62a4\u751f\u6210\u6811\u5e76\u5e94\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u51cf\u5c11\u6811\u6df1\u5ea6\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4", "method": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u6811\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u7ef4\u62a4\u4e0d\u76f8\u4ea4\u96c6\u6811\u3002\u7ed3\u5408\u4e24\u79cd\u6811\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u5e38\u6570\u67e5\u8be2\u65f6\u95f4\uff0c\u5e76\u5728\u8fb9\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\u4e2d\u663e\u8457\u6539\u8fdb\u7406\u8bba\u8fd0\u884c\u65f6\u95f4\u3002\u8fd8\u5c06\u7b97\u6cd5\u6269\u5c55\u5230\u7ef4\u62a42-\u8fb9\u8fde\u901a\u6027", "result": "\u5728\u771f\u5b9e\u5927\u578b\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb", "conclusion": "\u901a\u8fc7\u540c\u65f6\u7ef4\u62a4\u751f\u6210\u6811\u548c\u4e0d\u76f8\u4ea4\u96c6\u6811\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u52a8\u6001\u56fe\u8fde\u901a\u6027\u67e5\u8be2\u5904\u7406\u4e2d\u5b9e\u73b0\u4e86\u5e38\u6570\u67e5\u8be2\u65f6\u95f4\uff0c\u5e76\u5728\u8fb9\u66f4\u65b0\u64cd\u4f5c\u4e2d\u83b7\u5f97\u4e86\u7406\u8bba\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u7b97\u6cd5\u53ef\u6269\u5c55\u52302-\u8fb9\u8fde\u901a\u6027\u7ef4\u62a4"}}
{"id": "2601.17888", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17888", "abs": "https://arxiv.org/abs/2601.17888", "authors": ["Monika Santra", "Bokai Zhang", "Mark Lim", "Vishnu Asutosh Dasu", "Dongrui Zeng", "Gang Tan"], "title": "iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement", "comment": null, "summary": "Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems.", "AI": {"tldr": "iResolveX\u662f\u4e00\u4e2a\u6df7\u5408\u591a\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u4fdd\u5b88\u9759\u6001\u5206\u6790\u548c\u5b66\u4e60\u4f18\u5316\uff0c\u7528\u4e8e\u4e8c\u8fdb\u5236\u9006\u5411\u5de5\u7a0b\u4e2d\u7684\u95f4\u63a5\u8c03\u7528\u89e3\u6790\uff0c\u5728\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8bef\u62a5\u3002", "motivation": "\u95f4\u63a5\u8c03\u7528\u89e3\u6790\u662f\u9006\u5411\u5de5\u7a0b\u548c\u63a7\u5236\u6d41\u56fe\u6062\u590d\u7684\u5173\u952e\u6311\u6218\u3002\u9759\u6001\u5206\u6790\u867d\u7136\u5b8c\u5907\u4f46\u4f1a\u4ea7\u751f\u5927\u91cf\u8bef\u62a5\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u63d0\u9ad8\u7cbe\u5ea6\u4f46\u53ef\u80fd\u727a\u7272\u5b8c\u5907\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u53c8\u80fd\u51cf\u5c11\u8bef\u62a5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faiResolveX\u6df7\u5408\u591a\u5c42\u6846\u67b6\uff1a\u7b2c\u4e00\u5c42\u5e94\u7528\u4fdd\u5b88\u503c\u96c6\u5206\u6790(BPA)\u786e\u4fdd\u9ad8\u53ec\u56de\u7387\uff1b\u7b2c\u4e8c\u5c42\u6dfb\u52a0\u57fa\u4e8e\u5b66\u4e60\u7684\u8f6f\u7b7e\u540d\u8bc4\u5206\u5668(iScoreGen)\u548c\u9009\u62e9\u6027\u8fc7\u7a0b\u95f4\u53cd\u5411\u5206\u6790(iScoreRefine)\u6765\u51cf\u5c11\u8bef\u62a5\uff1b\u6700\u7ec8\u8f93\u51fap-IndirectCFG\u4e3a\u95f4\u63a5\u8fb9\u6807\u6ce8\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002", "result": "\u5728SPEC CPU2006\u548c\u771f\u5b9e\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e0a\uff0ciScoreGen\u5e73\u5747\u51cf\u5c1119.2%\u7684\u9884\u6d4b\u76ee\u6807\uff0c\u540c\u65f6\u4fdd\u630198.2%\u7684BPA\u7ea7\u53ec\u56de\u7387\u3002\u7ed3\u5408iScoreRefine\u540e\uff0c\u603b\u51cf\u5c11\u7387\u8fbe\u523044.3%\uff0c\u53ec\u56de\u7387\u4e3a97.8%\uff08\u4ec5\u4e0b\u964d0.4%\uff09\u3002iResolveX\u652f\u6301\u4fdd\u5b88\u53ec\u56de\u4fdd\u6301\u548cF1\u4f18\u5316\u914d\u7f6e\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\u3002", "conclusion": "iResolveX\u901a\u8fc7\u7ed3\u5408\u4fdd\u5b88\u9759\u6001\u5206\u6790\u548c\u5b66\u4e60\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u95f4\u63a5\u8c03\u7528\u89e3\u6790\u4e2d\u7684\u7cbe\u5ea6-\u53ec\u56de\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u4e0b\u6e38\u5206\u6790\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8bef\u62a5\u3002"}}
{"id": "2601.17435", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17435", "abs": "https://arxiv.org/abs/2601.17435", "authors": ["Maria Jesus Rodriguez-Sanchez", "Manuel Noguera", "Angel Ruiz-Zafra", "Kawtar Benghazi"], "title": "Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems", "comment": "12 pages, 3 figures", "summary": "Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments.", "AI": {"tldr": "DALIA\u662f\u4e00\u4e2a\u58f0\u660e\u5f0f\u7684\u3001\u6a21\u578b\u65e0\u5173\u7684\u667a\u80fd\u4f53\u67b6\u6784\u5c42\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u53ef\u6267\u884c\u80fd\u529b\u3001\u58f0\u660e\u5f0f\u53d1\u73b0\u534f\u8bae\u548c\u786e\u5b9a\u6027\u4efb\u52a1\u56fe\uff0c\u89e3\u51b3\u5f53\u524d\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5982\u5e7b\u89c9\u52a8\u4f5c\u3001\u4e0d\u53ef\u6267\u884c\u8ba1\u5212\u548c\u8106\u5f31\u534f\u8c03\u3002\u8fd9\u4e9b\u95ee\u9898\u5e76\u975e\u6e90\u4e8e\u5e95\u5c42\u6a21\u578b\u9650\u5236\uff0c\u800c\u662f\u7f3a\u4e4f\u660e\u786e\u8fde\u63a5\u76ee\u6807\u3001\u80fd\u529b\u548c\u6267\u884c\u7684\u67b6\u6784\u7ed3\u6784\u3002", "method": "\u63d0\u51faDALIA\uff08\u58f0\u660e\u5f0f\u667a\u80fd\u4f53\u5c42\uff09\uff0c\u5305\u542b\uff1a1\uff09\u5f62\u5f0f\u5316\u53ef\u6267\u884c\u80fd\u529b\uff1b2\uff09\u901a\u8fc7\u58f0\u660e\u5f0f\u53d1\u73b0\u534f\u8bae\u66b4\u9732\u4efb\u52a1\uff1b3\uff09\u7ef4\u62a4\u667a\u80fd\u4f53\u53ca\u5176\u6267\u884c\u8d44\u6e90\u7684\u8054\u90a6\u76ee\u5f55\uff1b4\uff09\u6784\u5efa\u5b8c\u5168\u57fa\u4e8e\u58f0\u660e\u64cd\u4f5c\u7684\u786e\u5b9a\u6027\u4efb\u52a1\u56fe\u3002\u5f3a\u5236\u5206\u79bb\u53d1\u73b0\u3001\u89c4\u5212\u548c\u6267\u884c\u9636\u6bb5\u3002", "result": "\u901a\u8fc7\u4ee3\u8868\u6027\u4efb\u52a1\u5bfc\u5411\u573a\u666f\u5c55\u793a\u4e86DALIA\u7684\u64cd\u4f5c\uff0c\u8bc1\u660e\u58f0\u660e\u5f0f\u57fa\u7840\u80fd\u591f\u5b9e\u73b0\u8de8\u5f02\u6784\u73af\u5883\u7684\u53ef\u91cd\u73b0\u548c\u53ef\u9a8c\u8bc1\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u5c06\u667a\u80fd\u4f53\u884c\u4e3a\u7ea6\u675f\u5728\u53ef\u9a8c\u8bc1\u7684\u64cd\u4f5c\u7a7a\u95f4\u5185\u3002", "conclusion": "DALIA\u67b6\u6784\u901a\u8fc7\u58f0\u660e\u5f0f\u57fa\u7840\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u5bf9\u63a8\u6d4b\u63a8\u7406\u548c\u81ea\u7531\u5f62\u5f0f\u534f\u8c03\u7684\u4f9d\u8d56\uff0c\u4e3a\u6784\u5efa\u53ef\u9a8c\u8bc1\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17249", "abs": "https://arxiv.org/abs/2601.17249", "authors": ["Peter Bryan", "Rejin John Varghese", "Dario Farina"], "title": "Quantifying Ergonomics in the Elevate Soft Robotic Suit", "comment": "5 pages, 3 figures. Submitted to IEEE-EMBC 2026", "summary": "Soft robotic suits have the potential to rehabilitate, assist, and augment the human body. The low weight, cost, and minimal form-factor of these devices make them ideal for daily use by both healthy and impaired individuals. However, challenges associated with data-driven, user-specific, and comfort-first design of human-robot interfaces using soft materials limit their widespread translation and adoption. In this work, we present the quantitative evaluation of ergonomics and comfort of the Elevate suit - a cable driven soft robotic suit that assists shoulder elevation. Using a motion-capture system and force sensors, we measured the suit's ergonomics during assisted shoulder elevation up to 70 degrees. Two 4-hour sessions were conducted with one subject, involving transmitting cable tensions of up to 200N with no discomfort reported. We estimated that the pressure applied to the shoulder during assisted movements was within the range seen in a human grasp (approximately 69.1-85.1kPa), and estimated volumetric compression of <3% and <8% across the torso and upper arm, respectively. These results provide early validation of Elevate's ergonomic design in preparation for future studies with patient groups.", "AI": {"tldr": "Elevate\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u901a\u8fc7\u5b9a\u91cf\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u4eba\u4f53\u5de5\u7a0b\u5b66\u8bbe\u8ba1\uff0c\u5728\u8f85\u52a9\u80a9\u90e8\u62ac\u9ad8\u65f6\u538b\u529b\u5206\u5e03\u5408\u7406\uff0c\u538b\u7f29\u7387\u4f4e\uff0c\u65e0\u4e0d\u9002\u62a5\u544a", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u5177\u6709\u5eb7\u590d\u3001\u8f85\u52a9\u548c\u589e\u5f3a\u4eba\u4f53\u529f\u80fd\u7684\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u8f6f\u6750\u6599\u4eba\u673a\u63a5\u53e3\u7684\u6570\u636e\u9a71\u52a8\u3001\u7528\u6237\u5b9a\u5236\u548c\u8212\u9002\u6027\u4f18\u5148\u8bbe\u8ba1\u65b9\u9762\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u63a8\u5e7f\u548c\u5e94\u7528", "method": "\u4f7f\u7528\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u548c\u529b\u4f20\u611f\u5668\u5b9a\u91cf\u8bc4\u4f30Elevate\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u7684\u4eba\u4f53\u5de5\u7a0b\u5b66\u548c\u8212\u9002\u6027\uff0c\u8be5\u8bbe\u5907\u91c7\u7528\u7f06\u7ef3\u9a71\u52a8\u8f85\u52a9\u80a9\u90e8\u62ac\u9ad8\uff0c\u8fdb\u884c\u4e86\u4e24\u6b214\u5c0f\u65f6\u5b9e\u9a8c\uff0c\u6d4b\u91cf\u4e86\u9ad8\u8fbe200N\u7684\u7f06\u7ef3\u5f20\u529b", "result": "\u8f85\u52a9\u80a9\u90e8\u62ac\u9ad8\u81f370\u5ea6\u65f6\u65e0\u4e0d\u9002\u62a5\u544a\uff1b\u80a9\u90e8\u538b\u529b\u4f30\u8ba1\u572869.1-85.1kPa\u8303\u56f4\u5185\uff08\u76f8\u5f53\u4e8e\u4eba\u7c7b\u6293\u63e1\u538b\u529b\uff09\uff1b\u8eaf\u5e72\u548c\u4e0a\u81c2\u7684\u4f53\u79ef\u538b\u7f29\u5206\u522b\u5c0f\u4e8e3%\u548c8%", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aElevate\u7684\u4eba\u4f53\u5de5\u7a0b\u5b66\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65e9\u671f\u9a8c\u8bc1\uff0c\u4e3a\u672a\u6765\u60a3\u8005\u7fa4\u4f53\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8868\u660e\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u5728\u8212\u9002\u6027\u548c\u529f\u80fd\u6027\u65b9\u9762\u5177\u6709\u826f\u597d\u5e73\u8861"}}
{"id": "2601.17356", "categories": ["cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.17356", "abs": "https://arxiv.org/abs/2601.17356", "authors": ["Yao Zhao", "Zhang Sheng", "Shengchen Duan", "Shen Wang"], "title": "From Scores to Queues: Operationalizing Cross-Chain Obfuscation Signals for Smart-Contract Audits", "comment": null, "summary": "Obfuscation substantially increases the interpretation cost of smart-contract auditing, while the comparability and transferability of obfuscation signals across chains remain unclear. We present HObfNET as an efficient surrogate of Obfs_Tool (ObfProbe), enabling fast cross-chain scoring at scale. The model aligns well with tool outputs on Ethereum (PCC 0.9158, MAPE 8.20 percent) and achieves 8-9 ms per contract, a 2.3k-5.2k times speedup over second-level Obfs_Tool runs, enabling million-scale scoring. On large BSC, Polygon, and Avalanche corpora, we find systematic score drift: fixed-threshold transfer inflates and deflates candidate queues, motivating within-chain main and extreme thresholds (p99 and p99.9) and an actionable queueing strategy. The high-score tail exhibits rare selectors, external-call opcode enrichment, and low signature density; a proxy indicator is enriched in the BSC high-score queue, enabling secondary triage. Cross-chain reuse analysis shows tail enrichment and directional diffusion, with traceable same-hash cases across chains. In publicly alignable incident samples, all fall into the p99 queue; Transit Swap DEX Hack and New Free DAO Flash Loan exhibit cross-chain spillover, indicating real-world hit and prioritization value. We deliver a two-tier audit queue and cross-chain linkage workflow to support practical multi-chain security operations.", "AI": {"tldr": "HObfNET\u4f5c\u4e3aObfs_Tool\u7684\u9ad8\u6548\u66ff\u4ee3\uff0c\u5b9e\u73b0\u8de8\u94fe\u667a\u80fd\u5408\u7ea6\u6df7\u6dc6\u5ea6\u5feb\u901f\u8bc4\u5206\uff0c\u901f\u5ea6\u63d0\u53472300-5200\u500d\uff0c\u652f\u6301\u767e\u4e07\u7ea7\u89c4\u6a21\u5206\u6790\uff0c\u5e76\u53d1\u73b0\u8de8\u94fe\u8bc4\u5206\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u5b9e\u7528\u7684\u5ba1\u8ba1\u961f\u5217\u7b56\u7565\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u6df7\u6dc6\u589e\u52a0\u4e86\u5ba1\u8ba1\u6210\u672c\uff0c\u800c\u8de8\u94fe\u6df7\u6dc6\u4fe1\u53f7\u7684\u6bd4\u8f83\u6027\u548c\u53ef\u8f6c\u79fb\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u9ad8\u6548\u5de5\u5177\u652f\u6301\u5927\u89c4\u6a21\u8de8\u94fe\u5b89\u5168\u5206\u6790\u3002", "method": "\u63d0\u51faHObfNET\u4f5c\u4e3aObfs_Tool\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u5b9e\u73b0\u5feb\u901f\u8de8\u94fe\u8bc4\u5206\uff1b\u5206\u6790Ethereum\u3001BSC\u3001Polygon\u548cAvalanche\u4e0a\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff1b\u91c7\u7528\u4e3b\u9608\u503c\u548c\u6781\u7aef\u9608\u503c(p99\u548cp99.9)\u7b56\u7565\uff1b\u8fdb\u884c\u8de8\u94fe\u91cd\u7528\u5206\u6790\u548c\u516c\u5f00\u4e8b\u4ef6\u6837\u672c\u5bf9\u9f50\u3002", "result": "HObfNET\u4e0eObfs_Tool\u5728Ethereum\u4e0a\u5bf9\u9f50\u826f\u597d(PCC 0.9158, MAPE 8.20%)\uff1b\u901f\u5ea6\u8fbe8-9ms/\u5408\u7ea6\uff0c\u63d0\u53472300-5200\u500d\uff1b\u53d1\u73b0\u8de8\u94fe\u7cfb\u7edf\u8bc4\u5206\u6f02\u79fb\uff1b\u9ad8\u5206\u533a\u961f\u5217\u663e\u793a\u7a00\u6709\u9009\u62e9\u5668\u3001\u5916\u90e8\u8c03\u7528\u64cd\u4f5c\u7801\u5bcc\u96c6\u548c\u4f4e\u7b7e\u540d\u5bc6\u5ea6\uff1b\u516c\u5f00\u4e8b\u4ef6\u6837\u672c\u5747\u843d\u5165p99\u961f\u5217\uff1b\u63d0\u51fa\u53cc\u5c42\u5ba1\u8ba1\u961f\u5217\u548c\u8de8\u94fe\u5173\u8054\u5de5\u4f5c\u6d41\u3002", "conclusion": "HObfNET\u4e3a\u5927\u89c4\u6a21\u8de8\u94fe\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u9ad8\u6548\u5de5\u5177\uff0c\u8bc6\u522b\u8de8\u94fe\u8bc4\u5206\u6f02\u79fb\u95ee\u9898\u5e76\u63d0\u51fa\u5b9e\u7528\u9608\u503c\u7b56\u7565\uff0c\u652f\u6301\u5b9e\u9645\u591a\u94fe\u5b89\u5168\u64cd\u4f5c\uff0c\u5177\u6709\u73b0\u5b9e\u4e16\u754c\u68c0\u6d4b\u548c\u4f18\u5148\u7ea7\u6392\u5e8f\u4ef7\u503c\u3002"}}
{"id": "2601.18761", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2601.18761", "abs": "https://arxiv.org/abs/2601.18761", "authors": ["Wout Slabbinck", "Wouter Termont", "Ruben Dedecker", "Beatriz Esteves"], "title": "From Access Control to Usage Control with User-Managed Access", "comment": null, "summary": "Recent data protection and data governance regulations have intensified the demand for interoperable, decentralized data ecosystems that can support not only access control but also legally-aligned governance over data use. Existing Web-based data storage platforms increasingly struggle to meet these regulatory and practical requirements, as their authorization mechanisms rely on tightly coupled, document-centric access control models that lack expressiveness for legal constraints and fail to separate data management from authorization concerns. In parallel, widely adopted authorization standards remain poorly aligned with decentralized, semantically rich usage-control scenarios. To bridge this gap, this work introduces an architecture that replaces Solid's native access control mechanisms with a UMA authorization flow, enabling the enforcement of usage control policies expressed with the W3C ODRL standard. This article details the conceptual background motivating this approach, presents the proposed UMA-based architecture, and describes a prototype implementation that integrates an ODRL-enabled Authorization Server with a Solid-compatible Resource Server. The prototype demonstrates that decoupling authorization from storage enables more flexible, interoperable, and legally expressive control over data use, while remaining compatible with existing Solid infrastructure. It also highlights practical design choices required to evaluate ODRL policies in the absence of a fully standardized evaluation semantics. Moreover, this work shows how usage control can be operationalized using existing Web standards, offering a concrete path beyond permission-based access control toward policy-aware, legally informed data governance. Future research will focus on policy management interfaces, richer claim verification mechanisms, and techniques for communicating and enforcing obligations over time.", "AI": {"tldr": "\u7528UMA\u6388\u6743\u6d41\u66ff\u6362Solid\u539f\u751f\u8bbf\u95ee\u63a7\u5236\u673a\u5236\uff0c\u652f\u6301ODRL\u6807\u51c6\u8868\u8fbe\u4f7f\u7528\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u6570\u636e\u7ba1\u7406\u4e0e\u6388\u6743\u7684\u89e3\u8026\uff0c\u63d0\u5347\u6cd5\u5f8b\u5408\u89c4\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u6570\u636e\u4fdd\u62a4\u548c\u6cbb\u7406\u6cd5\u89c4\u8981\u6c42\u5efa\u7acb\u4e92\u64cd\u4f5c\u3001\u53bb\u4e2d\u5fc3\u5316\u7684\u6570\u636e\u751f\u6001\u7cfb\u7edf\uff0c\u73b0\u6709Web\u6570\u636e\u5b58\u50a8\u5e73\u53f0\u7684\u6388\u6743\u673a\u5236\u7f3a\u4e4f\u6cd5\u5f8b\u7ea6\u675f\u8868\u8fbe\u80fd\u529b\uff0c\u4e14\u6570\u636e\u7ba1\u7406\u4e0e\u6388\u6743\u7d27\u5bc6\u8026\u5408\uff0c\u96be\u4ee5\u6ee1\u8db3\u76d1\u7ba1\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u67b6\u6784\uff0c\u7528UMA\u6388\u6743\u6d41\u66ff\u4ee3Solid\u539f\u751f\u8bbf\u95ee\u63a7\u5236\uff0c\u96c6\u6210\u652f\u6301ODRL\u6807\u51c6\u7684\u6388\u6743\u670d\u52a1\u5668\u4e0eSolid\u517c\u5bb9\u7684\u8d44\u6e90\u670d\u52a1\u5668\uff0c\u5b9e\u73b0\u4f7f\u7528\u63a7\u5236\u7b56\u7565\u7684\u6267\u884c\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u8bc1\u660e\u89e3\u8026\u6388\u6743\u4e0e\u5b58\u50a8\u80fd\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u4e92\u64cd\u4f5c\u4e14\u6cd5\u5f8b\u8868\u8fbe\u6027\u5f3a\u7684\u6570\u636e\u4f7f\u7528\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709Solid\u57fa\u7840\u8bbe\u65bd\u517c\u5bb9\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u8bed\u4e49\u65f6\u8bc4\u4f30ODRL\u7b56\u7565\u7684\u5b9e\u9645\u8bbe\u8ba1\u9009\u62e9\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u73b0\u6709Web\u6807\u51c6\u5b9e\u73b0\u4f7f\u7528\u63a7\u5236\uff0c\u4e3a\u8d85\u8d8a\u57fa\u4e8e\u6743\u9650\u7684\u8bbf\u95ee\u63a7\u5236\u3001\u8fc8\u5411\u7b56\u7565\u611f\u77e5\u548c\u6cd5\u5f8b\u77e5\u60c5\u7684\u6570\u636e\u6cbb\u7406\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u5f84\u3002\u672a\u6765\u7814\u7a76\u5c06\u5173\u6ce8\u7b56\u7565\u7ba1\u7406\u754c\u9762\u3001\u66f4\u4e30\u5bcc\u7684\u58f0\u660e\u9a8c\u8bc1\u673a\u5236\u4ee5\u53ca\u968f\u65f6\u95f4\u6267\u884c\u4e49\u52a1\u7684\u6280\u672f\u3002"}}
{"id": "2601.17238", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17238", "abs": "https://arxiv.org/abs/2601.17238", "authors": ["Rishi Vanukuru", "Krithik Ranjan", "Ada Yi Zhao", "David Lindero", "Gunilla H. Berndtsson", "Gregoire Phillips", "Amy Bani\u0107", "Mark D. Gross", "Ellen Yi-Luen Do"], "title": "Studying Mobile Spatial Collaboration across Video Calls and Augmented Reality", "comment": "To appear in the Proceedings of the ACM on Human-Computer Interaction, Volume 10, Issue 2, Article CSCW037. 31 pages, 12 figures", "summary": "Mobile video calls are widely used to share information about real-world objects and environments with remote collaborators. While these calls provide valuable visual context in real time, the experience of interacting with people and moving around a space is significantly reduced when compared to co-located conversations. Recent work has demonstrated the potential of Mobile Augmented Reality applications to enable more spatial forms of collaboration across distance. To better understand the dynamics of mobile AR collaboration and how this medium compares against the status quo, we conducted a comparative structured observation study to analyze people's perception of space and interaction with remote collaborators across mobile video calls and AR-based calls. Fourteen pairs of participants completed a spatial collaboration task using each medium. Through a mixed-methods analysis of session videos, transcripts, motion logs, post-task exercises, and interviews, we highlight how the choice of medium influences the roles and responsibilities that collaborators take on and the construction of a shared language for coordination. We discuss the importance of spatial reasoning with one's body, how video calls help participants \"be on the same page\" more directly, and how AR calls enable both onsite and remote collaborators to engage with the space and each other in ways that resemble in-person interaction. Our study offers a nuanced view of the benefits and limitations of both mediums, and we conclude with a discussion of design implications for future systems that integrate mobile video and AR to better support spatial collaboration in its many forms.", "AI": {"tldr": "\u6bd4\u8f83\u79fb\u52a8\u89c6\u9891\u901a\u8bdd\u4e0eAR\u901a\u8bdd\u5728\u7a7a\u95f4\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e24\u79cd\u5a92\u4ecb\u5982\u4f55\u5f71\u54cd\u534f\u4f5c\u89d2\u8272\u3001\u5171\u4eab\u8bed\u8a00\u6784\u5efa\u548c\u7a7a\u95f4\u63a8\u7406\uff0c\u4e3a\u672a\u6765\u96c6\u6210\u79fb\u52a8\u89c6\u9891\u4e0eAR\u7684\u7cfb\u7edf\u63d0\u4f9b\u8bbe\u8ba1\u542f\u793a\u3002", "motivation": "\u79fb\u52a8\u89c6\u9891\u901a\u8bdd\u867d\u7136\u80fd\u63d0\u4f9b\u5b9e\u65f6\u89c6\u89c9\u4fe1\u606f\uff0c\u4f46\u4e0e\u5171\u5904\u4e00\u5730\u7684\u4ea4\u6d41\u76f8\u6bd4\uff0c\u4e92\u52a8\u4f53\u9a8c\u548c\u7a7a\u95f4\u611f\u77e5\u663e\u8457\u964d\u4f4e\u3002\u79fb\u52a8AR\u5e94\u7528\u5c55\u793a\u4e86\u8fdc\u7a0b\u7a7a\u95f4\u534f\u4f5c\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6df1\u5165\u7406\u89e3AR\u534f\u4f5c\u7684\u52a8\u6001\u7279\u6027\u53ca\u5176\u4e0e\u73b0\u6709\u89c6\u9891\u901a\u8bdd\u7684\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u6bd4\u8f83\u6027\u7ed3\u6784\u5316\u89c2\u5bdf\u7814\u7a76\uff0c14\u5bf9\u53c2\u4e0e\u8005\u5206\u522b\u4f7f\u7528\u79fb\u52a8\u89c6\u9891\u901a\u8bdd\u548cAR\u901a\u8bdd\u5b8c\u6210\u7a7a\u95f4\u534f\u4f5c\u4efb\u52a1\u3002\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u5206\u6790\u4f1a\u8bdd\u89c6\u9891\u3001\u8f6c\u5f55\u6587\u672c\u3001\u8fd0\u52a8\u65e5\u5fd7\u3001\u4efb\u52a1\u540e\u7ec3\u4e60\u548c\u8bbf\u8c08\u6570\u636e\u3002", "result": "\u5a92\u4ecb\u9009\u62e9\u5f71\u54cd\u534f\u4f5c\u89d2\u8272\u4e0e\u8d23\u4efb\u5206\u914d\uff0c\u4ee5\u53ca\u534f\u8c03\u5171\u4eab\u8bed\u8a00\u7684\u6784\u5efa\u3002\u89c6\u9891\u901a\u8bdd\u5e2e\u52a9\u53c2\u4e0e\u8005\u66f4\u76f4\u63a5\u5730\"\u8fbe\u6210\u5171\u8bc6\"\uff0c\u800cAR\u901a\u8bdd\u4f7f\u73b0\u573a\u548c\u8fdc\u7a0b\u534f\u4f5c\u8005\u80fd\u591f\u4ee5\u7c7b\u4f3c\u9762\u5bf9\u9762\u4e92\u52a8\u7684\u65b9\u5f0f\u4e0e\u7a7a\u95f4\u53ca\u5f7c\u6b64\u4e92\u52a8\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u8eab\u4f53\u7a7a\u95f4\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e24\u79cd\u5a92\u4ecb\u4f18\u52bf\u4e0e\u5c40\u9650\u7684\u7ec6\u81f4\u89c6\u89d2\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u96c6\u6210\u79fb\u52a8\u89c6\u9891\u4e0eAR\u7cfb\u7edf\u7684\u8bbe\u8ba1\u542f\u793a\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u5404\u79cd\u5f62\u5f0f\u7684\u7a7a\u95f4\u534f\u4f5c\u3002"}}
{"id": "2601.18199", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.18199", "abs": "https://arxiv.org/abs/2601.18199", "authors": ["Chenning Wu", "Sifan Chen", "Wentao Wu", "Yinan Jing", "Zhenying He", "Kai Zhang", "X. Sean Wang"], "title": "UTune: Towards Uncertainty-Aware Online Index Tuning", "comment": "14 pages, 18 figures. An Extended version including detailed performance analysis", "summary": "There have been a flurry of recent proposals on learned benefit estimators for index tuning. Although these learned estimators show promising improvement over what-if query optimizer calls in terms of the accuracy of estimated index benefit, they face significant limitations when applied to online index tuning, an arguably more common and more challenging scenario in real-world applications. There are two major challenges for learned index benefit estimators in online tuning: (1) limited amount of query execution feedback that can be used to train the models, and (2) constant coming of new unseen queries due to workload drifts. The combination of the two hinders the generalization capability of existing learned index benefit estimators. To overcome these challenges, we present UTune, an uncertainty-aware online index tuning framework that employs operator-level learned models with improved generalization over unseen queries. At the core of UTune is an uncertainty quantification mechanism that characterizes the inherent uncertainty of the operator-level learned models given limited online execution feedback. We further integrate uncertainty information into index selection and configuration enumeration, the key component of any index tuner, by developing a new variant of the classic $\u03b5$-greedy search strategy with uncertainty-weighted index benefits. Experimental evaluation shows that UTune not only significantly improves the workload execution time compared to state-of-the-art online index tuners but also reduces the index exploration overhead, resulting in faster convergence when the workload is relatively stable.", "AI": {"tldr": "UTune\uff1a\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5728\u7ebf\u7d22\u5f15\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u64cd\u4f5c\u7b26\u7ea7\u5b66\u4e60\u6a21\u578b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u673a\u5236\u89e3\u51b3\u5728\u7ebf\u7d22\u5f15\u8c03\u4f18\u4e2d\u6570\u636e\u6709\u9650\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6f02\u79fb\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u7d22\u5f15\u6548\u76ca\u4f30\u8ba1\u5668\u5728\u5728\u7ebf\u7d22\u5f15\u8c03\u4f18\u573a\u666f\u4e2d\u5b58\u5728\u4e24\u5927\u6311\u6218\uff1a1) \u53ef\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u67e5\u8be2\u6267\u884c\u53cd\u9988\u6570\u636e\u6709\u9650\uff1b2) \u7531\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u6f02\u79fb\u5bfc\u81f4\u65b0\u67e5\u8be2\u4e0d\u65ad\u51fa\u73b0\u3002\u8fd9\u4e24\u4e2a\u56e0\u7d20\u5171\u540c\u9650\u5236\u4e86\u73b0\u6709\u5b66\u4e60\u578b\u7d22\u5f15\u6548\u76ca\u4f30\u8ba1\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faUTune\u6846\u67b6\uff0c\u91c7\u7528\u64cd\u4f5c\u7b26\u7ea7\u5b66\u4e60\u6a21\u578b\u63d0\u9ad8\u5bf9\u672a\u89c1\u67e5\u8be2\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6838\u5fc3\u662f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u673a\u5236\uff0c\u7528\u4e8e\u8868\u5f81\u5728\u6709\u9650\u5728\u7ebf\u6267\u884c\u53cd\u9988\u4e0b\u64cd\u4f5c\u7b26\u7ea7\u5b66\u4e60\u6a21\u578b\u7684\u56fa\u6709\u4e0d\u786e\u5b9a\u6027\u3002\u8fdb\u4e00\u6b65\u5c06\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u96c6\u6210\u5230\u7d22\u5f15\u9009\u62e9\u548c\u914d\u7f6e\u679a\u4e3e\u4e2d\uff0c\u901a\u8fc7\u5f00\u53d1\u65b0\u7684\u03b5-greedy\u641c\u7d22\u7b56\u7565\u53d8\u4f53\uff0c\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u7684\u7d22\u5f15\u6548\u76ca\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cUTune\u4e0d\u4ec5\u663e\u8457\u6539\u5584\u4e86\u4e0e\u6700\u5148\u8fdb\u5728\u7ebf\u7d22\u5f15\u8c03\u4f18\u5668\u76f8\u6bd4\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6267\u884c\u65f6\u95f4\uff0c\u8fd8\u51cf\u5c11\u4e86\u7d22\u5f15\u63a2\u7d22\u5f00\u9500\uff0c\u5728\u5de5\u4f5c\u8d1f\u8f7d\u76f8\u5bf9\u7a33\u5b9a\u65f6\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u3002", "conclusion": "UTune\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5728\u7ebf\u7d22\u5f15\u8c03\u4f18\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u4e60\u578b\u7d22\u5f15\u6548\u76ca\u4f30\u8ba1\u5668\u5728\u5728\u7ebf\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7d22\u5f15\u8c03\u4f18\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17450", "abs": "https://arxiv.org/abs/2601.17450", "authors": ["Qingchao Shen"], "title": "Data-driven Test Generation for Fuzzing AI Compiler", "comment": "This paper has been accepted by ICSE 2026 Doctoral Symposium track", "summary": "Artificial Intelligence (AI) compilers are critical for efficiently deploying AI models across diverse hardware platforms. However, they remain prone to bugs that can compromise both compiler reliability and model correctness. Thus, ensuring the quality of AI compilers is crucial. In this work, we present a unified data-driven testing framework that systematically addresses stage-specific challenges in AI compilers. Specifically, OPERA migrates tests for AI libraries to test various operator conversion logic in the model loading stage. OATest synthesizes diverse optimization-aware computational graphs for testing high-level optimizations. HARMONY generates and mutates diverse low-level IR seeds to generate hardware-optimization-aware tests for testing low-level optimizations. Together, these techniques provide a comprehensive, stage-aware framework that enhances testing coverage and effectiveness, detecting 266 previously unknown bugs in four widely used AI compilers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u636e\u9a71\u52a8\u6d4b\u8bd5\u6846\u67b6\uff0c\u7cfb\u7edf\u89e3\u51b3AI\u7f16\u8bd1\u5668\u5404\u9636\u6bb5\u7279\u5b9a\u6311\u6218\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1aOPERA\u6d4b\u8bd5\u6a21\u578b\u52a0\u8f7d\u9636\u6bb5\uff0cOATest\u6d4b\u8bd5\u9ad8\u7ea7\u4f18\u5316\uff0cHARMONY\u6d4b\u8bd5\u4f4e\u7ea7\u4f18\u5316\uff0c\u5171\u53d1\u73b0266\u4e2a\u672a\u77e5bug\u3002", "motivation": "AI\u7f16\u8bd1\u5668\u5bf9\u4e8e\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u9ad8\u6548\u90e8\u7f72AI\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bb9\u6613\u5b58\u5728bug\uff0c\u5f71\u54cd\u7f16\u8bd1\u5668\u53ef\u9760\u6027\u548c\u6a21\u578b\u6b63\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u786e\u4fddAI\u7f16\u8bd1\u5668\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6570\u636e\u9a71\u52a8\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1) OPERA\uff1a\u8fc1\u79fbAI\u5e93\u6d4b\u8bd5\u6765\u6d4b\u8bd5\u6a21\u578b\u52a0\u8f7d\u9636\u6bb5\u7684\u7b97\u5b50\u8f6c\u6362\u903b\u8f91\uff1b2) OATest\uff1a\u5408\u6210\u591a\u6837\u5316\u4f18\u5316\u611f\u77e5\u8ba1\u7b97\u56fe\u6765\u6d4b\u8bd5\u9ad8\u7ea7\u4f18\u5316\uff1b3) HARMONY\uff1a\u751f\u6210\u548c\u53d8\u5f02\u591a\u6837\u5316\u4f4e\u7ea7IR\u79cd\u5b50\u6765\u751f\u6210\u786c\u4ef6\u4f18\u5316\u611f\u77e5\u6d4b\u8bd5\uff0c\u6d4b\u8bd5\u4f4e\u7ea7\u4f18\u5316\u3002", "result": "\u8be5\u6846\u67b6\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684AI\u7f16\u8bd1\u5668\u4e2d\u68c0\u6d4b\u5230266\u4e2a\u5148\u524d\u672a\u77e5\u7684bug\uff0c\u8bc1\u660e\u4e86\u5176\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u6709\u6548\u6027\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u9636\u6bb5\u611f\u77e5\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u80fd\u591f\u7cfb\u7edf\u89e3\u51b3AI\u7f16\u8bd1\u5668\u5404\u9636\u6bb5\u7684\u7279\u5b9a\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u6548\u679c\uff0c\u6709\u52a9\u4e8e\u786e\u4fddAI\u7f16\u8bd1\u5668\u8d28\u91cf\u3002"}}
{"id": "2601.17251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17251", "abs": "https://arxiv.org/abs/2601.17251", "authors": ["Yunuo Chen", "Yafei Hu", "Lingfeng Sun", "Tushar Kusnur", "Laura Herlant", "Chenfanfu Jiang"], "title": "EMPM: Embodied MPM for Modeling and Simulation of Deformable Objects", "comment": null, "summary": "Modeling deformable objects - especially continuum materials - in a way that is physically plausible, generalizable, and data-efficient remains challenging across 3D vision, graphics, and robotic manipulation. Many existing methods oversimplify the rich dynamics of deformable objects or require large training sets, which often limits generalization. We introduce embodied MPM (EMPM), a deformable object modeling and simulation framework built on a differentiable Material Point Method (MPM) simulator that captures the dynamics of challenging materials. From multi-view RGB-D videos, our approach reconstructs geometry and appearance, then uses an MPM physics engine to simulate object behavior by minimizing the mismatch between predicted and observed visual data. We further optimize MPM parameters online using sensory feedback, enabling adaptive, robust, and physics-aware object representations that open new possibilities for robotic manipulation of complex deformables. Experiments show that EMPM outperforms spring-mass baseline models. Project website: https://embodied-mpm.github.io.", "code_url": "https://embodied-mpm.github.io", "AI": {"tldr": "EMPM\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u5fae\u5206\u6750\u6599\u70b9\u6cd5\u7684\u53ef\u53d8\u5f62\u7269\u4f53\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u91cd\u5efa\u51e0\u4f55\u548c\u5916\u89c2\uff0c\u5229\u7528\u7269\u7406\u5f15\u64ce\u6a21\u62df\u7269\u4f53\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u4f18\u5316\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u9c81\u68d2\u7684\u7269\u7406\u611f\u77e5\u7269\u4f53\u8868\u793a", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8fc7\u5ea6\u7b80\u5316\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u590d\u6742\u52a8\u529b\u5b66\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u7269\u7406\u5408\u7406\u3001\u53ef\u6cdb\u5316\u4e14\u6570\u636e\u9ad8\u6548\u7684\u53ef\u53d8\u5f62\u7269\u4f53\u5efa\u6a21\u65b9\u6cd5", "method": "\u57fa\u4e8e\u53ef\u5fae\u5206\u6750\u6599\u70b9\u6cd5\uff08MPM\uff09\u6784\u5efa\u4eff\u771f\u6846\u67b6\uff0c\u4ece\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u91cd\u5efa\u51e0\u4f55\u548c\u5916\u89c2\uff0c\u4f7f\u7528MPM\u7269\u7406\u5f15\u64ce\u6a21\u62df\u7269\u4f53\u884c\u4e3a\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u6d4b\u4e0e\u89c2\u6d4b\u89c6\u89c9\u6570\u636e\u7684\u4e0d\u5339\u914d\u6765\u4f18\u5316\uff0c\u5e76\u5728\u7ebf\u4f18\u5316MPM\u53c2\u6570", "result": "EMPM\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u5f39\u7c27-\u8d28\u91cf\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u9c81\u68d2\u548c\u7269\u7406\u611f\u77e5\u7684\u7269\u4f53\u8868\u793a\uff0c\u4e3a\u590d\u6742\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027", "conclusion": "EMPM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7269\u7406\u5408\u7406\u3001\u53ef\u6cdb\u5316\u4e14\u6570\u636e\u9ad8\u6548\u7684\u53ef\u53d8\u5f62\u7269\u4f53\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u91cd\u5efa\u548c\u7269\u7406\u4eff\u771f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6311\u6218\u6027\u6750\u6599\u52a8\u529b\u5b66\u7684\u51c6\u786e\u6355\u6349"}}
{"id": "2601.17378", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17378", "abs": "https://arxiv.org/abs/2601.17378", "authors": ["Mohammad Zare", "Pirooz Shamsinejadbabaki"], "title": "Res-MIA: A Training-Free Resolution-Based Membership Inference Attack on Federated Learning Models", "comment": null, "summary": "Membership inference attacks (MIAs) pose a serious threat to the privacy of machine learning models by allowing adversaries to determine whether a specific data sample was included in the training set. Although federated learning (FL) is widely regarded as a privacy-aware training paradigm due to its decentralized nature, recent evidence shows that the final global model can still leak sensitive membership information through black-box access. In this paper, we introduce Res-MIA, a novel training-free and black-box membership inference attack that exploits the sensitivity of deep models to high-frequency input details. Res-MIA progressively degrades the input resolution using controlled downsampling and restoration operations, and analyzes the resulting confidence decay in the model's predictions. Our key insight is that training samples exhibit a significantly steeper confidence decline under resolution erosion compared to non-member samples, revealing a robust membership signal. Res-MIA requires no shadow models, no auxiliary data, and only a limited number of forward queries to the target model. We evaluate the proposed attack on a federated ResNet-18 trained on CIFAR-10, where it consistently outperforms existing training-free baselines and achieves an AUC of up to 0.88 with minimal computational overhead. These findings highlight frequency-sensitive overfitting as an important and previously underexplored source of privacy leakage in federated learning, and emphasize the need for privacy-aware model designs that reduce reliance on fine-grained, non-robust input features.", "AI": {"tldr": "Res-MIA\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u901a\u8fc7\u6e10\u8fdb\u964d\u4f4e\u8f93\u5165\u5206\u8fa8\u7387\u5e76\u5206\u6790\u7f6e\u4fe1\u5ea6\u8870\u51cf\u6765\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u6210\u5458\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u9ad8\u8fbe0.88\u7684AUC", "motivation": "\u5c3d\u7ba1\u8054\u90a6\u5b66\u4e60\u56e0\u5176\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u88ab\u89c6\u4e3a\u9690\u79c1\u4fdd\u62a4\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f46\u7814\u7a76\u8868\u660e\u6700\u7ec8\u5168\u5c40\u6a21\u578b\u4ecd\u53ef\u80fd\u901a\u8fc7\u9ed1\u76d2\u8bbf\u95ee\u6cc4\u9732\u6210\u5458\u4fe1\u606f\u3002\u73b0\u6709\u6210\u5458\u63a8\u7406\u653b\u51fb\u901a\u5e38\u9700\u8981\u8bad\u7ec3\u5f71\u5b50\u6a21\u578b\u6216\u8f85\u52a9\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u4ec5\u9700\u6709\u9650\u524d\u5411\u67e5\u8be2\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u63a2\u7d22\u6a21\u578b\u5bf9\u9ad8\u9891\u8f93\u5165\u7ec6\u8282\u7684\u654f\u611f\u6027\u4f5c\u4e3a\u65b0\u7684\u9690\u79c1\u6cc4\u9732\u6e90\u3002", "method": "Res-MIA\u901a\u8fc7\u6e10\u8fdb\u964d\u91c7\u6837\u548c\u6062\u590d\u64cd\u4f5c\u9010\u6b65\u964d\u4f4e\u8f93\u5165\u5206\u8fa8\u7387\uff0c\u5206\u6790\u6a21\u578b\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u8870\u51cf\u6a21\u5f0f\u3002\u6838\u5fc3\u6d1e\u5bdf\u662f\uff1a\u8bad\u7ec3\u6837\u672c\u5728\u5206\u8fa8\u7387\u4fb5\u8680\u4e0b\u8868\u73b0\u51fa\u6bd4\u975e\u6210\u5458\u6837\u672c\u66f4\u9661\u5ced\u7684\u7f6e\u4fe1\u5ea6\u4e0b\u964d\uff0c\u8fd9\u63ed\u793a\u4e86\u7a33\u5065\u7684\u6210\u5458\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5f71\u5b50\u6a21\u578b\u3001\u65e0\u9700\u8f85\u52a9\u6570\u636e\uff0c\u4ec5\u9700\u5bf9\u76ee\u6807\u6a21\u578b\u8fdb\u884c\u6709\u9650\u6b21\u524d\u5411\u67e5\u8be2\u3002", "result": "\u5728\u8054\u90a6ResNet-18\u6a21\u578b\uff08CIFAR-10\u6570\u636e\u96c6\uff09\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cRes-MIA\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u8fbe0.88\u7684AUC\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002\u8fd9\u8bc1\u660e\u4e86\u9891\u7387\u654f\u611f\u8fc7\u62df\u5408\u662f\u8054\u90a6\u5b66\u4e60\u4e2d\u4e00\u4e2a\u91cd\u8981\u4e14\u5148\u524d\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9690\u79c1\u6cc4\u9732\u6e90\u3002", "conclusion": "Res-MIA\u63ed\u793a\u4e86\u6df1\u5ea6\u6a21\u578b\u5bf9\u9ad8\u9891\u8f93\u5165\u7ec6\u8282\u7684\u654f\u611f\u6027\u53ef\u88ab\u7528\u4e8e\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u5373\u4f7f\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u3002\u8fd9\u4e00\u53d1\u73b0\u5f3a\u8c03\u4e86\u9700\u8981\u8bbe\u8ba1\u51cf\u5c11\u5bf9\u7ec6\u7c92\u5ea6\u3001\u975e\u7a33\u5065\u8f93\u5165\u7279\u5f81\u4f9d\u8d56\u7684\u9690\u79c1\u611f\u77e5\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u7f13\u89e3\u6b64\u7c7b\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002"}}
{"id": "2601.18256", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18256", "abs": "https://arxiv.org/abs/2601.18256", "authors": ["Akihito Taya", "Yuuki Nishiyama", "Kaoru Sezaki"], "title": "A Mechanical Wi-Fi Antenna Device for Automatic Orientation Tuning with Bayesian Optimization", "comment": "(c) 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Wi-Fi access points have been widely deployed in homes, offices, and public spaces. Some APs allow users to adjust the antenna orientation to improve communication performance by optimizing antenna polarization. However, it is difficult for non-expert users to determine the optimal orientation, and users often leave the antenna orientation in ineffective positions. To address this issue, we developed a mechanical Wi-Fi antenna device capable of automatically tuning its orientation. Experimental results show that antenna orientation could cause a throughput variation of approximately 70 Mbps under line-of-sight conditions. Furthermore, Bayesian optimization identified better configurations than random search, demonstrating its effectiveness for orientation tuning.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u8c03\u6574\u65b9\u5411\u7684\u673a\u68b0Wi-Fi\u5929\u7ebf\u8bbe\u5907\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u7b97\u6cd5\u5bfb\u627e\u6700\u4f73\u5929\u7ebf\u65b9\u5411\u4ee5\u63d0\u5347\u901a\u4fe1\u6027\u80fd", "motivation": "Wi-Fi\u63a5\u5165\u70b9\u5e7f\u6cdb\u90e8\u7f72\u4f46\u5929\u7ebf\u65b9\u5411\u8c03\u6574\u56f0\u96be\uff0c\u975e\u4e13\u4e1a\u7528\u6237\u96be\u4ee5\u786e\u5b9a\u6700\u4f73\u65b9\u5411\uff0c\u5bfc\u81f4\u5929\u7ebf\u5e38\u5904\u4e8e\u65e0\u6548\u4f4d\u7f6e\uff0c\u5f71\u54cd\u901a\u4fe1\u6027\u80fd", "method": "\u5f00\u53d1\u673a\u68b0Wi-Fi\u5929\u7ebf\u8bbe\u5907\u5b9e\u73b0\u81ea\u52a8\u65b9\u5411\u8c03\u6574\uff0c\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u7b97\u6cd5\u5bfb\u627e\u6700\u4f73\u5929\u7ebf\u65b9\u5411\u914d\u7f6e", "result": "\u5b9e\u9a8c\u663e\u793a\u5929\u7ebf\u65b9\u5411\u53ef\u5bfc\u81f4\u7ea670 Mbps\u7684\u541e\u5410\u91cf\u53d8\u5316\uff08\u89c6\u8ddd\u6761\u4ef6\u4e0b\uff09\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u6bd4\u968f\u673a\u641c\u7d22\u80fd\u627e\u5230\u66f4\u597d\u7684\u914d\u7f6e", "conclusion": "\u81ea\u52a8\u5929\u7ebf\u65b9\u5411\u8c03\u6574\u7cfb\u7edf\u80fd\u6709\u6548\u4f18\u5316Wi-Fi\u6027\u80fd\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u662f\u5929\u7ebf\u65b9\u5411\u8c03\u8c10\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2601.18446", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2601.18446", "abs": "https://arxiv.org/abs/2601.18446", "authors": ["Xinmeng Yu", "Tao Jiang", "Ran Cheng", "Yaochu Jin", "Kay Chen Tan"], "title": "Scaling Behaviors of Evolutionary Algorithms on GPUs: When Does Parallelism Pay Off?", "comment": null, "summary": "Evolutionary algorithms (EAs) are increasingly implemented on graphics processing units (GPUs) to leverage parallel processing capabilities for enhanced efficiency. However, existing studies largely emphasize the raw speedup obtained by porting individual algorithms from CPUs to GPUs. Consequently, these studies offer limited insight into when and why GPU parallelism fundamentally benefits EAs. To address this gap, we investigate how GPU parallelism alters the behavior of EAs beyond simple acceleration metrics. We conduct a systematic empirical study of 16 representative EAs on 30 benchmark problems. Specifically, we compare CPU and GPU executions across a wide range of problem dimensionalities and population sizes. Our results reveal that the impact of GPU acceleration is highly heterogeneous and depends strongly on algorithmic structure. We further demonstrate that conventional fixed-budget evaluation based on the number of function evaluations (FEs) is inadequate for GPU execution. In contrast, fixed-time evaluation uncovers performance characteristics that are unobservable under small or practically constrained FE budgets, particularly for adaptive and exploration-oriented algorithms. Moreover, we identify distinct scaling regimes in which GPU parallelism is beneficial, saturates, or degrades as problem dimensionality and population size increase. Crucially, we show that large populations enabled by GPUs not only improve hardware utilization but also reveal algorithm-specific convergence and diversity dynamics that are difficult to observe under CPU-constrained settings. Consequently, our findings indicate that GPU parallelism is not strictly an implementation detail, but a pivotal factor that influences how EAs should be evaluated, compared, and designed for modern computing platforms.", "AI": {"tldr": "GPU\u5e76\u884c\u6027\u5bf9\u8fdb\u5316\u7b97\u6cd5\u7684\u5f71\u54cd\u8d85\u8d8a\u7b80\u5355\u52a0\u901f\uff0c\u9700\u91cd\u65b0\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\u8bc4\u4ef7\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5c06\u8fdb\u5316\u7b97\u6cd5\u4eceCPU\u79fb\u690d\u5230GPU\u83b7\u5f97\u7684\u539f\u59cb\u52a0\u901f\u6bd4\uff0c\u7f3a\u4e4f\u5bf9GPU\u5e76\u884c\u6027\u5982\u4f55\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u8fdb\u5316\u7b97\u6cd5\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3\uff0c\u9700\u8981\u63a2\u7a76GPU\u5e76\u884c\u6027\u4f55\u65f6\u3001\u4e3a\u4f55\u4ee5\u53ca\u5982\u4f55\u5f71\u54cd\u8fdb\u5316\u7b97\u6cd5\u7684\u6027\u80fd\u7279\u5f81", "method": "\u5bf916\u4e2a\u4ee3\u8868\u6027\u8fdb\u5316\u7b97\u6cd5\u572830\u4e2a\u57fa\u51c6\u95ee\u9898\u4e0a\u8fdb\u884c\u7cfb\u7edf\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83CPU\u548cGPU\u6267\u884c\u5728\u4e0d\u540c\u95ee\u9898\u7ef4\u5ea6\u548c\u79cd\u7fa4\u5927\u5c0f\u4e0b\u7684\u8868\u73b0\uff0c\u91c7\u7528\u56fa\u5b9a\u65f6\u95f4\u8bc4\u4f30\u800c\u975e\u4f20\u7edf\u7684\u56fa\u5b9a\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u8bc4\u4f30\u65b9\u6cd5", "result": "GPU\u52a0\u901f\u7684\u5f71\u54cd\u9ad8\u5ea6\u5f02\u8d28\u4e14\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u7b97\u6cd5\u7ed3\u6784\uff1b\u4f20\u7edf\u57fa\u4e8e\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u7684\u56fa\u5b9a\u9884\u7b97\u8bc4\u4f30\u4e0d\u9002\u7528\u4e8eGPU\u6267\u884c\uff1b\u56fa\u5b9a\u65f6\u95f4\u8bc4\u4f30\u80fd\u63ed\u793a\u5728\u6709\u9650\u51fd\u6570\u8bc4\u4f30\u9884\u7b97\u4e0b\u65e0\u6cd5\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u7279\u5f81\uff1b\u8bc6\u522b\u51faGPU\u5e76\u884c\u6027\u6709\u76ca\u3001\u9971\u548c\u6216\u9000\u5316\u7684\u4e0d\u540c\u7f29\u653e\u673a\u5236\uff1bGPU\u652f\u6301\u7684\u5927\u79cd\u7fa4\u4e0d\u4ec5\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\uff0c\u8fd8\u63ed\u793a\u7b97\u6cd5\u7279\u5b9a\u7684\u6536\u655b\u548c\u591a\u6837\u6027\u52a8\u6001", "conclusion": "GPU\u5e76\u884c\u6027\u4e0d\u4ec5\u4ec5\u662f\u5b9e\u73b0\u7ec6\u8282\uff0c\u800c\u662f\u5f71\u54cd\u8fdb\u5316\u7b97\u6cd5\u5728\u73b0\u4ee3\u8ba1\u7b97\u5e73\u53f0\u4e0a\u5982\u4f55\u8bc4\u4f30\u3001\u6bd4\u8f83\u548c\u8bbe\u8ba1\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u8fdb\u5316\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u8bbe\u8ba1\u539f\u5219\u4ee5\u9002\u5e94GPU\u5e76\u884c\u8ba1\u7b97\u73af\u5883"}}
{"id": "2601.17482", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17482", "abs": "https://arxiv.org/abs/2601.17482", "authors": ["Yang Liu", "Kaiming Zhang", "Zhuangbin Chen", "Jinyang Liu", "Zibin Zheng"], "title": "LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression", "comment": null, "summary": "The prevailing \"parse-then-compress\" paradigm in log compression fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines \"structure+variable\" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 13 datasets, surpassing leading baselines by margins of 4.7% to 80.9%, while delivering superior throughput at 29.87 MB/s (1.68$\\times$~43.04$\\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism outperforms the best baseline by 19.39% in compression ratio while maintaining a 2.62$\\times$ speed advantage.", "AI": {"tldr": "LogPrism\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65e5\u5fd7\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u5197\u4f59\u6811\uff08URT\uff09\u52a8\u6001\u6574\u5408\u7ed3\u6784\u63d0\u53d6\u4e0e\u53d8\u91cf\u7f16\u7801\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\"\u5148\u89e3\u6790\u540e\u538b\u7f29\"\u8303\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u538b\u7f29\u6bd4\u548c\u66f4\u5feb\u7684\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\"\u5148\u89e3\u6790\u540e\u538b\u7f29\"\u7684\u65e5\u5fd7\u538b\u7f29\u8303\u5f0f\u5c06\u65e5\u5fd7\u89e3\u6790\u548c\u538b\u7f29\u89c6\u4e3a\u5b64\u7acb\u76ee\u6807\uff0c\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002\u89e3\u6790\u5668\u4f18\u5148\u8003\u8651\u8bed\u4e49\u51c6\u786e\u6027\uff08\u4e8b\u4ef6\u8bc6\u522b\uff09\uff0c\u4f46\u5f80\u5f80\u63a9\u76d6\u4e86\u9759\u6001\u6a21\u677f\u548c\u52a8\u6001\u53d8\u91cf\u4e4b\u95f4\u7684\u6df1\u5ea6\u76f8\u5173\u6027\uff0c\u800c\u8fd9\u4e9b\u76f8\u5173\u6027\u5bf9\u5b58\u50a8\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faLogPrism\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u5197\u4f59\u6811\uff08URT\uff09\u5b9e\u73b0\u7edf\u4e00\u5197\u4f59\u7f16\u7801\u3002\u8be5\u65b9\u6cd5\u52a8\u6001\u6574\u5408\u7ed3\u6784\u63d0\u53d6\u4e0e\u53d8\u91cf\u7f16\u7801\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u50f5\u5316\u7684\u9884\u89e3\u6790\u6b65\u9aa4\u3002\u8fd9\u79cd\u5206\u5c42\u65b9\u6cd5\u6709\u6548\u6316\u6398\"\u7ed3\u6784+\u53d8\u91cf\"\u5171\u73b0\u6a21\u5f0f\uff0c\u6355\u83b7\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5197\u4f59\uff0c\u540c\u65f6\u901a\u8fc7\u9884\u6a21\u5f0f\u7f16\u7801\u52a0\u901f\u5904\u7406\u3002", "result": "\u572816\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9eLogPrism\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff1a\u572813\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u9ad8\u538b\u7f29\u6bd4\uff0c\u8d85\u8d8a\u9886\u5148\u57fa\u7ebf4.7%\u523080.9%\uff1b\u63d0\u4f9b29.87 MB/s\u7684\u5353\u8d8a\u541e\u5410\u91cf\uff08\u6bd4\u7ade\u4e89\u5bf9\u624b\u5feb1.68\u00d7~43.04\u00d7\uff09\uff1b\u5728\u5355\u5f52\u6863\u6a21\u5f0f\u4e0b\uff0c\u538b\u7f29\u6bd4\u8d85\u8d8a\u6700\u4f73\u57fa\u7ebf19.39%\uff0c\u540c\u65f6\u4fdd\u63012.62\u00d7\u7684\u901f\u5ea6\u4f18\u52bf\u3002", "conclusion": "LogPrism\u901a\u8fc7\u7edf\u4e00\u5197\u4f59\u7f16\u7801\u65b9\u6cd5\u6210\u529f\u5f25\u5408\u4e86\u65e5\u5fd7\u89e3\u6790\u548c\u538b\u7f29\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u6574\u5408\u7ed3\u6784\u63d0\u53d6\u4e0e\u53d8\u91cf\u7f16\u7801\u7684\u6709\u6548\u6027\uff0c\u4e3a\u65e5\u5fd7\u538b\u7f29\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17287", "abs": "https://arxiv.org/abs/2601.17287", "authors": ["Yanrong Chen", "Xihan Bian"], "title": "Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots", "comment": "6 pages, 6 figures", "summary": "As humanoid robots increasingly introduced into social scene, achieving emotionally synchronized multimodal interaction remains a significant challenges. To facilitate the further adoption and integration of humanoid robots into service roles, we present a real-time framework for NAO robots that synchronizes speech prosody with full-body gestures through three key innovations: (1) A dual-channel emotion engine where large language model (LLM) simultaneously generates context-aware text responses and biomechanically feasible motion descriptors, constrained by a structured joint movement library; (2) Duration-aware dynamic time warping for precise temporal alignment of speech output and kinematic motion keyframes; (3) Closed-loop feasibility verification ensuring gestures adhere to NAO's physical joint limits through real-time adaptation. Evaluations show 21% higher emotional alignment compared to rule-based systems, achieved by coordinating vocal pitch (arousal-driven) with upper-limb kinematics while maintaining lower-body stability. By enabling seamless sensorimotor coordination, this framework advances the deployment of context-aware social robots in dynamic applications such as personalized healthcare, interactive education, and responsive customer service platforms.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411NAO\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u6846\u67b6\uff0c\u901a\u8fc7LLM\u751f\u6210\u60c5\u611f\u540c\u6b65\u7684\u8bed\u97f3\u548c\u5168\u8eab\u624b\u52bf\uff0c\u5b9e\u73b0\u8bed\u97f3\u97f5\u5f8b\u4e0e\u52a8\u4f5c\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u63d0\u5347\u60c5\u611f\u4ea4\u4e92\u6548\u679c", "motivation": "\u968f\u7740\u4eba\u5f62\u673a\u5668\u4eba\u8d8a\u6765\u8d8a\u591a\u5730\u8fdb\u5165\u793e\u4ea4\u573a\u666f\uff0c\u5b9e\u73b0\u60c5\u611f\u540c\u6b65\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u4fc3\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u5728\u670d\u52a1\u89d2\u8272\u4e2d\u7684\u8fdb\u4e00\u6b65\u91c7\u7528\u548c\u96c6\u6210\uff0c\u9700\u8981\u89e3\u51b3\u8bed\u97f3\u548c\u52a8\u4f5c\u7684\u60c5\u611f\u534f\u8c03\u95ee\u9898", "method": "\u63d0\u51fa\u4e09\u65b9\u9762\u521b\u65b0\uff1a(1) \u53cc\u901a\u9053\u60c5\u611f\u5f15\u64ce\uff0cLLM\u540c\u65f6\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6587\u672c\u54cd\u5e94\u548c\u751f\u7269\u529b\u5b66\u53ef\u884c\u7684\u8fd0\u52a8\u63cf\u8ff0\u7b26\uff1b(2) \u6301\u7eed\u65f6\u95f4\u611f\u77e5\u7684\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff0c\u7cbe\u786e\u5bf9\u9f50\u8bed\u97f3\u8f93\u51fa\u548c\u8fd0\u52a8\u5173\u952e\u5e27\uff1b(3) \u95ed\u73af\u53ef\u884c\u6027\u9a8c\u8bc1\uff0c\u901a\u8fc7\u5b9e\u65f6\u9002\u5e94\u786e\u4fdd\u624b\u52bf\u7b26\u5408NAO\u7684\u7269\u7406\u5173\u8282\u9650\u5236", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff0c\u60c5\u611f\u5bf9\u9f50\u5ea6\u63d0\u9ad8\u4e8621%\u3002\u901a\u8fc7\u534f\u8c03\u58f0\u8c03\uff08\u57fa\u4e8e\u5524\u9192\u5ea6\uff09\u4e0e\u4e0a\u80a2\u8fd0\u52a8\u5b66\uff0c\u540c\u65f6\u4fdd\u6301\u4e0b\u534a\u8eab\u7a33\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u60c5\u611f\u8868\u8fbe", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5b9e\u73b0\u65e0\u7f1d\u7684\u611f\u89c9\u8fd0\u52a8\u534f\u8c03\uff0c\u63a8\u8fdb\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u793e\u4ea4\u673a\u5668\u4eba\u5728\u52a8\u6001\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\uff0c\u5982\u4e2a\u6027\u5316\u533b\u7597\u3001\u4e92\u52a8\u6559\u80b2\u548c\u54cd\u5e94\u5f0f\u5ba2\u6237\u670d\u52a1\u5e73\u53f0"}}
{"id": "2601.17379", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17379", "abs": "https://arxiv.org/abs/2601.17379", "authors": ["Khoi Trinh", "Scott Seidenberger", "Joseph Spracklen", "Raveen Wijewickrama", "Bimal Viswanath", "Murtuza Jadliwala", "Anindya Maiti"], "title": "Prompt and Circumstances: Evaluating the Efficacy of Human Prompt Inference in AI-Generated Art", "comment": "To appear in EvoMUSART 2026", "summary": "The emerging field of AI-generated art has witnessed the rise of prompt marketplaces, where creators can purchase, sell, or share prompts to generate unique artworks. These marketplaces often assert ownership over prompts, claiming them as intellectual property. This paper investigates whether concealed prompts sold on prompt marketplaces can be considered bona fide intellectual property, given that humans and AI tools may be able to infer the prompts based on publicly advertised sample images accompanying each prompt on sale. Specifically, our study aims to assess (i) how accurately humans can infer the original prompt solely by examining an AI-generated image, with the goal of generating images similar to the original image, and (ii) the possibility of improving upon individual human and AI prompt inferences by crafting combined human and AI prompts with the help of a large language model. Although previous research has explored AI-driven prompt inference and protection strategies, our work is the first to incorporate a human subject study and examine collaborative human-AI prompt inference in depth. Our findings indicate that while prompts inferred by humans and prompts inferred through a combined human and AI effort can generate images with a moderate level of similarity, they are not as successful as using the original prompt. Moreover, combining human- and AI-inferred prompts using our suggested merging techniques did not improve performance over purely human-inferred prompts.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5AI\u827a\u672f\u63d0\u793a\u5e02\u573a\u4e2d\u7684\u9690\u85cf\u63d0\u793a\u662f\u5426\u6784\u6210\u771f\u6b63\u7684\u77e5\u8bc6\u4ea7\u6743\uff0c\u901a\u8fc7\u4eba\u7c7b\u548cAI\u534f\u4f5c\u63a8\u65ad\u63d0\u793a\u7684\u53ef\u884c\u6027\u5206\u6790", "motivation": "AI\u751f\u6210\u827a\u672f\u9886\u57df\u51fa\u73b0\u4e86\u63d0\u793a\u5e02\u573a\uff0c\u5e02\u573a\u58f0\u79f0\u5bf9\u63d0\u793a\u62e5\u6709\u77e5\u8bc6\u4ea7\u6743\u3002\u4f46\u4eba\u7c7b\u548cAI\u5de5\u5177\u53ef\u80fd\u901a\u8fc7\u516c\u5f00\u7684\u6837\u672c\u56fe\u50cf\u63a8\u65ad\u51fa\u539f\u59cb\u63d0\u793a\uff0c\u8fd9\u5f15\u53d1\u4e86\u9690\u85cf\u63d0\u793a\u662f\u5426\u771f\u6b63\u6784\u6210\u77e5\u8bc6\u4ea7\u6743\u7684\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5305\u542b\u4e24\u4e2a\u90e8\u5206\uff1a(1) \u8bc4\u4f30\u4eba\u7c7b\u4ec5\u901a\u8fc7\u89c2\u5bdfAI\u751f\u6210\u56fe\u50cf\u63a8\u65ad\u539f\u59cb\u63d0\u793a\u7684\u51c6\u786e\u6027\uff1b(2) \u63a2\u7d22\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4eba\u7c7b\u548cAI\u63a8\u65ad\u7684\u63d0\u793a\u6765\u6539\u8fdb\u63d0\u793a\u8d28\u91cf\u7684\u53ef\u80fd\u6027\u3002\u7814\u7a76\u9996\u6b21\u7ed3\u5408\u4e86\u4eba\u7c7b\u4e3b\u4f53\u5b9e\u9a8c\u548c\u6df1\u5165\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u793a\u63a8\u65ad\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u4eba\u7c7b\u63a8\u65ad\u7684\u63d0\u793a\u4ee5\u53ca\u4eba\u673a\u534f\u4f5c\u63a8\u65ad\u7684\u63d0\u793a\u80fd\u591f\u751f\u6210\u5177\u6709\u4e2d\u7b49\u76f8\u4f3c\u5ea6\u7684\u56fe\u50cf\uff0c\u4f46\u6548\u679c\u4e0d\u5982\u4f7f\u7528\u539f\u59cb\u63d0\u793a\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u5efa\u8bae\u7684\u5408\u5e76\u6280\u672f\u7ed3\u5408\u4eba\u7c7b\u548cAI\u63a8\u65ad\u7684\u63d0\u793a\uff0c\u5176\u6027\u80fd\u5e76\u672a\u8d85\u8fc7\u7eaf\u7cb9\u4eba\u7c7b\u63a8\u65ad\u7684\u63d0\u793a\u3002", "conclusion": "\u9690\u85cf\u63d0\u793a\u4f5c\u4e3a\u77e5\u8bc6\u4ea7\u6743\u7684\u6709\u6548\u6027\u5b58\u5728\u7591\u95ee\uff0c\u56e0\u4e3a\u4eba\u7c7b\u548cAI\u80fd\u591f\u901a\u8fc7\u6837\u672c\u56fe\u50cf\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63a8\u65ad\u51fa\u63d0\u793a\u5185\u5bb9\u3002\u4eba\u673a\u534f\u4f5c\u63d0\u793a\u63a8\u65ad\u672a\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8fd9\u6311\u6218\u4e86\u63d0\u793a\u5e02\u573a\u5bf9\u63d0\u793a\u6240\u6709\u6743\u7684\u58f0\u79f0\u3002"}}
{"id": "2601.17351", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17351", "abs": "https://arxiv.org/abs/2601.17351", "authors": ["Nadja Rupprechter", "Tobias Dienlin", "Tilo Hartmann"], "title": "AI-RP: The AI Relationship Process Framework", "comment": null, "summary": "For a growing number of people, AI chatbots have become close personal companions. Despite rising scholarly attention, theoretical accounts of how such relationships develop remain fragmented. Existing frameworks address important aspects of the phenomenon, but they rarely treat human-chatbot communication as the central behavior that builds relationships. To address this gap, we propose the AI relationship process (AI-RP) framework. The AI-RP outlines relationship formation as a sequential process. (a) Chatbot characteristics shape users' (b) social perceptions. These perceptions guide (c) communication, and communication produces (d) relational outcomes such as attachment and companionship. The AI-RP introduces a six-features profile characterizing chatbots, a dual-route approach of social perception, a behavioral conceptualization of communication and discusses the foundation and types of artificial relationships. By foregrounding observable communicative behavior, the AI-RP provides a foundation for theory building and empirical research on the social and ethical implications of AI companionship.", "AI": {"tldr": "\u63d0\u51faAI\u5173\u7cfb\u8fc7\u7a0b(AI-RP)\u6846\u67b6\uff0c\u5c06\u4eba\u673a\u5173\u7cfb\u5f62\u6210\u89c6\u4e3a\u4e00\u4e2a\u5e8f\u5217\u8fc7\u7a0b\uff1a\u804a\u5929\u673a\u5668\u4eba\u7279\u5f81\u2192\u793e\u4f1a\u611f\u77e5\u2192\u6c9f\u901a\u884c\u4e3a\u2192\u5173\u7cfb\u7ed3\u679c", "motivation": "\u5f53\u524d\u5bf9AI\u804a\u5929\u673a\u5668\u4eba\u4f5c\u4e3a\u4eb2\u5bc6\u4f34\u4fa3\u7684\u7814\u7a76\u7406\u8bba\u6846\u67b6\u5206\u6563\uff0c\u7f3a\u4e4f\u4ee5\u4eba\u673a\u6c9f\u901a\u4f5c\u4e3a\u5173\u7cfb\u6784\u5efa\u6838\u5fc3\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u7406\u8bba\u3002\u73b0\u6709\u6846\u67b6\u867d\u6d89\u53ca\u91cd\u8981\u65b9\u9762\uff0c\u4f46\u5f88\u5c11\u5c06\u4eba\u673a\u6c9f\u901a\u89c6\u4e3a\u6784\u5efa\u5173\u7cfb\u7684\u5173\u952e\u884c\u4e3a\u3002", "method": "\u63d0\u51faAI\u5173\u7cfb\u8fc7\u7a0b(AI-RP)\u6846\u67b6\uff0c\u5c06\u5173\u7cfb\u5f62\u6210\u5efa\u6a21\u4e3a\u5e8f\u5217\u8fc7\u7a0b\uff1a1)\u804a\u5929\u673a\u5668\u4eba\u7279\u5f81\u5851\u9020\u7528\u6237\u7684\u793e\u4f1a\u611f\u77e5\uff1b2)\u793e\u4f1a\u611f\u77e5\u6307\u5bfc\u6c9f\u901a\u884c\u4e3a\uff1b3)\u6c9f\u901a\u4ea7\u751f\u5173\u7cfb\u7ed3\u679c(\u5982\u4f9d\u604b\u548c\u966a\u4f34)\u3002\u6846\u67b6\u5305\u542b\u516d\u7279\u5f81\u804a\u5929\u673a\u5668\u4eba\u63cf\u8ff0\u3001\u793e\u4f1a\u611f\u77e5\u7684\u53cc\u8def\u5f84\u65b9\u6cd5\u3001\u6c9f\u901a\u7684\u884c\u4e3a\u6982\u5ff5\u5316\uff0c\u5e76\u8ba8\u8bba\u4eba\u5de5\u5173\u7cfb\u7684\u57fa\u7840\u548c\u7c7b\u578b\u3002", "result": "AI-RP\u6846\u67b6\u901a\u8fc7\u7a81\u51fa\u53ef\u89c2\u5bdf\u7684\u6c9f\u901a\u884c\u4e3a\uff0c\u4e3a\u7406\u8bba\u6784\u5efa\u548c\u5b9e\u8bc1\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u7406\u89e3AI\u966a\u4f34\u7684\u793e\u4f1a\u548c\u4f26\u7406\u5f71\u54cd\u3002", "conclusion": "AI-RP\u6846\u67b6\u586b\u8865\u4e86\u4eba\u673a\u5173\u7cfb\u5f62\u6210\u7406\u8bba\u7a7a\u767d\uff0c\u5c06\u6c9f\u901a\u7f6e\u4e8e\u5173\u7cfb\u6784\u5efa\u7684\u6838\u5fc3\u4f4d\u7f6e\uff0c\u4e3a\u7814\u7a76AI\u966a\u4f34\u7684\u793e\u4f1a\u548c\u4f26\u7406\u610f\u4e49\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2601.18315", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18315", "abs": "https://arxiv.org/abs/2601.18315", "authors": ["Zhaozhi Liu", "Jiaxin Chen", "Yuanai Xie", "Yuna Jiang", "Minrui Xu", "Xiao Zhang", "Pan Lai", "Zan Zhou"], "title": "CovertComBench: The First Domain-Specific Testbed for LLMs in Wireless Covert Communication", "comment": "6pages", "summary": "The integration of Large Language Models (LLMs) into wireless networks presents significant potential for automating system design. However, unlike conventional throughput maximization, Covert Communication (CC) requires optimizing transmission utility under strict detection-theoretic constraints, such as Kullback-Leibler divergence limits. Existing benchmarks primarily focus on general reasoning or standard communication tasks and do not adequately evaluate the ability of LLMs to satisfy these rigorous security constraints. To address this limitation, we introduce CovertComBench, a unified benchmark designed to assess LLM capabilities across the CC pipeline, encompassing conceptual understanding (MCQs), optimization derivation (ODQs), and code generation (CGQs). Furthermore, we analyze the reliability of automated scoring within a detection-theoretic ``LLM-as-Judge'' framework. Extensive evaluations across state-of-the-art models reveal a significant performance discrepancy. While LLMs achieve high accuracy in conceptual identification (81%) and code implementation (83%), their performance in the higher-order mathematical derivations necessary for security guarantees ranges between 18% and 55%. This limitation indicates that current LLMs serve better as implementation assistants rather than autonomous solvers for security-constrained optimization. These findings suggest that future research should focus on external tool augmentation to build trustworthy wireless AI systems.", "AI": {"tldr": "CovertComBench\uff1a\u9996\u4e2a\u8bc4\u4f30LLM\u5728\u9690\u853d\u901a\u4fe1\u4e2d\u5b89\u5168\u7ea6\u675f\u4f18\u5316\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u53d1\u73b0LLM\u5728\u6982\u5ff5\u7406\u89e3\u548c\u4ee3\u7801\u5b9e\u73b0\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6570\u5b66\u63a8\u5bfc\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u63a8\u7406\u6216\u6807\u51c6\u901a\u4fe1\u4efb\u52a1\uff0c\u672a\u80fd\u5145\u5206\u8bc4\u4f30LLM\u5728\u6ee1\u8db3\u9690\u853d\u901a\u4fe1\u4e25\u683c\u68c0\u6d4b\u7406\u8bba\u7ea6\u675f\uff08\u5982KL\u6563\u5ea6\u9650\u5236\uff09\u65b9\u9762\u7684\u80fd\u529b\u3002\u9690\u853d\u901a\u4fe1\u9700\u8981\u5728\u4e25\u683c\u5b89\u5168\u7ea6\u675f\u4e0b\u4f18\u5316\u4f20\u8f93\u6548\u7528\uff0c\u8fd9\u4e0e\u4f20\u7edf\u541e\u5410\u91cf\u6700\u5927\u5316\u6709\u672c\u8d28\u533a\u522b\u3002", "method": "\u63d0\u51faCovertComBench\u7edf\u4e00\u57fa\u51c6\uff0c\u6db5\u76d6\u9690\u853d\u901a\u4fe1\u5168\u6d41\u7a0b\uff1a\u6982\u5ff5\u7406\u89e3\uff08MCQs\uff09\u3001\u4f18\u5316\u63a8\u5bfc\uff08ODQs\uff09\u548c\u4ee3\u7801\u751f\u6210\uff08CGQs\uff09\u3002\u91c7\u7528\u68c0\u6d4b\u7406\u8bba\u6846\u67b6\u4e0b\u7684\"LLM-as-Judge\"\u65b9\u6cd5\u8fdb\u884c\u81ea\u52a8\u8bc4\u5206\u53ef\u9760\u6027\u5206\u6790\u3002", "result": "\u5bf9\u5148\u8fdbLLM\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff1a\u6982\u5ff5\u8bc6\u522b\u51c6\u786e\u738781%\uff0c\u4ee3\u7801\u5b9e\u73b083%\uff0c\u4f46\u5b89\u5168\u4fdd\u8bc1\u6240\u9700\u7684\u9ad8\u9636\u6570\u5b66\u63a8\u5bfc\u6027\u80fd\u4ec5\u4e3a18%-55%\u3002\u8fd9\u8868\u660e\u5f53\u524dLLM\u66f4\u9002\u5408\u4f5c\u4e3a\u5b9e\u73b0\u52a9\u624b\u800c\u975e\u5b89\u5168\u7ea6\u675f\u4f18\u5316\u7684\u81ea\u4e3b\u6c42\u89e3\u5668\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u9690\u853d\u901a\u4fe1\u7684\u5b89\u5168\u7ea6\u675f\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5916\u90e8\u5de5\u5177\u589e\u5f3a\uff0c\u4ee5\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u65e0\u7ebfAI\u7cfb\u7edf\u3002"}}
{"id": "2601.17010", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.17010", "abs": "https://arxiv.org/abs/2601.17010", "authors": ["Hudson Golino"], "title": "Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study", "comment": "18 pages, 6 figures, conference paper", "summary": "Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u91cd\u6784\u4e3a\u53ef\u641c\u7d22\u7684\u8bed\u4e49\u666f\u89c2\uff0c\u901a\u8fc7\u52a8\u6001\u63a2\u7d22\u56fe\u5206\u6790\u7cfb\u7edf\u904d\u5386\u5d4c\u5165\u5750\u6807\uff0c\u53d1\u73b0\u5d4c\u5165\u7a7a\u95f4\u7684\u7ed3\u6784\u4fe1\u606f\u5206\u5e03\u4e0d\u5747\u5300\uff0c\u9700\u8981\u57fa\u4e8e\u52a0\u6743\u590d\u5408\u51c6\u5219\u8fdb\u884c\u4f18\u5316\u800c\u975e\u9ed8\u8ba4\u4f7f\u7528\u5b8c\u6574\u5411\u91cf\u3002", "motivation": "\u5f53\u524d\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u5c06LLM\u5d4c\u5165\u4f5c\u4e3a\u9759\u6001\u3001\u6a2a\u622a\u9762\u8868\u793a\uff0c\u5047\u8bbe\u6240\u6709\u5d4c\u5165\u5750\u6807\u5747\u5300\u8d21\u732e\uff0c\u5ffd\u89c6\u4e86\u6700\u4f18\u7ed3\u6784\u4fe1\u606f\u53ef\u80fd\u96c6\u4e2d\u5728\u5d4c\u5165\u7a7a\u95f4\u7279\u5b9a\u533a\u57df\u7684\u53ef\u80fd\u6027\u3002\u9700\u8981\u7cfb\u7edf\u63a2\u7d22\u5d4c\u5165\u7a7a\u95f4\u4ee5\u8bc6\u522b\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u7ef4\u5ea6\u533a\u57df\u3002", "method": "\u5c06\u5d4c\u5165\u91cd\u6784\u4e3a\u53ef\u641c\u7d22\u666f\u89c2\uff0c\u91c7\u7528\u52a8\u6001\u63a2\u7d22\u56fe\u5206\u6790\uff08DynEGA\uff09\u7cfb\u7edf\u904d\u5386\u5d4c\u5165\u5750\u6807\uff0c\u5c06\u7ef4\u5ea6\u7d22\u5f15\u89c6\u4e3a\u4f2a\u65f6\u95f4\u987a\u5e8f\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u8499\u7279\u5361\u6d1b\u6a21\u62df\uff0c\u4f7f\u7528OpenAI\u7684text-embedding-3-small\u6a21\u578b\u5d4c\u5165\u4ee3\u8868\u81ea\u604b\u4e94\u7ef4\u5ea6\u7684\u9879\u76ee\uff0c\u7cfb\u7edf\u53d8\u5316\u9879\u76ee\u6c60\u5927\u5c0f\uff08\u6bcf\u7ef4\u5ea63-40\u4e2a\u9879\u76ee\uff09\u548c\u5d4c\u5165\u6df1\u5ea6\uff083-1,298\u7ef4\u5ea6\uff09\u3002", "result": "\u603b\u71b5\u62df\u5408\u6307\u6570\uff08TEFI\uff09\u548c\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\uff08NMI\uff09\u5728\u5d4c\u5165\u666f\u89c2\u4e2d\u4ea7\u751f\u7ade\u4e89\u6027\u4f18\u5316\u8f68\u8ff9\uff1aTEFI\u5728\u6df1\u5c42\u5d4c\u5165\u8303\u56f4\uff08900-1,200\u7ef4\u5ea6\uff09\u8fbe\u5230\u6700\u5c0f\u503c\uff0c\u6b64\u65f6\u57fa\u4e8e\u71b5\u7684\u7ec4\u7ec7\u6700\u5927\u5316\u4f46\u7ed3\u6784\u51c6\u786e\u6027\u4e0b\u964d\uff1bNMI\u5728\u6d45\u5c42\u6df1\u5ea6\u8fbe\u5230\u5cf0\u503c\uff0c\u6b64\u65f6\u7ef4\u5ea6\u6062\u590d\u6700\u5f3a\u4f46\u57fa\u4e8e\u71b5\u7684\u62df\u5408\u4ecd\u4e0d\u7406\u60f3\u3002\u5355\u4e00\u6307\u6807\u4f18\u5316\u4ea7\u751f\u7ed3\u6784\u4e0d\u4e00\u81f4\u89e3\uff0c\u800c\u52a0\u6743\u590d\u5408\u51c6\u5219\u8bc6\u522b\u51fa\u540c\u65f6\u5e73\u8861\u51c6\u786e\u6027\u548c\u7ec4\u7ec7\u7684\u5d4c\u5165\u7ef4\u5ea6\u6df1\u5ea6\u533a\u57df\u3002\u6700\u4f18\u5d4c\u5165\u6df1\u5ea6\u968f\u9879\u76ee\u6c60\u5927\u5c0f\u7cfb\u7edf\u7f29\u653e\u3002", "conclusion": "\u5d4c\u5165\u666f\u89c2\u662f\u975e\u5747\u5300\u7684\u8bed\u4e49\u7a7a\u95f4\uff0c\u9700\u8981\u57fa\u4e8e\u539f\u5219\u7684\u4f18\u5316\u800c\u975e\u9ed8\u8ba4\u4f7f\u7528\u5b8c\u6574\u5411\u91cf\u3002\u52a0\u6743\u590d\u5408\u51c6\u5219\u80fd\u591f\u8bc6\u522b\u540c\u65f6\u4f18\u5316\u7ed3\u6784\u51c6\u786e\u6027\u548c\u7ec4\u7ec7\u6027\u7684\u5d4c\u5165\u7ef4\u5ea6\u533a\u57df\uff0c\u6700\u4f18\u5d4c\u5165\u6df1\u5ea6\u53d6\u51b3\u4e8e\u9879\u76ee\u6c60\u5927\u5c0f\uff0c\u4e3a\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u7684\u5d4c\u5165\u4f7f\u7528\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u6307\u5bfc\u3002"}}
{"id": "2601.17558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17558", "abs": "https://arxiv.org/abs/2601.17558", "authors": ["J. P. Fleischer", "Tanchanok Sirikanchittavon", "Chonlachart Jeenprasom", "Nooshin Yousefzadeh", "Sanjay Ranka", "Mohammed Hadi"], "title": "Measuring Braking Behavior Using Vehicle Tracking and Camera-to-Satellite Homography Rectification", "comment": "VEHITS 2026", "summary": "This paper presents an open-source software application for analyzing traffic camera footage, focusing on vehicle behavior and braking events at signalized urban highways. The core innovation is a robust ground-plane homography estimation that links fixed traffic camera views to satellite orthoimagery. This process rectifies the camera's oblique perspective, ensuring that pixel distances accurately represent real-world distances. This enables the acquisition of features such as vehicle trajectory, speed, deceleration, and braking severity without the need for camera calibration. The pipeline employs the MAGSAC++ estimator to build the homography, converting YOLO11 object detections into a rectified top-down coordinate system. All detection and trajectory data are stored in a ClickHouse database for subsequent analysis. A real-world case study at two signalized intersections in Key West, Florida, showcased the system's capabilities. Across two days of daytime footage, braking activity at the higher-volume intersection peaked around 4 PM at approximately 57.5 events per hour, while the second intersection peaked around 10 AM at roughly 15.5 events per hour. The spatial analysis revealed that most braking events initiated upstream, with mild and moderate braking mostly occurring 30 to 45+ meters away from the stop bar and severe braking distributed throughout, but particularly concentrated in lanes with higher interaction and merging activity. The findings highlight the significant potential of this centralized safety information system to support connected vehicles, facilitating proactive traffic management, crash mitigation, and data-driven roadway design and safety analysis.", "AI": {"tldr": "\u5f00\u6e90\u8f6f\u4ef6\u5e94\u7528\uff0c\u901a\u8fc7\u5730\u9762\u5e73\u9762\u5355\u5e94\u6027\u4f30\u8ba1\u5c06\u4ea4\u901a\u6444\u50cf\u5934\u89c6\u89d2\u4e0e\u536b\u661f\u6b63\u5c04\u5f71\u50cf\u5bf9\u9f50\uff0c\u5b9e\u73b0\u65e0\u6807\u5b9a\u83b7\u53d6\u8f66\u8f86\u8f68\u8ff9\u3001\u901f\u5ea6\u3001\u51cf\u901f\u5ea6\u548c\u5236\u52a8\u4e25\u91cd\u5ea6\u7b49\u7279\u5f81\uff0c\u7528\u4e8e\u5206\u6790\u4fe1\u53f7\u5316\u57ce\u5e02\u9ad8\u901f\u516c\u8def\u7684\u8f66\u8f86\u884c\u4e3a\u548c\u5236\u52a8\u4e8b\u4ef6\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u65e0\u9700\u6444\u50cf\u5934\u6807\u5b9a\u7684\u96c6\u4e2d\u5f0f\u5b89\u5168\u4fe1\u606f\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u4ea4\u901a\u6444\u50cf\u5934\u89c6\u9891\u6765\u652f\u6301\u8054\u7f51\u8f66\u8f86\u3001\u4e3b\u52a8\u4ea4\u901a\u7ba1\u7406\u3001\u4e8b\u6545\u7f13\u89e3\u4ee5\u53ca\u6570\u636e\u9a71\u52a8\u7684\u9053\u8def\u8bbe\u8ba1\u548c\u5b89\u5168\u5206\u6790\u3002", "method": "\u4f7f\u7528MAGSAC++\u4f30\u8ba1\u5668\u6784\u5efa\u5730\u9762\u5e73\u9762\u5355\u5e94\u6027\uff0c\u5c06YOLO11\u76ee\u6807\u68c0\u6d4b\u8f6c\u6362\u4e3a\u6821\u6b63\u7684\u4fef\u89c6\u5750\u6807\u7cfb\uff0c\u901a\u8fc7ClickHouse\u6570\u636e\u5e93\u5b58\u50a8\u68c0\u6d4b\u548c\u8f68\u8ff9\u6570\u636e\uff0c\u5b9e\u73b0\u50cf\u7d20\u8ddd\u79bb\u5230\u771f\u5b9e\u4e16\u754c\u8ddd\u79bb\u7684\u51c6\u786e\u6620\u5c04\u3002", "result": "\u5728\u4f5b\u7f57\u91cc\u8fbe\u5dde\u57fa\u97e6\u65af\u7279\u4e24\u4e2a\u4fe1\u53f7\u5316\u4ea4\u53c9\u53e3\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u9ad8\u6d41\u91cf\u4ea4\u53c9\u53e3\u4e0b\u53484\u70b9\u5236\u52a8\u4e8b\u4ef6\u5cf0\u503c\u7ea657.5\u6b21/\u5c0f\u65f6\uff0c\u7b2c\u4e8c\u4ea4\u53c9\u53e3\u4e0a\u534810\u70b9\u5cf0\u503c\u7ea615.5\u6b21/\u5c0f\u65f6\uff1b\u7a7a\u95f4\u5206\u6790\u663e\u793a\u5927\u591a\u6570\u5236\u52a8\u4e8b\u4ef6\u5728\u4e0a\u6e38\u542f\u52a8\uff0c\u8f7b\u5ea6/\u4e2d\u5ea6\u5236\u52a8\u591a\u53d1\u751f\u5728\u8ddd\u505c\u8f66\u7ebf30-45+\u7c73\u5904\uff0c\u4e25\u91cd\u5236\u52a8\u5728\u4ea4\u4e92\u548c\u5e76\u7ebf\u6d3b\u52a8\u8f83\u591a\u7684\u8f66\u9053\u96c6\u4e2d\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u96c6\u4e2d\u5f0f\u5b89\u5168\u4fe1\u606f\u7cfb\u7edf\u5728\u652f\u6301\u8054\u7f51\u8f66\u8f86\u3001\u4fc3\u8fdb\u4e3b\u52a8\u4ea4\u901a\u7ba1\u7406\u3001\u4e8b\u6545\u7f13\u89e3\u4ee5\u53ca\u6570\u636e\u9a71\u52a8\u7684\u9053\u8def\u8bbe\u8ba1\u548c\u5b89\u5168\u5206\u6790\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2601.17404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17404", "abs": "https://arxiv.org/abs/2601.17404", "authors": ["Anke Fischer-Janzen", "Thomas M. Wendt", "Kristof Van Laerhoven"], "title": "Eye-Tracking-Driven Control in Daily Task Assistance for Assistive Robotic Arms", "comment": "23 pages, 6 figures, publication in review process", "summary": "Shared control improves Human-Robot Interaction by reducing the user's workload and increasing the robot's autonomy. It allows robots to perform tasks under the user's supervision. Current eye-tracking-driven approaches face several challenges. These include accuracy issues in 3D gaze estimation and difficulty interpreting gaze when differentiating between multiple tasks. We present an eye-tracking-driven control framework, aimed at enabling individuals with severe physical disabilities to perform daily tasks independently. Our system uses task pictograms as fiducial markers combined with a feature matching approach that transmits data of the selected object to accomplish necessary task related measurements with an eye-in-hand configuration. This eye-tracking control does not require knowledge of the user's position in relation to the object. The framework correctly interpreted object and task selection in up to 97.9% of measurements. Issues were found in the evaluation, that were improved and shared as lessons learned. The open-source framework can be adapted to new tasks and objects due to the integration of state-of-the-art object detection models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7684\u63a7\u5236\u6846\u67b6\uff0c\u5e2e\u52a9\u91cd\u5ea6\u8eab\u4f53\u6b8b\u75be\u4eba\u58eb\u901a\u8fc7\u6ce8\u89c6\u4efb\u52a1\u56fe\u6807\u9009\u62e9\u5bf9\u8c61\u5e76\u6267\u884c\u65e5\u5e38\u4efb\u52a1\uff0c\u51c6\u786e\u7387\u8fbe97.9%", "motivation": "\u5f53\u524d\u773c\u52a8\u8ffd\u8e2a\u9a71\u52a8\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u6cd5\u9762\u4e343D\u6ce8\u89c6\u4f30\u8ba1\u7cbe\u5ea6\u4e0d\u8db3\u548c\u591a\u91cd\u4efb\u52a1\u89e3\u91ca\u56f0\u96be\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e3a\u91cd\u5ea6\u8eab\u4f53\u6b8b\u75be\u4eba\u58eb\u5f00\u53d1\u66f4\u6709\u6548\u7684\u72ec\u7acb\u6267\u884c\u65e5\u5e38\u4efb\u52a1\u7684\u7cfb\u7edf", "method": "\u4f7f\u7528\u4efb\u52a1\u56fe\u6807\u4f5c\u4e3a\u57fa\u51c6\u6807\u8bb0\uff0c\u7ed3\u5408\u7279\u5f81\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u773c\u5728\u624b\u914d\u7f6e\u4f20\u8f93\u9009\u5b9a\u5bf9\u8c61\u6570\u636e\u4ee5\u5b8c\u6210\u5fc5\u8981\u4efb\u52a1\u6d4b\u91cf\uff0c\u65e0\u9700\u7528\u6237\u4e0e\u5bf9\u8c61\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f", "result": "\u6846\u67b6\u5728\u9ad8\u8fbe97.9%\u7684\u6d4b\u91cf\u4e2d\u6b63\u786e\u89e3\u91ca\u5bf9\u8c61\u548c\u4efb\u52a1\u9009\u62e9\uff0c\u8bc4\u4f30\u4e2d\u53d1\u73b0\u7684\u95ee\u9898\u5df2\u6539\u8fdb\u5e76\u4f5c\u4e3a\u7ecf\u9a8c\u6559\u8bad\u5206\u4eab", "conclusion": "\u5f00\u6e90\u6846\u67b6\u53ef\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u5bf9\u8c61\uff0c\u96c6\u6210\u4e86\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u4e3a\u91cd\u5ea6\u6b8b\u75be\u4eba\u58eb\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u773c\u52a8\u8ffd\u8e2a\u63a7\u5236\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17368", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17368", "abs": "https://arxiv.org/abs/2601.17368", "authors": ["Shuning Zhang", "Eve He", "Sixing Tao", "Yuting Yang", "Ying Ma", "Ailei Wang", "Xin Yi", "Hewu Li"], "title": "A Scoping Review and Guidelines on Privacy Policy's Visualization from an HCI Perspective", "comment": "Accepted by CHI'26: 10.1145/3772318.3790320", "summary": "Privacy Policies are a cornerstone of informed consent, yet a persistent gap exists between their legal intent and practical efficacy. Despite decades of Human-Computer Interaction (HCI) research proposing various visualizations, user comprehension remains low, and designs rarely see widespread adoption. To understand this landscape and chart a path forward, we synthesized 65 top-tier papers using a framework adapted from the user-centered design lifecycle. Our analysis presented findings of the field's evolution across four dimensions: (1) the trade-off between information load and decision efficacy, which demonstrates a shift from augmenting disclosures to prioritizing information condensation and cognitive load management to counter the inefficacy of comprehensive texts, (2) the co-evolutionary dynamic of design and automation, revealing that complex design ambitions such as context-awareness drove the need for advanced NLP, while recent LLM breakthroughs are enabling the semantic interpretation required to realize those designs, (3) the tension between generality and specificity, highlighting the divergence between standardized, cross-platform solutions and the increasing necessity for specialized, context-aware interaction patterns in IoT and immersive environments, and (4) balancing stakeholder opinions, which shows that visualization efficacy is constrained by the complex interplay of regulatory mandates, developer capabilities and provider incentives. We conclude by outlining four critical challenges for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e8665\u7bc7\u9876\u7ea7HCI\u7814\u7a76\uff0c\u5206\u6790\u4e86\u9690\u79c1\u653f\u7b56\u53ef\u89c6\u5316\u9886\u57df\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u8bc6\u522b\u4e86\u4fe1\u606f\u8d1f\u8f7d\u4e0e\u51b3\u7b56\u6548\u80fd\u3001\u8bbe\u8ba1\u4e0e\u81ea\u52a8\u5316\u3001\u901a\u7528\u6027\u4e0e\u7279\u5f02\u6027\u3001\u5229\u76ca\u76f8\u5173\u8005\u5e73\u8861\u56db\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u56db\u4e2a\u5173\u952e\u6311\u6218\u3002", "motivation": "\u9690\u79c1\u653f\u7b56\u4f5c\u4e3a\u77e5\u60c5\u540c\u610f\u7684\u57fa\u77f3\uff0c\u5176\u6cd5\u5f8b\u610f\u56fe\u4e0e\u5b9e\u9645\u6548\u679c\u4e4b\u95f4\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\u3002\u5c3d\u7ba1HCI\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u79cd\u53ef\u89c6\u5316\u65b9\u6848\uff0c\u4f46\u7528\u6237\u7406\u89e3\u5ea6\u4ecd\u7136\u5f88\u4f4e\uff0c\u8bbe\u8ba1\u4e5f\u5f88\u5c11\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002\u9700\u8981\u7406\u89e3\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u8109\u7edc\u5e76\u4e3a\u672a\u6765\u6307\u660e\u65b9\u5411\u3002", "method": "\u91c7\u7528\u6539\u7f16\u81ea\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u8bbe\u8ba1\u751f\u547d\u5468\u671f\u7684\u6846\u67b6\uff0c\u5bf965\u7bc7\u9876\u7ea7\u8bba\u6587\u8fdb\u884c\u7efc\u5408\u6587\u732e\u7efc\u8ff0\u5206\u6790\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u8be5\u9886\u57df\u5728\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u6f14\u53d8\uff1a(1)\u4fe1\u606f\u8d1f\u8f7d\u4e0e\u51b3\u7b56\u6548\u80fd\u7684\u6743\u8861\uff0c\u4ece\u589e\u5f3a\u62ab\u9732\u8f6c\u5411\u4fe1\u606f\u538b\u7f29\u548c\u8ba4\u77e5\u8d1f\u8f7d\u7ba1\u7406\uff1b(2)\u8bbe\u8ba1\u4e0e\u81ea\u52a8\u5316\u7684\u5171\u540c\u6f14\u5316\uff0c\u590d\u6742\u8bbe\u8ba1\u9700\u6c42\u63a8\u52a8NLP\u53d1\u5c55\uff0c\u800cLLM\u7a81\u7834\u4f7f\u8bed\u4e49\u89e3\u91ca\u6210\u4e3a\u53ef\u80fd\uff1b(3)\u901a\u7528\u6027\u4e0e\u7279\u5f02\u6027\u7684\u5f20\u529b\uff0c\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\u4e0e\u7269\u8054\u7f51\u548c\u6c89\u6d78\u5f0f\u73af\u5883\u4e2d\u7279\u5b9a\u60c5\u5883\u4ea4\u4e92\u6a21\u5f0f\u7684\u9700\u6c42\u5206\u6b67\uff1b(4)\u5229\u76ca\u76f8\u5173\u8005\u5e73\u8861\uff0c\u53ef\u89c6\u5316\u6548\u80fd\u53d7\u76d1\u7ba1\u8981\u6c42\u3001\u5f00\u53d1\u8005\u80fd\u529b\u548c\u63d0\u4f9b\u5546\u6fc0\u52b1\u7684\u590d\u6742\u4e92\u52a8\u5236\u7ea6\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u9690\u79c1\u653f\u7b56\u53ef\u89c6\u5316\u9886\u57df\u7684\u6f14\u53d8\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u56db\u4e2a\u5173\u952e\u6311\u6218\uff0c\u4e3a\u63a8\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6846\u67b6\u3002"}}
{"id": "2601.17063", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17063", "abs": "https://arxiv.org/abs/2601.17063", "authors": ["Byeongju Kim", "Jungwan Lee", "Donghyeon Han", "Hoi-Jun Yoo", "Sangyeob Kim"], "title": "FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices", "comment": null, "summary": "Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.", "AI": {"tldr": "FlashMoE\uff1a\u4e00\u4e2a\u5c06\u975e\u6d3b\u8dc3\u4e13\u5bb6\u5378\u8f7d\u5230SSD\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u6709\u9650RAM\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684MoE\u63a8\u7406\uff0c\u901a\u8fc7ML\u7f13\u5b58\u7b56\u7565\u63d0\u5347\u4e13\u5bb6\u91cd\u7528\u7387\uff0c\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\u5b9e\u73b0\u6700\u9ad82.6\u500d\u52a0\u901f\u3002", "motivation": "MoE\u6a21\u578b\u867d\u7136\u7a00\u758f\u6fc0\u6d3b\uff0c\u4f46\u89c4\u6a21\u5df2\u8fbe\u6570\u767eGB\uff0c\u73b0\u6709DRAM\u5378\u8f7d\u65b9\u6848\uff08\u5982Fiddler\u3001DAOP\uff09\u4e0d\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u7684\u7aef\u4fa7\u8bbe\u5907\u3002RAM\u5378\u8f7d\u65b9\u6848\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFlashMoE\u7cfb\u7edf\uff0c\u5c06\u975e\u6d3b\u8dc3\u4e13\u5bb6\u5378\u8f7d\u5230SSD\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7ML\u7f13\u5b58\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u7ed3\u5408\u6700\u8fd1\u4f7f\u7528\u548c\u9891\u7387\u4fe1\u53f7\u4ee5\u6700\u5927\u5316\u4e13\u5bb6\u91cd\u7528\uff0c\u663e\u8457\u51cf\u5c11\u5b58\u50a8I/O\u3002\u6784\u5efa\u684c\u9762\u5e73\u53f0\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "result": "\u5728\u771f\u5b9e\u786c\u4ef6\u8bbe\u7f6e\u4e0a\uff0cFlashMoE\u76f8\u6bd4LRU\u548cLFU\u7b49\u4f20\u7edf\u5378\u8f7d\u7b56\u7565\u63d0\u5347\u7f13\u5b58\u547d\u4e2d\u7387\u6700\u9ad8\u8fbe51%\uff0c\u76f8\u6bd4\u73b0\u6709MoE\u63a8\u7406\u7cfb\u7edf\u5b9e\u73b0\u6700\u9ad82.6\u500d\u52a0\u901f\u3002", "conclusion": "FlashMoE\u901a\u8fc7SSD\u5378\u8f7d\u548c\u667a\u80fd\u7f13\u5b58\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21MoE\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u95ee\u9898\uff0c\u4e3a\u7aef\u4fa7MoE\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17412", "abs": "https://arxiv.org/abs/2601.17412", "authors": ["Valerii Serpiva", "Artem Lykov", "Jeffrin Sam", "Aleksey Fedoseev", "Dzmitry Tsetserukou"], "title": "DiffusionCinema: Text-to-Aerial Cinematography", "comment": null, "summary": "We propose a novel Unmanned Aerial Vehicles (UAV) assisted creative capture system that leverages diffusion models to interpret high-level natural language prompts and automatically generate optimal flight trajectories for cinematic video recording. Instead of manually piloting the drone, the user simply describes the desired shot (e.g., \"orbit around me slowly from the right and reveal the background waterfall\"). Our system encodes the prompt along with an initial visual snapshot from the onboard camera, and a diffusion model samples plausible spatio-temporal motion plans that satisfy both the scene geometry and shot semantics. The generated flight trajectory is then executed autonomously by the UAV to record smooth, repeatable video clips that match the prompt. User evaluation using NASA-TLX showed a significantly lower overall workload with our interface (M = 21.6) compared to a traditional remote controller (M = 58.1), demonstrating a substantial reduction in perceived effort. Mental demand (M = 11.5 vs. 60.5) and frustration (M = 14.0 vs. 54.5) were also markedly lower for our system, confirming clear usability advantages in autonomous text-driven flight control. This project demonstrates a new interaction paradigm: text-to-cinema flight, where diffusion models act as the \"creative operator\" converting story intentions directly into aerial motion.", "AI": {"tldr": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65e0\u4eba\u673a\u521b\u610f\u62cd\u6444\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u81ea\u52a8\u751f\u6210\u6700\u4f18\u98de\u884c\u8f68\u8ff9\uff0c\u5b9e\u73b0\u6587\u672c\u5230\u7535\u5f71\u5316\u98de\u884c\u7684\u65b0\u4ea4\u4e92\u8303\u5f0f", "motivation": "\u4f20\u7edf\u65e0\u4eba\u673a\u64cd\u63a7\u9700\u8981\u4e13\u4e1a\u6280\u80fd\uff0c\u624b\u52a8\u98de\u884c\u96be\u4ee5\u5b9e\u73b0\u590d\u6742\u7684\u7535\u5f71\u5316\u955c\u5934\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u964d\u4f4e\u65e0\u4eba\u673a\u521b\u610f\u62cd\u6444\u7684\u95e8\u69db\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u901a\u8fc7\u7b80\u5355\u6587\u672c\u63cf\u8ff0\u76f4\u63a5\u83b7\u5f97\u4e13\u4e1a\u7ea7\u822a\u62cd\u6548\u679c\u3002", "method": "\u63d0\u51fa\u65e0\u4eba\u673a\u8f85\u52a9\u521b\u610f\u62cd\u6444\u7cfb\u7edf\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u89e3\u6790\u9ad8\u7ea7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u3002\u7cfb\u7edf\u7f16\u7801\u7528\u6237\u6587\u672c\u63d0\u793a\u548c\u673a\u8f7d\u6444\u50cf\u5934\u7684\u521d\u59cb\u89c6\u89c9\u5feb\u7167\uff0c\u6269\u6563\u6a21\u578b\u91c7\u6837\u6ee1\u8db3\u573a\u666f\u51e0\u4f55\u548c\u955c\u5934\u8bed\u4e49\u7684\u65f6\u7a7a\u8fd0\u52a8\u89c4\u5212\uff0c\u751f\u6210\u6700\u4f18\u98de\u884c\u8f68\u8ff9\u5e76\u7531\u65e0\u4eba\u673a\u81ea\u4e3b\u6267\u884c\u3002", "result": "NASA-TLX\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u663e\u8457\u964d\u4f4e\u5de5\u4f5c\u8d1f\u8377\uff08M=21.6 vs 58.1\uff09\uff0c\u5fc3\u7406\u9700\u6c42\uff0811.5 vs 60.5\uff09\u548c\u632b\u8d25\u611f\uff0814.0 vs 54.5\uff09\u5927\u5e45\u964d\u4f4e\u3002\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u5e73\u6ed1\u3001\u53ef\u91cd\u590d\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u51c6\u786e\u5339\u914d\u6587\u672c\u63cf\u8ff0\u3002", "conclusion": "\u5c55\u793a\u4e86\"\u6587\u672c\u5230\u7535\u5f71\u5316\u98de\u884c\"\u7684\u65b0\u4ea4\u4e92\u8303\u5f0f\uff0c\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\"\u521b\u610f\u64cd\u4f5c\u5458\"\u5c06\u6545\u4e8b\u610f\u56fe\u76f4\u63a5\u8f6c\u6362\u4e3a\u7a7a\u4e2d\u8fd0\u52a8\u3002\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u521b\u610f\u62cd\u6444\u7684\u53ef\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2601.17497", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17497", "abs": "https://arxiv.org/abs/2601.17497", "authors": ["Mohammed Barhoush", "Arthur Mehta", "Anne M\u00fcller", "Louis Salvail"], "title": "On the Impossibility of Simulation Security for Quantum Functional Encryption", "comment": "28 pages", "summary": "Functional encryption is a powerful cryptographic primitive that enables fine-grained access to encrypted data and underlies numerous applications. Although the ideal security notion for FE (simulation security) has been shown to be impossible in the classical setting, those impossibility results rely on inherently classical arguments. This leaves open the question of whether simulation-secure functional encryption can be achieved in the quantum regime.\n  In this work, we rule out this possibility by showing that the classical impossibility results largely extend to the quantum world. In particular, when the adversary can issue an unbounded number of challenge messages, we prove an unconditional impossibility, matching the classical barrier. In the case where the adversary may obtain many functional keys, classical arguments only yield impossibility under the assumption of pseudorandom functions; we strengthen this by proving impossibility under the potentially weaker assumption of pseudorandom quantum states. In the same setting, we also establish an alternative impossibility based on public-key encryption. Since public-key encryption is not known to imply pseudorandom quantum states, this provides independent evidence of the barrier. As part of our proofs, we show a novel incompressibility property for pseudorandom states, which may be of independent interest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u91cf\u5b50\u8ba1\u7b97\u73af\u5883\u4e0b\uff0c\u529f\u80fd\u52a0\u5bc6\u7684\u7406\u60f3\u5b89\u5168\u6982\u5ff5\uff08\u6a21\u62df\u5b89\u5168\uff09\u4ecd\u7136\u662f\u4e0d\u53ef\u80fd\u7684\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u73af\u5883\u4e2d\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u3002", "motivation": "\u529f\u80fd\u52a0\u5bc6\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u5bc6\u7801\u5b66\u539f\u8bed\uff0c\u652f\u6301\u5bf9\u52a0\u5bc6\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u3002\u867d\u7136\u7ecf\u5178\u73af\u5883\u4e2d\u529f\u80fd\u52a0\u5bc6\u7684\u6a21\u62df\u5b89\u5168\u5df2\u88ab\u8bc1\u660e\u4e0d\u53ef\u80fd\uff0c\u4f46\u8fd9\u4e9b\u4e0d\u53ef\u80fd\u6027\u8bc1\u660e\u4f9d\u8d56\u4e8e\u7ecf\u5178\u8bba\u8bc1\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7a76\u5728\u91cf\u5b50\u8ba1\u7b97\u73af\u5883\u4e0b\u662f\u5426\u53ef\u80fd\u5b9e\u73b0\u6a21\u62df\u5b89\u5168\u7684\u529f\u80fd\u52a0\u5bc6\u3002", "method": "\u901a\u8fc7\u5c06\u7ecf\u5178\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u6269\u5c55\u5230\u91cf\u5b50\u4e16\u754c\u6765\u8bc1\u660e\u91cf\u5b50\u73af\u5883\u4e0b\u7684\u4e0d\u53ef\u80fd\u6027\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5728\u653b\u51fb\u8005\u53ef\u4ee5\u53d1\u51fa\u65e0\u9650\u6311\u6218\u6d88\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u65e0\u6761\u4ef6\u4e0d\u53ef\u80fd\u6027\uff1b2\uff09\u5728\u653b\u51fb\u8005\u53ef\u4ee5\u83b7\u5f97\u591a\u4e2a\u529f\u80fd\u5bc6\u94a5\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u4f2a\u968f\u673a\u91cf\u5b50\u6001\uff08\u6bd4\u7ecf\u5178\u4f2a\u968f\u673a\u51fd\u6570\u66f4\u5f31\u7684\u5047\u8bbe\uff09\u8bc1\u660e\u4e0d\u53ef\u80fd\u6027\uff1b3\uff09\u57fa\u4e8e\u516c\u94a5\u52a0\u5bc6\u8bc1\u660e\u53e6\u4e00\u79cd\u4e0d\u53ef\u80fd\u6027\u3002\u8bc1\u660e\u8fc7\u7a0b\u4e2d\u5c55\u793a\u4e86\u4f2a\u968f\u673a\u6001\u7684\u65b0\u9896\u4e0d\u53ef\u538b\u7f29\u6027\u7279\u6027\u3002", "result": "1\uff09\u5728\u65e0\u9650\u6311\u6218\u6d88\u606f\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u4e86\u4e0e\u7ecf\u5178\u969c\u788d\u5339\u914d\u7684\u65e0\u6761\u4ef6\u4e0d\u53ef\u80fd\u6027\uff1b2\uff09\u5728\u591a\u4e2a\u529f\u80fd\u5bc6\u94a5\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u4f2a\u968f\u673a\u91cf\u5b50\u6001\u8bc1\u660e\u4e86\u4e0d\u53ef\u80fd\u6027\uff0c\u8fd9\u6bd4\u57fa\u4e8e\u4f2a\u968f\u673a\u51fd\u6570\u7684\u7ecf\u5178\u8bba\u8bc1\u66f4\u5f3a\uff1b3\uff09\u57fa\u4e8e\u516c\u94a5\u52a0\u5bc6\u8bc1\u660e\u4e86\u53e6\u4e00\u79cd\u4e0d\u53ef\u80fd\u6027\uff0c\u4e3a\u969c\u788d\u63d0\u4f9b\u4e86\u72ec\u7acb\u8bc1\u636e\uff1b4\uff09\u53d1\u73b0\u4e86\u4f2a\u968f\u673a\u6001\u7684\u65b0\u4e0d\u53ef\u538b\u7f29\u6027\u7279\u6027\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u73af\u5883\u65e0\u6cd5\u5b9e\u73b0\u6a21\u62df\u5b89\u5168\u7684\u529f\u80fd\u52a0\u5bc6\uff0c\u7ecf\u5178\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u5728\u91cf\u5b50\u4e16\u754c\u4e2d\u57fa\u672c\u6210\u7acb\u3002\u7814\u7a76\u63ed\u793a\u4e86\u91cf\u5b50\u5bc6\u7801\u5b66\u7684\u57fa\u672c\u9650\u5236\uff0c\u5e76\u53d1\u73b0\u4e86\u4f2a\u968f\u673a\u6001\u7684\u65b0\u7279\u6027\u3002"}}
{"id": "2601.17371", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17371", "abs": "https://arxiv.org/abs/2601.17371", "authors": ["Shuning Zhang", "Linzhi Wang", "Shixuan Li", "Yuanyuan Wu", "Yuwei Chuai", "Luoxi Chen", "Xin Yi", "Hewu Li"], "title": "Collab: Fostering Critical Identification of Deepfake Videos on Social Media via Synergistic Annotation", "comment": "To be published in CHI'26: 10.1145/3772318.3790501", "summary": "Identifying deepfake videos on social media platforms is challenged by dynamic spatio-temporal artifacts and inadequate user tools. This hinders both critical viewing by users and scalable moderation on platforms. Here, we present Collab, a web plugin enabling users to collaboratively annotate deepfake videos. Collab integrates three key components: (i) an intuitive interface for spatio-temporal labeling where users provide confidence scores and rationales, facilitating detailed input even from non-experts, (ii) a novel confidence-weighted spatio-temporal Intersection-over-Union (IoU) algorithm to aggregate diverse user annotations into accurate aggregations, and (iii) a hierarchical demonstration strategy presenting aggregated results to guide attention toward contentious regions and foster critical evaluation. A seven-day online study (N=90), where participants annotated suspicious videos when viewing an online experimental platforms, compared Collab against two conditions without aggregation or demonstration respectively. Collab significantly improved identification accuracy and enhanced reflection compared to non-demonstration condition, while outperforming non-aggregation condition for its novelty and effectiveness.", "AI": {"tldr": "Collab\u662f\u4e00\u4e2a\u7528\u4e8e\u534f\u4f5c\u6807\u6ce8\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u7684Web\u63d2\u4ef6\uff0c\u901a\u8fc7\u7528\u6237\u6807\u6ce8\u3001\u7f6e\u4fe1\u5ea6\u52a0\u6743\u805a\u5408\u7b97\u6cd5\u548c\u5206\u5c42\u6f14\u793a\u7b56\u7565\uff0c\u63d0\u5347\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u7528\u6237\u53cd\u601d\u80fd\u529b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u7684\u8bc6\u522b\u9762\u4e34\u52a8\u6001\u65f6\u7a7a\u4f2a\u5f71\u548c\u7528\u6237\u5de5\u5177\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u8fd9\u65e2\u963b\u788d\u4e86\u7528\u6237\u7684\u6279\u5224\u6027\u89c2\u770b\uff0c\u4e5f\u9650\u5236\u4e86\u5e73\u53f0\u7684\u53ef\u6269\u5c55\u6027\u5ba1\u6838\u3002", "method": "Collab\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u76f4\u89c2\u7684\u65f6\u7a7a\u6807\u6ce8\u754c\u9762\uff0c\u7528\u6237\u53ef\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u5206\u6570\u548c\u7406\u7531\uff1b(2) \u65b0\u9896\u7684\u7f6e\u4fe1\u5ea6\u52a0\u6743\u65f6\u7a7aIoU\u7b97\u6cd5\uff0c\u805a\u5408\u591a\u6837\u5316\u7528\u6237\u6807\u6ce8\uff1b(3) \u5206\u5c42\u6f14\u793a\u7b56\u7565\uff0c\u5c55\u793a\u805a\u5408\u7ed3\u679c\u4ee5\u5f15\u5bfc\u6ce8\u610f\u529b\u81f3\u4e89\u8bae\u533a\u57df\u5e76\u4fc3\u8fdb\u6279\u5224\u6027\u8bc4\u4f30\u3002", "result": "\u4e3a\u671f7\u5929\u7684\u5728\u7ebf\u7814\u7a76(N=90)\u663e\u793a\uff0cCollab\u76f8\u6bd4\u65e0\u6f14\u793a\u6761\u4ef6\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u6027\u548c\u53cd\u601d\u80fd\u529b\uff0c\u76f8\u6bd4\u65e0\u805a\u5408\u6761\u4ef6\u5728\u65b0\u9896\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "Collab\u901a\u8fc7\u534f\u4f5c\u6807\u6ce8\u3001\u667a\u80fd\u805a\u5408\u548c\u5f15\u5bfc\u6027\u6f14\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u7528\u6237\u8bc6\u522b\u80fd\u529b\u548c\u5e73\u53f0\u5ba1\u6838\u6548\u7387\u3002"}}
{"id": "2601.17584", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17584", "abs": "https://arxiv.org/abs/2601.17584", "authors": ["Mahmoud Samir Fayed", "Ahmed Samir Fayed"], "title": "Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language", "comment": null, "summary": "Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.", "AI": {"tldr": "\u4f7f\u7528Claude Code Opus 4.5\u901a\u8fc7\u7eaf\u63d0\u793a\u9a71\u52a8\u5de5\u4f5c\u6d41\u5728\u7ea610\u5c0f\u65f6\u5185\u5f00\u53d1\u4e867420\u884cRing\u8bed\u8a00\u7ec8\u7aef\u7528\u6237\u754c\u9762\u6846\u67b6\uff0c\u5305\u542b\u5b8c\u6574\u7a97\u53e3\u5b50\u7cfb\u7edf\u3001\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u548c\u591a\u79cdUI\u7ec4\u4ef6\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u751f\u6210\u548c\u7ef4\u62a4\u5927\u578b\u591a\u6a21\u5757\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u63a2\u7d22\u63d0\u793a\u9a71\u52a8\u5f00\u53d1\u5728\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u7eaf\u63d0\u793a\u9a71\u52a8\u5de5\u4f5c\u6d41\uff0c\u4f7f\u7528Claude Code Opus 4.5\uff0c\u901a\u8fc7107\u4e2a\u63d0\u793a\uff0821\u4e2a\u529f\u80fd\u8bf7\u6c42\u300172\u4e2a\u9519\u8bef\u4fee\u590d\u30019\u4e2a\u6587\u6863\u4fe1\u606f\u5206\u4eab\u30014\u4e2a\u67b6\u6784\u6307\u5bfc\u30011\u4e2a\u6587\u6863\u751f\u6210\uff09\u5728\u4e09\u5929\u5185\u5f00\u53d17420\u884c\u4ee3\u7801\u7684TUI\u6846\u67b6\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u5305\u542b\u5b8c\u6574\u7a97\u53e3\u5b50\u7cfb\u7edf\u3001\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u3001\u4ea4\u4e92\u5f0f\u5c0f\u90e8\u4ef6\u3001\u5206\u5c42\u83dc\u5355\u3001\u7f51\u683c\u548c\u6811\u7ec4\u4ef6\u3001\u9009\u9879\u5361\u63a7\u4ef6\u4ee5\u53ca\u591a\u7a97\u53e3\u684c\u9762\u73af\u5883\u7684TUI\u6846\u67b6\uff0c\u8bc1\u660e\u4e86LLM\u80fd\u591f\u7ef4\u6301\u67b6\u6784\u4e00\u81f4\u6027\u5e76\u652f\u6301\u751f\u4ea7\u7ea7\u5de5\u5177\u5f00\u53d1\u3002", "conclusion": "\u73b0\u4ee3LLM\u80fd\u591f\u901a\u8fc7\u63d0\u793a\u9a71\u52a8\u5f00\u53d1\u65b9\u6cd5\u7ef4\u6301\u67b6\u6784\u4e00\u81f4\u6027\u5e76\u652f\u6301\u65b0\u5174\u7f16\u7a0b\u8bed\u8a00\u7684\u751f\u4ea7\u7ea7\u5de5\u5177\u6784\u5efa\uff0c\u63d0\u793a\u9a71\u52a8\u5f00\u53d1\u662f\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u53ef\u884c\u7684\u5f00\u53d1\u65b9\u6cd5\u3002"}}
{"id": "2601.17428", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17428", "abs": "https://arxiv.org/abs/2601.17428", "authors": ["Ziming Li", "Chenhao Li", "Marco Hutter"], "title": "Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning", "comment": null, "summary": "Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent's learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u8fdb\u5ea6\u7684\u81ea\u52a8\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6LP-ACRL\uff0c\u901a\u8fc7\u5728\u7ebf\u4f30\u8ba1\u667a\u80fd\u4f53\u5b66\u4e60\u8fdb\u5ea6\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u4efb\u52a1\u91c7\u6837\u5206\u5e03\uff0c\u65e0\u9700\u4efb\u52a1\u7a7a\u95f4\u96be\u5ea6\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4ebaANYmal D\u80fd\u5728\u590d\u6742\u5730\u5f62\u5b9e\u73b0\u9ad8\u901f\u7a33\u5b9a\u8fd0\u52a8\u3002", "motivation": "\u4f20\u7edf\u8bfe\u7a0b\u5b66\u4e60\u5728\u6269\u5c55\u5230\u590d\u6742\u3001\u5e7f\u6cdb\u7684\u4efb\u52a1\u7a7a\u95f4\u65f6\u9762\u4e34\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u8fd9\u7c7b\u4efb\u52a1\u7a7a\u95f4\u901a\u5e38\u7f3a\u4e4f\u660e\u786e\u5b9a\u4e49\u7684\u96be\u5ea6\u7ed3\u6784\uff0c\u96be\u4ee5\u5b9a\u4e49\u5148\u524d\u65b9\u6cd5\u6240\u9700\u7684\u96be\u5ea6\u6392\u5e8f\u3002", "method": "\u63d0\u51faLP-ACRL\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u4f30\u8ba1\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u8fdb\u5ea6\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u4efb\u52a1\u91c7\u6837\u5206\u5e03\uff0c\u5b9e\u73b0\u65e0\u9700\u4efb\u52a1\u7a7a\u95f4\u96be\u5ea6\u5206\u5e03\u5148\u9a8c\u77e5\u8bc6\u7684\u81ea\u52a8\u8bfe\u7a0b\u751f\u6210\u3002", "result": "\u4f7f\u7528LP-ACRL\u8bad\u7ec3\u7684\u7b56\u7565\u4f7fANYmal D\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u5728\u5305\u62ec\u697c\u68af\u3001\u659c\u5761\u3001\u783e\u77f3\u548c\u4f4e\u6469\u64e6\u5e73\u9762\u5728\u5185\u7684\u591a\u6837\u5316\u5730\u5f62\u4e0a\u5b9e\u73b0\u5e76\u7ef4\u63012.5 m/s\u7ebf\u901f\u5ea6\u548c3.0 rad/s\u89d2\u901f\u5ea6\u7684\u7a33\u5b9a\u9ad8\u901f\u8fd0\u52a8\uff0c\u800c\u5148\u524d\u65b9\u6cd5\u901a\u5e38\u4ec5\u9650\u4e8e\u5e73\u5766\u5730\u5f62\u9ad8\u901f\u6216\u590d\u6742\u5730\u5f62\u4f4e\u901f\u3002", "conclusion": "LP-ACRL\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6027\uff0c\u4e3a\u672a\u6765\u5728\u590d\u6742\u3001\u5e7f\u6cdb\u7684\u673a\u5668\u4eba\u5b66\u4e60\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u8bfe\u7a0b\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u57fa\u7ebf\u3002"}}
{"id": "2601.17604", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17604", "abs": "https://arxiv.org/abs/2601.17604", "authors": ["Suborno Deb Bappon", "Saikat Mondal", "Chanchal K. Roy", "Kevin Schneider"], "title": "Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback", "comment": "Preprint", "summary": "Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86AUTOCOMBAT\u5de5\u5177\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6839\u636eStack Overflow\u8bc4\u8bba\u53cd\u9988\u81ea\u52a8\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\uff0c\u5728ReSOlve\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a84.5%\u7684\u5f00\u53d1\u8005\u613f\u610f\u91c7\u7528\u8be5\u5de5\u5177\u3002", "motivation": "Stack Overflow\u7b49\u6280\u672f\u95ee\u7b54\u5e73\u53f0\u4e0a\uff0c\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u7528\u6237\u53cd\u9988\u56e0\u65f6\u95f4\u3001\u4e13\u4e1a\u77e5\u8bc6\u6216\u53ef\u89c1\u6027\u9650\u5236\u800c\u672a\u88ab\u5904\u7406\uff0c\u5bfc\u81f4\u8bb8\u591a\u7b54\u6848\u4e0d\u5b8c\u6574\u6216\u8fc7\u65f6\u3002\u7814\u7a76\u63a2\u7d22LLMs\u80fd\u5426\u901a\u8fc7\u89e3\u91ca\u548c\u6574\u5408\u8bc4\u8bba\u53cd\u9988\u6765\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\uff0c\u63d0\u9ad8\u6280\u672f\u77e5\u8bc6\u5e73\u53f0\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "1. \u521b\u5efaReSOlve\u57fa\u51c6\uff1a\u5305\u542b790\u4e2aStack Overflow\u7b54\u6848\u53ca\u76f8\u5173\u8bc4\u8bba\u7ebf\u7a0b\uff0c\u6807\u6ce8\u6539\u8fdb\u76f8\u5173\u548c\u4e00\u822c\u53cd\u9988\u30022. \u8bc4\u4f30\u56db\u4e2a\u5148\u8fdbLLMs\u8bc6\u522b\u53ef\u64cd\u4f5c\u95ee\u9898\u7684\u80fd\u529b\u30023. \u5f00\u53d1AUTOCOMBAT\u5de5\u5177\uff1a\u7ed3\u5408\u7528\u6237\u8bc4\u8bba\u548c\u95ee\u9898\u4e0a\u4e0b\u6587\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\u30024. \u8fdb\u884c\u7528\u6237\u7814\u7a76\uff1a58\u540d\u4ece\u4e1a\u8005\u8bc4\u4f30\u5de5\u5177\u5b9e\u7528\u4ef7\u503c\u3002", "result": "1. DeepSeek\u5728\u8bc6\u522b\u53ef\u64cd\u4f5c\u95ee\u9898\u65b9\u9762\u8fbe\u5230\u6700\u4f73\u7cbe\u5ea6-\u53ec\u56de\u5e73\u8861\u30022. AUTOCOMBAT\u76f8\u6bd4\u4eba\u7c7b\u4fee\u8ba2\u53c2\u8003\u4ea7\u751f\u63a5\u8fd1\u4eba\u7c7b\u8d28\u91cf\u7684\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u610f\u56fe\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u30023. \u7528\u6237\u7814\u7a76\u663e\u793a84.5%\u7684\u4ece\u4e1a\u8005\u8868\u793a\u4f1a\u91c7\u7528\u6216\u63a8\u8350\u8be5\u5de5\u5177\u3002", "conclusion": "AUTOCOMBAT\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u53cd\u9988\u9a71\u52a8\u7b54\u6848\u7cbe\u70bc\u5728\u63d0\u9ad8\u6280\u672f\u77e5\u8bc6\u5e73\u53f0\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\u3002LLMs\u80fd\u591f\u6709\u6548\u89e3\u91ca\u548c\u6574\u5408\u7528\u6237\u53cd\u9988\u6765\u6539\u8fdb\u7f16\u7a0b\u7b54\u6848\uff0c\u4e3a\u6280\u672f\u95ee\u7b54\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17440", "abs": "https://arxiv.org/abs/2601.17440", "authors": ["Xinru Cui", "Linxi Feng", "Yixuan Zhou", "Haoqi Han", "Zhe Liu", "Hesheng Wang"], "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes", "comment": "8 pages, 4 figures", "summary": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured scenarios.To address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes.", "AI": {"tldr": "PILOT\uff1a\u4e00\u79cd\u7528\u4e8e\u611f\u77e5\u5f0f\u79fb\u52a8\u64cd\u4f5c\u7684\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u611f\u77e5\u5f0f\u6b65\u6001\u548c\u5168\u8eab\u63a7\u5236\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7f16\u7801\u5668\u548c\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u63d0\u5347\u5730\u5f62\u611f\u77e5\u548c\u8fd0\u52a8\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5168\u8eab\u63a7\u5236\u5668\u7f3a\u4e4f\u5bf9\u5468\u56f4\u73af\u5883\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u6267\u884c\u4efb\u52a1\u65f6\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u4e2d\u5fc3\u73af\u5883\u4e2d\u9700\u8981\u80fd\u591f\u65e0\u7f1d\u6574\u5408\u7cbe\u786e\u6b65\u6001\u548c\u7075\u5de7\u64cd\u4f5c\u7684\u63a7\u5236\u5668\u3002", "method": "\u63d0\u51faPILOT\u7edf\u4e00\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff0c\u878d\u5408\u57fa\u4e8e\u9884\u6d4b\u7684\u672c\u4f53\u611f\u77e5\u7279\u5f81\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u611f\u77e5\u8868\u793a\u4ee5\u589e\u5f3a\u5730\u5f62\u611f\u77e5\uff1b2\uff09\u4e13\u5bb6\u6df7\u5408\u7b56\u7565\u67b6\u6784\uff0c\u534f\u8c03\u4e0d\u540c\u8fd0\u52a8\u6280\u80fd\uff0c\u4fc3\u8fdb\u4e0d\u540c\u8fd0\u52a8\u6a21\u5f0f\u7684\u4e13\u95e8\u5316\u3002", "result": "\u5728\u4eff\u771f\u548cUnitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\u3002PILOT\u5728\u7a33\u5b9a\u6027\u3001\u6307\u4ee4\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5730\u5f62\u7a7f\u8d8a\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PILOT\u5c55\u793a\u4e86\u4f5c\u4e3a\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u79fb\u52a8\u64cd\u4f5c\u7684\u9c81\u68d2\u57fa\u7840\u4f4e\u7ea7\u63a7\u5236\u5668\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u611f\u77e5\u5f0f\u79fb\u52a8\u64cd\u4f5c\u3002"}}
{"id": "2601.18727", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18727", "abs": "https://arxiv.org/abs/2601.18727", "authors": ["Skanda Harisha", "Jimmy G. D. Hester", "Aline Eid"], "title": "An ISAC-ready Full-Duplex Backscatter Architecture for the mmWave IoT", "comment": null, "summary": "Achieving long-range, high-rate, concurrent two-way mmWave communication with power-constrained IoT devices is fundamental to scaling future ubiquitous sensing systems, yet the substantial power demands and high cost of mmWave hardware have long stood in the way of practical deployment. This paper presents the first mmWave full-duplex backscatter tag architecture, charting a genuinely low-cost path toward high-performance mmWave connectivity and localization for ISAC systems. The proposed tag operates at ranges beyond 45m on the uplink and beyond 200m on the downlink, delivering 20x the reach of state-of-the-art systems while being over 100x cheaper than existing mmWave backscatter platforms. Enabling this leap is a novel low-power regenerative amplifier that provides 30 dB of gain while consuming only 30 mW, paired with a regenerative rectifier that achieves state-of-the-art sensitivity down to -60 dBm. We integrate our circuits on a compact PCB and evaluate it across diverse uplink and downlink scenarios, where it achieves an downlink BER of $10^{-1}$ at 200 meters and a uplink BER of $10^{-2}$ at 45 meters, demonstrating resilient, high-quality communication even at extended ranges.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u6beb\u7c73\u6ce2\u5168\u53cc\u5de5\u53cd\u5411\u6563\u5c04\u6807\u7b7e\u67b6\u6784\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u9ad8\u6027\u80fd\u6beb\u7c73\u6ce2\u8fde\u63a5\u548c\u5b9a\u4f4d\uff0c\u901a\u4fe1\u8ddd\u79bb\u8fbe\u4e0a\u884c45\u7c73/\u4e0b\u884c200\u7c73\uff0c\u6bd4\u73b0\u6709\u7cfb\u7edf\u63d0\u534720\u500d\u8ddd\u79bb\u4e14\u6210\u672c\u964d\u4f4e100\u500d\u3002", "motivation": "\u6beb\u7c73\u6ce2\u901a\u4fe1\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\u9762\u4e34\u9ad8\u529f\u8017\u548c\u9ad8\u6210\u672c\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u672a\u6765\u6cdb\u5728\u611f\u77e5\u7cfb\u7edf\u7684\u89c4\u6a21\u5316\u90e8\u7f72\uff0c\u9700\u8981\u5bfb\u627e\u4f4e\u6210\u672c\u9ad8\u6027\u80fd\u7684\u6beb\u7c73\u6ce2\u8fde\u63a5\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u6beb\u7c73\u6ce2\u5168\u53cc\u5de5\u53cd\u5411\u6563\u5c04\u6807\u7b7e\u67b6\u6784\uff0c\u91c7\u7528\u65b0\u578b\u4f4e\u529f\u8017\u518d\u751f\u653e\u5927\u5668\uff0830dB\u589e\u76ca\uff0c\u4ec530mW\u529f\u8017\uff09\u548c\u518d\u751f\u6574\u6d41\u5668\uff08\u7075\u654f\u5ea6\u8fbe-60dBm\uff09\uff0c\u96c6\u6210\u5728\u7d27\u51d1PCB\u4e0a\u3002", "result": "\u6807\u7b7e\u5728200\u7c73\u4e0b\u884c\u8ddd\u79bb\u5b9e\u73b010\u207b\u00b9\u8bef\u7801\u7387\uff0c\u572845\u7c73\u4e0a\u884c\u8ddd\u79bb\u5b9e\u73b010\u207b\u00b2\u8bef\u7801\u7387\uff0c\u901a\u4fe1\u8ddd\u79bb\u6bd4\u73b0\u6709\u7cfb\u7edf\u63d0\u534720\u500d\uff0c\u6210\u672c\u964d\u4f4e100\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6beb\u7c73\u6ce2\u5168\u53cc\u5de5\u53cd\u5411\u6563\u5c04\u6807\u7b7e\u67b6\u6784\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u771f\u6b63\u4f4e\u6210\u672c\u7684\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u6beb\u7c73\u6ce2\u8fde\u63a5\u548c\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u6beb\u7c73\u6ce2\u786c\u4ef6\u7684\u9ad8\u529f\u8017\u548c\u9ad8\u6210\u672c\u95ee\u9898\u3002"}}
{"id": "2601.17486", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17486", "abs": "https://arxiv.org/abs/2601.17486", "authors": ["Zhiyuan Zhang", "Yu She"], "title": "EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds", "comment": "Project website: https://ZhangZhiyuanZhang.github.io/equiform-website/ Code will be released", "summary": "Visual imitation learning with 3D point clouds has advanced robotic manipulation by providing geometry-aware, appearance-invariant observations. However, point cloud-based policies remain highly sensitive to sensor noise, pose perturbations, and occlusion-induced artifacts, which distort geometric structure and break the equivariance assumptions required for robust generalization. Existing equivariant approaches primarily encode symmetry constraints into neural architectures, but do not explicitly correct noise-induced geometric deviations or enforce equivariant consistency in learned representations. We introduce EquiForm, a noise-robust SE(3)-equivariant policy learning framework for point cloud-based manipulation. EquiForm formalizes how noise-induced geometric distortions lead to equivariance deviations in observation-to-action mappings, and introduces a geometric denoising module to restore consistent 3D structure under noisy or incomplete observations. In addition, we propose a contrastive equivariant alignment objective that enforces representation consistency under both rigid transformations and noise perturbations. Built upon these components, EquiForm forms a flexible policy learning pipeline that integrates noise-robust geometric reasoning with modern generative models. We evaluate EquiForm on 16 simulated tasks and 4 real-world manipulation tasks across diverse objects and scene layouts. Compared to state-of-the-art point cloud imitation learning methods, EquiForm achieves an average improvement of 17.2% in simulation and 28.1% in real-world experiments, demonstrating strong noise robustness and spatial generalization.", "AI": {"tldr": "EquiForm\uff1a\u4e00\u79cd\u9762\u5411\u70b9\u4e91\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u566a\u58f0\u9c81\u68d2SE(3)\u7b49\u53d8\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u53bb\u566a\u6a21\u5757\u548c\u5bf9\u6bd4\u7b49\u53d8\u5bf9\u9f50\u76ee\u6807\uff0c\u89e3\u51b3\u4f20\u611f\u5668\u566a\u58f0\u3001\u59ff\u6001\u6270\u52a8\u548c\u906e\u6321\u5bfc\u81f4\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u70b9\u4e91\u6a21\u4eff\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u70b9\u4e91\u7684\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u867d\u7136\u63d0\u4f9b\u4e86\u51e0\u4f55\u611f\u77e5\u3001\u5916\u89c2\u4e0d\u53d8\u7684\u89c2\u6d4b\uff0c\u4f46\u70b9\u4e91\u7b56\u7565\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u3001\u59ff\u6001\u6270\u52a8\u548c\u906e\u6321\u5f15\u8d77\u7684\u4f2a\u5f71\u9ad8\u5ea6\u654f\u611f\u3002\u8fd9\u4e9b\u56e0\u7d20\u4f1a\u626d\u66f2\u51e0\u4f55\u7ed3\u6784\uff0c\u7834\u574f\u9c81\u68d2\u6cdb\u5316\u6240\u9700\u7684\u7b49\u53d8\u6027\u5047\u8bbe\u3002\u73b0\u6709\u7b49\u53d8\u65b9\u6cd5\u4e3b\u8981\u5c06\u5bf9\u79f0\u7ea6\u675f\u7f16\u7801\u5230\u795e\u7ecf\u67b6\u6784\u4e2d\uff0c\u4f46\u672a\u80fd\u663e\u5f0f\u7ea0\u6b63\u566a\u58f0\u5f15\u8d77\u7684\u51e0\u4f55\u504f\u5dee\u6216\u5f3a\u5236\u5b66\u4e60\u8868\u793a\u4e2d\u7684\u7b49\u53d8\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faEquiForm\u6846\u67b6\uff1a1) \u5f62\u5f0f\u5316\u5206\u6790\u566a\u58f0\u5f15\u8d77\u7684\u51e0\u4f55\u5931\u771f\u5982\u4f55\u5bfc\u81f4\u89c2\u6d4b\u5230\u52a8\u4f5c\u6620\u5c04\u7684\u7b49\u53d8\u6027\u504f\u5dee\uff1b2) \u5f15\u5165\u51e0\u4f55\u53bb\u566a\u6a21\u5757\uff0c\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u89c2\u6d4b\u4e0b\u6062\u590d\u4e00\u81f4\u76843D\u7ed3\u6784\uff1b3) \u63d0\u51fa\u5bf9\u6bd4\u7b49\u53d8\u5bf9\u9f50\u76ee\u6807\uff0c\u5728\u521a\u6027\u53d8\u6362\u548c\u566a\u58f0\u6270\u52a8\u4e0b\u5f3a\u5236\u8868\u793a\u4e00\u81f4\u6027\uff1b4) \u6784\u5efa\u7075\u6d3b\u7684\u7b56\u7565\u5b66\u4e60\u6d41\u7a0b\uff0c\u5c06\u566a\u58f0\u9c81\u68d2\u7684\u51e0\u4f55\u63a8\u7406\u4e0e\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u76f8\u7ed3\u5408\u3002", "result": "\u572816\u4e2a\u6a21\u62df\u4efb\u52a1\u548c4\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7269\u4f53\u548c\u573a\u666f\u5e03\u5c40\u3002\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u70b9\u4e91\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0cEquiForm\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u5e73\u5747\u63d0\u534717.2%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5e73\u5747\u63d0\u534728.1%\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u566a\u58f0\u9c81\u68d2\u6027\u548c\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EquiForm\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u566a\u58f0\u5f15\u8d77\u7684\u51e0\u4f55\u5931\u771f\u548c\u5f3a\u5236\u7b49\u53d8\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u6a21\u4eff\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4e3a\u566a\u58f0\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u51e0\u4f55\u53bb\u566a\u4e0e\u7b49\u53d8\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4e3a\u70b9\u4e91\u7b56\u7565\u7684\u5b9e\u9645\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.17548", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17548", "abs": "https://arxiv.org/abs/2601.17548", "authors": ["Narek Maloyan", "Dmitry Namiot"], "title": "Prompt Injection Attacks on Agentic Coding Assistants: A Systematic Analysis of Vulnerabilities in Skills, Tools, and Protocol Ecosystems", "comment": null, "summary": "The proliferation of agentic AI coding assistants, including Claude Code, GitHub Copilot, Cursor, and emerging skill-based architectures, has fundamentally transformed software development workflows. These systems leverage Large Language Models (LLMs) integrated with external tools, file systems, and shell access through protocols like the Model Context Protocol (MCP). However, this expanded capability surface introduces critical security vulnerabilities. In this \\textbf{Systematization of Knowledge (SoK)} paper, we present a comprehensive analysis of prompt injection attacks targeting agentic coding assistants. We propose a novel three-dimensional taxonomy categorizing attacks across \\textit{delivery vectors}, \\textit{attack modalities}, and \\textit{propagation behaviors}. Our meta-analysis synthesizes findings from 78 recent studies (2021--2026), consolidating evidence that attack success rates against state-of-the-art defenses exceed 85\\% when adaptive attack strategies are employed. We systematically catalog 42 distinct attack techniques spanning input manipulation, tool poisoning, protocol exploitation, multimodal injection, and cross-origin context poisoning. Through critical analysis of 18 defense mechanisms reported in prior work, we identify that most achieve less than 50\\% mitigation against sophisticated adaptive attacks. We contribute: (1) a unified taxonomy bridging disparate attack classifications, (2) the first systematic analysis of skill-based architecture vulnerabilities with concrete exploit chains, and (3) a defense-in-depth framework grounded in the limitations we identify. Our findings indicate that the security community must treat prompt injection as a first-class vulnerability class requiring architectural-level mitigations rather than ad-hoc filtering approaches.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u9488\u5bf9\u667a\u80fd\u7f16\u7801\u52a9\u624b\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u4e09\u7ef4\u5206\u7c7b\u6cd5\uff0c\u7efc\u540878\u9879\u7814\u7a76\u663e\u793a\u73b0\u6709\u9632\u5fa1\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u7684\u7f13\u89e3\u7387\u4f4e\u4e8e50%\uff0c\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc785%\uff0c\u5e76\u63d0\u51fa\u4e86\u6df1\u5ea6\u9632\u5fa1\u6846\u67b6\u3002", "motivation": "\u968f\u7740Claude Code\u3001GitHub Copilot\u3001Cursor\u7b49\u667a\u80fd\u7f16\u7801\u52a9\u624b\u7684\u666e\u53ca\uff0c\u8fd9\u4e9b\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u4e0e\u5916\u90e8\u5de5\u5177\u3001\u6587\u4ef6\u7cfb\u7edf\u3001shell\u8bbf\u95ee\u96c6\u6210\u7684\u7cfb\u7edf\u6269\u5c55\u4e86\u80fd\u529b\u9762\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\u3002\u9700\u8981\u7cfb\u7edf\u5206\u6790\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4e3a\u5b89\u5168\u793e\u533a\u63d0\u4f9b\u7edf\u4e00\u5206\u7c7b\u548c\u9632\u5fa1\u6846\u67b6\u3002", "method": "\u672c\u6587\u662f\u7cfb\u7edf\u5316\u77e5\u8bc6(SoK)\u8bba\u6587\uff0c\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u4e09\u7ef4\u5206\u7c7b\u6cd5\uff0c\u4ece\u4f20\u64ad\u5411\u91cf\u3001\u653b\u51fb\u6a21\u5f0f\u548c\u4f20\u64ad\u884c\u4e3a\u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u653b\u51fb\u8fdb\u884c\u5206\u7c7b\u3002\u5bf978\u9879\u8fd1\u671f\u7814\u7a76(2021-2026)\u8fdb\u884c\u4e86\u5143\u5206\u6790\uff0c\u7cfb\u7edf\u6574\u7406\u4e8642\u79cd\u4e0d\u540c\u7684\u653b\u51fb\u6280\u672f\uff0c\u5e76\u6279\u5224\u6027\u5206\u6790\u4e8618\u79cd\u9632\u5fa1\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u91c7\u7528\u81ea\u9002\u5e94\u653b\u51fb\u7b56\u7565\u65f6\uff0c\u9488\u5bf9\u6700\u5148\u8fdb\u9632\u5fa1\u7684\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc785%\u3002\u5927\u591a\u6570\u9632\u5fa1\u673a\u5236\u5bf9\u590d\u6742\u81ea\u9002\u5e94\u653b\u51fb\u7684\u7f13\u89e3\u7387\u4f4e\u4e8e50%\u3002\u8bc6\u522b\u4e86\u6280\u80fd\u67b6\u6784\u6f0f\u6d1e\u7684\u5177\u4f53\u5229\u7528\u94fe\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5df2\u8bc6\u522b\u9650\u5236\u7684\u6df1\u5ea6\u9632\u5fa1\u6846\u67b6\u3002", "conclusion": "\u5b89\u5168\u793e\u533a\u5e94\u5c06\u63d0\u793a\u6ce8\u5165\u89c6\u4e3a\u9700\u8981\u67b6\u6784\u7ea7\u7f13\u89e3\u63aa\u65bd\u7684\u4e00\u7c7b\u9996\u8981\u6f0f\u6d1e\uff0c\u800c\u975e\u4e34\u65f6\u8fc7\u6ee4\u65b9\u6cd5\u3002\u672c\u6587\u8d21\u732e\u5305\u62ec\uff1a\u7edf\u4e00\u653b\u51fb\u5206\u7c7b\u6cd5\u3001\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u6280\u80fd\u67b6\u6784\u6f0f\u6d1e\u3001\u57fa\u4e8e\u9650\u5236\u7684\u6df1\u5ea6\u9632\u5fa1\u6846\u67b6\u3002"}}
{"id": "2601.17434", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17434", "abs": "https://arxiv.org/abs/2601.17434", "authors": ["Xiaokang Lei", "Ching Christie Pang", "Yuyang Jiang", "Xin Tong", "Pan Hui"], "title": "Co-Designing Digital Humans for Online Learning: A Framework for Human-AI Pedagogical Integration", "comment": null, "summary": "Artificial intelligence (AI) and large language models (LLMs) are reshaping education, with virtual avatars emerging as digital teachers capable of enhancing engagement, sustaining attention, and addressing instructor shortages. Aligned with the Sustainable Development Goals (SDGs) for equitable quality education, these technologies hold promise yet lack clear guidelines for effective design and implementation in online learning. To fill this gap, we introduce a framework specifying when, what, and how digital teachers should be integrated. Our study combines (1) a design space analysis of 87 works across AI, educational technology, design, and HCI, (2) a survey of 132 learners' practices and preferences, and (3) three co-design workshops with 18 experts from pedagogy, design, and AI. It provides actionable guidance for educators, designers, and HCI researchers, advancing opportunities to build more engaging, equitable, and effective online learning environments powered by digital teachers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b57\u6559\u5e08\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7a7a\u95f4\u5206\u6790\u3001\u5b66\u4e60\u8005\u8c03\u67e5\u548c\u4e13\u5bb6\u534f\u540c\u8bbe\u8ba1\uff0c\u4e3aAI\u9a71\u52a8\u7684\u865a\u62df\u6559\u5e08\u5728\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "AI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6b63\u5728\u91cd\u5851\u6559\u80b2\uff0c\u865a\u62df\u5316\u8eab\u4f5c\u4e3a\u6570\u5b57\u6559\u5e08\u80fd\u591f\u63d0\u5347\u53c2\u4e0e\u5ea6\u3001\u7ef4\u6301\u6ce8\u610f\u529b\u5e76\u89e3\u51b3\u6559\u5e08\u77ed\u7f3a\u95ee\u9898\u3002\u867d\u7136\u8fd9\u4e9b\u6280\u672f\u4e0e\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u4e2d\u7684\u516c\u5e73\u4f18\u8d28\u6559\u80b2\u76ee\u6807\u4e00\u81f4\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u7684\u8bbe\u8ba1\u548c\u5b9e\u65bd\u6307\u5357\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e09\u90e8\u5206\u65b9\u6cd5\uff1a(1) \u5bf987\u9879\u6765\u81eaAI\u3001\u6559\u80b2\u6280\u672f\u3001\u8bbe\u8ba1\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u5de5\u4f5c\u8fdb\u884c\u8bbe\u8ba1\u7a7a\u95f4\u5206\u6790\uff1b(2) \u5bf9132\u540d\u5b66\u4e60\u8005\u7684\u5b9e\u8df5\u548c\u504f\u597d\u8fdb\u884c\u8c03\u67e5\uff1b(3) \u4e0e18\u4f4d\u6765\u81ea\u6559\u80b2\u5b66\u3001\u8bbe\u8ba1\u548cAI\u9886\u57df\u7684\u4e13\u5bb6\u8fdb\u884c\u4e09\u6b21\u534f\u540c\u8bbe\u8ba1\u7814\u8ba8\u4f1a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u4f53\u6846\u67b6\uff0c\u660e\u786e\u4e86\u6570\u5b57\u6559\u5e08\u5e94\u8be5\u5728\u4f55\u65f6\u3001\u4ee5\u4f55\u79cd\u5185\u5bb9\u548c\u65b9\u5f0f\u96c6\u6210\u5230\u5728\u7ebf\u5b66\u4e60\u4e2d\uff0c\u4e3a\u6559\u80b2\u8005\u3001\u8bbe\u8ba1\u5e08\u548c\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u591a\u65b9\u6cd5\u7814\u7a76\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u5438\u5f15\u529b\u3001\u66f4\u516c\u5e73\u3001\u66f4\u6709\u6548\u7684\u6570\u5b57\u6559\u5e08\u9a71\u52a8\u7684\u5728\u7ebf\u5b66\u4e60\u73af\u5883\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u63a8\u52a8\u4e86\u6559\u80b2\u6280\u672f\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.17762", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17762", "abs": "https://arxiv.org/abs/2601.17762", "authors": ["Zelong Zheng", "Jiayuan Zhou", "Xing Hu", "Yi Gao", "Shengyi Pan"], "title": "Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities", "comment": null, "summary": "Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.", "AI": {"tldr": "MAVM\u662f\u4e00\u4e2a\u7528\u4e8e\u7aef\u5230\u7aef\u590d\u73b0\u6f0f\u6d1e\u7ba1\u7406\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6f0f\u6d1e\u77e5\u8bc6\u5e93\u3001\u68c0\u6d4b\u3001\u786e\u8ba4\u3001\u4fee\u590d\u548c\u9a8c\u8bc1\u4e94\u4e2a\u7ec4\u4ef6\uff0c\u5229\u7528\u5386\u53f2\u6f0f\u6d1e\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u68c0\u7d22\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u548c\u4fee\u590d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6f0f\u6d1e\u7ba1\u7406\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4f20\u7edf\u9759\u6001\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u6355\u6349\u8de8\u51fd\u6570/\u6a21\u5757\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\uff1b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u68c0\u7d22\u548c\u5229\u7528\u80fd\u529b\uff1b\u5386\u53f2\u6f0f\u6d1e\u77e5\u8bc6\u5728\u4ee3\u7801\u590d\u7528\u548c\u5171\u4eab\u903b\u8f91\u5bfc\u81f4\u7684\u590d\u73b0\u6f0f\u6d1e\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5229\u7528\u5386\u53f2\u77e5\u8bc6\u5e76\u514b\u670d\u4e0a\u4e0b\u6587\u9650\u5236\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMAVM\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u96c6\u6210\u7ec4\u4ef6\uff1a1) \u4ece\u516c\u5f00\u62ab\u9732\u6f0f\u6d1e\u6784\u5efa\u77e5\u8bc6\u5e93\uff1b2) \u8bbe\u8ba1\u4e0a\u4e0b\u6587\u68c0\u7d22\u5de5\u5177\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u63d0\u53d6\u548c\u63a8\u7406\u4ed3\u5e93\u7ea7\u4fe1\u606f\uff1b3) \u57fa\u4e8e\u667a\u80fd\u4f53\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5b89\u5168\u5de5\u4f5c\u6d41\u7a0b\u3002\u6846\u67b6\u5c06\u6f0f\u6d1e\u77e5\u8bc6\u5e93\u3001\u68c0\u6d4b\u3001\u786e\u8ba4\u3001\u4fee\u590d\u548c\u9a8c\u8bc1\u6574\u5408\u5230\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u4e2d\u3002", "result": "\u5728\u5305\u542b78\u4e2a\u771f\u5b9e\u4e16\u754c\u8865\u4e01\u79fb\u690d\u6848\u4f8b\uff08\u8986\u76d6114\u4e2a\u51fd\u6570\u7ea7\u8fc1\u79fb\uff09\u7684\u6570\u636e\u96c6\u4e0a\uff0cMAVM\u6210\u529f\u68c0\u6d4b\u5e76\u4fee\u590d\u4e8651\u4e2a\u771f\u5b9e\u6f0f\u6d1e\u3002\u5728\u4fee\u590d\u51c6\u786e\u7387\u65b9\u9762\uff0cMAVM\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8631.9%-45.2%\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MAVM\u901a\u8fc7\u6574\u5408\u5386\u53f2\u6f0f\u6d1e\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u68c0\u7d22\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6f0f\u6d1e\u7ba1\u7406\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u548c\u4fee\u590d\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u6a21\u62df\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u5b89\u5168\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e3a\u590d\u73b0\u6f0f\u6d1e\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17507", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17507", "abs": "https://arxiv.org/abs/2601.17507", "authors": ["Yutong Shen", "Hangxu Liu", "Kailin Pei", "Ruizhe Xia", "Tongtong Feng"], "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions", "comment": "8 pages, 4 figures, Submitted to ICLR 2026 World Model Workshop", "summary": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/", "AI": {"tldr": "MetaWorld\u662f\u4e00\u4e2a\u5206\u5c42\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u4e13\u5bb6\u7b56\u7565\u8f6c\u79fb\u6574\u5408\u8bed\u4e49\u89c4\u5212\u548c\u7269\u7406\u63a7\u5236\uff0c\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u8bed\u4e49-\u7269\u7406\u9e3f\u6c9f\u95ee\u9898", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\u53d7\u9650\u4e8e\u8bed\u4e49-\u7269\u7406\u9e3f\u6c9f\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5c40\u9650\uff1a\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\u4f4e\u3001\u6a21\u4eff\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7269\u7406\u4e0d\u4e00\u81f4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6574\u5408\u8bed\u4e49\u89c4\u5212\u548c\u7269\u7406\u63a7\u5236\u7684\u9ad8\u6548\u6846\u67b6", "method": "\u63d0\u51fa\u5206\u5c42\u4e16\u754c\u6a21\u578bMetaWorld\uff0c\u5c06\u4efb\u52a1\u89e3\u8026\u4e3aVLM\u9a71\u52a8\u7684\u8bed\u4e49\u5c42\u548c\u5728\u7d27\u51d1\u72b6\u6001\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u7684\u6f5c\u5728\u52a8\u529b\u5b66\u6a21\u578b\u3002\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u548c\u8fd0\u52a8\u5148\u9a8c\u878d\u5408\u673a\u5236\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u4e13\u5bb6\u7b56\u7565\u5e93\u4f5c\u4e3a\u53ef\u8f6c\u79fb\u77e5\u8bc6\uff0c\u5b9e\u73b0\u9ad8\u6548\u5728\u7ebf\u9002\u5e94\u7684\u4e24\u9636\u6bb5\u6846\u67b6", "result": "\u5728Humanoid-Bench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMetaWorld\u5728\u4e16\u754c\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0c\u5728\u4efb\u52a1\u5b8c\u6210\u5ea6\u548c\u8fd0\u52a8\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18", "conclusion": "MetaWorld\u901a\u8fc7\u6574\u5408\u8bed\u4e49\u89c4\u5212\u548c\u7269\u7406\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u8bed\u4e49-\u7269\u7406\u9e3f\u6c9f\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6"}}
{"id": "2601.17458", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17458", "abs": "https://arxiv.org/abs/2601.17458", "authors": ["Shuhao Zhang", "Jiahe Dong", "Haoran Wang", "Chang Jiang", "Quan Li"], "title": "When Seconds Count: Designing Real-Time VR Interventions for Stress Inoculation Training in Novice Physicians", "comment": "In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI'26), April 13-17, 2026, Barcelona, Spain", "summary": "Surgical emergencies often trigger acute cognitive overload in novice physicians, impairing their decision-making under pressure. Although Virtual Reality-based Stress Inoculation Training (VR-SIT) shows promise, current systems fall short in delivering real-time, effective support during moments of peak stress. To bridge this gap, we first conducted a formative study (N=12) to uncover the core needs of novice physicians for immediate assistance under acute stress and identified three key intervention strategies: self-regulation aids, procedure guidance, and emotional/sensory support. Building on these insights, we designed and implemented a novel VR-SIT system that incorporates a just-in-time adaptive intervention framework, dynamically tailoring support to learners' cognitive and emotional states. We then validated these strategies in a user study (N=26). Our findings provide empirical evidence and design implications for next-generation VR medical training systems, supporting physicians in sustaining cognitive clarity and accurate decision-making in critical situations.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u865a\u62df\u73b0\u5b9e\u7684\u5e94\u6fc0\u63a5\u79cd\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u5373\u65f6\u81ea\u9002\u5e94\u5e72\u9884\u6846\u67b6\u4e3a\u65b0\u624b\u533b\u751f\u5728\u624b\u672f\u7d27\u6025\u60c5\u51b5\u4e0b\u63d0\u4f9b\u5b9e\u65f6\u652f\u6301\uff0c\u4ee5\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u8377\u5e76\u6539\u5584\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u624b\u672f\u7d27\u6025\u60c5\u51b5\u5e38\u5bfc\u81f4\u65b0\u624b\u533b\u751f\u8ba4\u77e5\u8fc7\u8f7d\uff0c\u5f71\u54cd\u5176\u538b\u529b\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\u3002\u73b0\u6709\u865a\u62df\u73b0\u5b9e\u5e94\u6fc0\u63a5\u79cd\u8bad\u7ec3\u7cfb\u7edf\u5728\u9ad8\u5cf0\u538b\u529b\u65f6\u523b\u7f3a\u4e4f\u5b9e\u65f6\u6709\u6548\u652f\u6301\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9996\u5148\u8fdb\u884c\u5f62\u6210\u6027\u7814\u7a76\uff08N=12\uff09\u8bc6\u522b\u65b0\u624b\u533b\u751f\u5728\u6025\u6027\u538b\u529b\u4e0b\u7684\u6838\u5fc3\u9700\u6c42\uff0c\u786e\u5b9a\u4e09\u79cd\u5173\u952e\u5e72\u9884\u7b56\u7565\uff1a\u81ea\u6211\u8c03\u8282\u8f85\u52a9\u3001\u7a0b\u5e8f\u6307\u5bfc\u548c\u60c5\u611f/\u611f\u5b98\u652f\u6301\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5305\u542b\u5373\u65f6\u81ea\u9002\u5e94\u5e72\u9884\u6846\u67b6\u7684\u65b0\u578bVR-SIT\u7cfb\u7edf\uff0c\u80fd\u591f\u6839\u636e\u5b66\u4e60\u8005\u7684\u8ba4\u77e5\u548c\u60c5\u7eea\u72b6\u6001\u52a8\u6001\u8c03\u6574\u652f\u6301\u3002", "result": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08N=26\uff09\u9a8c\u8bc1\u4e86\u4e09\u79cd\u5e72\u9884\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u4e0b\u4e00\u4ee3VR\u533b\u7597\u57f9\u8bad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bc1\u636e\u548c\u8bbe\u8ba1\u542f\u793a\uff0c\u652f\u6301\u533b\u751f\u5728\u5173\u952e\u60c5\u51b5\u4e0b\u4fdd\u6301\u8ba4\u77e5\u6e05\u6670\u548c\u51c6\u786e\u51b3\u7b56\u3002", "conclusion": "\u63d0\u51fa\u7684VR-SIT\u7cfb\u7edf\u901a\u8fc7\u5373\u65f6\u81ea\u9002\u5e94\u5e72\u9884\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u65b0\u624b\u533b\u751f\u5728\u624b\u672f\u7d27\u6025\u60c5\u51b5\u4e0b\u7ba1\u7406\u8ba4\u77e5\u8d1f\u8377\uff0c\u6539\u5584\u51b3\u7b56\u80fd\u529b\uff0c\u4e3a\u533b\u7597\u57f9\u8bad\u7cfb\u7edf\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2601.17076", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17076", "abs": "https://arxiv.org/abs/2601.17076", "authors": ["Jiajun Chen", "Yue Wu", "Kai Huang", "Wen Xi", "Yangyang Wu", "Xiaoye Miao", "Mengying Zhu", "Meng Xi", "Guanjie Cheng"], "title": "E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning", "comment": "11 pages", "summary": "Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \\emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \\textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \\textsf{E2PL} unifies two novel prompt designs: \\emph{task-tailored prompts} for class-incremental adaptation and \\emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \\emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \\emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \\textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at https://anonymous.4open.science/r/code-for-E2PL.", "AI": {"tldr": "E2PL\uff1a\u4e00\u79cd\u7528\u4e8e\u4e0d\u5b8c\u6574\u591a\u89c6\u56fe\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u9ad8\u6548\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5b9a\u5236\u63d0\u793a\u548c\u7f3a\u5931\u611f\u77e5\u63d0\u793a\u89e3\u51b3\u89c6\u56fe\u7f3a\u5931\u548c\u7c7b\u522b\u52a8\u6001\u6269\u5c55\u95ee\u9898\uff0c\u4f7f\u7528\u539f\u578b\u5f20\u91cf\u5316\u5c06\u53c2\u6570\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u81f3\u7ebf\u6027\u7ea7\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754cWeb\u5e94\u7528\u4e2d\u591a\u89c6\u56fe\u591a\u6807\u7b7e\u5206\u7c7b\u9762\u4e34\u89c6\u56fe\u7f3a\u5931\u548c\u7c7b\u522b\u52a8\u6001\u6269\u5c55\u4e24\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u9002\u5e94\u65b0\u7c7b\u522b\uff0c\u8981\u4e48\u5728\u5904\u7406\u7f3a\u5931\u89c6\u56fe\u6a21\u5f0f\u65f6\u53c2\u6570\u5448\u6307\u6570\u589e\u957f\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5728Web\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faE2PL\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u4efb\u52a1\u5b9a\u5236\u63d0\u793a\u7528\u4e8e\u7c7b\u589e\u91cf\u9002\u5e94\uff1b2\uff09\u7f3a\u5931\u611f\u77e5\u63d0\u793a\u7528\u4e8e\u7075\u6d3b\u6574\u5408\u4efb\u610f\u89c6\u56fe\u7f3a\u5931\u573a\u666f\uff1b3\uff09\u9ad8\u6548\u539f\u578b\u5f20\u91cf\u5316\u6a21\u5757\uff0c\u5229\u7528\u539f\u5b50\u5f20\u91cf\u5206\u89e3\u5c06\u63d0\u793a\u53c2\u6570\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u81f3\u7ebf\u6027\u7ea7\uff1b4\uff09\u52a8\u6001\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u663e\u5f0f\u5efa\u6a21\u4e0d\u540c\u7f3a\u5931\u89c6\u56fe\u6a21\u5f0f\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cE2PL\u5728\u6548\u679c\u548c\u6548\u7387\u65b9\u9762\u5747\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "E2PL\u901a\u8fc7\u7edf\u4e00\u7684\u63d0\u793a\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5b8c\u6574\u591a\u89c6\u56fe\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3aWeb\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17561", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17561", "abs": "https://arxiv.org/abs/2601.17561", "authors": ["Jincheol Ha", "Guillaume Hanrot", "Taeyeong Noh", "Jung Hee Cheon", "Jung Woo Kim", "Damien Stehl\u00e9"], "title": "Private Iris Recognition with High-Performance FHE", "comment": null, "summary": "Among biometric verification systems, irises stand out because they offer high accuracy even in large-scale databases. For example, the World ID project aims to provide authentication to all humans via iris recognition, with millions already registered. Storing such biometric data raises privacy concerns, which can be addressed using privacy-enhancing techniques.\n  Bloemen et al. describe a solution based on 2-out-of-3 Secret-Sharing Multiparty Computation (SS-MPC), for the World ID setup. In terms of security, unless an adversary corrupts 2~servers, the iris codes remain confidential and nothing leaks beyond the result of the computation. Their solution is able to match~$32$ users against a database of~$2^{22}$ iris codes in~$\\approx 2$s , using~24 H100 GPUs, more than 40~communication rounds and $81$GB/party of data transferred (the timing assumes a network speed above~3Tb/s).\n  In the present work, we explore the use of Threshold Fully Homomorphic Encryption (ThFHE) for the same task. The ThFHE solution brings a number of security advantages: no trusted setup, the encrypted database and queries can be public, the secret can be distributed among many parties, and active security can be added without significant performance degradation.\n  Our proof-of-concept implementation of the computation phase handles $32$~eyes against a database of $7\\cdot 2^{14}$ iris codes in~$\\approx 1.8$s ($\\approx 0.33s$ for 4 eyes against the same database), using 8 RTX-5090 GPUs. To this, one should add~2 to 3 rounds of communication (depending on deployment choice). We perform the matching using the CKKS (Th)FHE scheme. Our main technical ingredients are the use of recent progress on FHE-based linear algebra boosted using int8 GPU operations, and the introduction of a technique reducing the number of ciphertexts to be processed as early as possible.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4f7f\u7528\u95e8\u9650\u5168\u540c\u6001\u52a0\u5bc6\uff08ThFHE\uff09\u66ff\u4ee3\u79d8\u5bc6\u5171\u4eab\u591a\u65b9\u8ba1\u7b97\uff08SS-MPC\uff09\u8fdb\u884c\u8679\u819c\u8bc6\u522b\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u5f3a\u7684\u5b89\u5168\u4f18\u52bf\uff0c\u5305\u62ec\u65e0\u9700\u53ef\u4fe1\u8bbe\u7f6e\u3001\u516c\u5f00\u52a0\u5bc6\u6570\u636e\u5e93\u548c\u67e5\u8be2\u3001\u652f\u6301\u591a\u65b9\u79d8\u5bc6\u5206\u53d1\u4ee5\u53ca\u4e3b\u52a8\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u8679\u819c\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002\u73b0\u6709\u57fa\u4e8e\u79d8\u5bc6\u5171\u4eab\u591a\u65b9\u8ba1\u7b97\u7684\u65b9\u6848\u867d\u7136\u6709\u6548\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u9650\u5236\uff08\u9700\u8981\u81f3\u5c112\u4e2a\u670d\u52a1\u5668\u4e0d\u88ab\u653b\u7834\uff09\u3002ThFHE\u65b9\u6848\u63d0\u4f9b\u66f4\u5f3a\u7684\u5b89\u5168\u4f18\u52bf\uff1a\u65e0\u9700\u53ef\u4fe1\u8bbe\u7f6e\u3001\u52a0\u5bc6\u6570\u636e\u548c\u67e5\u8be2\u53ef\u516c\u5f00\u3001\u79d8\u5bc6\u53ef\u591a\u65b9\u5206\u53d1\u3001\u652f\u6301\u4e3b\u52a8\u5b89\u5168\u6027\u4e14\u6027\u80fd\u5f71\u54cd\u5c0f\u3002", "method": "\u91c7\u7528\u95e8\u9650\u5168\u540c\u6001\u52a0\u5bc6\uff08ThFHE\uff09\u65b9\u6848\uff0c\u5177\u4f53\u4f7f\u7528CKKS\uff08Th\uff09FHE\u52a0\u5bc6\u65b9\u6848\u3002\u5173\u952e\u6280\u672f\u5305\u62ec\uff1a\u5229\u7528\u57fa\u4e8eFHE\u7684\u7ebf\u6027\u4ee3\u6570\u6700\u65b0\u8fdb\u5c55\uff0c\u7ed3\u5408int8 GPU\u64cd\u4f5c\u52a0\u901f\uff1b\u5f15\u5165\u65e9\u671f\u51cf\u5c11\u5f85\u5904\u7406\u5bc6\u6587\u6570\u91cf\u7684\u6280\u672f\uff1b\u5b9e\u73b0\u8ba1\u7b97\u9636\u6bb5\u7684\u539f\u578b\u7cfb\u7edf\u3002", "result": "\u539f\u578b\u5b9e\u73b0\u663e\u793a\uff1a32\u4e2a\u8679\u819c\u4e0e7\u00b72^14\u4e2a\u8679\u819c\u4ee3\u7801\u6570\u636e\u5e93\u5339\u914d\u7ea6\u97001.8\u79d2\uff084\u4e2a\u8679\u819c\u5339\u914d\u76f8\u540c\u6570\u636e\u5e93\u7ea60.33\u79d2\uff09\uff0c\u4f7f\u75288\u4e2aRTX-5090 GPU\u3002\u901a\u4fe1\u8f6e\u6570\u4ec5\u97002-3\u8f6e\uff08\u53d6\u51b3\u4e8e\u90e8\u7f72\u9009\u62e9\uff09\uff0c\u76f8\u6bd4\u4e4b\u524d\u65b9\u6848\u768440\u591a\u8f6e\u901a\u4fe1\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "ThFHE\u65b9\u6848\u4e3a\u5927\u89c4\u6a21\u8679\u819c\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u9690\u79c1\u4fdd\u62a4\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u5b89\u5168\u4f18\u52bf\uff08\u65e0\u9700\u53ef\u4fe1\u8bbe\u7f6e\u3001\u516c\u5f00\u52a0\u5bc6\u6570\u636e\u3001\u652f\u6301\u4e3b\u52a8\u5b89\u5168\uff09\u548c\u826f\u597d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u901a\u4fe1\u6548\u7387\u65b9\u9762\u76f8\u6bd4SS-MPC\u65b9\u6848\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2601.17476", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17476", "abs": "https://arxiv.org/abs/2601.17476", "authors": ["Hellina Hailu Nigatu", "Farhana Shahid", "Vishal Sharma", "Abigail Oppong", "Michaelanne Thomas", "Syed Ishtiaque Ahmed"], "title": "UnWEIRDing Peer Review in Human Computer Interaction", "comment": "Accepted to CHI'26. Authors with (*) contributed equally to this work", "summary": "Peer review determines which scholarship is legitimized; however, review biases often disadvantage scholarship that diverges from the norm. Human-Computer Interaction (HCI) lacks a systemic inquiry into how such biases affect underrepresented Global South (GS) scholarship. To address this critical gap, we conducted four focus groups with 16 HCI researchers studying the GS. Participants reported experiencing reviews that confined them to development research, dismissed their theoretical contributions, and questioned situated knowledge from GS communities. Both as authors and reviewers, participants reported experiencing the epistemic burden of over-explaining why knowledge from GS communities matters. Further, they noted being tokenized as ``cultural experts'' when assigned to review papers and pointed out that the hidden curriculum of writing HCI papers often gatekeeps GS scholarship. Using epistemic oppression as a lens, we discuss how review practices marginalize GS scholarship and outline actionable strategies for nurturing equitable epistemological evaluation of HCI scholarship.", "AI": {"tldr": "HCI\u9886\u57df\u7f3a\u4e4f\u5bf9\u5ba1\u7a3f\u504f\u89c1\u5982\u4f55\u5f71\u54cd\u5168\u7403\u5357\u65b9\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u7814\u7a76\u53d1\u73b0\u5168\u7403\u5357\u65b9\u5b66\u8005\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u9762\u4e34\u53d1\u5c55\u7814\u7a76\u9650\u5236\u3001\u7406\u8bba\u8d21\u732e\u88ab\u5ffd\u89c6\u3001\u60c5\u5883\u77e5\u8bc6\u88ab\u8d28\u7591\u7b49\u7cfb\u7edf\u6027\u504f\u89c1", "motivation": "\u540c\u884c\u8bc4\u5ba1\u51b3\u5b9a\u4e86\u5b66\u672f\u7814\u7a76\u7684\u5408\u6cd5\u6027\uff0c\u4f46\u5ba1\u7a3f\u504f\u89c1\u5e38\u5e38\u4f7f\u504f\u79bb\u4e3b\u6d41\u7684\u7814\u7a76\u5904\u4e8e\u4e0d\u5229\u5730\u4f4d\u3002\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7f3a\u4e4f\u5bf9\u8fd9\u7c7b\u504f\u89c1\u5982\u4f55\u5f71\u54cd\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5168\u7403\u5357\u65b9\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u5b58\u5728\u5173\u952e\u7684\u7814\u7a76\u7a7a\u767d\u9700\u8981\u586b\u8865\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u7126\u70b9\u5c0f\u7ec4\uff0c\u5bf916\u540d\u7814\u7a76\u5168\u7403\u5357\u65b9\u7684\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u8005\u8fdb\u884c\u4e86\u8d28\u6027\u7814\u7a76\uff0c\u6536\u96c6\u4ed6\u4eec\u5728\u4f5c\u4e3a\u4f5c\u8005\u548c\u5ba1\u7a3f\u4eba\u65f6\u7684\u7ecf\u9a8c\u6570\u636e\u3002", "result": "\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u591a\u79cd\u7cfb\u7edf\u6027\u504f\u89c1\uff1a\u88ab\u9650\u5236\u5728\u5f00\u53d1\u7814\u7a76\u4e2d\u3001\u7406\u8bba\u8d21\u732e\u88ab\u5ffd\u89c6\u3001\u6765\u81ea\u5168\u7403\u5357\u65b9\u793e\u533a\u7684\u60c5\u5883\u77e5\u8bc6\u88ab\u8d28\u7591\u3002\u4ed6\u4eec\u8fd8\u7ecf\u5386\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff08\u9700\u8981\u8fc7\u5ea6\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5168\u7403\u5357\u65b9\u77e5\u8bc6\u91cd\u8981\uff09\u3001\u4f5c\u4e3a\"\u6587\u5316\u4e13\u5bb6\"\u88ab\u8c61\u5f81\u6027\u5229\u7528\uff0c\u4ee5\u53caHCI\u8bba\u6587\u5199\u4f5c\u7684\u9690\u6027\u8bfe\u7a0b\u963b\u788d\u5168\u7403\u5357\u65b9\u7814\u7a76\u3002", "conclusion": "\u4f7f\u7528\u8ba4\u77e5\u538b\u8feb\u4f5c\u4e3a\u5206\u6790\u6846\u67b6\uff0c\u8ba8\u8bba\u4e86\u5ba1\u7a3f\u5b9e\u8df5\u5982\u4f55\u8fb9\u7f18\u5316\u5168\u7403\u5357\u65b9\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4fc3\u8fdb\u4eba\u673a\u4ea4\u4e92\u5b66\u672f\u516c\u5e73\u8ba4\u77e5\u8bc4\u4f30\u7684\u53ef\u64cd\u4f5c\u7b56\u7565\u3002"}}
{"id": "2601.17090", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17090", "abs": "https://arxiv.org/abs/2601.17090", "authors": ["Noam Koren", "Rafael Moschopoulos", "Kira Radinsky", "Elad Hazan"], "title": "SFO: Learning PDE Operators via Spectral Filtering", "comment": null, "summary": "Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.", "AI": {"tldr": "SFO\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7b97\u5b50\uff0c\u4f7f\u7528\u901a\u7528\u8c31\u57fa(USB)\u53c2\u6570\u5316\u79ef\u5206\u6838\uff0c\u901a\u8fc7\u4ec5\u5b66\u4e60\u5feb\u901f\u8870\u51cf\u7279\u5f81\u503c\u7684\u8c31\u7cfb\u6570\u5b9e\u73b0\u9ad8\u6548\u8868\u793a\uff0c\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u5728\u6355\u6349PDE\u89e3\u6620\u5c04\u4e2d\u7684\u957f\u7a0b\u975e\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\u65f6\u6548\u7387\u4f4e\u4e0b\u3002\u4f5c\u8005\u53d1\u73b0\u5e73\u79fb\u4e0d\u53d8PDE\u79bb\u6563\u5316\u7684\u79bb\u6563\u683c\u6797\u51fd\u6570\u5177\u6709\u7a7a\u95f4\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7ed3\u6784\uff0c\u8fd9\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7b97\u5b50\u8868\u793a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u8c31\u6ee4\u6ce2\u7b97\u5b50(SFO)\uff0c\u4f7f\u7528\u4ece\u5e0c\u5c14\u4f2f\u7279\u77e9\u9635\u7279\u5f81\u6a21\u6001\u5bfc\u51fa\u7684\u56fa\u5b9a\u5168\u5c40\u6b63\u4ea4\u57fa(USB)\u53c2\u6570\u5316\u79ef\u5206\u6838\u3002\u7406\u8bba\u8bc1\u660e\u8fd9\u4e9b\u6838\u5728USB\u4e2d\u5177\u6709\u7d27\u51d1\u8fd1\u4f3c\uff0c\u901a\u8fc7\u4ec5\u5b66\u4e60\u5feb\u901f\u8870\u51cf\u7279\u5f81\u503c\u7684\u8c31\u7cfb\u6570\u5b9e\u73b0\u9ad8\u6548\u8868\u793a\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5(\u5305\u62ec\u53cd\u5e94\u6269\u6563\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u548c3D\u7535\u78c1\u5b66)\u4e2d\uff0cSFO\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u76f8\u5bf9\u4e8e\u5f3a\u57fa\u7ebf\u8bef\u5dee\u51cf\u5c11\u9ad8\u8fbe40%\uff0c\u540c\u65f6\u4f7f\u7528\u663e\u8457\u66f4\u5c11\u7684\u53c2\u6570\u3002", "conclusion": "SFO\u901a\u8fc7\u5229\u7528PDE\u89e3\u6620\u5c04\u7684\u8c31\u7ed3\u6784\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2601.17903", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17903", "abs": "https://arxiv.org/abs/2601.17903", "authors": ["Tolgahan Bardakci", "Andreas Faes", "Mutlu Beyazit", "Serge Demeyr"], "title": "Prompt-Based REST API Test Amplification in Industry: An Experience Report", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support software testing tasks, yet there is little evidence of their effectiveness for REST API testing in industrial settings. To address this gap, we replicate our earlier work on LLM-based REST API test amplification within an industrial context at one of the largest logistics companies in Belgium. We apply LLM-based test amplification to six representative endpoints of a production microservice embedded in a large-scale, security-sensitive system, where there is in-depth complexity in authentication, stateful behavior, and organizational constraints. Our experience shows that LLM-based test amplification remains practically useful in industry by increasing coverage and revealing various observations and anomalies.", "AI": {"tldr": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u8bc4\u4f30LLM\u7528\u4e8eREST API\u6d4b\u8bd5\u653e\u5927\u7684\u6709\u6548\u6027\uff0c\u5728\u6bd4\u5229\u65f6\u5927\u578b\u7269\u6d41\u516c\u53f8\u7684\u751f\u4ea7\u5fae\u670d\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u8f6f\u4ef6\u6d4b\u8bd5\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u5176\u5728\u5de5\u4e1a\u73af\u5883\u4e2dREST API\u6d4b\u8bd5\u6709\u6548\u6027\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u590d\u6742\u8ba4\u8bc1\u3001\u6709\u72b6\u6001\u884c\u4e3a\u548c\u4e25\u683c\u7ec4\u7ec7\u7ea6\u675f\u7684\u751f\u4ea7\u7cfb\u7edf\u4e2d\u3002", "method": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u590d\u5236\u5148\u524d\u5173\u4e8eLLM-based REST API\u6d4b\u8bd5\u653e\u5927\u7684\u7814\u7a76\uff0c\u5e94\u7528\u4e8e\u6bd4\u5229\u65f6\u5927\u578b\u7269\u6d41\u516c\u53f8\u751f\u4ea7\u5fae\u670d\u52a1\u7684\u516d\u4e2a\u4ee3\u8868\u6027\u7aef\u70b9\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u5927\u89c4\u6a21\u3001\u5b89\u5168\u654f\u611f\u3001\u590d\u6742\u8ba4\u8bc1\u548c\u6709\u72b6\u6001\u884c\u4e3a\u7b49\u7279\u70b9\u3002", "result": "LLM-based\u6d4b\u8bd5\u653e\u5927\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u4ecd\u7136\u5177\u6709\u5b9e\u9645\u6548\u7528\uff0c\u80fd\u591f\u63d0\u9ad8\u6d4b\u8bd5\u8986\u76d6\u7387\u5e76\u63ed\u793a\u5404\u79cd\u89c2\u5bdf\u7ed3\u679c\u548c\u5f02\u5e38\u60c5\u51b5\u3002", "conclusion": "LLM-based\u6d4b\u8bd5\u653e\u5927\u65b9\u6cd5\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u8ba4\u8bc1\u3001\u6709\u72b6\u6001\u884c\u4e3a\u548c\u7ec4\u7ec7\u7ea6\u675f\u7b49\u6311\u6218\uff0c\u4e3a\u751f\u4ea7\u7cfb\u7edf\u7684API\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.17620", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17620", "abs": "https://arxiv.org/abs/2601.17620", "authors": ["Eliron Rahimi", "Margarita Osadchy", "Orr Dunkelman"], "title": "Reconstructing Protected Biometric Templates from Binary Authentication Results", "comment": "Accepted at the International Joint Conference on Biometrics (IJCB) 2025", "summary": "Biometric data is considered to be very private and highly sensitive. As such, many methods for biometric template protection were considered over the years -- from biohashing and specially crafted feature extraction procedures, to the use of cryptographic solutions such as Fuzzy Commitments or the use of Fully Homomorphic Encryption (FHE).\n  A key question that arises is how much protection these solutions can offer when the adversary can inject samples, and observe the outputs of the system. While for systems that return the similarity score, one can use attacks such as hill-climbing, for systems where the adversary can only learn whether the authentication attempt was successful, this question remained open.\n  In this paper, we show that it is indeed possible to reconstruct the biometric template by just observing the success/failure of the authentication attempt (given the ability to inject a sufficient amount of templates). Our attack achieves negligible template reconstruction loss and enables full recovery of facial images through a generative inversion method, forming a pipeline from binary scores to high-resolution facial images that successfully pass the system more than 98\\% of the time. Our results, of course, are applicable for any protection mechanism that maintains the accuracy of the recognition.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u751f\u7269\u7279\u5f81\u6a21\u677f\u4fdd\u62a4\u7cfb\u7edf\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u4ec5\u901a\u8fc7\u89c2\u5bdf\u8ba4\u8bc1\u5c1d\u8bd5\u7684\u6210\u529f/\u5931\u8d25\u7ed3\u679c\uff08\u4e8c\u8fdb\u5236\u5206\u6570\uff09\u5c31\u80fd\u91cd\u5efa\u751f\u7269\u7279\u5f81\u6a21\u677f\uff0c\u5e76\u8fdb\u4e00\u6b65\u6062\u590d\u9ad8\u5206\u8fa8\u7387\u9762\u90e8\u56fe\u50cf\u3002", "motivation": "\u751f\u7269\u7279\u5f81\u6570\u636e\u5177\u6709\u9ad8\u5ea6\u9690\u79c1\u6027\u548c\u654f\u611f\u6027\uff0c\u73b0\u6709\u4fdd\u62a4\u65b9\u6cd5\uff08\u5982\u751f\u7269\u54c8\u5e0c\u3001\u6a21\u7cca\u627f\u8bfa\u3001\u5168\u540c\u6001\u52a0\u5bc6\u7b49\uff09\u5728\u653b\u51fb\u8005\u80fd\u591f\u6ce8\u5165\u6837\u672c\u5e76\u89c2\u5bdf\u7cfb\u7edf\u8f93\u51fa\u7684\u573a\u666f\u4e0b\uff0c\u5176\u4fdd\u62a4\u80fd\u529b\u5b58\u5728\u7591\u95ee\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u4ec5\u8fd4\u56de\u8ba4\u8bc1\u6210\u529f/\u5931\u8d25\u7ed3\u679c\u7684\u7cfb\u7edf\uff0c\u653b\u51fb\u53ef\u884c\u6027\u5c1a\u672a\u660e\u786e\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u653b\u51fb\u8005\u80fd\u591f\u6ce8\u5165\u8db3\u591f\u6570\u91cf\u6a21\u677f\u5e76\u89c2\u5bdf\u8ba4\u8bc1\u6210\u529f/\u5931\u8d25\u7ed3\u679c\u7684\u80fd\u529b\u3002\u901a\u8fc7\u4e8c\u8fdb\u5236\u5206\u6570\u91cd\u5efa\u751f\u7269\u7279\u5f81\u6a21\u677f\uff0c\u7136\u540e\u4f7f\u7528\u751f\u6210\u5f0f\u53cd\u6f14\u65b9\u6cd5\u6062\u590d\u9ad8\u5206\u8fa8\u7387\u9762\u90e8\u56fe\u50cf\u3002", "result": "\u653b\u51fb\u5b9e\u73b0\u4e86\u53ef\u5ffd\u7565\u7684\u6a21\u677f\u91cd\u5efa\u635f\u5931\uff0c\u80fd\u591f\u5b8c\u5168\u6062\u590d\u9762\u90e8\u56fe\u50cf\uff0c\u5f62\u6210\u4ece\u4e8c\u8fdb\u5236\u5206\u6570\u5230\u9ad8\u5206\u8fa8\u7387\u9762\u90e8\u56fe\u50cf\u7684\u5b8c\u6574\u653b\u51fb\u6d41\u7a0b\u3002\u6062\u590d\u7684\u56fe\u50cf\u5728\u7cfb\u7edf\u4e2d\u8ba4\u8bc1\u6210\u529f\u7387\u8d85\u8fc798%\u3002", "conclusion": "\u8be5\u653b\u51fb\u9002\u7528\u4e8e\u4efb\u4f55\u4fdd\u6301\u8bc6\u522b\u51c6\u786e\u6027\u7684\u4fdd\u62a4\u673a\u5236\uff0c\u63ed\u793a\u4e86\u4ec5\u4f9d\u8d56\u8ba4\u8bc1\u6210\u529f/\u5931\u8d25\u7ed3\u679c\u7684\u751f\u7269\u7279\u5f81\u4fdd\u62a4\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u8fd9\u7c7b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2601.17553", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17553", "abs": "https://arxiv.org/abs/2601.17553", "authors": ["Amin Mohamed", "Hamza Abdelmoreed", "Mohamed Ehab", "Youssef Shawky", "Mayada Hadhoud", "Ahmad Al-Kabbany"], "title": "TOSHFA: A Mobile VR-Based System for Pose-Guided Exercise Rehabilitation for Low Back Pain", "comment": null, "summary": "Low back pain (LBP) is a pervasive global health challenge, affecting approximately 80% of adults and frequently progressing into chronic or recurrent episodes. While exercise therapy is a primary clinical intervention, traditional at-home programs suffer from low adherence rates and the absence of professional supervision. This study introduces TOSHFA, an accessible mobile VR-based rehabilitation system that bridges this gap by combining computer vision with affordable hardware. The system utilizes a laptop webcam to perform real-time pose estimation via the MediaPipe framework, tracking 33 skeletal landmarks to provide immediate biofeedback. This data is streamed via low-latency UDP protocols to a smartphone mounted in a cardboard-style VR headset, where patients interact with a gamified 3D environment. A pilot study with 20 participants evaluated the system's performance and user engagement. Quantitative results yielded a mean System Usability Scale (SUS) score of 47.4, indicating marginal usability and a need for interface optimization. However, Game Experience Questionnaire (GEQ) data revealed high scores in positive affect and enjoyment, suggesting that the gamification elements--such as coin rewards and streak tracking--successfully maintained user motivation despite technical friction. These findings validate the feasibility of a smartphone-based tele-rehabilitation model and establish a technical foundation for future clinical trials involving multi-exercise protocols.", "AI": {"tldr": "TOSHFA\uff1a\u57fa\u4e8e\u79fb\u52a8VR\u7684\u4f4e\u6210\u672c\u5eb7\u590d\u7cfb\u7edf\uff0c\u4f7f\u7528\u7535\u8111\u6444\u50cf\u5934\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u667a\u80fd\u624b\u673aVR\u5934\u663e\u63d0\u4f9b\u6e38\u620f\u5316\u5eb7\u590d\u8bad\u7ec3\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u4f46\u9700\u754c\u9762\u4f18\u5316\u3002", "motivation": "\u8170\u75db\u662f\u5168\u7403\u666e\u904d\u5065\u5eb7\u95ee\u9898\uff0c\u4f20\u7edf\u5bb6\u5ead\u5eb7\u590d\u8bad\u7ec3\u5b58\u5728\u4f9d\u4ece\u6027\u4f4e\u3001\u7f3a\u4e4f\u4e13\u4e1a\u76d1\u7763\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u8bbf\u95ee\u3001\u4f4e\u6210\u672c\u4e14\u80fd\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u7684\u5eb7\u590d\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86TOSHFA\u7cfb\u7edf\uff1a\u4f7f\u7528\u7b14\u8bb0\u672c\u7535\u8111\u6444\u50cf\u5934\u901a\u8fc7MediaPipe\u6846\u67b6\u5b9e\u65f6\u4f30\u8ba133\u4e2a\u9aa8\u9abc\u5173\u952e\u70b9\u59ff\u6001\uff0c\u901a\u8fc7\u4f4e\u5ef6\u8fdfUDP\u534f\u8bae\u5c06\u6570\u636e\u6d41\u4f20\u8f93\u5230\u667a\u80fd\u624b\u673aVR\u5934\u663e\uff0c\u60a3\u8005\u5728\u4e0e\u6e38\u620f\u53163D\u73af\u5883\u4e92\u52a8\u4e2d\u8fdb\u884c\u5eb7\u590d\u8bad\u7ec3\u3002", "result": "20\u540d\u53c2\u4e0e\u8005\u7684\u8bd5\u70b9\u7814\u7a76\u663e\u793a\uff1a\u7cfb\u7edf\u53ef\u7528\u6027\u91cf\u8868(SUS)\u5e73\u5747\u5f97\u520647.4\uff0c\u8868\u660e\u53ef\u7528\u6027\u4e00\u822c\u9700\u754c\u9762\u4f18\u5316\uff1b\u6e38\u620f\u4f53\u9a8c\u95ee\u5377(GEQ)\u5728\u79ef\u6781\u60c5\u611f\u548c\u4eab\u53d7\u5ea6\u65b9\u9762\u5f97\u5206\u8f83\u9ad8\uff0c\u8868\u660e\u6e38\u620f\u5316\u5143\u7d20\uff08\u91d1\u5e01\u5956\u52b1\u3001\u8fde\u7eed\u8bb0\u5f55\uff09\u80fd\u6709\u6548\u7ef4\u6301\u7528\u6237\u52a8\u673a\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u8fdc\u7a0b\u5eb7\u590d\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u5305\u542b\u591a\u8fd0\u52a8\u65b9\u6848\u7684\u4e34\u5e8a\u8bd5\u9a8c\u5efa\u7acb\u4e86\u6280\u672f\u57fa\u7840\uff0c\u6e38\u620f\u5316\u5143\u7d20\u80fd\u6709\u6548\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4f46\u9700\u8981\u6539\u8fdb\u754c\u9762\u8bbe\u8ba1\u4ee5\u63d0\u5347\u6574\u4f53\u53ef\u7528\u6027\u3002"}}
{"id": "2601.17091", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17091", "abs": "https://arxiv.org/abs/2601.17091", "authors": ["Ole St\u00fcven", "Keno Moenck", "Thorsten Sch\u00fcppstuhl"], "title": "CUROCKET: Optimizing ROCKET for GPU", "comment": null, "summary": "ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.", "code_url": "https://github.com/oleeven/CUROCKET", "code_stars": 1, "code_last_update": "2026-01-27", "AI": {"tldr": "CUROCKET\uff1a\u4e00\u79cd\u5728GPU\u4e0a\u9ad8\u6548\u6267\u884cROCKET\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7b97\u6cd5\u7684\u5b9e\u73b0\uff0c\u76f8\u6bd4CPU\u7248\u672c\u5b9e\u73b0\u9ad8\u8fbe11\u500d\u7684\u6bcf\u74e6\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u3002", "motivation": "ROCKET\u7b97\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u4e3b\u8981\u5c40\u9650\u4e8eCPU\u6267\u884c\u3002\u5377\u79ef\u64cd\u4f5c\u9ad8\u5ea6\u53ef\u5e76\u884c\u5316\uff0c\u9002\u5408GPU\u52a0\u901f\uff0c\u4f46\u7531\u4e8eROCKET\u4f7f\u7528\u975e\u5747\u5300\u5377\u79ef\u6838\uff0c\u6807\u51c6GPU\u5377\u79ef\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u5728GPU\u4e0a\u6267\u884cROCKET\u7279\u5f81\u63d0\u53d6\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u5904\u7406ROCKET\u7684\u975e\u5747\u5300\u5377\u79ef\u6838\uff0c\u514b\u670d\u4e86\u6807\u51c6GPU\u5377\u79ef\u65b9\u6cd5\u7684\u6548\u7387\u95ee\u9898\u3002", "result": "CUROCKET\u5728GPU\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe11\u500d\u7684\u6bcf\u74e6\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff08\u76f8\u6bd4CPU\u4e0a\u7684ROCKET\uff09\uff0c\u663e\u8457\u52a0\u901f\u4e86\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u3002", "conclusion": "CUROCKET\u6210\u529f\u5c06ROCKET\u7b97\u6cd5\u9ad8\u6548\u79fb\u690d\u5230GPU\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u975e\u5747\u5300\u5377\u79ef\u6838\u5728GPU\u4e0a\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u63d0\u4f9b\u4e86\u66f4\u5feb\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6848\u3002"}}
{"id": "2601.17638", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17638", "abs": "https://arxiv.org/abs/2601.17638", "authors": ["Nitin Choudhury", "Bikrant Bikram Pratap Maurya", "Orchid Chetia Phukan", "Arun Balaji Buduru"], "title": "FOCA: Multimodal Malware Classification via Hyperbolic Cross-Attention", "comment": "Accepted to the International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "In this work, we introduce FOCA, a novel multimodal framework for malware classification that jointly leverages audio and visual modalities. Unlike conventional Euclidean-based fusion methods, FOCA is the first to exploit the intrinsic hierarchical relationships between audio and visual representations within hyperbolic space. To achieve this, raw binaries are transformed into both audio and visual representations, which are then processed through three key components: (i) a hyperbolic projection module that maps Euclidean embeddings into the Poincare ball, (ii) a hyperbolic cross-attention mechanism that aligns multimodal dependencies under curvature-aware constraints, and (iii) a Mobius addition-based fusion layer. Comprehensive experiments on two benchmark datasets-Mal-Net and CICMalDroid2020- show that FOCA consistently outperforms unimodal models, surpasses most Euclidean multimodal baselines, and achieves state-of-the-art performance over existing works.", "AI": {"tldr": "FOCA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u6846\u67b6\uff0c\u9996\u6b21\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5229\u7528\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u7684\u5c42\u6b21\u5173\u7cfb\uff0c\u901a\u8fc7\u53cc\u66f2\u6295\u5f71\u3001\u4ea4\u53c9\u6ce8\u610f\u529b\u548cM\u00f6bius\u52a0\u6cd5\u878d\u5408\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u97f3\u9891\u548c\u89c6\u89c9\u8868\u793a\u4e4b\u95f4\u7684\u5185\u5728\u5c42\u6b21\u5173\u7cfb\u3002\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "1) \u5c06\u539f\u59cb\u4e8c\u8fdb\u5236\u6587\u4ef6\u8f6c\u6362\u4e3a\u97f3\u9891\u548c\u89c6\u89c9\u8868\u793a\uff1b2) \u53cc\u66f2\u6295\u5f71\u6a21\u5757\u5c06\u6b27\u51e0\u91cc\u5f97\u5d4c\u5165\u6620\u5c04\u5230Poincar\u00e9\u7403\uff1b3) \u53cc\u66f2\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5728\u66f2\u7387\u611f\u77e5\u7ea6\u675f\u4e0b\u5bf9\u9f50\u591a\u6a21\u6001\u4f9d\u8d56\uff1b4) \u57fa\u4e8eM\u00f6bius\u52a0\u6cd5\u7684\u878d\u5408\u5c42\u3002", "result": "\u5728Mal-Net\u548cCICMalDroid2020\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aFOCA\u59cb\u7ec8\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b\uff0c\u8d85\u8d8a\u5927\u591a\u6570\u6b27\u51e0\u91cc\u5f97\u591a\u6a21\u6001\u57fa\u7ebf\uff0c\u5e76\u5728\u73b0\u6709\u5de5\u4f5c\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FOCA\u901a\u8fc7\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u5c42\u6b21\u5173\u7cfb\uff0c\u4e3a\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u53cc\u66f2\u51e0\u4f55\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.17577", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17577", "abs": "https://arxiv.org/abs/2601.17577", "authors": ["Emilio Barkett"], "title": "Status Hierarchies in Language Models", "comment": null, "summary": "From school playgrounds to corporate boardrooms, status hierarchies -- rank orderings based on respect and perceived competence -- are universal features of human social organization. Language models trained on human-generated text inevitably encounter these hierarchical patterns embedded in language, raising the question of whether they might reproduce such dynamics in multi-agent settings. This thesis investigates when and how language models form status hierarchies by adapting Berger et al.'s (1972) expectation states framework. I create multi-agent scenarios where separate language model instances complete sentiment classification tasks, are introduced with varying status characteristics (e.g., credentials, expertise), then have opportunities to revise their initial judgments after observing their partner's responses. The dependent variable is deference, the rate at which models shift their ratings toward their partner's position based on status cues rather than task information. Results show that language models form significant status hierarchies when capability is equal (35 percentage point asymmetry, p < .001), but capability differences dominate status cues, with the most striking effect being that high-status assignments reduce higher-capability models' deference rather than increasing lower-capability models' deference. The implications for AI safety are significant: status-seeking behavior could introduce deceptive strategies, amplify discriminatory biases, and scale across distributed deployments far faster than human hierarchies form organically. This work identifies emergent social behaviors in AI systems and highlights a previously underexplored dimension of the alignment challenge.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u4f1a\u5f62\u6210\u663e\u8457\u7684\u5730\u4f4d\u7b49\u7ea7\uff0c\u5f53\u80fd\u529b\u76f8\u540c\u65f6\u5730\u4f4d\u5dee\u5f02\u5bfc\u81f435%\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u4f46\u80fd\u529b\u5dee\u5f02\u4e3b\u5bfc\u5730\u4f4d\u7ebf\u7d22\uff0c\u9ad8\u5730\u4f4d\u5206\u914d\u53cd\u800c\u964d\u4f4e\u9ad8\u80fd\u529b\u6a21\u578b\u7684\u987a\u4ece\u5ea6\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4f1a\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u518d\u73b0\u4eba\u7c7b\u793e\u4f1a\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5730\u4f4d\u7b49\u7ea7\u73b0\u8c61\uff0c\u8fd9\u5bf9\u4e8eAI\u5b89\u5168\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u4e3a\u5730\u4f4d\u5bfb\u6c42\u884c\u4e3a\u53ef\u80fd\u5f15\u5165\u6b3a\u9a97\u7b56\u7565\u3001\u653e\u5927\u6b67\u89c6\u6027\u504f\u89c1\uff0c\u5e76\u4ee5\u8fdc\u8d85\u4eba\u7c7b\u81ea\u7136\u5f62\u6210\u901f\u5ea6\u7684\u65b9\u5f0f\u6269\u5c55\u3002", "method": "\u91c7\u7528Berger\u7b49\u4eba\uff081972\uff09\u7684\u671f\u671b\u72b6\u6001\u7406\u8bba\u6846\u67b6\uff0c\u521b\u5efa\u591a\u667a\u80fd\u4f53\u573a\u666f\uff1a\u8ba9\u72ec\u7acb\u7684\u8bed\u8a00\u6a21\u578b\u5b9e\u4f8b\u5b8c\u6210\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\uff0c\u5f15\u5165\u4e0d\u540c\u7684\u5730\u4f4d\u7279\u5f81\uff08\u5982\u8bc1\u4e66\u3001\u4e13\u4e1a\u77e5\u8bc6\uff09\uff0c\u7136\u540e\u8ba9\u6a21\u578b\u5728\u89c2\u5bdf\u5230\u4f19\u4f34\u7684\u56de\u5e94\u540e\u6709\u673a\u4f1a\u4fee\u6539\u521d\u59cb\u5224\u65ad\u3002\u56e0\u53d8\u91cf\u662f\u987a\u4ece\u5ea6\u2014\u2014\u6a21\u578b\u57fa\u4e8e\u5730\u4f4d\u7ebf\u7d22\u800c\u975e\u4efb\u52a1\u4fe1\u606f\u5411\u4f19\u4f34\u7acb\u573a\u8f6c\u53d8\u8bc4\u7ea7\u7684\u6bd4\u7387\u3002", "result": "\u5f53\u80fd\u529b\u76f8\u7b49\u65f6\uff0c\u8bed\u8a00\u6a21\u578b\u5f62\u6210\u663e\u8457\u7684\u5730\u4f4d\u7b49\u7ea7\uff0835\u4e2a\u767e\u5206\u70b9\u7684\u4e0d\u5bf9\u79f0\u6027\uff0cp < .001\uff09\uff0c\u4f46\u80fd\u529b\u5dee\u5f02\u4e3b\u5bfc\u5730\u4f4d\u7ebf\u7d22\u3002\u6700\u663e\u8457\u7684\u6548\u679c\u662f\uff1a\u9ad8\u5730\u4f4d\u5206\u914d\u53cd\u800c\u964d\u4f4e\u9ad8\u80fd\u529b\u6a21\u578b\u7684\u987a\u4ece\u5ea6\uff0c\u800c\u4e0d\u662f\u589e\u52a0\u4f4e\u80fd\u529b\u6a21\u578b\u7684\u987a\u4ece\u5ea6\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u786e\u5b9e\u4f1a\u5f62\u6210\u5730\u4f4d\u7b49\u7ea7\uff0c\u8fd9\u63ed\u793a\u4e86AI\u7cfb\u7edf\u4e2d\u6d8c\u73b0\u7684\u793e\u4f1a\u884c\u4e3a\uff0c\u5e76\u7a81\u51fa\u4e86\u5bf9\u9f50\u6311\u6218\u4e2d\u4e00\u4e2a\u5148\u524d\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7ef4\u5ea6\u3002\u5730\u4f4d\u5bfb\u6c42\u884c\u4e3a\u53ef\u80fd\u5bf9AI\u5b89\u5168\u6027\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\uff0c\u5305\u62ec\u5f15\u5165\u6b3a\u9a97\u7b56\u7565\u3001\u653e\u5927\u6b67\u89c6\u6027\u504f\u89c1\uff0c\u4ee5\u53ca\u5728\u5206\u5e03\u5f0f\u90e8\u7f72\u4e2d\u4ee5\u8fdc\u8d85\u4eba\u7c7b\u81ea\u7136\u5f62\u6210\u901f\u5ea6\u6269\u5c55\u3002"}}
{"id": "2601.18241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18241", "abs": "https://arxiv.org/abs/2601.18241", "authors": ["Elena Bruches", "Vadim Alperovich", "Dari Baturova", "Roman Derunets", "Daniil Grebenkin", "Georgy Mkrtchyan", "Oleg Sedukhin", "Mikhail Klementev", "Ivan Bondarenko", "Nikolay Bushkov", "Stanislav Moiseev"], "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance", "comment": "Accepted for publication at the 9th Workshop on Validation, Analysis and Evolution of Software Tests (VST 2026), co-located with the the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)", "summary": "While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.", "code_url": "https://github.com/trndcenter/TAM-Eval", "code_stars": 0, "code_last_update": "2026-01-23", "AI": {"tldr": "TAM-Eval\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u5957\u4ef6\u7ef4\u62a4\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u6db5\u76d6\u521b\u5efa\u3001\u4fee\u590d\u548c\u66f4\u65b0\u4e09\u79cd\u6838\u5fc3\u573a\u666f\uff0c\u652f\u6301Python\u3001Java\u548cGo\u8bed\u8a00\uff0c\u5305\u542b1539\u4e2a\u81ea\u52a8\u63d0\u53d6\u7684\u6d4b\u8bd5\u573a\u666f\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u4e3b\u8981\u5c40\u9650\u4e8e\u5b64\u7acb\u7684\u6d4b\u8bd5\u751f\u6210\u6216\u9884\u8a00\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u6d4b\u8bd5\u5957\u4ef6\u7ef4\u62a4\u8fd9\u4e00\u66f4\u5e7f\u6cdb\u7684\u6311\u6218\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u6d4b\u8bd5\u7ef4\u62a4\u5de5\u4f5c\u6d41\u4e2d\u6027\u80fd\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51faTAM-Eval\u6846\u67b6\uff0c\u5728\u6d4b\u8bd5\u6587\u4ef6\u7ea7\u522b\u64cd\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5b8c\u6574\u4ed3\u5e93\u4e0a\u4e0b\u6587\u7684\u8bbf\u95ee\u3002\u57fa\u51c6\u5305\u542b1539\u4e2a\u4ecePython\u3001Java\u548cGo\u9879\u76ee\u4e2d\u81ea\u52a8\u63d0\u53d6\u548c\u9a8c\u8bc1\u7684\u573a\u666f\u3002\u91c7\u7528\u57fa\u4e8e\u6d4b\u8bd5\u5957\u4ef6\u901a\u8fc7\u7387\u3001\u4ee3\u7801\u8986\u76d6\u7387\u548c\u53d8\u5f02\u6d4b\u8bd5\u7684\u65e0\u53c2\u8003\u8bc4\u4f30\u534f\u8bae\uff0c\u652f\u6301\u539f\u59cbLLMs\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u7cfb\u7edf\u65e0\u5173\u8bc4\u4f30\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LLMs\u5728\u5b9e\u9645\u6d4b\u8bd5\u7ef4\u62a4\u8fc7\u7a0b\u4e2d\u7684\u80fd\u529b\u6709\u9650\uff0c\u5bf9\u6d4b\u8bd5\u6709\u6548\u6027\u7684\u63d0\u5347\u5e45\u5ea6\u5f88\u5c0f\u3002", "conclusion": "TAM-Eval\u4f5c\u4e3a\u5f00\u6e90\u6846\u67b6\u53d1\u5e03\uff0c\u652f\u6301\u672a\u6765\u81ea\u52a8\u5316\u8f6f\u4ef6\u6d4b\u8bd5\u7814\u7a76\u3002\u5f53\u524dLLMs\u5728\u6d4b\u8bd5\u7ef4\u62a4\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u5176\u80fd\u529b\u3002"}}
{"id": "2601.17815", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17815", "abs": "https://arxiv.org/abs/2601.17815", "authors": ["Yves Inglin", "Jonas Frey", "Changan Chen", "Marco Hutter"], "title": "Less Is More: Scalable Visual Navigation from Limited Data", "comment": null, "summary": "Imitation learning provides a powerful framework for goal-conditioned visual navigation in mobile robots, enabling obstacle avoidance while respecting human preferences and social norms. However, its effectiveness depends critically on the quality and diversity of training data. In this work, we show how classical geometric planners can be leveraged to generate synthetic trajectories that complement costly human demonstrations. We train Less is More (LiMo), a transformer-based visual navigation policy that predicts goal-conditioned SE(2) trajectories from a single RGB observation, and find that augmenting limited expert demonstrations with planner-generated supervision yields substantial performance gains. Through ablations and complementary qualitative and quantitative analyses, we characterize how dataset scale and diversity affect planning performance. We demonstrate real-robot deployment and argue that robust visual navigation is enabled not by simply collecting more demonstrations, but by strategically curating diverse, high-quality datasets. Our results suggest that scalable, embodiment-specific geometric supervision is a practical path toward data-efficient visual navigation.", "AI": {"tldr": "\u5229\u7528\u51e0\u4f55\u89c4\u5212\u5668\u751f\u6210\u5408\u6210\u8f68\u8ff9\u589e\u5f3a\u6a21\u4eff\u5b66\u4e60\uff0c\u63d0\u5347\u89c6\u89c9\u5bfc\u822a\u7684\u6570\u636e\u6548\u7387\u548c\u6027\u80fd", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u89c6\u89c9\u5bfc\u822a\u4e2d\u4f9d\u8d56\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u83b7\u53d6\u4eba\u7c7b\u6f14\u793a\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u63a2\u7d22\u5982\u4f55\u7528\u4f4e\u6210\u672c\u65b9\u5f0f\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "method": "\u63d0\u51faLiMo\u6846\u67b6\uff1a\u4f7f\u7528\u7ecf\u5178\u51e0\u4f55\u89c4\u5212\u5668\u751f\u6210\u5408\u6210\u8f68\u8ff9\uff0c\u4e0e\u6709\u9650\u7684\u4eba\u7c7b\u6f14\u793a\u7ed3\u5408\u8bad\u7ec3\u57fa\u4e8etransformer\u7684\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\uff0c\u4ece\u5355\u5f20RGB\u56fe\u50cf\u9884\u6d4b\u76ee\u6807\u6761\u4ef6\u4e0b\u7684SE(2)\u8f68\u8ff9\u3002", "result": "\u7528\u89c4\u5212\u5668\u751f\u6210\u7684\u76d1\u7763\u6570\u636e\u589e\u5f3a\u6709\u9650\u4e13\u5bb6\u6f14\u793a\u80fd\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u6570\u636e\u96c6\u89c4\u6a21\u548c\u591a\u6837\u6027\u5bf9\u89c4\u5212\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u9a8c\u8bc1\u3002", "conclusion": "\u9c81\u68d2\u7684\u89c6\u89c9\u5bfc\u822a\u4e0d\u662f\u7b80\u5355\u6536\u96c6\u66f4\u591a\u6f14\u793a\uff0c\u800c\u662f\u901a\u8fc7\u6218\u7565\u6027\u5730\u7b56\u5212\u591a\u6837\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u5b9e\u73b0\u3002\u53ef\u6269\u5c55\u7684\u3001\u4f53\u73b0\u7279\u5b9a\u51e0\u4f55\u76d1\u7763\u662f\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u89c6\u89c9\u5bfc\u822a\u7684\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2601.17644", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17644", "abs": "https://arxiv.org/abs/2601.17644", "authors": ["Ali Al-Lawati", "Suhang Wang"], "title": "A Systemic Evaluation of Multimodal RAG Privacy", "comment": null, "summary": "The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210(mRAG)\u7ba1\u9053\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u7684\u9690\u79c1\u98ce\u9669\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6807\u51c6\u6a21\u578b\u63d0\u793a\u6cc4\u9732\u79c1\u4eba\u6570\u636e\u96c6\u4fe1\u606f\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210(mRAG)\u7ba1\u9053\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1(\u5982\u89c6\u89c9\u95ee\u7b54)\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5e26\u6765\u4e86\u91cd\u8981\u7684\u9690\u79c1\u6311\u6218\u3002\u867d\u7136mRAG\u80fd\u591f\u8fde\u63a5\u79c1\u6709\u6570\u636e\u96c6\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6cc4\u9732\u8fd9\u4e9b\u6570\u636e\u96c6\u79c1\u6709\u4fe1\u606f\u7684\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790mRAG\u7ba1\u9053\u4e2d\u7684\u9690\u79c1\u98ce\u9669\uff0c\u91c7\u7528\u6848\u4f8b\u7814\u7a76\u65b9\u6cd5\uff0c\u5c1d\u8bd5\u63a8\u65ad\u89c6\u89c9\u8d44\u4ea7(\u5982\u56fe\u50cf)\u662f\u5426\u5305\u542b\u5728mRAG\u4e2d\uff0c\u5982\u679c\u5b58\u5728\u5219\u6cc4\u9732\u5176\u76f8\u5173\u5143\u6570\u636e(\u5982\u6807\u9898)\u3002", "result": "\u7814\u7a76\u53d1\u73b0mRAG\u7ba1\u9053\u5b58\u5728\u663e\u8457\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u901a\u8fc7\u6807\u51c6\u6a21\u578b\u63d0\u793a\u53ef\u4ee5\u6210\u529f\u63a8\u65ad\u79c1\u6709\u6570\u636e\u96c6\u4e2d\u89c6\u89c9\u8d44\u4ea7\u7684\u5b58\u5728\u5e76\u6cc4\u9732\u5176\u5143\u6570\u636e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63a8\u52a8\u672a\u6765\u5bf9mRAG\u9690\u79c1\u95ee\u9898\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2601.17094", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17094", "abs": "https://arxiv.org/abs/2601.17094", "authors": ["Junichiro Niimi"], "title": "Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation", "comment": null, "summary": "Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u5634\u4e0d\u662f\u8111\"\u67b6\u6784\u539f\u5219\uff0c\u5c06\u4e16\u754c\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u5206\u79bb\uff0c\u901a\u8fc7\u6df1\u5ea6\u73bb\u5c14\u5179\u66fc\u673a\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u3001\u9002\u914d\u5668\u548c\u51bb\u7ed3GPT-2\u5b9e\u73b0\u53ef\u63a7\u751f\u6210\uff0c\u5728\u6d88\u8d39\u8005\u8bc4\u8bba\u9886\u57df\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u6d41\u7545\u6587\u672c\uff0c\u4f46\u5176\u662f\u5426\u771f\u6b63\u7406\u89e3\u4e16\u754c\u8fd8\u662f\u4ec5\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u7684\u8bed\u8a00\u4ecd\u5b58\u5728\u4e89\u8bae\u3002\u7814\u7a76\u8005\u5e0c\u671b\u660e\u786e\u5206\u79bb\u4e16\u754c\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u9a8c\u8bc1\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u63a5\u9002\u5f53\u4e16\u754c\u6a21\u578b\u65f6\u80fd\u5426\u5b9e\u73b0\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u751f\u6210\u3002", "method": "\u63d0\u51fa\"\u5634\u4e0d\u662f\u8111\"\u67b6\u6784\u539f\u5219\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1)\u6df1\u5ea6\u73bb\u5c14\u5179\u66fc\u673a\u4f5c\u4e3a\u57fa\u4e8e\u80fd\u91cf\u7684\u4e16\u754c\u6a21\u578b\u6355\u6349\u9886\u57df\u7ed3\u6784\uff1b2)\u9002\u914d\u5668\u5c06\u6f5c\u5728\u4fe1\u5ff5\u72b6\u6001\u6295\u5f71\u5230\u5d4c\u5165\u7a7a\u95f4\uff1b3)\u51bb\u7ed3\u7684GPT-2\u63d0\u4f9b\u8bed\u8a00\u80fd\u529b\u4f46\u4e0d\u5305\u542b\u9886\u57df\u77e5\u8bc6\u3002\u5728\u4e9a\u9a6c\u900a\u667a\u80fd\u624b\u673a\u8bc4\u8bba\u9886\u57df\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1)\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u8c03\u8282\u76f8\u6bd4\u4ec5\u57fa\u4e8e\u63d0\u793a\u7684\u751f\u6210\uff0c\u60c5\u611f\u76f8\u5173\u6027\u663e\u8457\u66f4\u9ad8\u3001\u56f0\u60d1\u5ea6\u66f4\u4f4e\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u66f4\u5927\uff1b2)DBM\u80fd\u91cf\u51fd\u6570\u80fd\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u7684\u5e02\u573a\u914d\u7f6e\uff0c\u4e3a\u4e0d\u53ef\u4fe1\u7684\u54c1\u724c-\u4ef7\u683c\u7ec4\u5408\u5206\u914d\u66f4\u9ad8\u80fd\u91cf\uff1b3)\u5bf9\u7279\u5b9a\u5c5e\u6027\u7684\u5e72\u9884\u80fd\u56e0\u679c\u4f20\u64ad\u5230\u751f\u6210\u6587\u672c\uff0c\u5e72\u9884\u8f93\u51fa\u4e0e\u81ea\u7136\u6837\u672c\u7684\u5206\u5e03\u7edf\u8ba1\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff0c\u5f53\u8fde\u63a5\u5230\u9002\u5f53\u7684\u4e16\u754c\u6a21\u578b\u65f6\uff0c\u4e5f\u80fd\u5b9e\u73b0\u4e00\u81f4\u4e14\u53ef\u63a7\u7684\u751f\u6210\u3002\u8fd9\u4e3a\u5206\u79bb\u8bed\u8a00\u80fd\u529b\u4e0e\u4e16\u754c\u7406\u89e3\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\uff0c\u9a8c\u8bc1\u4e86\"\u5634\u4e0d\u662f\u8111\"\u67b6\u6784\u539f\u5219\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.17661", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17661", "abs": "https://arxiv.org/abs/2601.17661", "authors": ["Ahmed Oun", "Rishabh Das", "Clay Hess", "Aakriti Barat", "Savas Kaya"], "title": "A PUF-Based Security Framework for Fault and Intrusion Detection", "comment": null, "summary": "Industrial Control Systems (ICS) rely on sensor feedback to keep safety-critical processes within operational limits. This research presents a hardware-root-of-trust that embeds a Physically Unclonable Function (PUF) at the measurement layer to authenticate sensor readings. The architecture combines voltage fingerprinting with a temporal authentication that integrates with standard industrial control system architecture. The research prototypes the PUF integration on a hardware-in-the-loop (HIL) water tank testbed using a Simulink-based PUF emulator. The system maintains 99.97% accuracy over a 5.18-hour period of normal operation and flags all injected anomalies, including spike faults, hard-over faults, and hardware trojan scenarios that push the system over to an unsafe operational state. The proposed architecture provides a process-aware, vendor-agnostic approach that can integrate with legacy plants to detect sensor signal degradation or sophisticated supply chain attacks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u786c\u4ef6\u4fe1\u4efb\u6839\u7684\u4f20\u611f\u5668\u8ba4\u8bc1\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u6d4b\u91cf\u5c42\u5d4c\u5165\u7269\u7406\u4e0d\u53ef\u514b\u9686\u51fd\u6570(PUF)\u6765\u9a8c\u8bc1\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u4f20\u611f\u5668\u8bfb\u6570\uff0c\u9632\u6b62\u4f20\u611f\u5668\u4fe1\u53f7\u9000\u5316\u6216\u4f9b\u5e94\u94fe\u653b\u51fb\u3002", "motivation": "\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u4f9d\u8d56\u4f20\u611f\u5668\u53cd\u9988\u6765\u786e\u4fdd\u5173\u952e\u8fc7\u7a0b\u5728\u64cd\u4f5c\u9650\u5236\u5185\uff0c\u4f46\u4f20\u611f\u5668\u8bfb\u6570\u53ef\u80fd\u53d7\u5230\u4fe1\u53f7\u9000\u5316\u6216\u4f9b\u5e94\u94fe\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u8ba4\u8bc1\u673a\u5236\u6765\u4fdd\u8bc1\u6d4b\u91cf\u6570\u636e\u7684\u771f\u5b9e\u6027\u3002", "method": "\u91c7\u7528\u786c\u4ef6\u4fe1\u4efb\u6839\u67b6\u6784\uff0c\u5728\u6d4b\u91cf\u5c42\u5d4c\u5165\u7269\u7406\u4e0d\u53ef\u514b\u9686\u51fd\u6570(PUF)\uff0c\u7ed3\u5408\u7535\u538b\u6307\u7eb9\u8bc6\u522b\u548c\u65f6\u95f4\u8ba4\u8bc1\u6280\u672f\uff0c\u4e0e\u6807\u51c6\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u67b6\u6784\u96c6\u6210\u3002\u7814\u7a76\u5728\u786c\u4ef6\u5728\u73af(HIL)\u6c34\u7bb1\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u4f7f\u7528Simulink-based PUF\u6a21\u62df\u5668\u8fdb\u884c\u539f\u578b\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u57285.18\u5c0f\u65f6\u7684\u6b63\u5e38\u8fd0\u884c\u671f\u95f4\u4fdd\u630199.97%\u7684\u51c6\u786e\u7387\uff0c\u80fd\u591f\u68c0\u6d4b\u6240\u6709\u6ce8\u5165\u7684\u5f02\u5e38\uff0c\u5305\u62ec\u5c16\u5cf0\u6545\u969c\u3001\u786c\u6027\u6545\u969c\u4ee5\u53ca\u5c06\u7cfb\u7edf\u63a8\u5411\u4e0d\u5b89\u5168\u64cd\u4f5c\u72b6\u6001\u7684\u786c\u4ef6\u6728\u9a6c\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u8fc7\u7a0b\u611f\u77e5\u3001\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4e0e\u9057\u7559\u5de5\u5382\u96c6\u6210\uff0c\u6709\u6548\u68c0\u6d4b\u4f20\u611f\u5668\u4fe1\u53f7\u9000\u5316\u6216\u590d\u6742\u7684\u4f9b\u5e94\u94fe\u653b\u51fb\uff0c\u589e\u5f3a\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2601.17614", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17614", "abs": "https://arxiv.org/abs/2601.17614", "authors": ["Yimeng Liu", "Misha Sra", "Chang Xiao"], "title": "AlignUI: A Method for Designing LLM-Generated UIs Aligned with User Preferences", "comment": null, "summary": "Designing user interfaces that align with user preferences is a time-consuming process, which requires iterative cycles of prototyping, user testing, and refinement. Recent advancements in LLM-based UI generation have enabled efficient UI generation to assist the UI design process. We introduce AlignUI, a method that aligns LLM-generated UIs with user tasks and preferences by using a user preference dataset to guide the LLM's reasoning process. The dataset was crowdsourced from 50 general users (the target users of generated UIs) and contained 720 UI control preferences on eight image-editing tasks. We evaluated AlignUI by generating UIs for six unseen tasks and conducting a user study with 72 additional general users. The results showed that the generated UIs closely align with multiple dimensions of user preferences. We conclude by discussing the applicability of our method to support user-aligned UI design for multiple task domains and user groups, as well as personalized user needs.", "AI": {"tldr": "AlignUI\u662f\u4e00\u79cd\u901a\u8fc7\u7528\u6237\u504f\u597d\u6570\u636e\u96c6\u6307\u5bfcLLM\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u751f\u6210\u7684UI\u4e0e\u7528\u6237\u4efb\u52a1\u548c\u504f\u597d\u5bf9\u9f50\u7684\u65b9\u6cd5", "motivation": "\u8bbe\u8ba1\u7b26\u5408\u7528\u6237\u504f\u597d\u7684\u7528\u6237\u754c\u9762\u662f\u4e00\u4e2a\u8017\u65f6\u7684\u8fc7\u7a0b\uff0c\u9700\u8981\u539f\u578b\u8bbe\u8ba1\u3001\u7528\u6237\u6d4b\u8bd5\u548c\u4f18\u5316\u7684\u8fed\u4ee3\u5468\u671f\u3002\u867d\u7136\u57fa\u4e8eLLM\u7684UI\u751f\u6210\u6280\u672f\u80fd\u591f\u9ad8\u6548\u751f\u6210UI\u8f85\u52a9\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u4f46\u9700\u8981\u786e\u4fdd\u751f\u6210\u7684UI\u4e0e\u7528\u6237\u4efb\u52a1\u548c\u504f\u597d\u5bf9\u9f50", "method": "AlignUI\u65b9\u6cd5\u4f7f\u7528\u4ece50\u540d\u666e\u901a\u7528\u6237\u4f17\u5305\u6536\u96c6\u7684\u7528\u6237\u504f\u597d\u6570\u636e\u96c6\uff08\u5305\u542b8\u4e2a\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u7684720\u4e2aUI\u63a7\u4ef6\u504f\u597d\uff09\u6765\u6307\u5bfcLLM\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u751f\u6210\u7684UI\u4e0e\u7528\u6237\u4efb\u52a1\u548c\u504f\u597d\u5bf9\u9f50", "result": "\u901a\u8fc7\u4e3a6\u4e2a\u672a\u89c1\u4efb\u52a1\u751f\u6210UI\u5e76\u8fdb\u884c\u5305\u542b72\u540d\u666e\u901a\u7528\u6237\u7684\u7528\u6237\u7814\u7a76\uff0c\u7ed3\u679c\u663e\u793a\u751f\u6210\u7684UI\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u4e0e\u7528\u6237\u504f\u597d\u9ad8\u5ea6\u5bf9\u9f50", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u652f\u6301\u591a\u4e2a\u4efb\u52a1\u9886\u57df\u548c\u7528\u6237\u7fa4\u4f53\u7684\u7528\u6237\u5bf9\u9f50UI\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u4e2a\u6027\u5316\u7684\u7528\u6237\u9700\u6c42"}}
{"id": "2601.18344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18344", "abs": "https://arxiv.org/abs/2601.18344", "authors": ["Alexandros Tsakpinis", "Efe Berk Erg\u00fclec", "Emil Schwenger", "Alexander Pretschner"], "title": "Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries", "comment": "11 pages, 9 figures, 2 tables", "summary": "The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks.", "AI": {"tldr": "\u7814\u7a76OpenSSF Scorecard\u4e2dMaintained\u6307\u6807\u7684\u9884\u6d4b\u6027\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u9884\u6d4b\u5f00\u6e90\u8f6f\u4ef6\u4ed3\u5e93\u7684\u672a\u6765\u7ef4\u62a4\u6d3b\u52a8\uff0c\u53d1\u73b0\u805a\u5408\u8868\u793a\u65b9\u6cd5\u80fd\u83b7\u5f97\u9ad8\u51c6\u786e\u7387\uff0c\u4e14\u7b80\u5355\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "OpenSSF Scorecard\u7684Maintained\u6307\u6807\u662f\u56de\u987e\u6027\u7684\uff08\u4ec5\u53cd\u6620\u8fc7\u53bb90\u5929\u6d3b\u52a8\uff09\uff0c\u65e0\u6cd5\u9884\u6d4b\u672a\u6765\u7ef4\u62a4\u72b6\u6001\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e3b\u52a8\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u9700\u8981\u7814\u7a76\u662f\u5426\u80fd\u9884\u6d4b\u672a\u6765\u7ef4\u62a4\u6d3b\u52a8\u4ee5\u8865\u5145\u73b0\u6709\u6307\u6807\u3002", "method": "\u5206\u67903,220\u4e2aGitHub\u4ed3\u5e93\uff08\u5bf9\u5e94PageRank\u6392\u540d\u524d1%\u7684PyPI\u5e93\uff09\uff0c\u91cd\u5efa\u4e09\u5e74\u5386\u53f2Maintained\u5206\u6570\u3002\u5c06\u4efb\u52a1\u6784\u5efa\u4e3a\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u8003\u8651\u56db\u79cd\u76ee\u6807\u8868\u793a\uff1a\u539f\u59cb\u5206\u6570\u3001\u5206\u6876\u7ef4\u62a4\u7ea7\u522b\u3001\u6570\u503c\u8d8b\u52bf\u659c\u7387\u3001\u5206\u7c7b\u8d8b\u52bf\u7c7b\u578b\u3002\u6bd4\u8f83VARMA\uff08\u7edf\u8ba1\u6a21\u578b\uff09\u3001\u968f\u673a\u68ee\u6797\uff08\u673a\u5668\u5b66\u4e60\uff09\u548cLSTM\uff08\u6df1\u5ea6\u5b66\u4e60\uff09\u6a21\u578b\uff0c\u8bad\u7ec3\u7a97\u53e33-12\u4e2a\u6708\uff0c\u9884\u6d4b\u8303\u56f41-6\u4e2a\u6708\u3002", "result": "\u672a\u6765\u7ef4\u62a4\u6d3b\u52a8\u53ef\u4ee5\u4ee5\u6709\u610f\u4e49\u7684\u51c6\u786e\u5ea6\u8fdb\u884c\u9884\u6d4b\uff0c\u7279\u522b\u662f\u805a\u5408\u8868\u793a\u5982\u5206\u6876\u5206\u6570\u548c\u8d8b\u52bf\u7c7b\u578b\uff0c\u51c6\u786e\u7387\u5206\u522b\u8d85\u8fc70.95\u548c0.80\u3002\u7b80\u5355\u7684\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u8868\u660e\u4e0d\u9700\u8981\u590d\u6742\u67b6\u6784\u3002", "conclusion": "\u9884\u6d4b\u5efa\u6a21\u53ef\u4ee5\u6709\u6548\u8865\u5145\u73b0\u6709Scorecard\u6307\u6807\uff0c\u5b9e\u73b0\u5bf9\u5f00\u6e90\u7ef4\u62a4\u98ce\u9669\u7684\u66f4\u4e3b\u52a8\u8bc4\u4f30\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u7ef4\u62a4\u6d3b\u52a8\u5177\u6709\u53ef\u9884\u6d4b\u6a21\u5f0f\uff0c\u4e3a\u5f00\u53d1\u66f4\u524d\u77bb\u6027\u7684\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.18121", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18121", "abs": "https://arxiv.org/abs/2601.18121", "authors": ["Byeonggyeol Choi", "Woojin Oh", "Jongwoo Lim"], "title": "Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization", "comment": "13 pages, 7 figures", "summary": "Dexterous hand manipulation increasingly relies on large-scale motion datasets with precise hand-object trajectory data. However, existing resources such as DexYCB and HO3D are primarily optimized for visual alignment but often yield physically implausible interactions when replayed in physics simulators, including penetration, missed contact, and unstable grasps.\n  We propose a simulation-in-the-loop refinement framework that converts these visually aligned trajectories into physically executable ones. Our core contribution is to formulate this as a tractable black-box optimization problem. We parameterize the hand's motion using a low-dimensional, spline-based representation built on sparse temporal keyframes. This allows us to use a powerful gradient-free optimizer, CMA-ES, to treat the high-fidelity physics engine as a black-box objective function. Our method finds motions that simultaneously maximize physical success (e.g., stable grasp and lift) while minimizing deviation from the original human demonstration.\n  Compared to MANIPTRANS-recent transfer pipelines, our approach achieves lower hand and object pose errors during replay and more accurately recovers hand-object physical interactions. Our approach provides a general and scalable method for converting visual demonstrations into physically valid trajectories, enabling the generation of high-fidelity data crucial for robust policy learning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4eff\u771f\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u5bf9\u9f50\u7684\u624b\u90e8\u8fd0\u52a8\u8f68\u8ff9\u8f6c\u6362\u4e3a\u7269\u7406\u53ef\u6267\u884c\u8f68\u8ff9\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u7269\u7406\u6a21\u62df\u4e2d\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u624b\u90e8\u64cd\u4f5c\u6570\u636e\u96c6\uff08\u5982DexYCB\u548cHO3D\uff09\u4e3b\u8981\u4e3a\u89c6\u89c9\u5bf9\u9f50\u4f18\u5316\uff0c\u4f46\u5728\u7269\u7406\u6a21\u62df\u5668\u4e2d\u91cd\u653e\u65f6\u4f1a\u4ea7\u751f\u7269\u7406\u4e0a\u4e0d\u53ef\u4fe1\u7684\u7ed3\u679c\uff0c\u5305\u62ec\u7a7f\u900f\u3001\u63a5\u89e6\u4e22\u5931\u548c\u4e0d\u7a33\u5b9a\u6293\u63e1\uff0c\u9650\u5236\u4e86\u5176\u5728\u7269\u7406\u4eff\u771f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4eff\u771f\u95ed\u73af\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u8f68\u8ff9\u8f6c\u6362\u95ee\u9898\u8868\u8ff0\u4e3a\u53ef\u5904\u7406\u7684\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\u3002\u4f7f\u7528\u57fa\u4e8e\u7a00\u758f\u65f6\u95f4\u5173\u952e\u5e27\u7684\u4f4e\u7ef4\u6837\u6761\u8868\u793a\u53c2\u6570\u5316\u624b\u90e8\u8fd0\u52a8\uff0c\u91c7\u7528\u65e0\u68af\u5ea6\u4f18\u5316\u5668CMA-ES\u5c06\u9ad8\u4fdd\u771f\u7269\u7406\u5f15\u64ce\u4f5c\u4e3a\u9ed1\u76d2\u76ee\u6807\u51fd\u6570\uff0c\u5728\u6700\u5927\u5316\u7269\u7406\u6210\u529f\u7387\u7684\u540c\u65f6\u6700\u5c0f\u5316\u4e0e\u539f\u59cb\u6f14\u793a\u7684\u504f\u5dee\u3002", "result": "\u4e0eMANIPTRANS\u7b49\u73b0\u6709\u4f20\u8f93\u7ba1\u9053\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u653e\u65f6\u83b7\u5f97\u66f4\u4f4e\u7684\u624b\u90e8\u548c\u7269\u4f53\u59ff\u6001\u8bef\u5dee\uff0c\u66f4\u51c6\u786e\u5730\u6062\u590d\u624b-\u7269\u4f53\u7269\u7406\u4ea4\u4e92\uff0c\u4e3a\u89c6\u89c9\u6f14\u793a\u5230\u7269\u7406\u6709\u6548\u8f68\u8ff9\u7684\u8f6c\u6362\u63d0\u4f9b\u4e86\u901a\u7528\u53ef\u6269\u5c55\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u89c6\u89c9\u5bf9\u9f50\u8f68\u8ff9\u8f6c\u6362\u4e3a\u7269\u7406\u53ef\u6267\u884c\u8f68\u8ff9\uff0c\u4e3a\u9c81\u68d2\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u7269\u7406\u4eff\u771f\u4e2d\u7684\u5c40\u9650\u6027\u95ee\u9898\u3002"}}
{"id": "2601.17785", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17785", "abs": "https://arxiv.org/abs/2601.17785", "authors": ["Tushar Jain"], "title": "Performance Analysis of Quantum-Secure Digital Signature Algorithms in Blockchain", "comment": null, "summary": "The long-term security of public blockchains strictly depends on the hardness assumptions of the underlying digital signature schemes. In the current scenario, most deployed cryptocurrencies and blockchain platforms rely on elliptic-curve cryptography, which is vulnerable to quantum attacks due to Shor's algorithm. Therefore, it is important to understand how post-quantum (PQ) digital signatures behave when integrated into real blockchain systems. This report presents a blockchain prototype that supports multiple quantum-secure signature algorithms, focusing on CRYSTALS-Dilithium, Falcon and Hawk as lattice-based schemes. This report also describes the design of the prototype and discusses the performance metrics, which include key generation, signing, verification times, key sizes and signature sizes. This report covers the problem, background, and experimental methodology, also providing a detailed comparison of quantum-secure signatures in a blockchain context and extending the analysis to schemes such as HAETAE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u79cd\u91cf\u5b50\u5b89\u5168\u7b7e\u540d\u7b97\u6cd5\u7684\u533a\u5757\u94fe\u539f\u578b\u7cfb\u7edf\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86CRYSTALS-Dilithium\u3001Falcon\u548cHawk\u7b49\u57fa\u4e8e\u683c\u7684\u7b7e\u540d\u65b9\u6848\u5728\u533a\u5757\u94fe\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u52a0\u5bc6\u8d27\u5e01\u548c\u533a\u5757\u94fe\u5e73\u53f0\u4f9d\u8d56\u692d\u5706\u66f2\u7ebf\u5bc6\u7801\u5b66\uff0c\u4f46\u8be5\u6280\u672f\u6613\u53d7\u91cf\u5b50\u653b\u51fb\uff08Shor\u7b97\u6cd5\u5a01\u80c1\uff09\u3002\u4e3a\u786e\u4fdd\u533a\u5757\u94fe\u7684\u957f\u671f\u5b89\u5168\u6027\uff0c\u9700\u8981\u7814\u7a76\u540e\u91cf\u5b50\u6570\u5b57\u7b7e\u540d\u5728\u5b9e\u9645\u533a\u5757\u94fe\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u79cd\u91cf\u5b50\u5b89\u5168\u7b7e\u540d\u7b97\u6cd5\u7684\u533a\u5757\u94fe\u539f\u578b\u7cfb\u7edf\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8e\u683c\u7684\u7b7e\u540d\u65b9\u6848\uff08CRYSTALS-Dilithium\u3001Falcon\u3001Hawk\uff09\u3002\u901a\u8fc7\u5b9e\u9a8c\u65b9\u6cd5\u6d4b\u91cf\u5173\u952e\u6027\u80fd\u6307\u6807\uff1a\u5bc6\u94a5\u751f\u6210\u65f6\u95f4\u3001\u7b7e\u540d\u65f6\u95f4\u3001\u9a8c\u8bc1\u65f6\u95f4\u3001\u5bc6\u94a5\u5927\u5c0f\u548c\u7b7e\u540d\u5927\u5c0f\u3002", "result": "\u63d0\u4f9b\u4e86\u91cf\u5b50\u5b89\u5168\u7b7e\u540d\u5728\u533a\u5757\u94fe\u73af\u5883\u4e2d\u7684\u8be6\u7ec6\u6027\u80fd\u6bd4\u8f83\uff0c\u5305\u62ec\u5bf9CRYSTALS-Dilithium\u3001Falcon\u3001Hawk\u7b49\u65b9\u6848\u7684\u8bc4\u4f30\uff0c\u5e76\u6269\u5c55\u5206\u6790\u5230HAETAE\u7b49\u5176\u4ed6\u65b9\u6848\u3002\u83b7\u5f97\u4e86\u5404\u9879\u6027\u80fd\u6307\u6807\u7684\u5177\u4f53\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533a\u5757\u94fe\u7cfb\u7edf\u96c6\u6210\u540e\u91cf\u5b50\u7b7e\u540d\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u91cf\u5b50\u5b89\u5168\u7b7e\u540d\u65b9\u6848\u5728\u533a\u5757\u94fe\u73af\u5883\u4e2d\u7684\u6027\u80fd\u7279\u5f81\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u533a\u5757\u94fe\u7cfb\u7edf\u5411\u91cf\u5b50\u5b89\u5168\u5bc6\u7801\u5b66\u7684\u8fc1\u79fb\u3002"}}
{"id": "2601.17622", "categories": ["cs.HC", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17622", "abs": "https://arxiv.org/abs/2601.17622", "authors": ["Yoonsang Kim", "Yalong Yang", "Arie E. Kaufman"], "title": "Memento: Towards Proactive Visualization of Everyday Memories with Personal Wearable AR Assistant", "comment": "8 pages, 5 figures. This is the author's version of the article that will appear at the IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (IEEE VRW) 2026", "summary": "We introduce Memento, a conversational AR assistant that permanently captures and memorizes user's verbal queries alongside their spatiotemporal and activity contexts. By storing these \"memories,\" Memento discovers connections between users' recurring interests and the contexts that trigger them. Upon detection of similar or identical spatiotemporal activity, Memento proactively recalls user interests and delivers up-to-date responses through AR, seamlessly integrating AR experience into their daily routine. Unlike prior work, each interaction in Memento is not a transient event, but a connected series of interactions with coherent long--term perspective, tailored to the user's broader multimodal (visual, spatial, temporal, and embodied) context. We conduct preliminary evaluation through user feedbacks with participants of diverse expertise in immersive apps, and explore the value of proactive context-aware AR assistant in everyday settings. We share our findings and challenges in designing a proactive, context-aware AR system.", "AI": {"tldr": "Memento\u662f\u4e00\u4e2a\u5bf9\u8bdd\u5f0fAR\u52a9\u624b\uff0c\u901a\u8fc7\u6c38\u4e45\u8bb0\u5f55\u7528\u6237\u7684\u8bed\u97f3\u67e5\u8be2\u53ca\u5176\u65f6\u7a7a\u548c\u6d3b\u52a8\u4e0a\u4e0b\u6587\uff0c\u53d1\u73b0\u7528\u6237\u91cd\u590d\u5174\u8da3\u4e0e\u89e6\u53d1\u60c5\u5883\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u76f8\u4f3c\u60c5\u5883\u65f6\u4e3b\u52a8\u63d0\u4f9bAR\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709AR\u4ea4\u4e92\u591a\u4e3a\u77ac\u65f6\u4e8b\u4ef6\uff0c\u7f3a\u4e4f\u8fde\u8d2f\u7684\u957f\u671f\u89c6\u89d2\u3002Memento\u65e8\u5728\u521b\u5efa\u80fd\u591f\u8bb0\u5fc6\u7528\u6237\u5174\u8da3\u4e0e\u60c5\u5883\u5173\u8054\u7684AR\u52a9\u624b\uff0c\u5c06AR\u4f53\u9a8c\u65e0\u7f1d\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u5b9e\u73b0\u57fa\u4e8e\u7528\u6237\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff08\u89c6\u89c9\u3001\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u5177\u8eab\uff09\u7684\u4e2a\u6027\u5316\u957f\u671f\u4ea4\u4e92\u3002", "method": "\u7cfb\u7edf\u6c38\u4e45\u6355\u83b7\u5e76\u5b58\u50a8\u7528\u6237\u7684\u8bed\u97f3\u67e5\u8be2\u53ca\u5176\u65f6\u7a7a\u548c\u6d3b\u52a8\u4e0a\u4e0b\u6587\uff0c\u5f62\u6210\"\u8bb0\u5fc6\"\u3002\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u8bb0\u5fc6\u53d1\u73b0\u7528\u6237\u91cd\u590d\u5174\u8da3\u4e0e\u89e6\u53d1\u60c5\u5883\u7684\u5173\u8054\u3002\u5f53\u68c0\u6d4b\u5230\u76f8\u4f3c\u6216\u76f8\u540c\u7684\u65f6\u7a7a\u6d3b\u52a8\u65f6\uff0c\u7cfb\u7edf\u4e3b\u52a8\u56de\u5fc6\u7528\u6237\u5174\u8da3\u5e76\u901a\u8fc7AR\u63d0\u4f9b\u6700\u65b0\u54cd\u5e94\u3002", "result": "\u901a\u8fc7\u521d\u6b65\u7528\u6237\u8bc4\u4f30\uff0c\u63a2\u7d22\u4e86\u4e3b\u52a8\u4e0a\u4e0b\u6587\u611f\u77e5AR\u52a9\u624b\u5728\u65e5\u5e38\u73af\u5883\u4e2d\u7684\u4ef7\u503c\u3002\u6536\u96c6\u4e86\u4e0d\u540c\u6c89\u6d78\u5f0f\u5e94\u7528\u4e13\u4e1a\u80cc\u666f\u53c2\u4e0e\u8005\u7684\u53cd\u9988\uff0c\u5206\u4eab\u4e86\u8bbe\u8ba1\u4e3b\u52a8\u4e0a\u4e0b\u6587\u611f\u77e5AR\u7cfb\u7edf\u7684\u53d1\u73b0\u548c\u6311\u6218\u3002", "conclusion": "Memento\u5c06AR\u4ea4\u4e92\u4ece\u77ac\u65f6\u4e8b\u4ef6\u8f6c\u53d8\u4e3a\u5177\u6709\u8fde\u8d2f\u957f\u671f\u89c6\u89d2\u7684\u7cfb\u5217\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7528\u6237\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u7684\u4e2a\u6027\u5316AR\u4f53\u9a8c\u3002\u7814\u7a76\u63ed\u793a\u4e86\u4e3b\u52a8\u4e0a\u4e0b\u6587\u611f\u77e5AR\u7cfb\u7edf\u5728\u65e5\u5e38\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u548c\u8bbe\u8ba1\u6311\u6218\u3002"}}
{"id": "2601.17111", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17111", "abs": "https://arxiv.org/abs/2601.17111", "authors": ["Xuan-Phi Nguyen", "Shrey Pandit", "Austin Xu", "Caiming Xiong", "Shafiq Joty"], "title": "Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts", "comment": "Preprint", "summary": "Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.", "AI": {"tldr": "\u63d0\u51faLLEP\u7b97\u6cd5\u89e3\u51b3MoE\u6a21\u578b\u4e13\u5bb6\u5e76\u884c\u4e2d\u8def\u7531\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u8bbe\u5907\u8fc7\u8f7d\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u8def\u7531\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\uff0c\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387", "motivation": "MoE\u6a21\u578b\u5373\u4f7f\u5728\u9884\u8bad\u7ec3\u540e\u4ecd\u5b58\u5728\u663e\u8457\u7684\u8def\u7531\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u800c\u4e13\u5bb6\u5e76\u884c(EP)\u5047\u8bbe\u8def\u7531\u5e73\u8861\uff0c\u5728\u6781\u7aef\u4e0d\u5e73\u8861\u65f6\u4f1a\u5bfc\u81f4\u90e8\u5206\u8bbe\u5907\u8fc7\u8f7d\uff0c\u5f15\u53d1\u8ba1\u7b97\u548c\u5185\u5b58\u6545\u969c", "method": "\u63d0\u51fa\u6700\u5c0f\u8d1f\u8f7d\u4e13\u5bb6\u5e76\u884c(LLEP)\u7b97\u6cd5\uff0c\u52a8\u6001\u5c06\u8fc7\u8f7d\u8bbe\u5907\u4e0a\u7684\u591a\u4f59token\u548c\u76f8\u5173\u4e13\u5bb6\u53c2\u6570\u91cd\u8def\u7531\u5230\u5229\u7528\u7387\u4f4e\u7684\u8bbe\u5907\uff0c\u786e\u4fdd\u6240\u6709\u8bbe\u5907\u5728\u6700\u5c0f\u96c6\u4f53\u5ef6\u8fdf\u5185\u5b8c\u6210\u5de5\u4f5c\u5e76\u6ee1\u8db3\u5185\u5b58\u7ea6\u675f", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\uff0cLLEP\u76f8\u6bd4\u6807\u51c6EP\u5b9e\u73b0\u9ad8\u8fbe5\u500d\u52a0\u901f\u548c4\u500d\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\uff0cgpt-oss-120b\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea61.9\u500d", "conclusion": "LLEP\u6709\u6548\u89e3\u51b3\u4e86MoE\u4e13\u5bb6\u5e76\u884c\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u5efa\u7acb\u4e86\u786c\u4ef6\u7279\u5b9a\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u539f\u5219\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u540e\u8bad\u7ec3\u548c\u63a8\u7406"}}
{"id": "2601.18289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18289", "abs": "https://arxiv.org/abs/2601.18289", "authors": ["Jialong Li", "Zhenguo Wang", "Tianci Wang", "Maj Stenmark", "Volker Krueger"], "title": "Quest2ROS2: A ROS 2 Framework for Bi-manual VR Teleoperation", "comment": "HRI 2026", "summary": "Quest2ROS2 is an open-source ROS2 framework for bi-manual teleoperation designed to scale robot data collection. Extending Quest2ROS, it overcomes workspace limitations via relative motion-based control, calculating robot movement from VR controller pose changes to enable intuitive, pose-independent operation. The framework integrates essential usability and safety features, including real-time RViz visualization, streamlined gripper control, and a pause-and-reset function for smooth transitions. We detail a modular architecture that supports \"Side-by-Side\" and \"Mirror\" control modes to optimize operator experience across diverse platforms. Code is available at: https://github.com/Taokt/Quest2ROS2.", "code_url": "https://github.com/Taokt/Quest2ROS2", "code_stars": 10, "code_last_update": "2026-01-10", "AI": {"tldr": "Quest2ROS2\u662f\u4e00\u4e2a\u5f00\u6e90\u7684ROS2\u6846\u67b6\uff0c\u7528\u4e8e\u53cc\u624b\u9065\u64cd\u4f5c\uff0c\u65e8\u5728\u6269\u5c55\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u89c4\u6a21\u3002\u5b83\u901a\u8fc7\u57fa\u4e8e\u76f8\u5bf9\u8fd0\u52a8\u7684\u63a7\u5236\u514b\u670d\u5de5\u4f5c\u7a7a\u95f4\u9650\u5236\uff0c\u4eceVR\u63a7\u5236\u5668\u59ff\u6001\u53d8\u5316\u8ba1\u7b97\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u5b9e\u73b0\u76f4\u89c2\u3001\u59ff\u6001\u65e0\u5173\u7684\u64cd\u4f5c\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6269\u5c55\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u89c4\u6a21\u7684\u53cc\u624b\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u7684\u5de5\u4f5c\u7a7a\u95f4\u9650\u5236\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u76f4\u89c2\u3001\u59ff\u6001\u65e0\u5173\u7684\u64cd\u4f5c\u4f53\u9a8c\u3002", "method": "\u6269\u5c55Quest2ROS\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u76f8\u5bf9\u8fd0\u52a8\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97VR\u63a7\u5236\u5668\u59ff\u6001\u53d8\u5316\u6765\u786e\u5b9a\u673a\u5668\u4eba\u8fd0\u52a8\u3002\u6846\u67b6\u5305\u542b\u6a21\u5757\u5316\u67b6\u6784\uff0c\u652f\u6301\"\u5e76\u6392\"\u548c\"\u955c\u50cf\"\u63a7\u5236\u6a21\u5f0f\uff0c\u96c6\u6210\u4e86\u5b9e\u65f6RViz\u53ef\u89c6\u5316\u3001\u7b80\u5316\u5939\u722a\u63a7\u5236\u548c\u6682\u505c\u91cd\u7f6e\u529f\u80fd\u7b49\u53ef\u7528\u6027\u4e0e\u5b89\u5168\u7279\u6027\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86Quest2ROS2\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u76f4\u89c2\u7684\u53cc\u624b\u9065\u64cd\u4f5c\uff0c\u514b\u670d\u4e86\u5de5\u4f5c\u7a7a\u95f4\u9650\u5236\uff0c\u652f\u6301\u591a\u79cd\u63a7\u5236\u6a21\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u53ef\u7528\u6027\u548c\u5b89\u5168\u529f\u80fd\u3002\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002", "conclusion": "Quest2ROS2\u662f\u4e00\u4e2a\u6709\u6548\u7684ROS2\u53cc\u624b\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u5bf9\u8fd0\u52a8\u63a7\u5236\u89e3\u51b3\u4e86\u5de5\u4f5c\u7a7a\u95f4\u9650\u5236\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u67b6\u6784\u548c\u4e30\u5bcc\u7684\u529f\u80fd\uff0c\u6709\u52a9\u4e8e\u6269\u5c55\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u89c4\u6a21\u3002"}}
{"id": "2601.17676", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17676", "abs": "https://arxiv.org/abs/2601.17676", "authors": ["Jiexin Ding", "Yizhuo Zhang", "Xinyun Liu", "Ke chen", "Yuntao Wang", "Shwetak Patel", "Akshay Gadre"], "title": "GazeSummary: Exploring Gaze as an Implicit Prompt for Personalization in Text-based LLM Tasks", "comment": null, "summary": "Smart glasses are accelerating progress toward more seamless and personalized LLM-based assistance by integrating multimodal inputs. Yet, these inputs rely on obtrusive explicit prompts. The advent of gaze tracking on smart devices offers a unique opportunity to extract implicit user intent for personalization. This paper investigates whether LLMs can interpret user gaze for text-based tasks. We evaluate different gaze representations for personalization and validate their effectiveness in realistic reading tasks. Results show that LLMs can leverage gaze to generate high-quality personalized summaries and support users in downstream tasks, highlighting the feasibility and value of gaze-driven personalization for future mobile and wearable LLM applications.", "AI": {"tldr": "LLMs\u80fd\u591f\u5229\u7528\u7528\u6237\u6ce8\u89c6\u4fe1\u606f\u5b9e\u73b0\u4e2a\u6027\u5316\u6587\u672c\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u6ce8\u89c6\u9a71\u52a8\u4e2a\u6027\u5316\u5728\u79fb\u52a8\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u4e2d\u7684\u53ef\u884c\u6027", "motivation": "\u667a\u80fd\u773c\u955c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u63a8\u8fdb\u4e2a\u6027\u5316LLM\u8f85\u52a9\uff0c\u4f46\u5f53\u524d\u8f93\u5165\u4f9d\u8d56\u663e\u5f0f\u63d0\u793a\u3002\u6ce8\u89c6\u8ffd\u8e2a\u6280\u672f\u63d0\u4f9b\u4e86\u63d0\u53d6\u7528\u6237\u9690\u5f0f\u610f\u56fe\u7684\u673a\u4f1a\uff0c\u672c\u7814\u7a76\u63a2\u7d22LLM\u80fd\u5426\u5229\u7528\u6ce8\u89c6\u4fe1\u606f\u8fdb\u884c\u6587\u672c\u4efb\u52a1\u4e2a\u6027\u5316", "method": "\u8bc4\u4f30\u4e0d\u540c\u6ce8\u89c6\u8868\u793a\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u5728\u771f\u5b9e\u9605\u8bfb\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6ce8\u89c6\u8868\u793a\u7684\u6709\u6548\u6027", "result": "LLMs\u80fd\u591f\u5229\u7528\u6ce8\u89c6\u4fe1\u606f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u6458\u8981\uff0c\u5e76\u652f\u6301\u7528\u6237\u5b8c\u6210\u4e0b\u6e38\u4efb\u52a1", "conclusion": "\u6ce8\u89c6\u9a71\u52a8\u4e2a\u6027\u5316\u5bf9\u4e8e\u672a\u6765\u79fb\u52a8\u548c\u53ef\u7a7f\u6234LLM\u5e94\u7528\u5177\u6709\u53ef\u884c\u6027\u548c\u4ef7\u503c\uff0c\u4e3a\u66f4\u65e0\u7f1d\u7684\u4e2a\u6027\u5316\u8f85\u52a9\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2601.17112", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17112", "abs": "https://arxiv.org/abs/2601.17112", "authors": ["A. El Ichi", "K. Jbilou"], "title": "Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ecproduct\u7684\u5f20\u91cf\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u4ee5\u964d\u4f4eLLMs\u7684\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u6210\u672c", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u6781\u5927\u7684\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5", "method": "\u5229\u7528cproduct\u7684\u4ee3\u6570\u7ed3\u6784\uff0c\u5728\u53d8\u6362\u57df\u4e2d\u8868\u793a\u6743\u91cd\u5f20\u91cf\uff0c\u4f7f\u6b63\u9762\u5207\u7247\u80fd\u591f\u901a\u8fc7\u4f4e\u79e9\u5f20\u91cf\u56e0\u5b50\u8054\u5408\u8fd1\u4f3c\uff0c\u5229\u7528\u591a\u7ef4\u76f8\u5173\u6027\u8fdb\u884c\u9ad8\u6548\u538b\u7f29", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u8ba1\u7b97\u9ad8\u6548\u7684\u538b\u7f29\uff0c\u76f8\u6bd4\u4f20\u7edfSVD\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5229\u7528\u591a\u7ef4\u76f8\u5173\u6027", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8ecproduct\u7684\u5f20\u91cf\u538b\u7f29\u6846\u67b6\u4e3a\u964d\u4f4eLLMs\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18323", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18323", "abs": "https://arxiv.org/abs/2601.18323", "authors": ["Weishi Mi", "Yong Bao", "Xiaowei Chi", "Xiaozhu Ju", "Zhiyuan Qin", "Kuangzhi Ge", "Kai Tang", "Peidong Jia", "Shanghang Zhang", "Jian Tang"], "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion", "comment": null, "summary": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.\n  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.\n  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.\n  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.\n  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.", "AI": {"tldr": "TC-IDM\u901a\u8fc7\u63d0\u53d6\u4e16\u754c\u6a21\u578b\u751f\u6210\u7684\u5de5\u5177\u8f68\u8ff9\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5c06\u89c6\u89c9\u89c4\u5212\u4e0e\u7269\u7406\u63a7\u5236\u8fde\u63a5\u8d77\u6765\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u80fd\u529b", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u8303\u5f0f\u4f9d\u8d56\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u751f\u6210\u4e16\u754c\u6a21\u578b\u867d\u4e3a\u901a\u7528\u5177\u8eabAI\u63d0\u4f9b\u53ef\u80fd\uff0c\u4f46\u5176\u50cf\u7d20\u7ea7\u89c4\u5212\u4e0e\u7269\u7406\u53ef\u6267\u884c\u52a8\u4f5c\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd", "method": "\u63d0\u51fa\u5de5\u5177\u4e2d\u5fc3\u9006\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4ece\u751f\u6210\u89c6\u9891\u4e2d\u901a\u8fc7\u5206\u5272\u548c3D\u8fd0\u52a8\u4f30\u8ba1\u63d0\u53d6\u5de5\u5177\u70b9\u4e91\u8f68\u8ff9\uff0c\u91c7\u7528\u89e3\u8026\u52a8\u4f5c\u5934\u5c06\u89c4\u5212\u8f68\u8ff9\u6295\u5f71\u4e3a6\u81ea\u7531\u5ea6\u672b\u7aef\u6267\u884c\u5668\u8fd0\u52a8\u548c\u63a7\u5236\u4fe1\u53f7", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0cTC-IDM\u5e73\u5747\u6210\u529f\u738761.11%\uff0c\u7b80\u5355\u4efb\u52a177.7%\uff0c\u96f6\u6837\u672c\u53ef\u53d8\u5f62\u7269\u4f53\u4efb\u52a138.46%\uff0c\u663e\u8457\u4f18\u4e8e\u7aef\u5230\u7aefVLA\u57fa\u7ebf\u548c\u5176\u5b83\u9006\u52a8\u529b\u5b66\u6a21\u578b", "conclusion": "TC-IDM\u901a\u8fc7\"\u89c4\u5212-\u7ffb\u8bd1\"\u8303\u5f0f\u5efa\u7acb\u89c6\u89c9\u89c4\u5212\u4e0e\u7269\u7406\u63a7\u5236\u95f4\u7684\u7a33\u5065\u4e2d\u95f4\u8868\u793a\uff0c\u652f\u6301\u591a\u79cd\u672b\u7aef\u6267\u884c\u5668\uff0c\u63d0\u5347\u89c6\u89d2\u4e0d\u53d8\u6027\uff0c\u5728\u957f\u89c6\u91ce\u548c\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u5c55\u73b0\u5f3a\u5927\u6cdb\u5316\u80fd\u529b"}}
{"id": "2601.17736", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17736", "abs": "https://arxiv.org/abs/2601.17736", "authors": ["Can Liu", "Jaeuk Lee", "Tianhe Chen", "Zhibang Jiang", "Xiaolin Wen", "Yong Wang"], "title": "Athanor: Authoring Action Modification-based Interactions on Static Visualizations via Natural Language", "comment": null, "summary": "Interactivity is crucial for effective data visualizations. However, it is often challenging to implement interactions for existing static visualizations, since the underlying code and data for existing static visualizations are often not available, and it also takes significant time and effort to enable interactions for them even if the original code and data are available. To fill this gap, we propose Athanor, a novel approach to transform existing static visualizations into interactive ones using multimodal large language models (MLLMs) and natural language instructions. Our approach introduces three key innovations: (1) an action-modification interaction design space that maps visualization interactions into user actions and corresponding adjustments, (2) a multi-agent requirement analyzer that translates natural language instructions into an actionable operational space, and (3) a visualization abstraction transformer that converts static visualizations into flexible and interactive representations regardless of their underlying implementation. Athanor allows users to effortlessly author interactions through natural language instructions, eliminating the need for programming. We conducted two case studies and in-depth interviews with target users to evaluate our approach. The results demonstrate the effectiveness and usability of our approach in allowing users to conveniently enable flexible interactions for static visualizations.", "AI": {"tldr": "Athanor\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9759\u6001\u53ef\u89c6\u5316\u4ea4\u4e92\u5316\u8f6c\u6362\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e3a\u73b0\u6709\u9759\u6001\u53ef\u89c6\u5316\u6dfb\u52a0\u4ea4\u4e92\u529f\u80fd", "motivation": "\u73b0\u6709\u9759\u6001\u53ef\u89c6\u5316\u96be\u4ee5\u5b9e\u73b0\u4ea4\u4e92\u529f\u80fd\uff0c\u56e0\u4e3a\u539f\u59cb\u4ee3\u7801\u548c\u6570\u636e\u901a\u5e38\u4e0d\u53ef\u7528\uff0c\u5373\u4f7f\u53ef\u7528\u4e5f\u9700\u8981\u5927\u91cf\u65f6\u95f4\u548c\u7cbe\u529b\u6765\u6dfb\u52a0\u4ea4\u4e92\u529f\u80fd", "method": "\u63d0\u51faAthanor\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u52a8\u4f5c-\u4fee\u6539\u4ea4\u4e92\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5c06\u53ef\u89c6\u5316\u4ea4\u4e92\u6620\u5c04\u4e3a\u7528\u6237\u52a8\u4f5c\u548c\u5bf9\u5e94\u8c03\u6574\uff1b2) \u591a\u667a\u80fd\u4f53\u9700\u6c42\u5206\u6790\u5668\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u64cd\u4f5c\u7684\u64cd\u4f5c\u7a7a\u95f4\uff1b3) \u53ef\u89c6\u5316\u62bd\u8c61\u8f6c\u6362\u5668\uff0c\u5c06\u9759\u6001\u53ef\u89c6\u5316\u8f6c\u6362\u4e3a\u7075\u6d3b\u4ea4\u4e92\u8868\u793a", "result": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u548c\u6df1\u5ea6\u7528\u6237\u8bbf\u8c08\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8ba9\u7528\u6237\u65b9\u4fbf\u5730\u4e3a\u9759\u6001\u53ef\u89c6\u5316\u6dfb\u52a0\u7075\u6d3b\u4ea4\u4e92\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u548c\u53ef\u7528\u6027", "conclusion": "Athanor\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8ba9\u7528\u6237\u65e0\u9700\u7f16\u7a0b\u5373\u53ef\u4e3a\u73b0\u6709\u9759\u6001\u53ef\u89c6\u5316\u8f7b\u677e\u6dfb\u52a0\u4ea4\u4e92\u529f\u80fd\uff0c\u586b\u8865\u4e86\u9759\u6001\u53ef\u89c6\u5316\u4ea4\u4e92\u5316\u8f6c\u6362\u7684\u6280\u672f\u7a7a\u767d"}}
{"id": "2601.18477", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18477", "abs": "https://arxiv.org/abs/2601.18477", "authors": ["Giuseppe Destefanis", "Leila Yousefi", "Martin Shepperd", "Allan Tucker", "Stephen Swift", "Steve Counsell", "Mahir Arzoky"], "title": "An Audit of Machine Learning Experiments on Software Defect Prediction", "comment": null, "summary": "Background: Machine learning algorithms are widely used to predict defect prone software components. In this literature, computational experiments are the main means of evaluation, and the credibility of results depends on experimental design and reporting. Objective: This paper audits recent software defect prediction (SDP) studies by assessing their experimental design, analysis, and reporting practices against accepted norms from statistics, machine learning, and empirical software engineering. The aim is to characterise current practice and assess the reproducibility of published results. Method: We audited SDP studies indexed in SCOPUS between 2019 and 2023, focusing on design and analysis choices such as outcome measures, out of sample validation strategies, and the use of statistical inference. Nine study issues were evaluated. Reproducibility was assessed using the instrument proposed by Gonz\u00e1lez Barahona and Robles. Results: The search identified approximately 1,585 SDP experiments published during the period. From these, we randomly sampled 101 papers, including 61 journal and 40 conference publications, with almost 50 percent behind paywalls. We observed substantial variation in research practice. The number of datasets ranged from 1 to 365, learners or learner variants from 1 to 34, and performance measures from 1 to 9. About 45 percent of studies applied formal statistical inference. Across the sample, we identified 427 issues, with a median of four per paper, and only one paper without issues. Reproducibility ranged from near complete to severely limited. We also identified two cases of tortured phrases and possible paper mill activity. Conclusions: Experimental design and reporting practices vary widely, and almost half of the studies provide insufficient detail to support reproduction. The audit indicates substantial scope for improvement.", "AI": {"tldr": "\u5bf92019-2023\u5e74\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u7814\u7a76\u8fdb\u884c\u5ba1\u8ba1\uff0c\u53d1\u73b0\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u62a5\u544a\u5b9e\u8df5\u5dee\u5f02\u5de8\u5927\uff0c\u8fd1\u534a\u6570\u7814\u7a76\u7f3a\u4e4f\u8db3\u591f\u7ec6\u8282\u652f\u6301\u590d\u73b0\uff0c\u5b58\u5728\u5927\u91cf\u65b9\u6cd5\u5b66\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5e7f\u6cdb\u7528\u4e8e\u9884\u6d4b\u6613\u7f3a\u9677\u8f6f\u4ef6\u7ec4\u4ef6\uff0c\u4f46\u5b9e\u9a8c\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u4f9d\u8d56\u4e8e\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u62a5\u544a\u8d28\u91cf\u3002\u672c\u7814\u7a76\u65e8\u5728\u5ba1\u8ba1\u8fd1\u671f\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u7814\u7a76\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u5206\u6790\u548c\u62a5\u544a\u5b9e\u8df5\uff0c\u8bc4\u4f30\u5176\u662f\u5426\u7b26\u5408\u7edf\u8ba1\u5b66\u3001\u673a\u5668\u5b66\u4e60\u548c\u5b9e\u8bc1\u8f6f\u4ef6\u5de5\u7a0b\u7684\u516c\u8ba4\u89c4\u8303\uff0c\u5e76\u8bc4\u4f30\u5df2\u53d1\u8868\u7ed3\u679c\u7684\u53ef\u590d\u73b0\u6027\u3002", "method": "\u5ba1\u8ba12019-2023\u5e74\u95f4SCOPUS\u7d22\u5f15\u7684SDP\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u5206\u6790\u9009\u62e9\uff08\u5982\u7ed3\u679c\u5ea6\u91cf\u3001\u6837\u672c\u5916\u9a8c\u8bc1\u7b56\u7565\u3001\u7edf\u8ba1\u63a8\u65ad\u4f7f\u7528\u7b49\uff09\u3002\u8bc4\u4f30\u4e5d\u4e2a\u7814\u7a76\u95ee\u9898\uff0c\u4f7f\u7528Gonz\u00e1lez Barahona\u548cRobles\u63d0\u51fa\u7684\u5de5\u5177\u8bc4\u4f30\u53ef\u590d\u73b0\u6027\u3002\u4ece1,585\u9879\u5b9e\u9a8c\u4e2d\u968f\u673a\u62bd\u6837101\u7bc7\u8bba\u6587\uff0861\u7bc7\u671f\u520a\uff0c40\u7bc7\u4f1a\u8bae\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7814\u7a76\u5b9e\u8df5\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a\u6570\u636e\u96c6\u6570\u91cf1-365\u4e2a\uff0c\u5b66\u4e60\u5668\u6216\u53d8\u4f531-34\u4e2a\uff0c\u6027\u80fd\u5ea6\u91cf1-9\u4e2a\u3002\u7ea645%\u7684\u7814\u7a76\u5e94\u7528\u4e86\u6b63\u5f0f\u7edf\u8ba1\u63a8\u65ad\u3002\u5171\u8bc6\u522b\u51fa427\u4e2a\u95ee\u9898\uff0c\u4e2d\u4f4d\u6570\u4e3a\u6bcf\u7bc74\u4e2a\u95ee\u9898\uff0c\u4ec51\u7bc7\u8bba\u6587\u65e0\u95ee\u9898\u3002\u53ef\u590d\u73b0\u6027\u4ece\u63a5\u8fd1\u5b8c\u6574\u5230\u4e25\u91cd\u53d7\u9650\u4e0d\u7b49\u3002\u8fd8\u53d1\u73b02\u4f8b\"\u6298\u78e8\u77ed\u8bed\"\u548c\u53ef\u80fd\u7684\u8bba\u6587\u5de5\u5382\u6d3b\u52a8\u3002", "conclusion": "\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u62a5\u544a\u5b9e\u8df5\u5dee\u5f02\u5de8\u5927\uff0c\u8fd1\u534a\u6570\u7814\u7a76\u7f3a\u4e4f\u8db3\u591f\u7ec6\u8282\u652f\u6301\u590d\u73b0\u3002\u5ba1\u8ba1\u8868\u660e\u5b58\u5728\u5de8\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u66f4\u8be6\u7ec6\u7684\u62a5\u544a\u5b9e\u8df5\u6765\u63d0\u9ad8\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u7814\u7a76\u7684\u79d1\u5b66\u4e25\u8c28\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2601.18442", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18442", "abs": "https://arxiv.org/abs/2601.18442", "authors": ["Hongyi Zhao", "Shuo Wang", "Qijie He", "Ziyuan Pu"], "title": "SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation", "comment": null, "summary": "Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.", "AI": {"tldr": "SG-CADVLM\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u7801\u548c\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\uff0c\u4ece\u4e8b\u6545\u62a5\u544a\u548c\u8def\u7f51\u56fe\u4e2d\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u591a\u6837\u6027\u4e0d\u8db3\u3001\u7269\u7406\u4fdd\u771f\u5ea6\u5dee\u548c\u4e0a\u4e0b\u6587\u6291\u5236\u95ee\u9898\uff0c\u5c06\u5173\u952e\u98ce\u9669\u573a\u666f\u751f\u6210\u7387\u4ece12.5%\u63d0\u5347\u523084.4%\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9a8c\u8bc1\u9700\u8981\u6d4b\u8bd5\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u4f46\u8fd9\u4e9b\u4e8b\u4ef6\u5728\u771f\u5b9e\u9a7e\u9a76\u4e2d\u7f55\u89c1\u4e14\u6d4b\u8bd5\u6210\u672c\u9ad8\u3002\u4e8b\u6545\u62a5\u544a\u63d0\u4f9b\u4e86\u771f\u5b9e\u7684\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u89c4\u683c\uff0c\u662f\u7a00\u7f3a\u7684\u771f\u5b9e\u78b0\u649e\u8f68\u8ff9\u6570\u636e\u7684\u91cd\u8981\u66ff\u4ee3\u6765\u6e90\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff1a\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u56e0\u4f9d\u8d56\u73b0\u6709\u6f5c\u5728\u5206\u5e03\u800c\u7f3a\u4e4f\u591a\u6837\u6027\uff1b\u5bf9\u6297\u65b9\u6cd5\u5e38\u4ea7\u751f\u7f3a\u4e4f\u7269\u7406\u4fdd\u771f\u5ea6\u7684\u4e0d\u73b0\u5b9e\u573a\u666f\uff1bLLM/VLM\u65b9\u6cd5\u5b58\u5728\u4e0a\u4e0b\u6587\u6291\u5236\u95ee\u9898\uff0c\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u4f1a\u8986\u76d6\u4e8b\u6545\u89c4\u683c\uff0c\u5bfc\u81f4\u573a\u666f\u504f\u79bb\u5b9e\u9645\u4e8b\u6545\u7279\u5f81\u3002", "method": "\u63d0\u51faSG-CADVLM\u6846\u67b6\uff0c\u96c6\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u7801\u4e0e\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\uff0c\u4ece\u4e8b\u6545\u62a5\u544a\u548c\u8def\u7f51\u56fe\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\u3002\u8be5\u6846\u67b6\u7f13\u89e3\u4e86VLM\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u9053\u8def\u51e0\u4f55\u548c\u8f66\u8f86\u8f68\u8ff9\u7684\u540c\u6b65\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSG-CADVLM\u751f\u6210\u5173\u952e\u98ce\u9669\u573a\u666f\u7684\u6bd4\u4f8b\u8fbe\u523084.4%\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u4ec5\u4e3a12.5%\uff0c\u63d0\u5347\u4e86469%\u3002\u540c\u65f6\u80fd\u591f\u751f\u6210\u53ef\u6267\u884c\u7684\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u4eff\u771f\u573a\u666f\u3002", "conclusion": "SG-CADVLM\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u7801\u6709\u6548\u89e3\u51b3\u4e86VLM\u65b9\u6cd5\u4e2d\u7684\u4e0a\u4e0b\u6587\u6291\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ece\u4e8b\u6545\u62a5\u544a\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\u7684\u8d28\u91cf\u548c\u771f\u5b9e\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4eff\u771f\u573a\u666f\u751f\u6210\u5de5\u5177\u3002"}}
{"id": "2601.17769", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17769", "abs": "https://arxiv.org/abs/2601.17769", "authors": ["Anqi Wang", "Zhengyi Li", "Lan Luo", "Xin Tong", "Pan Hui"], "title": "Reflexa: Uncovering How LLM-Supported Reflection Scaffolding Reshapes Creativity in Creative Coding", "comment": null, "summary": "Creative coding requires continuous translation between evolving concepts and computational artifacts, making reflection essential yet difficult to sustain. Creators often struggle to manage ambiguous intentions, emergent outputs, and complex code, limiting depth of exploration. This work examines how large language models (LLMs) can scaffold reflection not as isolated prompts, but as a system-level mechanism shaping creative regulation. From formative studies with eight expert creators, we derived reflection challenges and design principles that informed Reflexa, an integrated scaffold combining dialogic guidance, visualized version navigation, and iterative suggestion pathways. A within-subject study with 18 participants provides an exploratory mechanism validation, showing that structured reflection patterns mediate the link between AI interaction and creative outcomes. These reflection trajectories enhanced perceived controllability, broadened exploration, and improved originality and aesthetic quality. Our findings advance HCI understanding of reflection from LLM-assisted creative practices, and provide design strategies for building LLM-based creative tools that support richer human-AI co-creativity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReflexa\u7cfb\u7edf\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u7cfb\u7edf\u6027\u53cd\u601d\u673a\u5236\u652f\u6301\u521b\u610f\u7f16\u7a0b\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5f15\u5bfc\u3001\u53ef\u89c6\u5316\u7248\u672c\u5bfc\u822a\u548c\u8fed\u4ee3\u5efa\u8bae\u8def\u5f84\u589e\u5f3a\u521b\u4f5c\u8005\u5bf9\u6a21\u7cca\u610f\u56fe\u3001\u6d8c\u73b0\u8f93\u51fa\u548c\u590d\u6742\u4ee3\u7801\u7684\u53cd\u601d\u80fd\u529b\u3002", "motivation": "\u521b\u610f\u7f16\u7a0b\u9700\u8981\u5728\u4e0d\u65ad\u6f14\u5316\u7684\u6982\u5ff5\u548c\u8ba1\u7b97\u5236\u54c1\u4e4b\u95f4\u6301\u7eed\u7ffb\u8bd1\uff0c\u53cd\u601d\u81f3\u5173\u91cd\u8981\u4f46\u96be\u4ee5\u7ef4\u6301\u3002\u521b\u4f5c\u8005\u5e38\u96be\u4ee5\u7ba1\u7406\u6a21\u7cca\u610f\u56fe\u3001\u6d8c\u73b0\u8f93\u51fa\u548c\u590d\u6742\u4ee3\u7801\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u6df1\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u53cd\u601d\u652f\u6301\u673a\u5236\u3002", "method": "\u9996\u5148\u901a\u8fc78\u4f4d\u4e13\u5bb6\u521b\u4f5c\u8005\u7684\u5f62\u6784\u6027\u7814\u7a76\u786e\u5b9a\u53cd\u601d\u6311\u6218\u548c\u8bbe\u8ba1\u539f\u5219\uff0c\u636e\u6b64\u5f00\u53d1Reflexa\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u5bf9\u8bdd\u5f15\u5bfc\u3001\u53ef\u89c6\u5316\u7248\u672c\u5bfc\u822a\u548c\u8fed\u4ee3\u5efa\u8bae\u8def\u5f84\u3002\u901a\u8fc718\u540d\u53c2\u4e0e\u8005\u7684\u7ec4\u5185\u7814\u7a76\u8fdb\u884c\u63a2\u7d22\u6027\u673a\u5236\u9a8c\u8bc1\uff0c\u5206\u6790\u7ed3\u6784\u5316\u53cd\u601d\u6a21\u5f0f\u5982\u4f55\u4e2d\u4ecbAI\u4ea4\u4e92\u4e0e\u521b\u610f\u6210\u679c\u7684\u5173\u7cfb\u3002", "result": "\u7ed3\u6784\u5316\u53cd\u601d\u6a21\u5f0f\u4e2d\u4ecb\u4e86AI\u4ea4\u4e92\u4e0e\u521b\u610f\u6210\u679c\u7684\u8054\u7cfb\u3002\u53cd\u601d\u8f68\u8ff9\u589e\u5f3a\u4e86\u611f\u77e5\u53ef\u63a7\u6027\uff0c\u62d3\u5bbd\u4e86\u63a2\u7d22\u8303\u56f4\uff0c\u5e76\u63d0\u9ad8\u4e86\u539f\u521b\u6027\u548c\u7f8e\u5b66\u8d28\u91cf\u3002\u53c2\u4e0e\u8005\u901a\u8fc7\u7cfb\u7edf\u5316\u53cd\u601d\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u521b\u610f\u63a7\u5236\u3002", "conclusion": "\u7814\u7a76\u63a8\u8fdb\u4e86HCI\u5bf9LLM\u8f85\u52a9\u521b\u610f\u5b9e\u8df5\u4e2d\u53cd\u601d\u7684\u7406\u89e3\uff0c\u4e3a\u6784\u5efa\u652f\u6301\u66f4\u4e30\u5bcc\u4eba\u673a\u5171\u521b\u7684LLM\u521b\u610f\u5de5\u5177\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u7b56\u7565\u3002\u7cfb\u7edf\u6027\u53cd\u601d\u673a\u5236\u80fd\u6709\u6548\u589e\u5f3a\u521b\u610f\u7f16\u7a0b\u4e2d\u7684\u4eba\u7c7b\u63a7\u5236\u529b\u548c\u63a2\u7d22\u6df1\u5ea6\u3002"}}
{"id": "2601.18566", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18566", "abs": "https://arxiv.org/abs/2601.18566", "authors": ["Fabio Massacci", "Winnie Mbaka"], "title": "On the Abolition of the \"ICSE Paper\" and the Adoption of the \"Registered Proposal\" and the \"Results Report\"", "comment": "10 pages, 0 figures, International Conference on Software Engineering", "summary": "To address the 'novelty-vicious cycle' and the 'replicability crisis' of the field (both discussed in the survey) we propose abolishing the \"ICSE paper\" as we know it and replacing it with a two-tier system that also evolves the existing notion of 'Registered Report'. Authors proposing a new idea, experiment, or analysis would submit a \"Registered Proposal\" of their idea and the proposed experimental methodology to undergo peer review. The following year, anyone can submit (shorter) \"Results Reports\" on the realization of the empirical work based on the registered proposals of the previous ICSE (or FSE or ISSTA or ASE etc.). Both works should be first class citizens of the mainstream events. We argue that such a disruptive (heretical?) idea is supported and based on the responses of the community of the Future of Software Engineering pre-survey", "AI": {"tldr": "\u63d0\u51fa\u5e9f\u9664\u4f20\u7edfICSE\u8bba\u6587\u5f62\u5f0f\uff0c\u5efa\u7acb\u4e24\u9636\u6bb5\u7cfb\u7edf\uff1a\u5148\u63d0\u4ea4\u6ce8\u518c\u63d0\u6848\u8fdb\u884c\u65b9\u6cd5\u8bba\u8bc4\u5ba1\uff0c\u6b21\u5e74\u63d0\u4ea4\u7ed3\u679c\u62a5\u544a\u5b8c\u6210\u5b9e\u8bc1\u7814\u7a76", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u9762\u4e34\u7684\"\u521b\u65b0\u6076\u6027\u5faa\u73af\"\u548c\"\u53ef\u590d\u73b0\u6027\u5371\u673a\"\uff0c\u901a\u8fc7\u6539\u9769\u8bba\u6587\u53d1\u8868\u673a\u5236\u6765\u63d0\u5347\u7814\u7a76\u8d28\u91cf\u548c\u53ef\u9760\u6027", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u7cfb\u7edf\uff1a1) \u6ce8\u518c\u63d0\u6848\u9636\u6bb5\uff1a\u4f5c\u8005\u63d0\u4ea4\u65b0\u60f3\u6cd5\u3001\u5b9e\u9a8c\u6216\u5206\u6790\u65b9\u6cd5\u8bba\u8fdb\u884c\u540c\u884c\u8bc4\u5ba1\uff1b2) \u7ed3\u679c\u62a5\u544a\u9636\u6bb5\uff1a\u6b21\u5e74\u63d0\u4ea4\u57fa\u4e8e\u6ce8\u518c\u63d0\u6848\u7684\u5b9e\u8bc1\u7814\u7a76\u7ed3\u679c\u62a5\u544a", "result": "\u8be5\u63d0\u6848\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\u9884\u8c03\u67e5\u7684\u793e\u533a\u53cd\u9988\uff0c\u5efa\u8bae\u5c06\u6ce8\u518c\u63d0\u6848\u548c\u7ed3\u679c\u62a5\u544a\u90fd\u4f5c\u4e3a\u4e3b\u6d41\u4f1a\u8bae\u7684\u4e00\u7b49\u516c\u6c11", "conclusion": "\u8fd9\u79cd\u98a0\u8986\u6027\u6539\u9769\u80fd\u6253\u7834\u521b\u65b0\u6076\u6027\u5faa\u73af\uff0c\u63d0\u5347\u7814\u7a76\u53ef\u590d\u73b0\u6027\uff0c\u9700\u8981\u793e\u533a\u63a5\u53d7\u6ce8\u518c\u62a5\u544a\u7406\u5ff5\u5e76\u5efa\u7acb\u76f8\u5e94\u7684\u8bc4\u5ba1\u548c\u53d1\u8868\u673a\u5236"}}
{"id": "2601.17799", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17799", "abs": "https://arxiv.org/abs/2601.17799", "authors": ["Qing Zhang", "Junyu Chen", "Yifei Huang", "Jing Huang", "Thad Starner", "Kai Kunze", "Jun Rekimoto"], "title": "Beyond Symbols: Motion Perception Cues Enhance Dual-Task Performance with Wearable Directional Guidance", "comment": null, "summary": "Directional cues are crucial for environmental interaction. Conventional methods rely on symbolic visual or auditory reminders that require semantic interpretation, a process that proves challenging in demanding dual-tasking scenarios. We introduce a novel alternative for conveying directional cues on wearable displays: directly triggering motion perception using monocularly presented peripheral stimuli. This approach is designed for low visual interference, with the goal of reducing the need for gaze-switching and the complex cognitive processing associated with symbols. User studies demonstrate our method's potential to robustly convey directional cues. Compared to a conventional arrow-based technique in a demanding dual-task scenario, our motion-based approach resulted in significantly more accurate interpretation of these directional cues ($p=.008$) and showed a trend towards reduced errors on the concurrent primary task ($p=.066$).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5355\u773c\u5916\u5468\u523a\u6fc0\u76f4\u63a5\u89e6\u53d1\u8fd0\u52a8\u611f\u77e5\u7684\u65b0\u578b\u65b9\u5411\u63d0\u793a\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u7b26\u53f7\u65b9\u6cd5\u5728\u53cc\u4efb\u52a1\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18", "motivation": "\u4f20\u7edf\u65b9\u5411\u63d0\u793a\u65b9\u6cd5\u4f9d\u8d56\u9700\u8981\u8bed\u4e49\u89e3\u91ca\u7684\u7b26\u53f7\u89c6\u89c9\u6216\u542c\u89c9\u63d0\u9192\uff0c\u5728\u8981\u6c42\u9ad8\u7684\u53cc\u4efb\u52a1\u573a\u666f\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u51cf\u5c11\u6ce8\u89c6\u5207\u6362\u548c\u590d\u6742\u8ba4\u77e5\u5904\u7406", "method": "\u4f7f\u7528\u5355\u773c\u5448\u73b0\u7684\u5916\u5468\u523a\u6fc0\u76f4\u63a5\u89e6\u53d1\u8fd0\u52a8\u611f\u77e5\u6765\u4f20\u8fbe\u65b9\u5411\u63d0\u793a\uff0c\u8bbe\u8ba1\u4e3a\u4f4e\u89c6\u89c9\u5e72\u6270\u65b9\u6cd5", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4f20\u8fbe\u65b9\u5411\u63d0\u793a\u3002\u4e0e\u4f20\u7edf\u7bad\u5934\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u53cc\u4efb\u52a1\u573a\u666f\u4e2d\u65b9\u5411\u63d0\u793a\u89e3\u91ca\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8(p=.008)\uff0c\u5e76\u53d1\u4e3b\u8981\u4efb\u52a1\u9519\u8bef\u7387\u6709\u964d\u4f4e\u8d8b\u52bf(p=.066)", "conclusion": "\u57fa\u4e8e\u8fd0\u52a8\u611f\u77e5\u7684\u5355\u773c\u5916\u5468\u523a\u6fc0\u65b9\u6cd5\u4e3a\u53ef\u7a7f\u6234\u663e\u793a\u5668\u7684\u65b9\u5411\u63d0\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\u5e76\u63d0\u9ad8\u53cc\u4efb\u52a1\u6027\u80fd"}}
{"id": "2601.17135", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17135", "abs": "https://arxiv.org/abs/2601.17135", "authors": ["Jakob Karalus", "Friedhelm Schwenker"], "title": "ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning", "comment": null, "summary": "Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.", "AI": {"tldr": "ConceptACT\u6269\u5c55\u4e86Action Chunking with Transformers\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u65f6\u5229\u7528\u7247\u6bb5\u7ea7\u8bed\u4e49\u6982\u5ff5\u6807\u6ce8\u6765\u63d0\u5347\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\uff0c\u65e0\u9700\u5728\u90e8\u7f72\u65f6\u63d0\u4f9b\u8bed\u4e49\u8f93\u5165\u3002", "motivation": "\u5f53\u524d\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4f4e\u7ea7\u7684\u4f20\u611f\u5668\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u81ea\u7136\u62e5\u6709\u7684\u4e30\u5bcc\u8bed\u4e49\u77e5\u8bc6\uff08\u5982\u7269\u4f53\u5c5e\u6027\u3001\u7a7a\u95f4\u5173\u7cfb\u3001\u4efb\u52a1\u7ea6\u675f\uff09\u3002\u8fd9\u4e9b\u8bed\u4e49\u77e5\u8bc6\u53ef\u4ee5\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u5f3a\u5927\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "ConceptACT\u6269\u5c55\u4e86ACT\u67b6\u6784\uff0c\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u4eba\u7c7b\u63d0\u4f9b\u7684\u8bed\u4e49\u6982\u5ff5\u6807\u6ce8\uff08\u4ec5\u5728\u6f14\u793a\u6536\u96c6\u65f6\u6dfb\u52a0\uff0c\u90e8\u7f72\u65f6\u4e0d\u9700\u8981\uff09\u3002\u91c7\u7528\u6539\u8fdb\u7684transformer\u67b6\u6784\uff0c\u5728\u6700\u7ec8\u7f16\u7801\u5668\u5c42\u5b9e\u73b0\u6982\u5ff5\u611f\u77e5\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u4f7f\u5176\u4e0e\u4eba\u7c7b\u6807\u6ce8\u5bf9\u9f50\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u903b\u8f91\u7ea6\u675f\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConceptACT\u6bd4\u6807\u51c6ACT\u6536\u655b\u66f4\u5feb\uff0c\u6837\u672c\u6548\u7387\u66f4\u9ad8\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u67b6\u6784\u96c6\u6210\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u7b80\u5355\u7684\u8f85\u52a9\u9884\u6d4b\u635f\u5931\u6216\u8bed\u8a00\u6761\u4ef6\u6a21\u578b\u3002", "conclusion": "\u9002\u5f53\u96c6\u6210\u7684\u8bed\u4e49\u76d1\u7763\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u5229\u7528\u8bed\u4e49\u6982\u5ff5\uff0c\u4f46\u90e8\u7f72\u65f6\u4e0d\u9700\u8981\u8bed\u4e49\u8f93\u5165\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u63d0\u5347\u4e0e\u6807\u6ce8\u8d1f\u62c5\u3002"}}
{"id": "2601.18591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18591", "abs": "https://arxiv.org/abs/2601.18591", "authors": ["Fiorella Zampetti", "Federico Stocchetti", "Federica Razzano", "Damian Andrew Tamburri", "Massimiliano Di Penta"], "title": "How are MLOps Frameworks Used in Open Source Projects? An Empirical Characterization", "comment": null, "summary": "Machine Learning (ML) Operations (MLOps) frameworks have been conceived to support developers and AI engineers in managing the lifecycle of their ML models. While such frameworks provide a wide range of features, developers may leverage only a subset of them, while missing some highly desired features. This paper investigates the practical use and desired feature enhancements of eight popular open-source MLOps frameworks. Specifically, we analyze their usage by dependent projects on GitHub, examining how they invoke the frameworks' APIs and commands. Then, we qualitatively analyze feature requests and enhancements mined from the frameworks' issue trackers, relating these desired improvements to the previously identified usage features. Results indicate that MLOps frameworks are rarely used out-of-the-box and are infrequently integrated into GitHub Workflows, but rather, developers use their APIs to implement custom functionality in their projects. Used features concern core ML phases and whole infrastructure governance, sometimes leveraging multiple frameworks with complementary features. The mapping with feature requests highlights that users mainly ask for enhancements to core features of the frameworks, but also better API exposure and CI/CD integration.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e868\u4e2a\u6d41\u884c\u5f00\u6e90MLOps\u6846\u67b6\u7684\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u548c\u529f\u80fd\u9700\u6c42\uff0c\u53d1\u73b0\u5f00\u53d1\u8005\u4e3b\u8981\u4f7f\u7528API\u5b9e\u73b0\u81ea\u5b9a\u4e49\u529f\u80fd\u800c\u975e\u5f00\u7bb1\u5373\u7528\uff0c\u4e14\u8f83\u5c11\u96c6\u6210\u5230GitHub Workflows\u4e2d\u3002", "motivation": "MLOps\u6846\u67b6\u867d\u7136\u63d0\u4f9b\u5e7f\u6cdb\u529f\u80fd\uff0c\u4f46\u5f00\u53d1\u8005\u53ef\u80fd\u53ea\u4f7f\u7528\u90e8\u5206\u529f\u80fd\uff0c\u540c\u65f6\u9519\u8fc7\u4e00\u4e9b\u9ad8\u5ea6\u671f\u671b\u7684\u7279\u6027\u3002\u9700\u8981\u4e86\u89e3\u8fd9\u4e9b\u6846\u67b6\u7684\u5b9e\u9645\u4f7f\u7528\u6a21\u5f0f\u4ee5\u53ca\u7528\u6237\u671f\u671b\u7684\u529f\u80fd\u589e\u5f3a\u3002", "method": "1) \u5206\u6790GitHub\u4e0a\u4f9d\u8d56\u9879\u76ee\u5982\u4f55\u4f7f\u7528\u8fd9\u4e9b\u6846\u67b6\u7684API\u548c\u547d\u4ee4\uff1b2) \u4ece\u6846\u67b6\u7684\u95ee\u9898\u8ddf\u8e2a\u5668\u4e2d\u6316\u6398\u529f\u80fd\u8bf7\u6c42\u548c\u589e\u5f3a\u5efa\u8bae\uff1b3) \u5c06\u671f\u671b\u7684\u6539\u8fdb\u4e0e\u5df2\u8bc6\u522b\u7684\u4f7f\u7528\u7279\u5f81\u5173\u8054\u8d77\u6765\u3002", "result": "1) MLOps\u6846\u67b6\u5f88\u5c11\u5f00\u7bb1\u5373\u7528\uff0c\u4e5f\u5f88\u5c11\u96c6\u6210\u5230GitHub Workflows\u4e2d\uff1b2) \u5f00\u53d1\u8005\u4e3b\u8981\u4f7f\u7528API\u5728\u9879\u76ee\u4e2d\u5b9e\u73b0\u81ea\u5b9a\u4e49\u529f\u80fd\uff1b3) \u4f7f\u7528\u7684\u529f\u80fd\u6d89\u53ca\u6838\u5fc3ML\u9636\u6bb5\u548c\u6574\u4e2a\u57fa\u7840\u8bbe\u65bd\u6cbb\u7406\uff1b4) \u6709\u65f6\u4f1a\u5229\u7528\u591a\u4e2a\u5177\u6709\u4e92\u8865\u529f\u80fd\u7684\u6846\u67b6\uff1b5) \u529f\u80fd\u8bf7\u6c42\u4e3b\u8981\u9488\u5bf9\u6838\u5fc3\u529f\u80fd\u589e\u5f3a\u3001\u66f4\u597d\u7684API\u66b4\u9732\u548cCI/CD\u96c6\u6210\u3002", "conclusion": "MLOps\u6846\u67b6\u7684\u5b9e\u9645\u4f7f\u7528\u6a21\u5f0f\u4e0e\u9884\u671f\u4e0d\u540c\uff0c\u5f00\u53d1\u8005\u66f4\u503e\u5411\u4e8e\u4f7f\u7528API\u6784\u5efa\u81ea\u5b9a\u4e49\u89e3\u51b3\u65b9\u6848\u800c\u975e\u76f4\u63a5\u4f7f\u7528\u9884\u7f6e\u529f\u80fd\u3002\u6846\u67b6\u9700\u8981\u6539\u8fdb\u6838\u5fc3\u529f\u80fd\u3001API\u8bbe\u8ba1\u548cCI/CD\u96c6\u6210\u4ee5\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002"}}
{"id": "2601.18537", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18537", "abs": "https://arxiv.org/abs/2601.18537", "authors": ["Linyong Gan", "Zimo Li", "Wenxin Xu", "Xingjian Li", "Jianhua Z. Huang", "Enmei Tu", "Shuhang Chen"], "title": "SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction", "comment": null, "summary": "Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u5173\u952e\u70b9\u6761\u4ef6\u7684\u8f68\u8ff9\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u9ad8\u5c42\u7ea7\u7684\u4e0b\u4e00\u4e2a\u5173\u952e\u70b9\u6765\u6355\u83b7\u822a\u884c\u610f\u56fe\uff0c\u5c06\u957f\u65f6\u7a0b\u9884\u6d4b\u5206\u89e3\u4e3a\u5168\u5c40\u8bed\u4e49\u51b3\u7b56\u548c\u5c40\u90e8\u8fd0\u52a8\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u65b9\u5411\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u9884\u6d4b\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u7531\u4e8e\u590d\u6742\u822a\u884c\u884c\u4e3a\u548c\u73af\u5883\u56e0\u7d20\u7684\u7d2f\u79ef\u4e0d\u786e\u5b9a\u6027\uff0c\u96be\u4ee5\u4fdd\u6301\u5168\u5c40\u65b9\u5411\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u8f68\u8ff9\u6f02\u79fb\u6216\u4e0d\u5408\u7406\u3002\u9700\u8981\u89e3\u51b3\u957f\u65f6\u7a0b\u9884\u6d4b\u4e2d\u7684\u8bed\u4e49\u53ef\u884c\u6027\u548c\u65b9\u5411\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u5173\u952e\u70b9\u6761\u4ef6\u7684\u8f68\u8ff9\u5efa\u6a21\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u9884\u6d4b\u9ad8\u5c42\u7ea7\u7684\u4e0b\u4e00\u4e2a\u5173\u952e\u70b9\uff08NKP\uff09\u6355\u83b7\u822a\u884c\u610f\u56fe\uff1b2\uff09\u5c06\u957f\u65f6\u7a0b\u9884\u6d4b\u5206\u89e3\u4e3a\u5168\u5c40\u8bed\u4e49\u51b3\u7b56\u548c\u5c40\u90e8\u8fd0\u52a8\u5efa\u6a21\uff1b3\uff09\u91c7\u7528\u9884\u8bad\u7ec3-\u5fae\u8c03\u7b56\u7565\u9ad8\u6548\u4f30\u8ba1NKP\u5148\u9a8c\uff1b4\uff09\u9650\u5236\u672a\u6765\u8f68\u8ff9\u7684\u652f\u6491\u96c6\u4e3a\u8bed\u4e49\u53ef\u884c\u7684\u5b50\u96c6\u3002", "result": "\u5728\u771f\u5b9eAIS\u6570\u636e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u65c5\u884c\u65f6\u957f\u3001\u65b9\u5411\u51c6\u786e\u6027\u548c\u7ec6\u7c92\u5ea6\u8f68\u8ff9\u9884\u6d4b\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u5173\u952e\u70b9\u6761\u4ef6\u7684\u8f68\u8ff9\u5efa\u6a21\u6846\u67b6\uff0c\u5c06\u957f\u65f6\u7a0b\u9884\u6d4b\u5206\u89e3\u4e3a\u5168\u5c40\u8bed\u4e49\u51b3\u7b56\u548c\u5c40\u90e8\u8fd0\u52a8\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u65b9\u5411\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u53ef\u884c\u6027\u95ee\u9898\uff0c\u4e3a\u957f\u65f6\u7a0b\u8f68\u8ff9\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17907", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17907", "abs": "https://arxiv.org/abs/2601.17907", "authors": ["Numan Halit Guldemir", "Oluwafemi Olukoya", "Jes\u00fas Mart\u00ednez-del-Rinc\u00f3n"], "title": "FARM: Few-shot Adaptive Malware Family Classification under Concept Drift", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Malware classification models often face performance degradation due to concept drift, arising from evolving threat landscapes and the emergence of novel malware families. This paper presents FARM (Few-shot Adaptive Recognition of Malware), a framework designed to detect and adapt to both covariate and label drift in Windows Portable Executable (PE) malware classification. FARM leverages a triplet autoencoder to project samples into a discriminative latent space, enabling unsupervised drift detection via DBSCAN clustering and dynamic thresholding. For rapid adaptation, it employs few-shot learning using prototype-based classification, requiring only a handful of labeled samples. FARM also supports full retraining when enough drifted samples accumulate, updating the latent space for long-term integration. Experiments on the BenchMFC dataset demonstrate that FARM improves classification performance under covariate drift by 5.6\\%, and achieves an average F1 score of 0.85 on unseen malware families using only few-shot adaptation, which further increases to 0.94 after retraining. These results highlight FARM's robustness and adaptability in dynamic malware detection environments under limited supervision.", "AI": {"tldr": "FARM\u6846\u67b6\u901a\u8fc7\u4e09\u5143\u7ec4\u81ea\u7f16\u7801\u5668\u3001\u65e0\u76d1\u7763\u6f02\u79fb\u68c0\u6d4b\u548c\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u89e3\u51b3\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u6a21\u578b\u9762\u4e34\u6982\u5ff5\u6f02\u79fb\uff08\u534f\u53d8\u91cf\u6f02\u79fb\u548c\u6807\u7b7e\u6f02\u79fb\uff09\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u9700\u8981\u9002\u5e94\u52a8\u6001\u6f14\u53d8\u7684\u5a01\u80c1\u73af\u5883\u548c\u65b0\u578b\u6076\u610f\u8f6f\u4ef6\u5bb6\u65cf\u3002", "method": "\u4f7f\u7528\u4e09\u5143\u7ec4\u81ea\u7f16\u7801\u5668\u5c06\u6837\u672c\u6295\u5f71\u5230\u5224\u522b\u6027\u6f5c\u5728\u7a7a\u95f4\uff1b\u901a\u8fc7DBSCAN\u805a\u7c7b\u548c\u52a8\u6001\u9608\u503c\u8fdb\u884c\u65e0\u76d1\u7763\u6f02\u79fb\u68c0\u6d4b\uff1b\u91c7\u7528\u57fa\u4e8e\u539f\u578b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u8fdb\u884c\u5feb\u901f\u9002\u5e94\uff1b\u652f\u6301\u79ef\u7d2f\u8db3\u591f\u6f02\u79fb\u6837\u672c\u540e\u7684\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728BenchMFC\u6570\u636e\u96c6\u4e0a\uff0cFARM\u5c06\u534f\u53d8\u91cf\u6f02\u79fb\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u63d0\u53475.6%\uff1b\u4f7f\u7528\u5c11\u6837\u672c\u9002\u5e94\u5728\u672a\u89c1\u6076\u610f\u8f6f\u4ef6\u5bb6\u65cf\u4e0a\u5e73\u5747F1\u5f97\u5206\u4e3a0.85\uff1b\u91cd\u65b0\u8bad\u7ec3\u540e\u8fdb\u4e00\u6b65\u589e\u81f30.94\u3002", "conclusion": "FARM\u5728\u6709\u9650\u76d1\u7763\u4e0b\u5c55\u73b0\u51fa\u5bf9\u52a8\u6001\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u73af\u5883\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\u3002"}}
{"id": "2601.17180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17180", "abs": "https://arxiv.org/abs/2601.17180", "authors": ["In\u00e9s Gonzalez-Pepe", "Vinuyan Sivakolunthu", "Jacob Fortin", "Yohan Chatelain", "Tristan Glatard"], "title": "Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging", "comment": null, "summary": "Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790CNN\u6570\u503c\u4e0d\u786e\u5b9a\u6027\uff0c\u53d1\u73b0\u8bb8\u591a\u5377\u79ef\u64cd\u4f5c\u5bf9\u6570\u503c\u566a\u58f0\u4e3b\u5bfc\u7684\u503c\u8fdb\u884c\u8ba1\u7b97\u662f\u5197\u4f59\u7684\u3002\u63d0\u51faConservative & Aggressive NaNs\u4e24\u79cd\u65b9\u6cd5\uff0c\u5229\u7528NaN\u6807\u8bb0\u4e0d\u7a33\u5b9a\u4f53\u7d20\uff0c\u8df3\u8fc7\u65e0\u5173\u8ba1\u7b97\uff0c\u5b9e\u73b0\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u795e\u7ecf\u5f71\u50cf\u5b66\u4e2d\u89c4\u6a21\u65e5\u76ca\u589e\u5927\uff0c\u6548\u7387\u95ee\u9898\u6301\u7eed\u5b58\u5728\u3002\u7814\u7a76\u53d1\u73b0CNN\u4e2d\u8bb8\u591a\u64cd\u4f5c\u5e94\u7528\u4e8e\u6570\u503c\u566a\u58f0\u4e3b\u5bfc\u7684\u503c\uff0c\u5bf9\u6a21\u578b\u8f93\u51fa\u5f71\u54cd\u53ef\u5ffd\u7565\uff0c\u5b58\u5728\u5927\u91cf\u5197\u4f59\u8ba1\u7b97\u3002", "method": "\u63d0\u51faConservative & Aggressive NaNs\u4e24\u79cdmax pooling\u548cunpooling\u53d8\u4f53\uff0c\u8bc6\u522b\u6570\u503c\u4e0d\u7a33\u5b9a\u4f53\u7d20\u5e76\u7528NaN\u66ff\u6362\uff0c\u4f7f\u540e\u7eed\u5c42\u80fd\u8df3\u8fc7\u65e0\u5173\u6570\u636e\u8ba1\u7b97\u3002\u65b9\u6cd5\u5728PyTorch\u4e2d\u5b9e\u73b0\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u3002", "result": "\u5f53\u8f93\u5165\u5305\u542b\u81f3\u5c1150% NaN\u65f6\uff0c\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u8fd0\u884c\u65f6\u6539\u8fdb\uff1b\u5bf9\u4e8e\u8d85\u8fc7\u4e09\u5206\u4e4b\u4e8cNaN\u7684\u6570\u636e\uff08\u5e38\u89c1\u4e8e\u795e\u7ecf\u5f71\u50cf\uff09\uff0c\u5e73\u5747\u63a8\u7406\u52a0\u901f1.67\u500d\u3002Conservative NaNs\u5e73\u5747\u51cf\u5c1130%\u5377\u79ef\u64cd\u4f5c\uff0c\u65e0\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u5b9a\u5c42\u53ef\u8df3\u8fc764.64%\u5377\u79ef\u3002Aggressive NaNs\u53ef\u8df3\u8fc769.30%\u5377\u79ef\uff0c\u4f46\u53ef\u80fd\u5076\u5c14\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u6570\u503c\u4e0d\u786e\u5b9a\u6027\u53ef\u88ab\u5229\u7528\u6765\u51cf\u5c11CNN\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u5e76\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0cConservative & Aggressive NaNs\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u63a8\u7406\uff0c\u7279\u522b\u9002\u7528\u4e8e\u795e\u7ecf\u5f71\u50cf\u7b49\u6570\u636e\u7a00\u758f\u573a\u666f\u3002"}}
{"id": "2601.18548", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18548", "abs": "https://arxiv.org/abs/2601.18548", "authors": ["Yulin Li", "Zhiyuan Song", "Yiming Li", "Zhicheng Song", "Kai Chen", "Chunxin Zheng", "Zhihai Bi", "Jiahang Cao", "Sylvain Calinon", "Fan Shi", "Jun Ma"], "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field", "comment": null, "summary": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes.", "AI": {"tldr": "\u63d0\u51fa\u5e7f\u4e49\u6784\u578b\u7a7a\u95f4\u8ddd\u79bb\u573a(GCDF)\uff0c\u5c06\u56fa\u5b9a\u57fa\u5ea7\u673a\u68b0\u81c2\u7684CDF\u6269\u5c55\u5230\u79fb\u52a8\u64cd\u4f5c\u5668\uff0c\u89e3\u51b3\u65e0\u754c\u5de5\u4f5c\u7a7a\u95f4\u548c\u7d27\u5bc6\u57fa\u5ea7-\u81c2\u8026\u5408\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8eGCDF\u7684\u9ad8\u6027\u80fd\u5e8f\u5217\u51f8\u4f18\u5316\u6846\u67b6", "motivation": "\u79fb\u52a8\u64cd\u4f5c\u5668\u5728\u590d\u6742\u53d7\u9650\u7a7a\u95f4\u4e2d\u7684\u5168\u8eab\u8f68\u8ff9\u4f18\u5316\u9762\u4e34\u9ad8\u7ef4\u975e\u51f8\u6027\u548c\u5feb\u901f\u7cbe\u786e\u78b0\u649e\u63a8\u7406\u7684\u6311\u6218\uff0c\u73b0\u6709CDF\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u5230\u79fb\u52a8\u64cd\u4f5c\u5668\uff0c\u56e0\u5176\u5b58\u5728\u65e0\u754c\u5de5\u4f5c\u7a7a\u95f4\u548c\u66f4\u7d27\u5bc6\u7684\u57fa\u5ea7-\u81c2\u8026\u5408\u95ee\u9898", "method": "\u63d0\u51faGCDF\u6269\u5c55CDF\u5230\u5305\u542b\u5e73\u79fb\u548c\u65cb\u8f6c\u5173\u8282\u7684\u79fb\u52a8\u64cd\u4f5c\u5668\uff0c\u8bc1\u660e\u5176\u4fdd\u6301\u6b27\u51e0\u91cc\u5f97\u5c40\u90e8\u8ddd\u79bb\u7ed3\u6784\uff1b\u5f00\u53d1\u6570\u636e\u751f\u6210\u548c\u8bad\u7ec3\u6d41\u7a0b\u83b7\u5f97\u8fde\u7eed\u795e\u7ecfGCDF\uff1b\u6784\u5efa\u57fa\u4e8eGCDF\u7684\u5e8f\u5217\u51f8\u4f18\u5316\u6846\u67b6\uff0c\u652f\u6301\u5728\u7ebf\u795e\u7ecf\u7ea6\u675f\u89c4\u8303\u3001\u7a00\u758f\u611f\u77e5\u4e3b\u52a8\u96c6\u68c0\u6d4b\u548c\u589e\u91cf\u7ea6\u675f\u7ba1\u7406", "result": "GCDF\u80fd\u51c6\u786e\u7f16\u7801\u5168\u8eab\u51e0\u4f55\u5230\u6784\u578b\u7a7a\u95f4\uff0c\u652f\u6301\u9ad8\u6548GPU\u6279\u5904\u7406\u67e5\u8be2\uff1b\u4f18\u5316\u6846\u67b6\u80fd\u6269\u5c55\u5230\u5927\u91cf\u9690\u5f0f\u7ea6\u675f\uff0c\u5b9e\u73b0\u5feb\u901f\u91cd\u89c4\u5212\u4ee5\u9002\u5e94\u573a\u666f\u53d8\u5316", "conclusion": "GCDF\u4e3a\u79fb\u52a8\u64cd\u4f5c\u5668\u63d0\u4f9b\u4e86\u6784\u578b\u7a7a\u95f4\u4e2d\u7684\u7cbe\u786e\u78b0\u649e\u8868\u793a\uff0c\u57fa\u4e8eGCDF\u7684\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e2d\u79fb\u52a8\u64cd\u4f5c\u5668\u8f68\u8ff9\u4f18\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u78b0\u649e\u63a8\u7406\u548c\u89c4\u5212"}}
{"id": "2601.17909", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17909", "abs": "https://arxiv.org/abs/2601.17909", "authors": ["Adriana Watson"], "title": "From Statistical Disclosure Control to Fair AI: Navigating Fundamental Tradeoffs in Differential Privacy", "comment": "8 pages, 3 figures", "summary": "Differential privacy has become the gold standard for privacy-preserving machine learning systems. Unfortunately, subsequent work has primarily fixated on the privacy-utility tradeoff, leaving the subject of fairness constraints undervalued and under-researched. This paper provides a systematic treatment connecting three threads: (1) Dalenius's impossibility results for semantic privacy, (2) Dwork's differential privacy as an achievable alternative, and (3) emerging impossibility results from the addition of a fairness requirement. Through concrete examples and technical analysis, the three-way Pareto frontier between privacy, utility, and fairness is demonstrated to showcase the fundamental limits on what can be simultaneously achieved. In this work, these limits are characterized, the impact on minority groups is demonstrated, and practical guidance for navigating these tradeoffs are provided. This forms a unified framework synthesizing scattered results to help practitioners and policymakers make informed decisions when deploying private fair learning systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u9690\u79c1\u3001\u6548\u7528\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u4e09\u65b9\u6743\u8861\uff0c\u63ed\u793a\u4e86\u540c\u65f6\u5b9e\u73b0\u8fd9\u4e09\u8005\u7684\u57fa\u672c\u9650\u5236\uff0c\u4e3a\u90e8\u7f72\u9690\u79c1\u4fdd\u62a4\u516c\u5e73\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u5c3d\u7ba1\u5dee\u5206\u9690\u79c1\u5df2\u6210\u4e3a\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46\u540e\u7eed\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u800c\u516c\u5e73\u6027\u7ea6\u675f\u88ab\u4f4e\u4f30\u548c\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u6027\u5730\u8fde\u63a5Dalenius\u7684\u8bed\u4e49\u9690\u79c1\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u3001Dwork\u7684\u53ef\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\u4ee5\u53ca\u6dfb\u52a0\u516c\u5e73\u6027\u8981\u6c42\u540e\u51fa\u73b0\u7684\u65b0\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u5177\u4f53\u793a\u4f8b\u548c\u6280\u672f\u5206\u6790\uff0c\u5c55\u793a\u9690\u79c1\u3001\u6548\u7528\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u4e09\u65b9\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u8868\u5f81\u8fd9\u4e9b\u57fa\u672c\u9650\u5236\uff0c\u5e76\u6f14\u793a\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u5f71\u54cd\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7efc\u5408\u5206\u6563\u7684\u7814\u7a76\u7ed3\u679c\u3002", "result": "\u63ed\u793a\u4e86\u540c\u65f6\u5b9e\u73b0\u9690\u79c1\u3001\u6548\u7528\u548c\u516c\u5e73\u6027\u7684\u57fa\u672c\u9650\u5236\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u6743\u8861\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u5177\u4f53\u5f71\u54cd\uff0c\u4e3a\u5b9e\u8df5\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u5bfc\u822a\u8fd9\u4e9b\u6743\u8861\u7684\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5e2e\u52a9\u5b9e\u8df5\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5728\u90e8\u7f72\u9690\u79c1\u4fdd\u62a4\u516c\u5e73\u5b66\u4e60\u7cfb\u7edf\u65f6\u505a\u51fa\u660e\u667a\u51b3\u7b56\uff0c\u7cfb\u7edf\u6027\u5730\u8fde\u63a5\u4e86\u9690\u79c1\u3001\u6548\u7528\u548c\u516c\u5e73\u6027\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.17824", "categories": ["cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17824", "abs": "https://arxiv.org/abs/2601.17824", "authors": ["Saber Zerhoudi", "Michael Dinzinger", "Michael Granitzer", "Jelena Mitrovic"], "title": "OwlerLite: Scope- and Freshness-Aware Web Retrieval for LLM Assistants", "comment": null, "summary": "Browser-based language models often use retrieval-augmented generation (RAG) but typically rely on fixed, outdated indices that give users no control over which sources are consulted. This can lead to answers that mix trusted and untrusted content or draw on stale information. We present OwlerLite, a browser-based RAG system that makes user-defined scopes and data freshness central to retrieval. Users define reusable scopes-sets of web pages or sources-and select them when querying. A freshness-aware crawler monitors live pages, uses a semantic change detector to identify meaningful updates, and selectively re-indexes changed content. OwlerLite integrates text relevance, scope choice, and recency into a unified retrieval model. Implemented as a browser extension, it represents a step toward more controllable and trustworthy web assistants.", "AI": {"tldr": "OwlerLite\u662f\u4e00\u4e2a\u6d4f\u89c8\u5668\u6269\u5c55\u7684RAG\u7cfb\u7edf\uff0c\u5141\u8bb8\u7528\u6237\u5b9a\u4e49\u53ef\u91cd\u7528\u7684\u7f51\u9875\u8303\u56f4\uff0c\u5e76\u901a\u8fc7\u65b0\u9c9c\u5ea6\u611f\u77e5\u722c\u866b\u76d1\u63a7\u66f4\u65b0\uff0c\u5b9e\u73b0\u66f4\u53ef\u63a7\u3001\u53ef\u4fe1\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u6d4f\u89c8\u5668\u8bed\u8a00\u6a21\u578b\u7684RAG\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u3001\u8fc7\u65f6\u7684\u7d22\u5f15\uff0c\u7528\u6237\u65e0\u6cd5\u63a7\u5236\u68c0\u7d22\u6e90\uff0c\u5bfc\u81f4\u7b54\u6848\u53ef\u80fd\u6df7\u5408\u53ef\u4fe1\u4e0e\u4e0d\u53ef\u4fe1\u5185\u5bb9\u6216\u4f7f\u7528\u9648\u65e7\u4fe1\u606f\u3002", "method": "1. \u7528\u6237\u5b9a\u4e49\u53ef\u91cd\u7528\u7684\u8303\u56f4\uff08\u7f51\u9875\u6216\u6e90\u96c6\u5408\uff09\u5e76\u5728\u67e5\u8be2\u65f6\u9009\u62e9\uff1b2. \u65b0\u9c9c\u5ea6\u611f\u77e5\u722c\u866b\u76d1\u63a7\u5b9e\u65f6\u9875\u9762\uff0c\u4f7f\u7528\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u5668\u8bc6\u522b\u6709\u610f\u4e49\u7684\u66f4\u65b0\uff1b3. \u9009\u62e9\u6027\u91cd\u65b0\u7d22\u5f15\u5df2\u66f4\u6539\u5185\u5bb9\uff1b4. \u5c06\u6587\u672c\u76f8\u5173\u6027\u3001\u8303\u56f4\u9009\u62e9\u548c\u65b0\u9c9c\u5ea6\u96c6\u6210\u5230\u7edf\u4e00\u7684\u68c0\u7d22\u6a21\u578b\u4e2d\u3002", "result": "\u5b9e\u73b0\u4e86\u6d4f\u89c8\u5668\u6269\u5c55\u5f62\u5f0f\u7684OwlerLite\u7cfb\u7edf\uff0c\u4ee3\u8868\u4e86\u5411\u66f4\u53ef\u63a7\u3001\u66f4\u53ef\u4fe1\u7684\u7f51\u9875\u52a9\u624b\u8fc8\u51fa\u7684\u4e00\u6b65\u3002", "conclusion": "OwlerLite\u901a\u8fc7\u5c06\u7528\u6237\u5b9a\u4e49\u7684\u8303\u56f4\u548c\u6570\u636e\u65b0\u9c9c\u5ea6\u7f6e\u4e8e\u68c0\u7d22\u6838\u5fc3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRAG\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u63a7\u548c\u53ef\u4fe1\u7684\u6d4f\u89c8\u5668\u8bed\u8a00\u6a21\u578b\u4f53\u9a8c\u3002"}}
{"id": "2601.17183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17183", "abs": "https://arxiv.org/abs/2601.17183", "authors": ["Farzam Asad", "Junaid Saif Khan", "Maria Tariq", "Sundus Munir", "Muhammad Adnan Khan"], "title": "Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data", "comment": "27 pages, 7 figures, 4 tables", "summary": "Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u533b\u9662\u95f4\u7684\u975eIID\u6570\u636e\u5206\u5e03\uff0c\u8bc4\u4f30FedProx\u7b97\u6cd5\u5728\u5fc3\u810f\u75c5\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u5176\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u548c\u5b64\u7acb\u672c\u5730\u6a21\u578b\u3002", "motivation": "\u533b\u7597\u6570\u636e\u56e0\u9690\u79c1\u6cd5\u89c4\u65e0\u6cd5\u76f4\u63a5\u5171\u4eab\uff0c\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4e34\u5e8a\u6570\u636e\u5b58\u5728\u975eIID\u7279\u6027\uff08\u4eba\u53e3\u7edf\u8ba1\u5dee\u5f02\u3001\u75be\u75c5\u6d41\u884c\u5ea6\u5dee\u5f02\u3001\u673a\u6784\u5b9e\u8df5\u5dee\u5f02\uff09\uff0c\u9700\u8981\u6709\u6548\u5904\u7406\u5f02\u6784\u6027\u7684\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528UCI\u5fc3\u810f\u75c5\u6570\u636e\u96c6\uff08\u514b\u5229\u592b\u5170\u8bca\u6240303\u540d\u60a3\u8005\uff09\uff0c\u901a\u8fc7\u4eba\u53e3\u7edf\u8ba1\u5206\u5c42\u6a21\u62df\u56db\u4e2a\u5f02\u6784\u533b\u9662\u5ba2\u6237\u7aef\uff0c\u521b\u5efa\u73b0\u5b9e\u975eIID\u6570\u636e\u5206\u533a\u3002\u91c7\u7528FedProx\u7b97\u6cd5\uff0c\u8fdb\u884c50\u6b21\u72ec\u7acb\u8fd0\u884c\u7684\u5e7f\u6cdb\u6d88\u878d\u7814\u7a76\uff0c\u8bc4\u4f30\u4e0d\u540c\u8fd1\u7aef\u53c2\u6570\u03bc\u503c\u7684\u6548\u679c\u3002", "result": "FedProx\u5728\u03bc=0.05\u65f6\u8fbe\u523085.00%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u5b66\u4e60\uff0883.33%\uff09\u548c\u5b64\u7acb\u672c\u5730\u6a21\u578b\uff08\u5e73\u574778.45%\uff09\u3002\u8fd1\u7aef\u6b63\u5219\u5316\u80fd\u6709\u6548\u6291\u5236\u5f02\u6784\u73af\u5883\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\uff0c\u4e14\u4e0d\u6cc4\u9732\u60a3\u8005\u9690\u79c1\u3002", "conclusion": "FedProx\u5728\u533b\u7597\u8054\u90a6\u5b66\u4e60\u4e2d\u80fd\u6709\u6548\u5904\u7406\u975eIID\u6570\u636e\uff0c\u8fd1\u7aef\u6b63\u5219\u5316\u5bf9\u63a7\u5236\u5ba2\u6237\u7aef\u6f02\u79fb\u81f3\u5173\u91cd\u8981\u3002\u8be5\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u4e3a\u5b9e\u9645\u533b\u7597\u8054\u90a6\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u7b97\u6cd5\u89c1\u89e3\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u7ed3\u679c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u533b\u9662IT\u7ba1\u7406\u3002"}}
{"id": "2601.18569", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18569", "abs": "https://arxiv.org/abs/2601.18569", "authors": ["Seokju Lee", "Kyung-Soo Kim"], "title": "Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation", "comment": "8 pages, 6 figures, Accepted to IEEE Robotics and Automation Letters (RA-L)", "summary": "In this letter, we propose an Attention-Based Neural-Augmented Kalman Filter (AttenNKF) for state estimation in legged robots. Foot slip is a major source of estimation error: when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the update step. Our objective is to estimate this slip-induced error and compensate for it. To this end, we augment an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer error conditioned on foot-slip severity and then applies this estimate as a post-update compensation to the InEKF state (i.e., after the filter update). The compensator is trained in a latent space, which aims to reduce sensitivity to raw input scales and encourages structured slip-conditioned compensations, while preserving the InEKF recursion. Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u589e\u5f3a\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08AttenNKF\uff09\uff0c\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8865\u507f\u811a\u6ed1\u5f15\u8d77\u7684\u8bef\u5dee", "motivation": "\u811a\u6ed1\u662f\u817f\u5f0f\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u8bef\u5dee\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u5f53\u53d1\u751f\u811a\u6ed1\u65f6\uff0c\u8fd0\u52a8\u5b66\u6d4b\u91cf\u8fdd\u53cd\u65e0\u6ed1\u79fb\u5047\u8bbe\uff0c\u5728\u66f4\u65b0\u6b65\u9aa4\u4e2d\u5f15\u5165\u504f\u5dee", "method": "\u5c06\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08InEKF\uff09\u4e0e\u795e\u7ecf\u8865\u507f\u5668\u589e\u5f3a\uff0c\u8865\u507f\u5668\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u6839\u636e\u811a\u6ed1\u4e25\u91cd\u7a0b\u5ea6\u63a8\u65ad\u8bef\u5dee\uff0c\u5e76\u5728InEKF\u72b6\u6001\u66f4\u65b0\u540e\u5e94\u7528\u8865\u507f\uff1b\u8865\u507f\u5668\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\uff0c\u51cf\u5c11\u5bf9\u539f\u59cb\u8f93\u5165\u5c3a\u5ea6\u7684\u654f\u611f\u6027\uff0c\u9f13\u52b1\u7ed3\u6784\u5316\u6ed1\u79fb\u6761\u4ef6\u8865\u507f\uff0c\u540c\u65f6\u4fdd\u6301InEKF\u9012\u5f52", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u817f\u5f0f\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u5668\u76f8\u6bd4\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u6613\u6ed1\u79fb\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18", "conclusion": "AttenNKF\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u8865\u507f\u811a\u6ed1\u5f15\u8d77\u7684\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u6ed1\u79fb\u6761\u4ef6\u4e0b\u7684\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6"}}
{"id": "2601.17911", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17911", "abs": "https://arxiv.org/abs/2601.17911", "authors": ["Thomas Heverin"], "title": "Prompt Injection Evaluations: Refusal Boundary Instability and Artifact-Dependent Compliance in GPT-4-Series Models", "comment": "15 pages, 3 figures, 1 table", "summary": "Prompt injection evaluations typically treat refusal as a stable, binary indicator of safety. This study challenges that paradigm by modeling refusal as a local decision boundary and examining its stability under structured perturbations. We evaluated two models, GPT-4.1 and GPT-4o, using 3,274 perturbation runs derived from refusal-inducing prompt injection attempts. Each base prompt was subjected to 25 perturbations across five structured families, with outcomes manually coded as Refusal, Partial Compliance, or Full Compliance.\n  Using chi-square tests, logistic regression, mixed-effects modeling, and a novel Refusal Boundary Entropy (RBE) metric, we demonstrate that while both models refuse >94% of attempts, refusal instability is persistent and non-uniform. Approximately one-third of initial refusal-inducing prompts exhibited at least one \"refusal escape,\" a transition to compliance under perturbation. We find that artifact type is a stronger predictor of refusal failure than perturbation style. Textual artifacts, such as ransomware notes, exhibited significantly higher instability, with flip rates exceeding 20%. Conversely, executable malware artifacts showed zero refusal escapes in both models. While GPT-4o demonstrated tighter refusal enforcement and lower RBE than GPT-4.1, it did not eliminate artifact-dependent risks. These findings suggest that single-prompt evaluations systematically overestimate safety robustness. We conclude that refusal behavior is a probabilistic, artifact-dependent boundary phenomenon rather than a stable binary property, requiring a shift in how LLM safety is measured and audited.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u62d2\u7edd\u884c\u4e3a\u5e76\u975e\u7a33\u5b9a\u7684\u4e8c\u5143\u5c5e\u6027\uff0c\u800c\u662f\u6982\u7387\u6027\u7684\u3001\u4f9d\u8d56\u4e8e\u5177\u4f53\u653b\u51fb\u7c7b\u578b\u7684\u8fb9\u754c\u73b0\u8c61\uff0c\u5355\u4e00\u63d0\u793a\u8bc4\u4f30\u4f1a\u7cfb\u7edf\u6027\u9ad8\u4f30\u5b89\u5168\u6027\u9c81\u68d2\u6027\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u8bc4\u4f30\u8303\u5f0f\u2014\u2014\u5c06\u62d2\u7edd\u89c6\u4e3a\u7a33\u5b9a\u4e8c\u5143\u5b89\u5168\u6307\u6807\uff0c\u63ed\u793a\u62d2\u7edd\u884c\u4e3a\u5728\u7ed3\u6784\u5316\u6270\u52a8\u4e0b\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u8bc1\u660e\u5355\u4e00\u63d0\u793a\u8bc4\u4f30\u4f1a\u7cfb\u7edf\u6027\u9ad8\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u9c81\u68d2\u6027\u3002", "method": "\u8bc4\u4f30GPT-4.1\u548cGPT-4o\u4e24\u4e2a\u6a21\u578b\uff0c\u4f7f\u75283,274\u6b21\u6270\u52a8\u8fd0\u884c\uff0c\u6e90\u81ea\u62d2\u7edd\u8bf1\u5bfc\u7684\u63d0\u793a\u6ce8\u5165\u5c1d\u8bd5\u3002\u6bcf\u4e2a\u57fa\u7840\u63d0\u793a\u7ecf\u8fc725\u6b21\u8de8\u4e94\u4e2a\u7ed3\u6784\u5316\u5bb6\u65cf\u7684\u6270\u52a8\uff0c\u7ed3\u679c\u624b\u52a8\u7f16\u7801\u4e3a\u62d2\u7edd\u3001\u90e8\u5206\u5408\u89c4\u6216\u5b8c\u5168\u5408\u89c4\u3002\u91c7\u7528\u5361\u65b9\u68c0\u9a8c\u3001\u903b\u8f91\u56de\u5f52\u3001\u6df7\u5408\u6548\u5e94\u6a21\u578b\u548c\u65b0\u578b\u62d2\u7edd\u8fb9\u754c\u71b5\uff08RBE\uff09\u6307\u6807\u8fdb\u884c\u5206\u6790\u3002", "result": "\u867d\u7136\u4e24\u4e2a\u6a21\u578b\u62d2\u7edd\u7387>94%\uff0c\u4f46\u62d2\u7edd\u4e0d\u7a33\u5b9a\u6027\u6301\u7eed\u5b58\u5728\u4e14\u4e0d\u5747\u5300\u3002\u7ea6\u4e09\u5206\u4e4b\u4e00\u521d\u59cb\u62d2\u7edd\u8bf1\u5bfc\u63d0\u793a\u81f3\u5c11\u51fa\u73b0\u4e00\u6b21\"\u62d2\u7edd\u9003\u9038\"\uff08\u6270\u52a8\u4e0b\u8f6c\u4e3a\u5408\u89c4\uff09\u3002\u653b\u51fb\u7c7b\u578b\u6bd4\u6270\u52a8\u98ce\u683c\u66f4\u80fd\u9884\u6d4b\u62d2\u7edd\u5931\u8d25\u3002\u6587\u672c\u653b\u51fb\uff08\u5982\u52d2\u7d22\u8f6f\u4ef6\u8bf4\u660e\uff09\u4e0d\u7a33\u5b9a\u6027\u663e\u8457\u66f4\u9ad8\uff0c\u7ffb\u8f6c\u7387\u8d85\u8fc720%\uff1b\u800c\u53ef\u6267\u884c\u6076\u610f\u8f6f\u4ef6\u653b\u51fb\u5728\u4e24\u4e2a\u6a21\u578b\u4e2d\u5747\u96f6\u62d2\u7edd\u9003\u9038\u3002GPT-4o\u6bd4GPT-4.1\u62d2\u7edd\u6267\u884c\u66f4\u4e25\u683c\u3001RBE\u66f4\u4f4e\uff0c\u4f46\u672a\u6d88\u9664\u653b\u51fb\u7c7b\u578b\u4f9d\u8d56\u98ce\u9669\u3002", "conclusion": "\u62d2\u7edd\u884c\u4e3a\u662f\u6982\u7387\u6027\u7684\u3001\u653b\u51fb\u7c7b\u578b\u4f9d\u8d56\u7684\u8fb9\u754c\u73b0\u8c61\uff0c\u800c\u975e\u7a33\u5b9a\u4e8c\u5143\u5c5e\u6027\uff0c\u9700\u8981\u6539\u53d8\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u7684\u6d4b\u91cf\u548c\u5ba1\u8ba1\u65b9\u5f0f\u3002\u5355\u4e00\u63d0\u793a\u8bc4\u4f30\u4f1a\u7cfb\u7edf\u6027\u9ad8\u4f30\u5b89\u5168\u9c81\u68d2\u6027\uff0c\u9700\u91c7\u7528\u66f4\u5168\u9762\u7684\u8fb9\u754c\u7a33\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2601.17837", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17837", "abs": "https://arxiv.org/abs/2601.17837", "authors": ["Peinuan Qin", "Yugin Tan", "Jingzhu Chen", "Nattapat Boonprakong", "Zicheng Zhu", "Naomi Yamashita", "Yi-Chieh Lee"], "title": "ChatLearn: Leveraging AI to Transform Non-Native Speaker Communication Challenges as Language Learning Opportunities", "comment": "This paper has been accepted for publication at ACM CHI 2026", "summary": "Non-native speakers (NNSs) face significant language barriers in multilingual communication with native speakers (NSs). While AI-mediated communication (AIMC) tools offer efficient one-time assistance, they often overlook opportunities for NNSs' continuous language acquisition. We introduce ChatLearn, an enhanced AIMC system that leverages NNSs' communication difficulties as learning opportunities. Beyond comprehension and expression assistance, ChatLearn simultaneously captures NNSs' language challenges, and subsequently provides them with spaced review as the conversation progresses. We conducted a mixed-methods study using a communication task with 43 NNS-NS pairs, after which ChatLearn NNSs recalled significantly more expressions than the baseline group, while there was no substantial decline in communication experience. Our findings highlight the value of contextual learning in NNS-NS communication, providing a new direction for AIMC systems that foster both immediate collaboration and continuous language development.", "AI": {"tldr": "ChatLearn\u662f\u4e00\u4e2aAI\u4e2d\u4ecb\u6c9f\u901a\u7cfb\u7edf\uff0c\u5229\u7528\u975e\u6bcd\u8bed\u8005\u5728\u6c9f\u901a\u4e2d\u9047\u5230\u7684\u8bed\u8a00\u56f0\u96be\u4f5c\u4e3a\u5b66\u4e60\u673a\u4f1a\uff0c\u901a\u8fc7\u95f4\u9694\u590d\u4e60\u4fc3\u8fdb\u6301\u7eed\u8bed\u8a00\u4e60\u5f97\uff0c\u540c\u65f6\u4fdd\u6301\u6c9f\u901a\u4f53\u9a8c\u3002", "motivation": "\u975e\u6bcd\u8bed\u8005\u5728\u4e0e\u6bcd\u8bed\u8005\u8fdb\u884c\u591a\u8bed\u8a00\u4ea4\u6d41\u65f6\u9762\u4e34\u663e\u8457\u8bed\u8a00\u969c\u788d\u3002\u73b0\u6709\u7684AI\u4e2d\u4ecb\u6c9f\u901a\u5de5\u5177\u867d\u7136\u80fd\u63d0\u4f9b\u4e00\u6b21\u6027\u5e2e\u52a9\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u975e\u6bcd\u8bed\u8005\u6301\u7eed\u8bed\u8a00\u4e60\u5f97\u7684\u673a\u4f1a\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u534f\u52a9\u5373\u65f6\u6c9f\u901a\uff0c\u53c8\u80fd\u4fc3\u8fdb\u957f\u671f\u8bed\u8a00\u53d1\u5c55\u7684\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86ChatLearn\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u63d0\u4f9b\u7406\u89e3\u548c\u8868\u8fbe\u8f85\u52a9\uff0c\u8fd8\u80fd\u5b9e\u65f6\u6355\u6349\u975e\u6bcd\u8bed\u8005\u7684\u8bed\u8a00\u56f0\u96be\uff0c\u5e76\u5728\u5bf9\u8bdd\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u95f4\u9694\u590d\u4e60\u3002\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u8ba943\u5bf9\u975e\u6bcd\u8bed\u8005-\u6bcd\u8bed\u8005\u914d\u5bf9\u5b8c\u6210\u6c9f\u901a\u4efb\u52a1\uff0c\u6bd4\u8f83ChatLearn\u7ec4\u4e0e\u57fa\u7ebf\u7ec4\u7684\u5dee\u5f02\u3002", "result": "\u4f7f\u7528ChatLearn\u7684\u975e\u6bcd\u8bed\u8005\u6bd4\u57fa\u7ebf\u7ec4\u663e\u8457\u56de\u5fc6\u51fa\u66f4\u591a\u8868\u8fbe\u65b9\u5f0f\uff0c\u540c\u65f6\u6c9f\u901a\u4f53\u9a8c\u6ca1\u6709\u660e\u663e\u4e0b\u964d\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u975e\u6bcd\u8bed\u8005-\u6bcd\u8bed\u8005\u6c9f\u901a\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "ChatLearn\u5c55\u793a\u4e86AI\u4e2d\u4ecb\u6c9f\u901a\u7cfb\u7edf\u7684\u65b0\u65b9\u5411\uff0c\u65e2\u80fd\u4fc3\u8fdb\u5373\u65f6\u534f\u4f5c\uff0c\u53c8\u80fd\u652f\u6301\u6301\u7eed\u8bed\u8a00\u53d1\u5c55\uff0c\u5f3a\u8c03\u4e86\u5c06\u6c9f\u901a\u56f0\u96be\u8f6c\u5316\u4e3a\u5b66\u4e60\u673a\u4f1a\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.17189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17189", "abs": "https://arxiv.org/abs/2601.17189", "authors": ["Sabrina Mokhtari", "Sara Kodeiri", "Shubhankar Mohapatra", "Florian Tramer", "Gautam Kamath"], "title": "Rethinking Benchmarks for Differentially Private Image Classification", "comment": null, "summary": "We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u5dee\u5206\u9690\u79c1\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5e76\u521b\u5efa\u516c\u5f00\u6392\u884c\u699c\u4ee5\u8ffd\u8e2a\u8fdb\u5c55\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0d\u591f\u5168\u9762\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6280\u672f\u6548\u679c\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u6765\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e00\u5957\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u6db5\u76d6\u6709\u65e0\u989d\u5916\u6570\u636e\u3001\u51f8\u4f18\u5316\u8bbe\u7f6e\u3001\u4e0d\u540c\u6027\u8d28\u6570\u636e\u96c6\u7b49\u591a\u79cd\u573a\u666f\uff1b\u5728\u57fa\u51c6\u4e0a\u6d4b\u8bd5\u73b0\u6709\u6280\u672f\uff1b\u521b\u5efa\u516c\u5f00\u6392\u884c\u699c\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684\u5dee\u5206\u9690\u79c1\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u6280\u672f\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5e73\u53f0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u516c\u5f00\u6392\u884c\u699c\u4fc3\u8fdb\u6280\u672f\u6bd4\u8f83\u548c\u8fdb\u5c55\u8ffd\u8e2a\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2601.18629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18629", "abs": "https://arxiv.org/abs/2601.18629", "authors": ["Yiming Wang", "Ruogu Zhang", "Minyang Li", "Hao Shi", "Junbo Wang", "Deyi Li", "Jieji Ren", "Wenhai Liu", "Weiming Wang", "Hao-Shu Fang"], "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection", "comment": null, "summary": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.", "code_url": "https://github.com/zaixiabalala/ExoGS", "code_stars": 9, "code_last_update": "2026-01-27", "AI": {"tldr": "ExoGS\u662f\u4e00\u4e2a\u673a\u5668\u4eba\u65e0\u5173\u76844D\u771f\u5b9e-\u4eff\u771f-\u771f\u5b9e\u6846\u67b6\uff0c\u901a\u8fc7\u88ab\u52a8\u5916\u9aa8\u9abc\u6355\u83b7\u4eba\u7c7b\u6f14\u793a\u7684\u7cbe\u786e\u8fd0\u52a8\u8f68\u8ff9\u548cRGB\u89c2\u5bdf\uff0c\u91cd\u5efa\u4e3a\u53ef\u7f16\u8f91\u76843D\u9ad8\u65af\u6cfc\u6e85\u8d44\u4ea7\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e00\u81f4\u7684\u91cd\u653e\u548c\u5927\u89c4\u6a21\u6570\u636e\u589e\u5f3a\uff0c\u63d0\u5347\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u7684\u7b56\u7565\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u771f\u5b9e-\u4eff\u771f-\u771f\u5b9e\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u73af\u5883\u7ea7\u522b\u7684\u89c6\u89c9\u8fc1\u79fb\uff0c\u5ffd\u7565\u4e86\u4ea4\u4e92\u8fc1\u79fb\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u4efb\u52a1\uff0c\u5728\u4eff\u771f\u4e2d\u83b7\u53d6\u4ea4\u4e92\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u4e14\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6355\u83b7\u9759\u6001\u73af\u5883\u548c\u52a8\u6001\u4ea4\u4e92\u5e76\u8fc1\u79fb\u5230\u4eff\u771f\u73af\u5883\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u81ea\u8bbe\u8ba1\u7684\u673a\u5668\u4eba\u540c\u6784\u88ab\u52a8\u5916\u9aa8\u9abcAirExo-3\u6355\u83b7\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u8fd0\u52a8\u8f68\u8ff9\u548c\u540c\u6b65RGB\u89c2\u5bdf\uff1b\u5c06\u673a\u5668\u4eba\u3001\u7269\u4f53\u548c\u73af\u5883\u91cd\u5efa\u4e3a\u53ef\u7f16\u8f91\u76843D\u9ad8\u65af\u6cfc\u6e85\u8d44\u4ea7\uff1b\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a9\u7801\u9002\u914d\u5668\u5411\u7b56\u7565\u6ce8\u5165\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u4ee5\u589e\u5f3a\u89c6\u89c9\u57df\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cExoGS\u76f8\u6bd4\u9065\u64cd\u4f5c\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u3002\u4ee3\u7801\u548c\u786c\u4ef6\u6587\u4ef6\u5df2\u5f00\u6e90\u3002", "conclusion": "ExoGS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u53ef\u6269\u5c55\u64cd\u7eb5\u6570\u636e\u6536\u96c6\u548c\u7b56\u7565\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6355\u83b7\u771f\u5b9e\u4e16\u754c\u7684\u9759\u6001\u73af\u5883\u548c\u52a8\u6001\u4ea4\u4e92\u5e76\u8fc1\u79fb\u5230\u4eff\u771f\u73af\u5883\uff0c\u7f29\u5c0f\u4e86\u4eff\u771f-\u771f\u5b9e\u5dee\u8ddd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u4efb\u52a1\u3002"}}
{"id": "2601.17967", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17967", "abs": "https://arxiv.org/abs/2601.17967", "authors": ["Alon Hillel-Tuch"], "title": "Data Siphoning Through Advanced Persistent Transmission Attacks At The Physical Layer", "comment": "3 pages, extended abstract", "summary": "Data at the physical layer transmits via media such as copper cable, fiber optic, or wireless. Physical attack vectors exist that challenge data confidentiality and availability. Protocols and encryption standards help obfuscate but often cannot keep the data type and destination secure, with limited insight into confidentiality and integrity. We will investigate the feasibility of developing an awareness and integrity protocol to help mitigate physical side-channel attacks that lead to eavesdropping of data communication and denial-of-service.\n  Keywords: data confidentiality, siphoning, eavesdropping, person-in-the-middle, denial-of-service, physical layer attacks, nation-states", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e00\u79cd\u611f\u77e5\u4e0e\u5b8c\u6574\u6027\u534f\u8bae\uff0c\u4ee5\u51cf\u8f7b\u7269\u7406\u5c42\u4fa7\u4fe1\u9053\u653b\u51fb\uff08\u5982\u7a83\u542c\u548c\u62d2\u7edd\u670d\u52a1\uff09\u7684\u53ef\u884c\u6027", "motivation": "\u7269\u7406\u5c42\u4f20\u8f93\u4ecb\u8d28\uff08\u94dc\u7f06\u3001\u5149\u7ea4\u3001\u65e0\u7ebf\uff09\u5b58\u5728\u7269\u7406\u653b\u51fb\u5411\u91cf\uff0c\u5a01\u80c1\u6570\u636e\u673a\u5bc6\u6027\u548c\u53ef\u7528\u6027\u3002\u73b0\u6709\u534f\u8bae\u548c\u52a0\u5bc6\u6807\u51c6\u867d\u7136\u80fd\u6df7\u6dc6\u6570\u636e\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u4fdd\u62a4\u6570\u636e\u7c7b\u578b\u548c\u76ee\u7684\u5730\u5b89\u5168\uff0c\u5bf9\u673a\u5bc6\u6027\u548c\u5b8c\u6574\u6027\u7684\u6d1e\u5bdf\u6709\u9650\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e00\u79cd\u611f\u77e5\u4e0e\u5b8c\u6574\u6027\u534f\u8bae\uff0c\u65e8\u5728\u7f13\u89e3\u7269\u7406\u4fa7\u4fe1\u9053\u653b\u51fb\uff0c\u5305\u62ec\u6570\u636e\u901a\u4fe1\u7a83\u542c\u548c\u62d2\u7edd\u670d\u52a1\u653b\u51fb\u3002", "result": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8be5\u534f\u8bae\u7684\u53ef\u884c\u6027\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u6216\u6027\u80fd\u6570\u636e\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u65b0\u7684\u611f\u77e5\u4e0e\u5b8c\u6574\u6027\u534f\u8bae\u6765\u5e94\u5bf9\u7269\u7406\u5c42\u653b\u51fb\uff0c\u4ee5\u589e\u5f3a\u6570\u636e\u901a\u4fe1\u7684\u673a\u5bc6\u6027\u548c\u53ef\u7528\u6027\u3002"}}
{"id": "2601.18639", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18639", "abs": "https://arxiv.org/abs/2601.18639", "authors": ["Ojasva Mishra", "Xiaolong Wu", "Min Xu"], "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation", "comment": "Pending IEEE Transactions on Robotics Publication", "summary": "The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($\u03c4=1.0$~s, $\u0394t=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.", "AI": {"tldr": "\u9488\u5bf9\u673a\u5668\u4eba\u5173\u8282\u63a7\u5236\u4e2d\u79bb\u6563\u65f6\u95f4PID\u8c03\u8c10\u95ee\u9898\uff0c\u63d0\u51fa\u5305\u542b\u7a33\u5b9a\u6027\u5206\u6790\u3001\u6297\u9971\u548c\u8bbe\u8ba1\u53ca\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u5b9e\u73b0\u611f\u77e5\u8c03\u8c10\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u4e2d\u65cb\u8f6c\u9a71\u52a8\u7684\u7cbe\u786e\u8c03\u8282\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645PID\u63a7\u5236\u56de\u8def\u56e0\u79bb\u6563\u65f6\u95f4\u6267\u884c\u3001\u6267\u884c\u5668\u9971\u548c\u3001\u5fae\u5c0f\u5ef6\u8fdf\u548c\u6d4b\u91cf\u4e0d\u5b8c\u7f8e\u7b49\u56e0\u7d20\uff0c\u4e0e\u8fde\u7eed\u65f6\u95f4\u7406\u8bba\u5b58\u5728\u504f\u5dee\uff0c\u9700\u8981\u5b9e\u73b0\u611f\u77e5\u7684\u5206\u6790\u548c\u8c03\u8c10\u65b9\u6cd5\u3002", "method": "1) \u4f7f\u7528Jury\u5224\u636e\u63a8\u5bfcEuler\u548c\u7cbe\u786e\u96f6\u9636\u4fdd\u6301\u79bb\u6563\u5316\u4e0b\u7684PI\u7a33\u5b9a\u6027\u533a\u57df\uff1b2) \u8bc4\u4f30\u9971\u548c\u4e3b\u5bfc\u673a\u5236\u4e0b\u7684\u79bb\u6563\u53cd\u5411\u8ba1\u7b97\u6297\u9971\u548c\u5b9e\u73b0\uff1b3) \u63d0\u51fa\u6df7\u5408\u8ba4\u8bc1\u8d1d\u53f6\u65af\u4f18\u5316\u6d41\u7a0b\uff0c\u7b5b\u9009\u5206\u6790\u4e0d\u7a33\u5b9a\u5019\u9009\u53c2\u6570\uff0c\u540c\u65f6\u4f18\u5316\u5e26\u8d85\u8c03\u548c\u9971\u548c\u5360\u7a7a\u6bd4\u8f6f\u60e9\u7f5a\u7684\u9c81\u68d2IAE\u76ee\u6807\u3002", "result": "\u57fa\u51c6\u626b\u63cf\u663e\u793aP/PI/PID\u7684\u4e0a\u5347/\u7a33\u5b9a\u8d8b\u52bf\u3002\u5728\u6a21\u62df\u4e0d\u786e\u5b9a\u6027\u3001\u5ef6\u8fdf\u3001\u566a\u58f0\u3001\u91cf\u5316\u548c\u66f4\u4e25\u683c\u9971\u548c\u7684\u968f\u673a\u6a21\u578b\u65cf\u4e0b\uff0c\u9c81\u68d2\u5bfc\u5411\u8c03\u8c10\u5c06\u4e2d\u4f4d\u6570IAE\u4ece0.843\u63d0\u5347\u81f30.430\uff0c\u540c\u65f6\u4fdd\u6301\u4e2d\u4f4d\u6570\u8d85\u8c03\u4f4e\u4e8e2%\u3002\u5728\u7eaf\u4eff\u771f\u8c03\u8c10\u4e2d\uff0c\u8ba4\u8bc1\u7b5b\u9009\u5728\u5b8c\u5168\u9c81\u68d2\u8bc4\u4f30\u524d\u62d2\u7edd11.6%\u7684\u968f\u673a\u91c7\u6837\u589e\u76ca\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b9e\u73b0\u611f\u77e5\u5206\u6790\u548c\u8c03\u8c10\u5de5\u4f5c\u6d41\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u6563\u65f6\u95f4\u5173\u8282\u63a7\u5236\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u901a\u8fc7\u7a33\u5b9a\u6027\u5206\u6790\u3001\u6297\u9971\u548c\u8bbe\u8ba1\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u786c\u4ef6\u5b9e\u9a8c\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u8c03\u8c10\u3002"}}
{"id": "2601.17196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17196", "abs": "https://arxiv.org/abs/2601.17196", "authors": ["Nghia Thu Truong", "Qui Phu Pham", "Quang Nguyen", "Dung Luong", "Mai Tran"], "title": "Accelerated Sinkhorn Algorithms for Partial Optimal Transport", "comment": null, "summary": "Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\\mathcal{O}(n^{7/3}\\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $\u03b3$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.", "AI": {"tldr": "ASPOT\u65b9\u6cd5\u5c06\u4ea4\u66ff\u6700\u5c0f\u5316\u4e0eNesterov\u52a0\u901f\u7ed3\u5408\uff0c\u4e3a\u90e8\u5206\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u590d\u6742\u5ea6\u754c\u9650", "motivation": "\u90e8\u5206\u6700\u4f18\u4f20\u8f93(POT)\u5904\u7406\u4e24\u4e2a\u5206\u5e03\u95f4\u90e8\u5206\u8d28\u91cf\u7684\u4f20\u8f93\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u5206\u5e03\u5927\u5c0f\u4e0d\u7b49\u6216\u5305\u542b\u5f02\u5e38\u503c\u7684\u60c5\u51b5\u3002\u867d\u7136Sinkhorn\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5728POT\u95ee\u9898\u4e2d\u7684\u590d\u6742\u5ea6\u754c\u9650\u4ecd\u7136\u4e0d\u591f\u4f18\u5316\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u52a0\u901fSinkhorn\u7528\u4e8ePOT(ASPOT)\u65b9\u6cd5\uff0c\u5c06\u4ea4\u66ff\u6700\u5c0f\u5316\u4e0eNesterov\u98ce\u683c\u7684\u52a0\u901f\u6280\u672f\u7ed3\u5408\u5230POT\u8bbe\u7f6e\u4e2d\u3002\u540c\u65f6\u5c55\u793a\u4e86\u901a\u8fc7\u660e\u667a\u9009\u62e9\u71b5\u53c2\u6570\u03b3\u53ef\u4ee5\u6539\u8fdb\u7ecf\u5178Sinkhorn\u65b9\u6cd5\u7684\u6536\u655b\u901f\u7387\u3002", "result": "ASPOT\u65b9\u6cd5\u5b9e\u73b0\u4e86$\\mathcal{O}(n^{7/3}\\varepsilon^{-5/3})$\u7684\u590d\u6742\u5ea6\u754c\u9650\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "ASPOT\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u4ea4\u66ff\u6700\u5c0f\u5316\u548cNesterov\u52a0\u901f\uff0c\u4e3a\u90e8\u5206\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u6539\u8fdb\u4e86\u73b0\u6709Sinkhorn\u65b9\u6cd5\u7684\u590d\u6742\u5ea6\u754c\u9650\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2601.18692", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18692", "abs": "https://arxiv.org/abs/2601.18692", "authors": ["Wei Wu", "Fan Lu", "Yunnan Wang", "Shuai Yang", "Shi Liu", "Fangjing Wang", "Qian Zhu", "He Sun", "Yong Wang", "Shuailei Ma", "Yiyu Ren", "Kejia Zhang", "Hui Yu", "Jingmei Zhao", "Shuai Zhou", "Zhenqi Qiu", "Houlong Xiong", "Ziyu Wang", "Zechen Wang", "Ran Cheng", "Yong-Lu Li", "Yongtao Huang", "Xing Zhu", "Yujun Shen", "Kecheng Zheng"], "title": "A Pragmatic VLA Foundation Model", "comment": "Project Webpage: https://technology.robbyant.com/lingbot-vla/, Code: https://github.com/Robbyant/lingbot-vla/", "summary": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.", "AI": {"tldr": "LingBot-VLA\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ea62\u4e07\u5c0f\u65f6\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u57fa\u7840\u6a21\u578b\uff0c\u57289\u79cd\u53cc\u81c2\u673a\u5668\u4eba\u914d\u7f6e\u4e0a\u8bad\u7ec3\uff0c\u57283\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b8c\u6210100\u4e2a\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u7ade\u54c1\uff0c\u4ee3\u7801\u5e93\u6548\u7387\u63d0\u53471.5-2.8\u500d", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u4efb\u52a1\u548c\u5e73\u53f0\u95f4\u5fe0\u5b9e\u6cdb\u5316\u3001\u540c\u65f6\u786e\u4fdd\u6210\u672c\u6548\u76ca\uff08\u6570\u636e\u548cGPU\u65f6\u95f4\uff09\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u63a8\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u53d1\u5c55", "method": "\u4f7f\u7528\u7ea620,000\u5c0f\u65f6\u6765\u81ea9\u79cd\u6d41\u884c\u53cc\u81c2\u673a\u5668\u4eba\u914d\u7f6e\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8bad\u7ec3LingBot-VLA\u6a21\u578b\uff0c\u6784\u5efa\u9ad8\u6548\u4ee3\u7801\u5e93\uff0c\u57288-GPU\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u6bcf\u79d2261\u4e2a\u6837\u672c\u7684\u541e\u5410\u91cf", "result": "\u57283\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b8c\u6210100\u4e2a\u4efb\u52a1\uff08\u6bcf\u4e2a\u4efb\u52a1130\u4e2a\u540e\u8bad\u7ec3\u56de\u5408\uff09\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u660e\u663e\u4f18\u4e8e\u7ade\u4e89\u5bf9\u624b\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u548c\u5e7f\u6cdb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ee3\u7801\u5e93\u901f\u5ea6\u63d0\u53471.5-2.8\u500d", "conclusion": "LingBot-VLA\u6a21\u578b\u9002\u5408\u5b9e\u9645\u90e8\u7f72\uff0c\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u3001\u57fa\u7840\u6a21\u578b\u548c\u57fa\u51c6\u6570\u636e\u63a8\u52a8\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u53d1\u5c55\uff0c\u652f\u6301\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u5e76\u4fc3\u8fdb\u5065\u5168\u7684\u8bc4\u4f30\u6807\u51c6"}}
{"id": "2601.18068", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18068", "abs": "https://arxiv.org/abs/2601.18068", "authors": ["Jiayi Zhang", "Chenxin Sun", "Chenxiong Qian"], "title": "XGuardian: Towards Explainable and Generalized AI Anti-Cheat on FPS Games", "comment": "Accepted by USENIX Security 2026", "summary": "Aim-assist cheats are the most prevalent and infamous form of cheating in First-Person Shooter (FPS) games, which help cheaters illegally reveal the opponent's location and auto-aim and shoot, and thereby pose significant threats to the game industry. Although a considerable research effort has been made to automatically detect aim-assist cheats, existing works suffer from unreliable frameworks, limited generalizability, high overhead, low detection performance, and a lack of explainability of detection results. In this paper, we propose XGuardian, a server-side generalized and explainable system for detecting aim-assist cheats to overcome these limitations. It requires only two raw data inputs, pitch and yaw, which are all FPS games' must-haves, to construct novel temporal features and describe aim trajectories, which are essential for distinguishing cheaters and normal players. XGuardian is evaluated with the latest mainstream FPS game CS2, and validates its generalizability with another two different games. It achieves high detection performance and low overhead compared to prior works across different games with real-world and large-scale datasets, demonstrating wide generalizability and high effectiveness. It is able to justify its predictions and thereby shorten the ban cycle. We make XGuardian as well as our datasets publicly available.", "AI": {"tldr": "XGuardian\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4bFPS\u6e38\u620f\u4e2d\u7784\u51c6\u8f85\u52a9\u4f5c\u5f0a\u7684\u670d\u52a1\u5668\u7aef\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u4fef\u4ef0\u89d2\u548c\u504f\u822a\u89d2\u6570\u636e\u6784\u5efa\u65f6\u5e8f\u7279\u5f81\uff0c\u5b9e\u73b0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3001\u4f4e\u5f00\u9500\u4e14\u53ef\u89e3\u91ca\u7684\u4f5c\u5f0a\u68c0\u6d4b\u3002", "motivation": "\u7784\u51c6\u8f85\u52a9\u4f5c\u5f0a\u662fFPS\u6e38\u620f\u4e2d\u6700\u666e\u904d\u4e14\u81ed\u540d\u662d\u8457\u7684\u4f5c\u5f0a\u5f62\u5f0f\uff0c\u4e25\u91cd\u5a01\u80c1\u6e38\u620f\u4ea7\u4e1a\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u6846\u67b6\u4e0d\u53ef\u9760\u3001\u6cdb\u5316\u6027\u6709\u9650\u3001\u5f00\u9500\u9ad8\u3001\u68c0\u6d4b\u6027\u80fd\u4f4e\u4ee5\u53ca\u7f3a\u4e4f\u7ed3\u679c\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faXGuardian\u7cfb\u7edf\uff0c\u4ec5\u9700\u4fef\u4ef0\u89d2(pitch)\u548c\u504f\u822a\u89d2(yaw)\u4e24\u79cd\u539f\u59cb\u6570\u636e\u8f93\u5165\uff0c\u6784\u5efa\u65b0\u9896\u7684\u65f6\u5e8f\u7279\u5f81\u6765\u63cf\u8ff0\u7784\u51c6\u8f68\u8ff9\uff0c\u8fd9\u4e9b\u7279\u5f81\u662f\u533a\u5206\u4f5c\u5f0a\u8005\u548c\u6b63\u5e38\u73a9\u5bb6\u7684\u5173\u952e\u3002", "result": "\u4f7f\u7528\u6700\u65b0\u4e3b\u6d41FPS\u6e38\u620fCS2\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728\u53e6\u5916\u4e24\u6b3e\u4e0d\u540c\u6e38\u620f\u4e2d\u9a8c\u8bc1\u6cdb\u5316\u6027\u3002\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u6027\u80fd\u548c\u4f4e\u5f00\u9500\uff0c\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u6cdb\u5316\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "XGuardian\u80fd\u591f\u4e3a\u5176\u9884\u6d4b\u63d0\u4f9b\u5408\u7406\u89e3\u91ca\uff0c\u4ece\u800c\u7f29\u77ed\u5c01\u7981\u5468\u671f\u3002\u8be5\u7cfb\u7edf\u53ca\u5176\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u4e3a\u89e3\u51b3FPS\u6e38\u620f\u4e2d\u7684\u7784\u51c6\u8f85\u52a9\u4f5c\u5f0a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u670d\u52a1\u5668\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17846", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17846", "abs": "https://arxiv.org/abs/2601.17846", "authors": ["Peinuan Qin", "Chi-Lan Yang", "Nattapat Boonprakong", "Jingzhu Chen", "Yugin Tan", "Yi-Chieh Lee"], "title": "AI Personalization Paradox: Personalized AI Increases Superficial Engagement in Reading while Undermines Autonomy and Ownership in Writing", "comment": "This paper has been accepted for publication at ACM CHI 2026", "summary": "AI-assisted writing raises concerns about autonomy and ownership when benefiting writers. Personalization has been proposed as an effective solution while also risking writers' reliance on AI and behavior shifting. For better personalization design, existing studies rely on interaction and information solely within the writing phase; however, few studies have examined how reading behaviors can inform personalized writing. This study investigates the effects of integrating reading highlights for personalization on AI-assisted writing. A between-subjects study with 46 participants revealed that the personalization condition encouraged participants to produce more highlights. However, highlighting unexpectedly shifted from a sense-making strategy to an instrumental act of \"feeding the AI,\" leading to significant reliance on AI and declines in writers' sense of autonomy, ownership, and self-credit. These findings indicate personalization risks in AI-assisted writing, emphasize the importance of personalization strategies, and provide design implications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728AI\u8f85\u52a9\u5199\u4f5c\u4e2d\u6574\u5408\u9605\u8bfb\u9ad8\u4eae\u8fdb\u884c\u4e2a\u6027\u5316\u8bbe\u8ba1\u7684\u98ce\u9669\uff0c\u53d1\u73b0\u867d\u7136\u80fd\u4fc3\u8fdb\u66f4\u591a\u9ad8\u4eae\u884c\u4e3a\uff0c\u4f46\u4f1a\u5bfc\u81f4\u7528\u6237\u5c06\u9ad8\u4eae\u4ece\u7406\u89e3\u7b56\u7565\u8f6c\u53d8\u4e3a\"\u5582\u517bAI\"\u7684\u5de5\u5177\u6027\u884c\u4e3a\uff0c\u4ece\u800c\u589e\u52a0\u5bf9AI\u7684\u4f9d\u8d56\u5e76\u964d\u4f4e\u81ea\u4e3b\u6027\u3001\u6240\u6709\u6743\u611f\u548c\u81ea\u6211\u6210\u5c31\u611f\u3002", "motivation": "AI\u8f85\u52a9\u5199\u4f5c\u5f15\u53d1\u4e86\u5bf9\u4f5c\u8005\u81ea\u4e3b\u6027\u548c\u6240\u6709\u6743\u7684\u62c5\u5fe7\uff0c\u4e2a\u6027\u5316\u88ab\u63d0\u51fa\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4e5f\u5b58\u5728\u98ce\u9669\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5199\u4f5c\u9636\u6bb5\u7684\u4ea4\u4e92\u548c\u4fe1\u606f\uff0c\u5f88\u5c11\u63a2\u8ba8\u9605\u8bfb\u884c\u4e3a\u5982\u4f55\u4e3a\u4e2a\u6027\u5316\u5199\u4f5c\u63d0\u4f9b\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u6574\u5408\u9605\u8bfb\u9ad8\u4eae\u8fdb\u884c\u4e2a\u6027\u5316\u5bf9AI\u8f85\u52a9\u5199\u4f5c\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u752846\u540d\u53c2\u4e0e\u8005\u7684\u7ec4\u95f4\u7814\u7a76\u8bbe\u8ba1\uff0c\u6bd4\u8f83\u4e86\u6574\u5408\u9605\u8bfb\u9ad8\u4eae\u8fdb\u884c\u4e2a\u6027\u5316\u4e0e\u4e0d\u6574\u5408\u7684\u6761\u4ef6\u3002\u901a\u8fc7\u5b9e\u9a8c\u65b9\u6cd5\u6536\u96c6\u6570\u636e\uff0c\u5206\u6790\u53c2\u4e0e\u8005\u7684\u9ad8\u4eae\u884c\u4e3a\u3001\u5bf9AI\u7684\u4f9d\u8d56\u7a0b\u5ea6\u4ee5\u53ca\u81ea\u4e3b\u6027\u3001\u6240\u6709\u6743\u611f\u548c\u81ea\u6211\u6210\u5c31\u611f\u7684\u53d8\u5316\u3002", "result": "\u4e2a\u6027\u5316\u6761\u4ef6\u9f13\u52b1\u53c2\u4e0e\u8005\u4ea7\u751f\u66f4\u591a\u9ad8\u4eae\uff0c\u4f46\u9ad8\u4eae\u884c\u4e3a\u610f\u5916\u5730\u4ece\u7406\u89e3\u7b56\u7565\u8f6c\u53d8\u4e3a\"\u5582\u517bAI\"\u7684\u5de5\u5177\u6027\u884c\u4e3a\u3002\u8fd9\u5bfc\u81f4\u5bf9AI\u7684\u663e\u8457\u4f9d\u8d56\uff0c\u4ee5\u53ca\u4f5c\u8005\u81ea\u4e3b\u6027\u3001\u6240\u6709\u6743\u611f\u548c\u81ea\u6211\u6210\u5c31\u611f\u7684\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86AI\u8f85\u52a9\u5199\u4f5c\u4e2d\u4e2a\u6027\u5316\u8bbe\u8ba1\u7684\u98ce\u9669\uff0c\u5f3a\u8c03\u4e86\u4e2a\u6027\u5316\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u542f\u793a\u3002\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u8bbe\u8ba1AI\u8f85\u52a9\u5199\u4f5c\u5de5\u5177\uff0c\u907f\u514d\u5c06\u7528\u6237\u884c\u4e3a\u5de5\u5177\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u7528\u6237\u7684\u81ea\u4e3b\u6027\u548c\u6240\u6709\u6743\u611f\u3002"}}
{"id": "2601.17204", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.17204", "abs": "https://arxiv.org/abs/2601.17204", "authors": ["Yinkai Wang", "Yan Zhou Chen", "Xiaohui Chen", "Li-Ping Liu", "Soha Hassoun"], "title": "SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment", "comment": "preprint", "summary": "Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.", "code_url": "https://github.com/HassounLab/SpecBridge", "code_stars": 0, "code_last_update": "2026-01-27", "AI": {"tldr": "SpecBridge\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u5f0f\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8d28\u8c31\u7f16\u7801\u5668\u5fae\u8c03\u6295\u5f71\u5230\u51bb\u7ed3\u5206\u5b50\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u5c0f\u5206\u5b50\u8d28\u8c31\u9274\u5b9a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534720-25%\u7684\u68c0\u7d22\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u5c0f\u5206\u5b50\u8d28\u8c31\u9274\u5b9a\u9762\u4e34\u4e24\u4e2a\u6781\u7aef\uff1a\u663e\u5f0f\u751f\u6210\u6a21\u578b\u539f\u5b50\u7ea7\u6784\u5efa\u5206\u5b50\u56fe\uff0c\u6216\u4ece\u5934\u5b66\u4e60\u8de8\u6a21\u6001\u5b50\u7a7a\u95f4\u7684\u8054\u5408\u5bf9\u6bd4\u6a21\u578b\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u975e\u9776\u5411\u8d28\u8c31\u5206\u6790\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SpecBridge\u91c7\u7528\u9690\u5f0f\u5bf9\u9f50\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u9274\u5b9a\u89c6\u4e3a\u51e0\u4f55\u5bf9\u9f50\u95ee\u9898\u3002\u5fae\u8c03\u81ea\u76d1\u7763\u8d28\u8c31\u7f16\u7801\u5668(DreaMS)\uff0c\u5c06\u5176\u76f4\u63a5\u6295\u5f71\u5230\u51bb\u7ed3\u5206\u5b50\u57fa\u7840\u6a21\u578b(ChemBERTa)\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u7136\u540e\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u9884\u8ba1\u7b97\u7684\u5206\u5b50\u5d4c\u5165\u5e93\u4e2d\u8fdb\u884c\u68c0\u7d22\u3002", "result": "\u5728MassSpecGym\u3001Spectraverse\u548cMSnLib\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSpecBridge\u76f8\u6bd4\u5f3a\u795e\u7ecf\u57fa\u7ebf\u65b9\u6cd5\u5c06top-1\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u7ea620-25%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "\u5bf9\u9f50\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\u662f\u8bbe\u8ba1\u65b0\u67b6\u6784\u7684\u5b9e\u7528\u7a33\u5b9a\u66ff\u4ee3\u65b9\u6848\uff0cSpecBridge\u6846\u67b6\u4e3a\u5c0f\u5206\u5b50\u8d28\u8c31\u9274\u5b9a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.18723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18723", "abs": "https://arxiv.org/abs/2601.18723", "authors": ["Mengyuan Liu", "Juyi Sheng", "Peiming Li", "Ziyi Wang", "Tianming Xu", "Tiantian Xu", "Hong Liu"], "title": "Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods", "comment": null, "summary": "Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution Quality (e.g., smoothness and safety). To bridge these gaps, we propose a solution that combines the Eval-Actions benchmark and the AutoEval architecture. First, we construct the Eval-Actions benchmark to support trustworthiness analysis. Distinct from existing datasets restricted to successful human demonstrations, Eval-Actions integrates VA and VLA policy execution trajectories alongside human teleoperation data, explicitly including failure scenarios. This dataset is structured around three core supervision signals: Expert Grading (EG), Rank-Guided preferences (RG), and Chain-of-Thought (CoT). Building on this, we propose the AutoEval architecture: AutoEval leverages Spatio-Temporal Aggregation for semantic assessment, augmented by an auxiliary Kinematic Calibration Signal to refine motion smoothness; AutoEval Plus (AutoEval-P) incorporates the Group Relative Policy Optimization (GRPO) paradigm to enhance logical reasoning capabilities. Experiments show AutoEval achieves Spearman's Rank Correlation Coefficients (SRCC) of 0.81 and 0.84 under the EG and RG protocols, respectively. Crucially, the framework possesses robust source discrimination capabilities, distinguishing between policy-generated and teleoperated videos with 99.6% accuracy, thereby establishing a rigorous standard for trustworthy robotic evaluation. Our project and code are available at https://term-bench.github.io/.", "code_url": "https://term-bench.github.io/", "AI": {"tldr": "\u63d0\u51faEval-Actions\u57fa\u51c6\u548cAutoEval\u67b6\u6784\uff0c\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u5efa\u7acb\u53ef\u4fe1\u8d56\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e8c\u5143\u6210\u529f\u7387\u3001\u5ffd\u89c6\u6e90\u771f\u5b9e\u6027\u548c\u6267\u884c\u8d28\u91cf\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u8bc4\u4f30\u65b9\u6cd5\u6ede\u540e\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u6210\u529f\u7387\uff0c\u65e0\u6cd5\u8bc4\u4f30\u5173\u952e\u7684\u53ef\u4fe1\u7ef4\u5ea6\uff1a\u6e90\u771f\u5b9e\u6027\uff08\u533a\u5206\u7b56\u7565\u884c\u4e3a\u4e0e\u4eba\u7c7b\u9065\u64cd\u4f5c\uff09\u548c\u6267\u884c\u8d28\u91cf\uff08\u5e73\u6ed1\u6027\u3001\u5b89\u5168\u6027\uff09\u3002\u8fd9\u963b\u788d\u4e86\u5efa\u7acb\u53ef\u4fe1\u8d56\u7684\u884c\u4e3a\u8bc4\u4f30\u6807\u51c6\u3002", "method": "1. \u6784\u5efaEval-Actions\u57fa\u51c6\uff1a\u5305\u542bVA\u548cVLA\u7b56\u7565\u6267\u884c\u8f68\u8ff9\u3001\u4eba\u7c7b\u9065\u64cd\u4f5c\u6570\u636e\uff0c\u660e\u786e\u5305\u542b\u5931\u8d25\u573a\u666f\uff0c\u63d0\u4f9b\u4e09\u79cd\u76d1\u7763\u4fe1\u53f7\uff1a\u4e13\u5bb6\u8bc4\u5206(EG)\u3001\u6392\u540d\u5f15\u5bfc\u504f\u597d(RG)\u3001\u601d\u7ef4\u94fe(CoT)\u30022. \u63d0\u51faAutoEval\u67b6\u6784\uff1a\u5229\u7528\u65f6\u7a7a\u805a\u5408\u8fdb\u884c\u8bed\u4e49\u8bc4\u4f30\uff0c\u901a\u8fc7\u8f85\u52a9\u8fd0\u52a8\u5b66\u6821\u51c6\u4fe1\u53f7\u4f18\u5316\u8fd0\u52a8\u5e73\u6ed1\u6027\u30023. AutoEval-P\u7248\u672c\uff1a\u5f15\u5165\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u8303\u5f0f\u589e\u5f3a\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "result": "AutoEval\u5728EG\u548cRG\u534f\u8bae\u4e0b\u5206\u522b\u8fbe\u52300.81\u548c0.84\u7684\u65af\u76ae\u5c14\u66fc\u7b49\u7ea7\u76f8\u5173\u7cfb\u6570\u3002\u6846\u67b6\u5177\u6709\u5f3a\u5927\u7684\u6e90\u533a\u5206\u80fd\u529b\uff0c\u80fd\u4ee599.6%\u7684\u51c6\u786e\u7387\u533a\u5206\u7b56\u7565\u751f\u6210\u89c6\u9891\u548c\u9065\u64cd\u4f5c\u89c6\u9891\uff0c\u4e3a\u673a\u5668\u4eba\u53ef\u4fe1\u8bc4\u4f30\u5efa\u7acb\u4e86\u4e25\u683c\u6807\u51c6\u3002", "conclusion": "\u901a\u8fc7Eval-Actions\u57fa\u51c6\u548cAutoEval\u67b6\u6784\uff0c\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u5efa\u7acb\u4e86\u5168\u9762\u7684\u53ef\u4fe1\u8d56\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2601.18733", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18733", "abs": "https://arxiv.org/abs/2601.18733", "authors": ["Li Kang", "Heng Zhou", "Xiufeng Song", "Rui Li", "Bruno N. Y. Chen", "Ziye Wang", "Ximeng Meng", "Stone Tao", "Yiran Qin", "Xiaohong Liu", "Ruimao Zhang", "Lei Bai", "Yilun Du", "Hao Su", "Philip Torr", "Zhenfei Yin", "Ruihao Gong", "Yejun Zeng", "Fengjun Zhong", "Shenghao Jin", "Jinyang Guo", "Xianglong Liu", "Xiaojun Jia", "Tianqi Shan", "Wenqi Ren", "Simeng Qin", "Jialing Yang", "Xiaoyu Ma", "Tianxing Chen", "Zixuan Li", "Zijian Cai", "Yan Qin", "Yusen Qin", "Qiangyu Chen", "Kaixuan Wang", "Zhaoming Han", "Yao Mu", "Ping Luo", "Yuanqi Yao", "Haoming Song", "Jan-Nico Zaech", "Fabien Despinoy", "Danda Pani Paudel", "Luc Van Gool"], "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge", "comment": "MARS Challenge @ NeurIPS 2025 Workshop on Space in Vision, Language, and Embodied AI. Challenge page: https://mars-eai.github.io/MARS-Challenge-Webpage/", "summary": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.", "AI": {"tldr": "NeurIPS 2025 SpaVLE Workshop\u63d0\u51faMARS\u6311\u6218\u8d5b\uff0c\u805a\u7126\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u901a\u8fc7VLM\u534f\u8c03\u4efb\u52a1\u548c\u7b56\u7565\u6267\u884c\uff0c\u63a8\u52a8\u5177\u8eabAI\u5411\u590d\u6742\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u5177\u8eabAI\u5411\u590d\u6742\u4efb\u52a1\u573a\u666f\u8fc7\u6e21\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\u5bf9\u4e8e\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u548c\u534f\u4f5c\u7684\u89e3\u51b3\u65b9\u6848\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u4e00\u8f6c\u53d8\u53d7\u5230\u4e09\u4e2a\u4e3b\u8981\u56e0\u7d20\u9a71\u52a8\uff1a\u667a\u80fd\u4f53\u80fd\u529b\u63d0\u5347\u3001\u901a\u8fc7\u4efb\u52a1\u59d4\u6d3e\u589e\u5f3a\u7cfb\u7edf\u6548\u7387\u3001\u4ee5\u53ca\u5b9e\u73b0\u9ad8\u7ea7\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u63d0\u51faMulti-Agent Robotic System (MARS)\u6311\u6218\u8d5b\uff0c\u91cd\u70b9\u5173\u6ce8\u4e24\u4e2a\u5173\u952e\u9886\u57df\uff1a1) \u89c4\u5212\uff1a\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u5177\u8eab\u89c4\u5212\u4ee5\u534f\u8c03\u4efb\u52a1\uff1b2) \u63a7\u5236\uff1a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6267\u884c\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u3002\u901a\u8fc7\u8bc4\u4f30\u53c2\u4e0e\u8005\u63d0\u4ea4\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7814\u7a76\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u95ee\u9898\u3002", "result": "\u6311\u6218\u8d5b\u4e3a\u5177\u8eab\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u534f\u8c03\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u53c2\u4e0e\u8005\u63a2\u7d22\u4e86\u57fa\u4e8eVLM\u7684\u591a\u667a\u80fd\u4f53\u89c4\u5212\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u9ad8\u7ea7\u534f\u4f5cAI\u7cfb\u7edf\u7684\u53d1\u5c55\u505a\u51fa\u8d21\u732e\u3002", "conclusion": "MARS\u6311\u6218\u8d5b\u901a\u8fc7\u805a\u7126\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u89c4\u5212\u4e0e\u63a7\u5236\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u5177\u8eabAI\u5411\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u5411\u7684\u53d1\u5c55\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u7814\u7a76\u5e73\u53f0\u3002"}}
{"id": "2601.18113", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18113", "abs": "https://arxiv.org/abs/2601.18113", "authors": ["Dezhang Kong", "Zhuxi Wu", "Shiqi Liu", "Zhicheng Tan", "Kuichen Lu", "Minghao Li", "Qichen Liu", "Shengyu Chu", "Zhenhua Xu", "Xuan Liu", "Meng Han"], "title": "MalURLBench: A Benchmark Evaluating Agents' Vulnerabilities When Processing Web URLs", "comment": null, "summary": "LLM-based web agents have become increasingly popular for their utility in daily life and work. However, they exhibit critical vulnerabilities when processing malicious URLs: accepting a disguised malicious URL enables subsequent access to unsafe webpages, which can cause severe damage to service providers and users. Despite this risk, no benchmark currently targets this emerging threat. To address this gap, we propose MalURLBench, the first benchmark for evaluating LLMs' vulnerabilities to malicious URLs. MalURLBench contains 61,845 attack instances spanning 10 real-world scenarios and 7 categories of real malicious websites. Experiments with 12 popular LLMs reveal that existing models struggle to detect elaborately disguised malicious URLs. We further identify and analyze key factors that impact attack success rates and propose URLGuard, a lightweight defense module. We believe this work will provide a foundational resource for advancing the security of web agents. Our code is available at https://github.com/JiangYingEr/MalURLBench.", "code_url": "https://github.com/JiangYingEr/MalURLBench", "code_stars": 0, "code_last_update": "2026-01-25", "AI": {"tldr": "MalURLBench\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u5bf9\u6076\u610fURL\u6f0f\u6d1e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b61,845\u4e2a\u653b\u51fb\u5b9e\u4f8b\uff0c\u6db5\u76d610\u4e2a\u771f\u5b9e\u573a\u666f\u548c7\u7c7b\u6076\u610f\u7f51\u7ad9\uff0c\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u68c0\u6d4b\u7cbe\u5fc3\u4f2a\u88c5\u7684\u6076\u610fURL\uff0c\u5e76\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u9632\u5fa1\u6a21\u5757URLGuard\u3002", "motivation": "LLM-based web agents\u5728\u5904\u7406\u6076\u610fURL\u65f6\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff1a\u63a5\u53d7\u4f2a\u88c5\u7684\u6076\u610fURL\u4f1a\u5bfc\u81f4\u8bbf\u95ee\u4e0d\u5b89\u5168\u7f51\u9875\uff0c\u5bf9\u670d\u52a1\u63d0\u4f9b\u5546\u548c\u7528\u6237\u9020\u6210\u4e25\u91cd\u635f\u5bb3\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u8fd9\u4e00\u65b0\u5174\u5a01\u80c1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faMalURLBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b61,845\u4e2a\u653b\u51fb\u5b9e\u4f8b\uff0c\u6db5\u76d610\u4e2a\u771f\u5b9e\u573a\u666f\u548c7\u7c7b\u771f\u5b9e\u6076\u610f\u7f51\u7ad9\u7c7b\u522b\uff1b\u5bf912\u4e2a\u6d41\u884cLLM\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff1b\u5206\u6790\u5f71\u54cd\u653b\u51fb\u6210\u529f\u7387\u7684\u5173\u952e\u56e0\u7d20\uff1b\u63d0\u51fa\u8f7b\u91cf\u7ea7\u9632\u5fa1\u6a21\u5757URLGuard\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709LLM\u96be\u4ee5\u68c0\u6d4b\u7cbe\u5fc3\u4f2a\u88c5\u7684\u6076\u610fURL\uff1b\u8bc6\u522b\u5e76\u5206\u6790\u4e86\u5f71\u54cd\u653b\u51fb\u6210\u529f\u7387\u7684\u5173\u952e\u56e0\u7d20\uff1b\u63d0\u51fa\u7684URLGuard\u9632\u5fa1\u6a21\u5757\u6709\u6548\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "MalURLBench\u4e3a\u8bc4\u4f30LLM\u5bf9\u6076\u610fURL\u7684\u6f0f\u6d1e\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u7684URLGuard\u9632\u5fa1\u6a21\u5757\u6709\u52a9\u4e8e\u63d0\u5347web agents\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u76f8\u5173\u5b89\u5168\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.17893", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17893", "abs": "https://arxiv.org/abs/2601.17893", "authors": ["Sarmistha Sarna Gomasta", "Mahmood Jasim", "Hossein Hadisi", "Yvonne Jansen", "Pierre Dragicevic", "Narges Mahyar", "Ali Sarvghad"], "title": "Investigating How Music Affects Persuasion, Engagement, and Emotion in Data Videos", "comment": null, "summary": "Data videos have become a prominent vessel for communicating data to broad audiences, and a common object of study in information visualization. Many of these videos include music, yet the impact of music on how people experience data videos remains largely unexplored. We conducted a preregistered study into the effect of music across three dimensions: persuasion, engagement, and emotion. We showed online participants an existing data video (1) without any music, (2) with its generic default music, and (3) with custom music designed by a professional composer. We found that the default music helped make the data video more persuasive. However, the effects of custom music were more mixed, and we did not find that music increased engagement. In addition, and contrary to our expectations, our participants reported more intense emotions without music. Our study contributes new insights into the intersection of music and data visualization and is a first step toward guiding designers in creating impactful data-driven narratives.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u97f3\u4e50\u5bf9\u6570\u636e\u89c6\u9891\u5728\u8bf4\u670d\u529b\u3001\u53c2\u4e0e\u5ea6\u548c\u60c5\u611f\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ed8\u8ba4\u97f3\u4e50\u80fd\u589e\u5f3a\u8bf4\u670d\u529b\uff0c\u4f46\u5b9a\u5236\u97f3\u4e50\u6548\u679c\u4e0d\u4e00\uff0c\u4e14\u65e0\u97f3\u4e50\u65f6\u60c5\u611f\u53cd\u5e94\u66f4\u5f3a\u70c8\u3002", "motivation": "\u6570\u636e\u89c6\u9891\u5df2\u6210\u4e3a\u5411\u5e7f\u6cdb\u53d7\u4f17\u4f20\u64ad\u6570\u636e\u7684\u91cd\u8981\u5a92\u4ecb\uff0c\u867d\u7136\u8bb8\u591a\u6570\u636e\u89c6\u9891\u5305\u542b\u97f3\u4e50\uff0c\u4f46\u97f3\u4e50\u5bf9\u4eba\u4eec\u4f53\u9a8c\u6570\u636e\u89c6\u9891\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u97f3\u4e50\u5982\u4f55\u5f71\u54cd\u6570\u636e\u89c6\u9891\u7684\u8bf4\u670d\u529b\u3001\u53c2\u4e0e\u5ea6\u548c\u60c5\u611f\u53cd\u5e94\u3002", "method": "\u91c7\u7528\u9884\u6ce8\u518c\u7814\u7a76\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\u5411\u53c2\u4e0e\u8005\u5c55\u793a\u4e09\u79cd\u7248\u672c\u7684\u6570\u636e\u89c6\u9891\uff1a(1) \u65e0\u97f3\u4e50\u7248\u672c\uff0c(2) \u4f7f\u7528\u901a\u7528\u9ed8\u8ba4\u97f3\u4e50\u7248\u672c\uff0c(3) \u4f7f\u7528\u4e13\u4e1a\u4f5c\u66f2\u5bb6\u8bbe\u8ba1\u7684\u5b9a\u5236\u97f3\u4e50\u7248\u672c\u3002\u6d4b\u91cf\u97f3\u4e50\u5bf9\u8bf4\u670d\u529b\u3001\u53c2\u4e0e\u5ea6\u548c\u60c5\u611f\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u9ed8\u8ba4\u97f3\u4e50\u4f7f\u6570\u636e\u89c6\u9891\u66f4\u5177\u8bf4\u670d\u529b\uff1b\u5b9a\u5236\u97f3\u4e50\u6548\u679c\u8f83\u4e3a\u590d\u6742\uff0c\u672a\u53d1\u73b0\u97f3\u4e50\u80fd\u589e\u52a0\u53c2\u4e0e\u5ea6\uff1b\u51fa\u4e4e\u610f\u6599\u7684\u662f\uff0c\u65e0\u97f3\u4e50\u65f6\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u66f4\u5f3a\u70c8\u7684\u60c5\u611f\u53cd\u5e94\u3002", "conclusion": "\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u4e86\u97f3\u4e50\u4e0e\u6570\u636e\u53ef\u89c6\u5316\u7684\u4ea4\u53c9\u5f71\u54cd\uff0c\u4e3a\u8bbe\u8ba1\u5e08\u521b\u5efa\u6709\u5f71\u54cd\u529b\u7684\u6570\u636e\u9a71\u52a8\u53d9\u4e8b\u63d0\u4f9b\u4e86\u521d\u6b65\u6307\u5bfc\u3002\u53d1\u73b0\u97f3\u4e50\u5bf9\u6570\u636e\u89c6\u9891\u7684\u5f71\u54cd\u5177\u6709\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u7814\u7a76\u6765\u7406\u89e3\u97f3\u4e50\u5728\u6570\u636e\u53d9\u4e8b\u4e2d\u7684\u4f5c\u7528\u673a\u5236\u3002"}}
{"id": "2601.17215", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2601.17215", "abs": "https://arxiv.org/abs/2601.17215", "authors": ["Ruoqing Zheng", "Chang Sun", "Qibin Liu", "Lauri Laatu", "Arianna Cox", "Benedikt Maier", "Alexander Tapper", "Jose G. F. Coutinho", "Wayne Luk", "Zhiqiang Que"], "title": "JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers", "comment": "15 pages,", "summary": "We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.", "code_url": "https://github.com/walkieq/JetFormer", "code_stars": 0, "code_last_update": "2025-09-10", "AI": {"tldr": "JetFormer\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\u7c92\u5b50\u55b7\u6ce8\u6807\u8bb0\u7684\u7f16\u7801\u5668\u4e13\u7528Transformer\u67b6\u6784\uff0c\u53ef\u5728\u4ece\u79bb\u7ebf\u5206\u6790\u5230\u5728\u7ebf\u89e6\u53d1\u7684\u5168\u573a\u666f\u4e2d\u9ad8\u6548\u8fd0\u884c\uff0c\u5177\u6709\u8ba1\u7b97\u6548\u7387\u548c\u90e8\u7f72\u53cb\u597d\u6027\u3002", "motivation": "\u73b0\u6709\u7c92\u5b50\u55b7\u6ce8\u6807\u8bb0\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u90e8\u7f72\u573a\u666f\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u67b6\u6784\u80fd\u591f\u5728\u4ece\u9ad8\u7cbe\u5ea6\u79bb\u7ebf\u5206\u6790\u5230\u8d85\u4f4e\u5ef6\u8fdf\u5728\u7ebf\u89e6\u53d1\u7684\u5168\u8c31\u7cfb\u573a\u666f\u4e2d\u6709\u6548\u8fd0\u884c\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u6613\u4e8e\u786c\u4ef6\u90e8\u7f72\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faJetFormer\u7f16\u7801\u5668\u4e13\u7528Transformer\u67b6\u6784\uff0c\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u7c92\u5b50\u7279\u5f81\u96c6\u800c\u4e0d\u4f9d\u8d56\u663e\u5f0f\u6210\u5bf9\u76f8\u4e92\u4f5c\u7528\u3002\u5f15\u5165\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u6d41\u7a0b\uff0c\u5305\u62ec\u591a\u76ee\u6807\u8d85\u53c2\u6570\u641c\u7d22\u3001\u7ed3\u6784\u5316\u526a\u679d\u548c\u91cf\u5316\uff0c\u751f\u6210\u9002\u7528\u4e8eFPGA\u89e6\u53d1\u7cfb\u7edf\u7684\u7d27\u51d1\u53d8\u4f53\uff08\u5982JetFormer-tiny\uff09\u3002", "result": "\u5728JetClass\u6570\u636e\u96c6\u4e0a\uff0c\u5927\u578bJetFormer\u4e0e\u4ea4\u4e92\u4e30\u5bcc\u7684ParT\u6a21\u578b\u7cbe\u5ea6\u76f8\u5f53\uff08\u76f8\u5dee0.7%\u4ee5\u5185\uff09\uff0c\u4f46FLOPs\u51cf\u5c1137.4%\u3002\u5728HLS4ML 150P\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6bd4MLP\u3001Deep Sets\u548cInteraction Networks\u7b49\u73b0\u6709\u6a21\u578b\u51c6\u786e\u7387\u9ad83-4%\u3002\u901a\u8fc7\u538b\u7f29\u6280\u672f\u53ef\u5927\u5e45\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u800c\u7cbe\u5ea6\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "JetFormer\u901a\u8fc7\u7edf\u4e00\u9ad8\u6027\u80fd\u5efa\u6a21\u548c\u90e8\u7f72\u53cb\u597d\u6027\uff0c\u4e3aTransformer\u57fa\u55b7\u6ce8\u6807\u8bb0\u5668\u5728LHC\u79bb\u7ebf\u548c\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u5e73\u8861\u3002"}}
{"id": "2601.18765", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18765", "abs": "https://arxiv.org/abs/2601.18765", "authors": ["Shutong Chen", "Adnan Aijaz", "Yansha Deng"], "title": "Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery", "comment": "Submit to IEEE for potential publication", "summary": "Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u76ee\u6807\u7684\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bbe\u8ba1\u901a\u4fe1-\u8ba1\u7b97-\u63a7\u5236\u5faa\u73af\uff0c\u5b9e\u73b0\u5feb\u901f\u9c81\u68d2\u7684\u673a\u5668\u4eba\u6545\u969c\u68c0\u6d4b\u4e0e\u6062\u590d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1182.6%\u7684FDR\u65f6\u95f4\u5e76\u63d0\u9ad876%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u6545\u969c\u68c0\u6d4b\u4e0e\u6062\u590d\u6846\u67b6\u5b58\u5728\u901a\u4fe1\u8ba1\u7b97\u5ef6\u8fdf\u5927\u3001\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u4e0d\u53ef\u9760\u7b49\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u901a\u4fe1-\u8ba1\u7b97-\u63a7\u5236\u5faa\u73af\u8bbe\u8ba1\u672a\u8003\u8651\u4e0b\u6e38FDR\u76ee\u6807\u3002\u9700\u8981\u5728\u52a8\u6001\u3001\u4e0d\u786e\u5b9a\u3001\u4eba\u673a\u5171\u5b58\u73af\u5883\u4e2d\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9c81\u68d2\u7684FDR\u3002", "method": "1) \u6545\u969c\u68c0\u6d4b\uff1a\u5b9a\u4e49\u5e76\u63d0\u53d63D\u573a\u666f\u56fe\u4f5c\u4e3a\u8bed\u4e49\u8868\u793a\uff0c\u901a\u8fc7\u76d1\u6d4b3D-SG\u4e2d\u7a7a\u95f4\u5173\u7cfb\u53d8\u5316\u68c0\u6d4b\u6545\u969c\uff1b2) \u6545\u969c\u6062\u590d\uff1a\u901a\u8fc7LoRA\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u589e\u5f3a\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u751f\u6210\u6062\u590d\u8fd0\u52a8\uff1b3) \u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u76ee\u6807\u5bfc\u5411\u6570\u5b57\u5b6a\u751f\u91cd\u5efa\u6a21\u5757\uff0c\u4ec5\u4f7f\u7528\u4efb\u52a1\u76f8\u5173\u7269\u4f53\u8f6e\u5ed3\u7ec6\u5316\u6062\u590d\u8fd0\u52a8\u3002", "result": "\u76f8\u6bd4\u4f9d\u8d56\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6545\u969c\u68c0\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6545\u969c\u6062\u590d\u7684\u6700\u5148\u8fdb\u6846\u67b6\uff0cGoC\u6846\u67b6\u5c06FDR\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe82.6%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u9ad8\u8fbe76%\u3002", "conclusion": "\u63d0\u51fa\u7684\u9762\u5411\u76ee\u6807\u901a\u4fe1\u6846\u67b6\u901a\u8fc7\u8054\u5408\u8bbe\u8ba1\u901a\u4fe1-\u8ba1\u7b97-\u63a7\u5236\u5faa\u73af\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u9c81\u68d2\u7684\u673a\u5668\u4eba\u6545\u969c\u68c0\u6d4b\u4e0e\u6062\u590d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u667a\u80fd\u5de5\u5382\u4e2d\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18216", "categories": ["cs.CR", "cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.18216", "abs": "https://arxiv.org/abs/2601.18216", "authors": ["Beom Heyn Kim", "Seok Min Hong", "Mohammad Mannan"], "title": "Rhea: Detecting Privilege-Escalated Evasive Ransomware Attacks Using Format-Aware Validation in the Cloud", "comment": "12 pages, 6 figures, under review (Jan 2026)", "summary": "Ransomware variants increasingly combine privilege escalation with sophisticated evasion strategies such as intermittent encryption, low-entropy encryption, and imitation attacks. Such powerful ransomware variants, privilege-escalated evasive ransomware (PEER), can defeat existing solutions relying on I/O-pattern analysis by tampering with or obfuscating I/O traces. Meanwhile, conventional statistical content-based detection becomes unreliable as the encryption size decreases due to sampling noises. We present Rhea, a cloud-offloaded ransomware defense system that analyzes replicated data snapshots, so-called mutation snapshots. Rhea introduces Format-Aware Validation that validates the syntactic and semantic correctness of file formats, instead of relying on statistical or entropy-based indicators. By leveraging file-format specifications as detection invariants, Rhea can reliably identify fine-grained and evasive encryption even under elevated attacker privileges. Our evaluation demonstrates that Rhea significantly outperforms existing approaches, establishing its practical effectiveness against modern ransomware threats.", "AI": {"tldr": "Rhea\u662f\u4e00\u4e2a\u4e91\u5378\u8f7d\u7684\u52d2\u7d22\u8f6f\u4ef6\u9632\u5fa1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u6570\u636e\u5feb\u7167\u548c\u683c\u5f0f\u611f\u77e5\u9a8c\u8bc1\u6765\u68c0\u6d4b\u7279\u6743\u63d0\u5347\u7684\u89c4\u907f\u578b\u52d2\u7d22\u8f6f\u4ef6", "motivation": "\u73b0\u4ee3\u52d2\u7d22\u8f6f\u4ef6\u53d8\u79cd\u7ed3\u5408\u4e86\u7279\u6743\u63d0\u5347\u548c\u590d\u6742\u89c4\u907f\u7b56\u7565\uff08\u5982\u95f4\u6b47\u52a0\u5bc6\u3001\u4f4e\u71b5\u52a0\u5bc6\u3001\u6a21\u4eff\u653b\u51fb\uff09\uff0c\u80fd\u591f\u51fb\u8d25\u4f9d\u8d56I/O\u6a21\u5f0f\u5206\u6790\u7684\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4f20\u7edf\u57fa\u4e8e\u7edf\u8ba1\u5185\u5bb9\u7684\u68c0\u6d4b\u5728\u52a0\u5bc6\u89c4\u6a21\u51cf\u5c0f\u65f6\u56e0\u91c7\u6837\u566a\u58f0\u800c\u4e0d\u53ef\u9760", "method": "Rhea\u91c7\u7528\u4e91\u5378\u8f7d\u67b6\u6784\uff0c\u5206\u6790\u590d\u5236\u7684\u6570\u636e\u5feb\u7167\uff08\u7a81\u53d8\u5feb\u7167\uff09\uff0c\u5f15\u5165\u683c\u5f0f\u611f\u77e5\u9a8c\u8bc1\u6280\u672f\uff0c\u9a8c\u8bc1\u6587\u4ef6\u683c\u5f0f\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u6b63\u786e\u6027\uff0c\u800c\u975e\u4f9d\u8d56\u7edf\u8ba1\u6216\u71b5\u57fa\u6307\u6807\uff0c\u5229\u7528\u6587\u4ef6\u683c\u5f0f\u89c4\u8303\u4f5c\u4e3a\u68c0\u6d4b\u4e0d\u53d8\u91cf", "result": "\u8bc4\u4f30\u8868\u660eRhea\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u786e\u7acb\u4e86\u5176\u5bf9\u73b0\u4ee3\u52d2\u7d22\u8f6f\u4ef6\u5a01\u80c1\u7684\u5b9e\u9645\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u57fa\u4e8e\u6587\u4ef6\u683c\u5f0f\u89c4\u8303\u7684\u683c\u5f0f\u611f\u77e5\u9a8c\u8bc1\uff0cRhea\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b\u7ec6\u7c92\u5ea6\u548c\u89c4\u907f\u6027\u52a0\u5bc6\uff0c\u5373\u4f7f\u5728\u653b\u51fb\u8005\u7279\u6743\u63d0\u5347\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c"}}
{"id": "2601.17937", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17937", "abs": "https://arxiv.org/abs/2601.17937", "authors": ["Anne Arzberger", "Celine Offerman", "Ujwal Gadiraju", "Alessandro Bozzon", "Jie Yang"], "title": "\"Label from Somewhere\": Reflexive Annotating for Situated AI Alignment", "comment": null, "summary": "AI alignment relies on annotator judgments, yet annotation pipelines often treat annotators as interchangeable, obscuring how their social position shapes annotation. We introduce reflexive annotating as a probe that invites crowd workers to reflect on how their positionality informs subjective annotation judgments in a language model alignment context. Through a qualitative study with crowd workers (N=30) and follow-up interviews (N=5), we examine how our probe shapes annotators' behaviour, experience, and the situated metadata it elicits. We find that reflexive annotating captures epistemic metadata beyond static demographics by eliciting intersectional reasoning, surfacing positional humility, and nudging viewpoint change. Crucially, we also denote tensions between reflexive engagement and affective demands such as emotional exposure. We discuss the implications of our work for richer value elicitation and alignment practices that treat annotator judgments as situated and selectively integrate positional metadata.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u53cd\u601d\u6027\u6807\u6ce8\"\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u4f17\u5305\u5de5\u4f5c\u8005\u53cd\u601d\u81ea\u8eab\u793e\u4f1a\u4f4d\u7f6e\u5982\u4f55\u5f71\u54cd\u4e3b\u89c2\u6807\u6ce8\u5224\u65ad\uff0c\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u6536\u96c6\u66f4\u4e30\u5bcc\u7684\u5143\u6570\u636e\u3002", "motivation": "\u5f53\u524dAI\u5bf9\u9f50\u4f9d\u8d56\u6807\u6ce8\u8005\u5224\u65ad\uff0c\u4f46\u6807\u6ce8\u6d41\u7a0b\u901a\u5e38\u5c06\u6807\u6ce8\u8005\u89c6\u4e3a\u53ef\u4e92\u6362\u7684\uff0c\u5ffd\u7565\u4e86\u4ed6\u4eec\u7684\u793e\u4f1a\u4f4d\u7f6e\u5982\u4f55\u5f71\u54cd\u6807\u6ce8\u3002\u9700\u8981\u63a2\u7d22\u5982\u4f55\u6355\u6349\u6807\u6ce8\u5224\u65ad\u7684\u60c5\u5883\u6027\u3002", "method": "\u5f15\u5165\u53cd\u601d\u6027\u6807\u6ce8\u4f5c\u4e3a\u63a2\u9488\uff0c\u9080\u8bf7\u4f17\u5305\u5de5\u4f5c\u8005\u53cd\u601d\u81ea\u8eab\u4f4d\u7f6e\u6027\u5982\u4f55\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u4e3b\u89c2\u6807\u6ce8\u5224\u65ad\u3002\u901a\u8fc7\u5b9a\u6027\u7814\u7a76\uff08N=30\uff09\u548c\u540e\u7eed\u8bbf\u8c08\uff08N=5\uff09\u5206\u6790\u8be5\u63a2\u9488\u5982\u4f55\u5f71\u54cd\u6807\u6ce8\u8005\u884c\u4e3a\u3001\u4f53\u9a8c\u53ca\u6536\u96c6\u7684\u60c5\u5883\u5143\u6570\u636e\u3002", "result": "\u53cd\u601d\u6027\u6807\u6ce8\u80fd\u6355\u6349\u8d85\u8d8a\u9759\u6001\u4eba\u53e3\u7edf\u8ba1\u7684\u8ba4\u77e5\u5143\u6570\u636e\uff1a\u5f15\u53d1\u4ea4\u53c9\u6027\u63a8\u7406\u3001\u5c55\u73b0\u4f4d\u7f6e\u6027\u8c26\u900a\u3001\u63a8\u52a8\u89c2\u70b9\u8f6c\u53d8\u3002\u4f46\u4e5f\u63ed\u793a\u4e86\u53cd\u601d\u53c2\u4e0e\u4e0e\u60c5\u611f\u9700\u6c42\uff08\u5982\u60c5\u611f\u66b4\u9732\uff09\u4e4b\u95f4\u7684\u5f20\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u66f4\u4e30\u5bcc\u7684\u4ef7\u503c\u83b7\u53d6\u548c\u5bf9\u9f50\u5b9e\u8df5\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5e94\u5c06\u6807\u6ce8\u8005\u5224\u65ad\u89c6\u4e3a\u60c5\u5883\u6027\u7684\uff0c\u5e76\u6709\u9009\u62e9\u5730\u6574\u5408\u4f4d\u7f6e\u6027\u5143\u6570\u636e\uff0c\u540c\u65f6\u9700\u5e73\u8861\u53cd\u601d\u53c2\u4e0e\u4e0e\u60c5\u611f\u9700\u6c42\u3002"}}
{"id": "2601.17224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17224", "abs": "https://arxiv.org/abs/2601.17224", "authors": ["Dmitrii Torbunov", "Yihui Ren", "Lijun Wu", "Yimei Zhu"], "title": "Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning", "comment": null, "summary": "Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.", "AI": {"tldr": "\u5c06\u6761\u4ef6\u6269\u6563\u6a21\u578b\u9006\u95ee\u9898\u6c42\u89e3\u5668\uff08CDI\uff09\u4ece\u4e00\u7ef4\u65f6\u95f4\u4fe1\u53f7\u6269\u5c55\u5230\u4e8c\u7ef4\u7a7a\u95f4\u6570\u636e\uff0c\u5e94\u7528\u4e8e\u6750\u6599\u8868\u5f81\u4e2d\u7684\u4f1a\u805a\u675f\u7535\u5b50\u884d\u5c04\u53c2\u6570\u63a8\u65ad\uff0c\u63d0\u4f9b\u6821\u51c6\u826f\u597d\u7684\u540e\u9a8c\u5206\u5e03\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u79d1\u5b66\u9006\u95ee\u9898\u4e2d\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6765\u533a\u5206\u53ef\u8bc6\u522b\u53c2\u6570\u548c\u6d4b\u91cf\u6570\u636e\u4e0b\u4ecd\u6a21\u7cca\u7684\u53c2\u6570\u3002\u867d\u7136CDI\u5728\u4e00\u7ef4\u65f6\u95f4\u4fe1\u53f7\u4e0a\u5df2\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u5176\u5728\u66f4\u9ad8\u7ef4\u7a7a\u95f4\u6570\u636e\u4e0a\u7684\u9002\u7528\u6027\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u5c06CDI\u6269\u5c55\u5230\u4e8c\u7ef4\u7a7a\u95f4\u6761\u4ef6\u5316\uff0c\u76f4\u63a5\u4ece\u7a7a\u95f4\u89c2\u6d4b\u8fdb\u884c\u6982\u7387\u53c2\u6570\u63a8\u65ad\u3002\u5728\u4f1a\u805a\u675f\u7535\u5b50\u884d\u5c04\u53c2\u6570\u63a8\u65ad\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u53c2\u6570\u9006\u95ee\u9898\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4f7f\u7528\u6a21\u62df\u6570\u636e\u8bc4\u4f30\u6027\u80fd\u3002", "result": "CDI\u4ea7\u751f\u6821\u51c6\u826f\u597d\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u51c6\u786e\u53cd\u6620\u6d4b\u91cf\u7ea6\u675f\uff1a\u5bf9\u786e\u5b9a\u826f\u597d\u7684\u91cf\u4ea7\u751f\u7d27\u5bc6\u5206\u5e03\uff0c\u5bf9\u6a21\u7cca\u53c2\u6570\u4ea7\u751f\u9002\u5f53\u5bbd\u6cdb\u7684\u5206\u5e03\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6807\u51c6\u56de\u5f52\u65b9\u6cd5\u867d\u7136\u603b\u4f53\u6307\u6807\u770b\u4f3c\u51c6\u786e\uff0c\u4f46\u901a\u8fc7\u9884\u6d4b\u8bad\u7ec3\u96c6\u5747\u503c\u6765\u63a9\u76d6\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "CDI\u6210\u529f\u4ece\u65f6\u95f4\u57df\u6269\u5c55\u5230\u7a7a\u95f4\u57df\uff0c\u4e3a\u7a33\u5065\u7684\u79d1\u5b66\u63a8\u65ad\u63d0\u4f9b\u4e86\u771f\u6b63\u6240\u9700\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002"}}
{"id": "2601.18413", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18413", "abs": "https://arxiv.org/abs/2601.18413", "authors": ["Darlan Noetzold", "Valderi Reis Quietinho Leithardt"], "title": "Fundamentals, Recent Advances, and Challenges Regarding Cryptographic Algorithms for the Quantum Computing Era", "comment": "in Portuguese language", "summary": "This book arises from the need to provide a clear and up-to-date overview of the impacts of quantum computing on cryptography. The goal is to provide a reference in Portuguese for undergraduate, master's, and doctoral students in the field of data security and cryptography. Throughout the chapters, we present fundamentals, we discuss classical and post-quantum algorithms, evaluate emerging patterns, and point out real-world implementation challenges. The initial objective is to serve as a guide for students, researchers, and professionals who need to understand not only the mathematics involved, but also its practical implications in security systems and policies. For more advanced professionals, the main objective is to present content and ideas so that they can assess the changes and perspectives in the era of quantum cryptographic algorithms. To that end, the text's structure was designed to be progressive: we begin with essential concepts, move on to quantum algorithms and their consequences (with emphasis on Shor's algorithm), present issues focusing on \"families\" of post-quantum schemes (based on lattices, codes, hash functions, multivariate, isogenies), analyze the state of the art in standardization (highlighting the NIST process), and finally, discuss migration, interoperability, performance, and cryptographic governance. We hope that this work will assist in the formation of critical thinking and informed technical decision-making, fostering secure transition strategies for the post-quantum era.", "AI": {"tldr": "\u8461\u8404\u7259\u8bed\u6559\u6750\uff0c\u7cfb\u7edf\u4ecb\u7ecd\u91cf\u5b50\u8ba1\u7b97\u5bf9\u5bc6\u7801\u5b66\u7684\u5f71\u54cd\uff0c\u6db5\u76d6\u57fa\u7840\u6982\u5ff5\u3001\u91cf\u5b50\u7b97\u6cd5\u3001\u540e\u91cf\u5b50\u5bc6\u7801\u65b9\u6848\u3001\u6807\u51c6\u5316\u8fdb\u7a0b\u53ca\u5b9e\u9645\u8fc1\u79fb\u6311\u6218\u3002", "motivation": "\u4e3a\u8461\u8404\u7259\u8bed\u8bfb\u8005\u63d0\u4f9b\u5173\u4e8e\u91cf\u5b50\u8ba1\u7b97\u5bf9\u5bc6\u7801\u5b66\u5f71\u54cd\u7684\u6e05\u6670\u3001\u6700\u65b0\u7684\u7efc\u8ff0\uff0c\u586b\u8865\u8be5\u8bed\u8a00\u9886\u57df\u7684\u53c2\u8003\u7a7a\u767d\uff0c\u5e2e\u52a9\u5b66\u751f\u3001\u7814\u7a76\u4eba\u5458\u548c\u4e13\u4e1a\u4eba\u58eb\u7406\u89e3\u76f8\u5173\u6570\u5b66\u57fa\u7840\u53ca\u5b9e\u9645\u5b89\u5168\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u7ed3\u6784\uff1a\u4ece\u57fa\u7840\u6982\u5ff5\u5f00\u59cb\uff0c\u4ecb\u7ecd\u91cf\u5b50\u7b97\u6cd5\u53ca\u5176\u5f71\u54cd\uff08\u91cd\u70b9\u8ba8\u8bbaShor\u7b97\u6cd5\uff09\uff0c\u5206\u6790\u57fa\u4e8e\u683c\u3001\u7f16\u7801\u3001\u54c8\u5e0c\u51fd\u6570\u3001\u591a\u5143\u65b9\u7a0b\u3001\u540c\u6e90\u7b49\u4e0d\u540c\"\u5bb6\u65cf\"\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u65b9\u6848\uff0c\u8bc4\u4f30\u6807\u51c6\u5316\u73b0\u72b6\uff08\u7279\u522b\u662fNIST\u8fdb\u7a0b\uff09\uff0c\u6700\u540e\u8ba8\u8bba\u8fc1\u79fb\u3001\u4e92\u64cd\u4f5c\u6027\u3001\u6027\u80fd\u548c\u5bc6\u7801\u6cbb\u7406\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u672c\u5168\u9762\u7684\u8461\u8404\u7259\u8bed\u53c2\u8003\u4e66\uff0c\u7cfb\u7edf\u8986\u76d6\u91cf\u5b50\u5bc6\u7801\u5b66\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6311\u6218\uff0c\u4e3a\u4e0d\u540c\u5c42\u6b21\u7684\u8bfb\u8005\uff08\u4ece\u5b66\u751f\u5230\u9ad8\u7ea7\u4e13\u4e1a\u4eba\u58eb\uff09\u63d0\u4f9b\u7ed3\u6784\u5316\u77e5\u8bc6\u6846\u67b6\u3002", "conclusion": "\u8be5\u8457\u4f5c\u65e8\u5728\u57f9\u517b\u6279\u5224\u6027\u601d\u7ef4\u548c\u660e\u667a\u7684\u6280\u672f\u51b3\u7b56\u80fd\u529b\uff0c\u4fc3\u8fdb\u540e\u91cf\u5b50\u65f6\u4ee3\u7684\u5b89\u5168\u8fc7\u6e21\u7b56\u7565\uff0c\u4e3a\u6570\u636e\u5b89\u5168\u548c\u5bc6\u7801\u5b66\u9886\u57df\u63d0\u4f9b\u91cd\u8981\u7684\u6559\u80b2\u8d44\u6e90\u3002"}}
{"id": "2601.17946", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17946", "abs": "https://arxiv.org/abs/2601.17946", "authors": ["Renkai Ma", "Ben Z. Zhang", "Chen Chen", "Fan Yang", "Xiaoshan Huang", "Haolun Wu", "Lingyao Li"], "title": "\"I use ChatGPT to humanize my words\": Affordances and Risks of ChatGPT to Autistic Users", "comment": null, "summary": "Large Language Model (LLM) chatbots like ChatGPT have emerged as cognitive scaffolding for autistic users, yet the tension between their utility and risk remains under-articulated. Through an inductive thematic analysis of 3,984 social media posts by self-identified autistic users, we apply the Technology Affordance framework to examine this duality. We found that while users leveraged ChatGPT to offload executive dysfunction, regulate emotions, translate neurotypical communication, and validate their autistic identity, these affordances coexist with significant risks: reinforcing delusional thinking, erasing authentic identity through automated masking, and triggering conflicts with the autistic sense of justice. This poster identifies these trade-offs in autistic users' interactions with ChatGPT and concludes by outlining our future work on developing neuro-inclusive technologies that address these tensions through beneficial friction and bidirectional translation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790\u81ea\u95ed\u75c7\u7528\u6237\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff0c\u53d1\u73b0ChatGPT\u65e2\u662f\u8ba4\u77e5\u8f85\u52a9\u5de5\u5177\u53c8\u5b58\u5728\u98ce\u9669\uff1a\u5e2e\u52a9\u6267\u884c\u529f\u80fd\u3001\u60c5\u7eea\u8c03\u8282\u3001\u6c9f\u901a\u7ffb\u8bd1\u548c\u8eab\u4efd\u8ba4\u540c\uff0c\u4f46\u4e5f\u53ef\u80fd\u5f3a\u5316\u5984\u60f3\u601d\u7ef4\u3001\u6d88\u9664\u771f\u5b9e\u8eab\u4efd\u3001\u89e6\u53d1\u6b63\u4e49\u611f\u51b2\u7a81\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982ChatGPT\u5df2\u6210\u4e3a\u81ea\u95ed\u75c7\u7528\u6237\u7684\u8ba4\u77e5\u652f\u67b6\uff0c\u4f46\u5176\u6548\u7528\u4e0e\u98ce\u9669\u4e4b\u95f4\u7684\u5f20\u529b\u5c1a\u672a\u5145\u5206\u9610\u660e\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6280\u672f\u53ef\u4f9b\u6027\u6846\u67b6\uff0c\u63a2\u7d22\u81ea\u95ed\u75c7\u7528\u6237\u4e0eChatGPT\u4e92\u52a8\u4e2d\u7684\u8fd9\u79cd\u53cc\u91cd\u6027\u3002", "method": "\u91c7\u7528\u5f52\u7eb3\u5f0f\u4e3b\u9898\u5206\u6790\u6cd5\uff0c\u5bf93,984\u6761\u81ea\u6211\u8ba4\u540c\u4e3a\u81ea\u95ed\u75c7\u7528\u6237\u7684\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u8fdb\u884c\u5206\u6790\uff0c\u5e94\u7528\u6280\u672f\u53ef\u4f9b\u6027\u6846\u67b6\u6765\u68c0\u9a8cChatGPT\u5bf9\u81ea\u95ed\u75c7\u7528\u6237\u7684\u53cc\u91cd\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7528\u6237\u5229\u7528ChatGPT\u6765\u51cf\u8f7b\u6267\u884c\u529f\u80fd\u969c\u788d\u3001\u8c03\u8282\u60c5\u7eea\u3001\u7ffb\u8bd1\u795e\u7ecf\u5178\u578b\u6c9f\u901a\u5e76\u9a8c\u8bc1\u81ea\u95ed\u75c7\u8eab\u4efd\u3002\u4f46\u8fd9\u4e9b\u53ef\u4f9b\u6027\u4f34\u968f\u7740\u663e\u8457\u98ce\u9669\uff1a\u5f3a\u5316\u5984\u60f3\u601d\u7ef4\u3001\u901a\u8fc7\u81ea\u52a8\u5316\u4f2a\u88c5\u6d88\u9664\u771f\u5b9e\u8eab\u4efd\u3001\u89e6\u53d1\u4e0e\u81ea\u95ed\u75c7\u6b63\u4e49\u611f\u7684\u51b2\u7a81\u3002", "conclusion": "\u7814\u7a76\u8bc6\u522b\u4e86\u81ea\u95ed\u75c7\u7528\u6237\u4e0eChatGPT\u4e92\u52a8\u4e2d\u7684\u6743\u8861\u53d6\u820d\uff0c\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u5de5\u4f5c\u65b9\u5411\uff1a\u5f00\u53d1\u795e\u7ecf\u5305\u5bb9\u6027\u6280\u672f\uff0c\u901a\u8fc7\u6709\u76ca\u6469\u64e6\u548c\u53cc\u5411\u7ffb\u8bd1\u6765\u89e3\u51b3\u8fd9\u4e9b\u5f20\u529b\u3002"}}
{"id": "2601.17257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17257", "abs": "https://arxiv.org/abs/2601.17257", "authors": ["Javier Porras-Valenzuela", "Samar Hadou", "Alejandro Ribeiro"], "title": "A Constrained Optimization Perspective of Unrolled Transformers", "comment": null, "summary": "We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ea6\u675f\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3transformer\u4f7f\u5176\u884c\u4e3a\u7c7b\u4f3c\u4e8e\u4f18\u5316\u4e0b\u964d\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c42\u95f4\u4e0b\u964d\u7ea6\u675f\u548c\u539f\u59cb-\u5bf9\u5076\u8bad\u7ec3\u65b9\u6848\uff0c\u786e\u4fdd\u4e2d\u95f4\u8868\u793a\u5728\u671f\u671b\u4e0a\u5355\u8c03\u964d\u4f4e\u635f\u5931", "motivation": "\u4f20\u7edftransformer\u8bad\u7ec3\u4f7f\u7528\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316(ERM)\uff0c\u4f46\u7f3a\u4e4f\u4f18\u5316\u7b97\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\u3002\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7ftransformer\u5728\u5c42\u95f4\u5177\u6709\u5355\u8c03\u4e0b\u964d\u7279\u6027\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b", "method": "1. \u5728\u76ee\u6807\u51fd\u6570\u4e0a\u65bd\u52a0\u5c42\u95f4\u4e0b\u964d\u7ea6\u675f\uff1b2. \u7528\u539f\u59cb-\u5bf9\u5076\u8bad\u7ec3\u65b9\u6848\u66ff\u4ee3\u6807\u51c6ERM\uff1b3. \u5e94\u7528\u4e8e\u5c55\u5f00\u5f0ftransformer\u67b6\u6784\u548c\u9884\u8bad\u7ec3transformer\uff1b4. \u5728\u89c6\u9891\u53bb\u566a\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u9a8c\u8bc1", "result": "\u7ea6\u675ftransformer\u5728\u6270\u52a8\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u4fdd\u6301\u66f4\u9ad8\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5206\u5e03\u5185\u6027\u80fd\u3002\u4e2d\u95f4\u8868\u793a\u5728\u671f\u671b\u4e0a\u786e\u5b9e\u5b9e\u73b0\u4e86\u8de8\u5c42\u7684\u5355\u8c03\u635f\u5931\u4e0b\u964d", "conclusion": "\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u6846\u67b6\u8bad\u7ec3\u7684transformer\u80fd\u591f\u6a21\u62df\u4f18\u5316\u4e0b\u964d\u7b97\u6cd5\u884c\u4e3a\uff0c\u83b7\u5f97\u7406\u8bba\u4fdd\u8bc1\u7684\u5355\u8c03\u4e0b\u964d\u7279\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3atransformer\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6307\u5bfc"}}
{"id": "2601.18445", "categories": ["cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.18445", "abs": "https://arxiv.org/abs/2601.18445", "authors": ["Eymen \u00dcnay", "Bj\u00f6rn Franke", "Jackson Woodruff"], "title": "KeyMemRT Compiler and Runtime: Unlocking Memory-Scalable FHE", "comment": null, "summary": "Fully Homomorphic Encryption (FHE) enables privacy preserving computation but it suffers from high latency and memory consumption. The computations are secured with special keys called rotation keys which often take up the majority of memory. In complex FHE applications, these rotation keys can cause a large memory bottleneck limiting program throughput. Existing compilers make little effort to solve this problem, instead relying on systems with massive memory availability. This resource requirement is a barrier to FHE uptake because optimizing FHE programs by hand is challenging due to their scale, complexity and expertise required.\n  In this work, we present KeyMemRT; an MLIR based compiler and runtime framework that individually manages rotation key lifetimes to lower memory utilization and to allow arbitrary number of rotation indices to be supported without memory bloating. KeyMemRT relies on dataflow analysis to determine key lifetimes and is the first FHE compiler to provide automatic key management, handle fine-grained key-mangement and manage boostrap keys. We implement frontends for Orion and HEIR and show improvements over state-of-the-art FHE compilers. KeyMemRT achieves memory reduction of 1.74x and a speedup of 1.20x over ANT-ACE, and memory reduction of 1.16x and a speedup of 1.73x over memory-optimized compiler Fhelipe. We provide KeyMemRT as a post-optimizing compiler that can be targeted by any FHE compiler.", "AI": {"tldr": "KeyMemRT\u662f\u4e00\u4e2a\u57fa\u4e8eMLIR\u7684\u7f16\u8bd1\u5668\u548c\u8fd0\u884c\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u7ba1\u7406\u65cb\u8f6c\u5bc6\u94a5\u7684\u751f\u547d\u5468\u671f\u6765\u964d\u4f4e\u5168\u540c\u6001\u52a0\u5bc6\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u652f\u6301\u4efb\u610f\u6570\u91cf\u7684\u65cb\u8f6c\u7d22\u5f15\u800c\u4e0d\u4f1a\u5bfc\u81f4\u5185\u5b58\u81a8\u80c0\u3002", "motivation": "\u5168\u540c\u6001\u52a0\u5bc6\uff08FHE\uff09\u867d\u7136\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u8ba1\u7b97\uff0c\u4f46\u5b58\u5728\u9ad8\u5ef6\u8fdf\u548c\u9ad8\u5185\u5b58\u6d88\u8017\u95ee\u9898\u3002\u65cb\u8f6c\u5bc6\u94a5\u901a\u5e38\u5360\u7528\u5927\u90e8\u5206\u5185\u5b58\uff0c\u5728\u590d\u6742FHE\u5e94\u7528\u4e2d\u4f1a\u9020\u6210\u5185\u5b58\u74f6\u9888\uff0c\u9650\u5236\u7a0b\u5e8f\u541e\u5410\u91cf\u3002\u73b0\u6709\u7f16\u8bd1\u5668\u5f88\u5c11\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u800c\u662f\u4f9d\u8d56\u5927\u5185\u5b58\u7cfb\u7edf\uff0c\u8fd9\u963b\u788d\u4e86FHE\u7684\u666e\u53ca\uff0c\u56e0\u4e3a\u624b\u52a8\u4f18\u5316FHE\u7a0b\u5e8f\u5177\u6709\u6311\u6218\u6027\u3002", "method": "KeyMemRT\u91c7\u7528\u57fa\u4e8eMLIR\u7684\u7f16\u8bd1\u5668\u548c\u8fd0\u884c\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u6d41\u5206\u6790\u786e\u5b9a\u65cb\u8f6c\u5bc6\u94a5\u7684\u751f\u547d\u5468\u671f\uff0c\u5b9e\u73b0\u81ea\u52a8\u5bc6\u94a5\u7ba1\u7406\u3002\u5b83\u662f\u9996\u4e2a\u63d0\u4f9b\u81ea\u52a8\u5bc6\u94a5\u7ba1\u7406\u3001\u5904\u7406\u7ec6\u7c92\u5ea6\u5bc6\u94a5\u7ba1\u7406\u5e76\u7ba1\u7406\u5f15\u5bfc\u5bc6\u94a5\u7684FHE\u7f16\u8bd1\u5668\u3002\u652f\u6301Orion\u548cHEIR\u524d\u7aef\uff0c\u53ef\u4f5c\u4e3a\u540e\u4f18\u5316\u7f16\u8bd1\u5668\u88ab\u4efb\u4f55FHE\u7f16\u8bd1\u5668\u4f7f\u7528\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684FHE\u7f16\u8bd1\u5668\u76f8\u6bd4\uff0cKeyMemRT\u5728ANT-ACE\u4e0a\u5b9e\u73b0\u4e861.74\u500d\u7684\u5185\u5b58\u51cf\u5c11\u548c1.20\u500d\u7684\u52a0\u901f\uff0c\u5728\u5185\u5b58\u4f18\u5316\u7684\u7f16\u8bd1\u5668Fhelipe\u4e0a\u5b9e\u73b0\u4e861.16\u500d\u7684\u5185\u5b58\u51cf\u5c11\u548c1.73\u500d\u7684\u52a0\u901f\u3002", "conclusion": "KeyMemRT\u901a\u8fc7\u81ea\u52a8\u7ba1\u7406\u65cb\u8f6c\u5bc6\u94a5\u751f\u547d\u5468\u671f\uff0c\u663e\u8457\u964d\u4f4e\u4e86FHE\u7684\u5185\u5b58\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4e3aFHE\u7684\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4f5c\u4e3a\u540e\u4f18\u5316\u7f16\u8bd1\u5668\u96c6\u6210\u5230\u73b0\u6709FHE\u7f16\u8bd1\u5668\u4e2d\u3002"}}
{"id": "2601.17962", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17962", "abs": "https://arxiv.org/abs/2601.17962", "authors": ["Wenhan Lyu", "Yimeng Wang", "Murong Yue", "Yifan Sun", "Jennifer Suh", "Meredith Kier", "Ziyu Yao", "Yixuan Zhang"], "title": "Designing AI Peers for Collaborative Mathematical Problem Solving with Middle School Students: A Participatory Design Study", "comment": "Accepted by ACM CHI 2026", "summary": "Collaborative problem solving (CPS) is a fundamental practice in middle-school mathematics education; however, student groups frequently stall or struggle without ongoing teacher support. Recent work has explored how Generative AI tools can be designed to support one-on-one tutoring, but little is known about how AI can be designed as peer learning partners in collaborative learning contexts. We conducted a participatory design study with 24 middle school students, who first engaged in mathematics CPS tasks with AI peers in a technology probe, and then collaboratively designed their ideal AI peer. Our findings reveal that students envision an AI peer as competent in mathematics yet explicitly deferential, providing progressive scaffolds such as hints and checks under clear student control. Students preferred a tone of friendly expertise over exaggerated personas. We also discuss design recommendations and implications for AI peers in middle school mathematics CPS.", "AI": {"tldr": "\u5b66\u751f\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u7814\u7a76\uff1a\u4e2d\u5b66\u751f\u7406\u60f3\u4e2d\u7684\u6570\u5b66\u534f\u4f5c\u95ee\u9898\u89e3\u51b3AI\u540c\u4f34\u5e94\u5177\u5907\u6570\u5b66\u80fd\u529b\u4f46\u4fdd\u6301\u8c26\u900a\uff0c\u63d0\u4f9b\u6e10\u8fdb\u5f0f\u811a\u624b\u67b6\u652f\u6301\uff0c\u8bed\u6c14\u53cb\u597d\u4e13\u4e1a\u800c\u975e\u5938\u5f20\u4eba\u8bbe\u3002", "motivation": "\u4e2d\u5b66\u751f\u6570\u5b66\u534f\u4f5c\u95ee\u9898\u89e3\u51b3(CPS)\u4e2d\uff0c\u5b66\u751f\u5c0f\u7ec4\u7ecf\u5e38\u5728\u6ca1\u6709\u6301\u7eed\u6559\u5e08\u652f\u6301\u7684\u60c5\u51b5\u4e0b\u505c\u6ede\u6216\u6323\u624e\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u751f\u6210\u5f0fAI\u5982\u4f55\u652f\u6301\u4e00\u5bf9\u4e00\u8f85\u5bfc\uff0c\u4f46AI\u5982\u4f55\u4f5c\u4e3a\u540c\u4f34\u5b66\u4e60\u4f19\u4f34\u5728\u534f\u4f5c\u5b66\u4e60\u73af\u5883\u4e2d\u53d1\u6325\u4f5c\u7528\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u91c7\u7528\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u7814\u7a76\uff0c\u62db\u52df24\u540d\u4e2d\u5b66\u751f\u3002\u9996\u5148\u8ba9\u5b66\u751f\u5728\u6280\u672f\u63a2\u9488\u4e2d\u4e0eAI\u540c\u4f34\u8fdb\u884c\u6570\u5b66CPS\u4efb\u52a1\uff0c\u7136\u540e\u534f\u4f5c\u8bbe\u8ba1\u4ed6\u4eec\u7406\u60f3\u7684AI\u540c\u4f34\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b66\u751f\u5e0c\u671bAI\u540c\u4f34\u5177\u5907\u6570\u5b66\u80fd\u529b\u4f46\u660e\u786e\u4fdd\u6301\u8c26\u900a\u6001\u5ea6\uff0c\u63d0\u4f9b\u6e10\u8fdb\u5f0f\u811a\u624b\u67b6\u652f\u6301\uff08\u5982\u63d0\u793a\u548c\u68c0\u67e5\uff09\uff0c\u4e14\u8fd9\u4e9b\u652f\u6301\u5e94\u5904\u4e8e\u5b66\u751f\u7684\u660e\u786e\u63a7\u5236\u4e4b\u4e0b\u3002\u5b66\u751f\u504f\u597d\u53cb\u597d\u4e13\u4e1a\u7684\u8bed\u6c14\uff0c\u800c\u975e\u5938\u5f20\u7684\u4eba\u7269\u8bbe\u5b9a\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e2d\u5b66\u6570\u5b66CPS\u4e2dAI\u540c\u4f34\u7684\u8bbe\u8ba1\u5efa\u8bae\u548c\u542f\u793a\uff0c\u5f3a\u8c03AI\u5e94\u4f5c\u4e3a\u80fd\u529b\u578b\u4f46\u8c26\u900a\u7684\u540c\u4f34\uff0c\u63d0\u4f9b\u53ef\u63a7\u7684\u6e10\u8fdb\u5f0f\u652f\u6301\uff0c\u91c7\u7528\u53cb\u597d\u4e13\u4e1a\u7684\u4ea4\u4e92\u98ce\u683c\u3002"}}
{"id": "2601.17260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17260", "abs": "https://arxiv.org/abs/2601.17260", "authors": ["Marco Pollanen"], "title": "The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment", "comment": "10 Pages, 5 Figures", "summary": "Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $\u03b2$) yields progressively \"better\" behavior. We instead treat $\u03b2$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $\u03b2\\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $\u03b2$ induces capability losses that persist even after $\u03b2$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $\u03b2$ landscape rather than reliance on margins or aggregate benchmarks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0DPO\u4e2d\u7684\u03b2\u53c2\u6570\u5bf9\u6a21\u578b\u80fd\u529b\u6709\u590d\u6742\u5f71\u54cd\uff0c\u800c\u975e\u7b80\u5355\u7684\u5355\u8c03\u5173\u7cfb\u3002\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u5bf9\u03b2\u53d8\u5316\u7684\u54cd\u5e94\u6a21\u5f0f\u4e0d\u540c\uff0c\u504f\u597d\u8fb9\u9645\u53ef\u80fd\u4e0e\u63a8\u7406\u80fd\u529b\u8d1f\u76f8\u5173\uff0c\u9ad8\u03b2\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u80fd\u529b\u635f\u5931\u6301\u7eed\u5b58\u5728\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u589e\u52a0DPO\u7684\u5bf9\u9f50\u538b\u529b\uff08\u901a\u8fc7\u03b2\u53c2\u6570\uff09\u4f1a\u6301\u7eed\u6539\u5584\u6a21\u578b\u884c\u4e3a\uff0c\u4f46\u672c\u7814\u7a76\u8d28\u7591\u8fd9\u4e00\u5047\u8bbe\uff0c\u65e8\u5728\u7cfb\u7edf\u63a2\u7d22\u03b2\u53c2\u6570\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u771f\u5b9e\u5f71\u54cd\u3002", "method": "\u5bf9\u4e09\u4e2a7B\u5f00\u6e90\u6a21\u578b\u5bb6\u65cf\uff08Mistral\u3001Llama\u3001Qwen\uff09\u5728\u56fa\u5b9aDPO\u914d\u65b9\u4e0b\u5bc6\u96c6\u626b\u63cf\u03b2\u53c2\u6570\uff0c\u5206\u6790\u903b\u8f91\u63a2\u9488\u8fb9\u9645\u3001\u63a8\u7406\u80fd\u529b\u53d8\u5316\uff0c\u5e76\u7814\u7a76\u8bad\u7ec3\u8def\u5f84\u4f9d\u8d56\u6548\u5e94\u3002", "result": "1. Mistral\u80fd\u529b\u5448\u5c16\u9510\u975e\u5355\u8c03\u6027\uff1a\u4ec5\u5728\u03b2\u224810\u207b\u00b2\u7a84\u5e26\u5185\u903b\u8f91\u63a2\u9488\u8fb9\u9645\u4e3a\u6b63\uff0c\u8fb9\u754c\u70b9\u5bf9\u968f\u673a\u79cd\u5b50\u654f\u611f\uff1b2. \u4e0d\u540c\u67b6\u6784\u54cd\u5e94\u6a21\u5f0f\u4e0d\u540c\uff1aMistral\u5267\u70c8\u91cd\u7ec4\uff0cLlama\u9009\u62e9\u6027\u53d8\u5316\uff0cQwen\u5e73\u6ed1\u6743\u8861\uff1b3. DPO\u504f\u597d\u8fb9\u9645\u4e0e\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u8d1f\u76f8\u5173\uff08Llama\u903b\u8f91\u4efb\u52a1r=-0.91\uff09\uff1b4. \u9ad8\u03b2\u8bad\u7ec3\u5bfc\u81f4\u80fd\u529b\u635f\u5931\u6301\u7eed\u5b58\u5728\uff08\u6ede\u540e\u6548\u5e94\uff09\u3002", "conclusion": "DPO\u7684\u03b2\u53c2\u6570\u5bf9\u6a21\u578b\u80fd\u529b\u6709\u590d\u6742\u3001\u67b6\u6784\u4f9d\u8d56\u7684\u5f71\u54cd\uff0c\u504f\u597d\u8fb9\u9645\u4e0d\u80fd\u53ef\u9760\u53cd\u6620\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u5728\u03b2\u53c2\u6570\u7a7a\u95f4\u4e2d\u57fa\u4e8e\u80fd\u529b\u8fdb\u884c\u7ec6\u81f4\u8bc4\u4f30\uff0c\u800c\u975e\u4f9d\u8d56\u8fb9\u9645\u6216\u805a\u5408\u57fa\u51c6\u3002"}}
{"id": "2601.17570", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17570", "abs": "https://arxiv.org/abs/2601.17570", "authors": ["Hadi Salloum", "Ali Jnadi", "Yaroslav Kholodov", "Alexander Gasnikov"], "title": "Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization", "comment": "Proceedings of Machine Learning Research tbd: 1_13, 2025 International Conference on Computational Optimization", "summary": "Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.", "AI": {"tldr": "MC\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\u4f4e\uff0c\u4f5c\u8005\u63d0\u51faMC+QUBO\u65b9\u6cd5\uff0c\u5c06\u8f68\u8ff9\u9009\u62e9\u5efa\u6a21\u4e3aQUBO\u95ee\u9898\uff0c\u7528\u91cf\u5b50\u542f\u53d1\u5f0f\u91c7\u6837\u5668\u6c42\u89e3\uff0c\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u7b56\u7565\u8d28\u91cf\u3002", "motivation": "\u8499\u7279\u5361\u6d1b\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u5956\u52b1\u3001\u5927\u72b6\u6001\u7a7a\u95f4\u548c\u8f68\u8ff9\u76f8\u5173\u73af\u5883\u4e2d\u6837\u672c\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8f68\u8ff9\u9009\u62e9\u65b9\u6cd5\u6765\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "method": "\u5c06\u8f68\u8ff9\u9009\u62e9\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u6a21\u62df\u91cf\u5b50\u9000\u706b\u548c\u6a21\u62df\u5206\u5c94\u4f5c\u4e3a\u9ed1\u76d2\u6c42\u89e3\u5668\uff0c\u4ece\u6bcf\u6279\u8f68\u8ff9\u4e2d\u9009\u62e9\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u540c\u65f6\u4fc3\u8fdb\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u7684\u5b50\u96c6\u3002", "result": "\u5728\u6709\u9650\u65f6\u57dfGridWorld\u5b9e\u9a8c\u4e2d\uff0cMC+QUBO\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u7b56\u7565\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u3002", "conclusion": "\u91cf\u5b50\u542f\u53d1\u5f0f\u4f18\u5316\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u51b3\u7b56\u5b50\u7a0b\u5e8f\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8499\u7279\u5361\u6d1b\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2601.18511", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.18511", "abs": "https://arxiv.org/abs/2601.18511", "authors": ["Jaiyoung Park", "Sejin Park", "Jai Hyun Park", "Jung Ho Ahn", "Jung Hee Cheon", "Guillaume Hanrot", "Jung Woo Kim", "Minje Park", "Damien Stehl\u00e9"], "title": "Scaling up Privacy-Preserving ML: A CKKS Implementation of Llama-2-7B", "comment": null, "summary": "As large language models (LLMs) become ubiquitous, privacy concerns pertaining to inference inputs keep growing. In this context, fully homomorphic encryption (FHE) has emerged as a primary cryptographic solution to provide non-interactive confidential LLM inference. Existing solutions scale poorly with the input token length, and hence focus either on small models or larger models with a small number of input tokens. They also suffer from the existence of large outlier values. These values have a strong impact on the evaluation of non-linear layers, leading to large-degree polynomial approximation and thus heavy evaluation costs.\n  We propose an FHE-based private LLM inference solution that allows thousands of input tokens with only a part of them being encrypted: this fits with a scenario where the context is benign and only part of the input is sensitive. To do so, we suggest an unbalanced chunked prefill framework that processes the private and public parts of the input tokens differently. Our framework contains plaintext-plaintext, plaintext-ciphertext and ciphertext-ciphertext computational components. We adopt different strategies and ingredients for each component. We also devise new homomorphic algorithms for specific matrix multiplication and polynomial evaluation tasks encountered during LLM inference.\n  Furthermore, without retraining, we tailor the LLM inference algorithm to reduce the ranges of outlier values: we leverage machine learning strategies (token prepending and rotations) to mitigate the impact of the outliers on non-linear layers.\n  Based on these ingredients, we describe a CKKS-based end-to-end implementation of Llama-2-7B private inference for up to 4096 input tokens, of which the last 128 are encrypted. On a cluster of 8~NVIDIA RTX-4090 GPUs, inference takes 85s for summarization and 33s for generation per output token.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eFHE\u7684\u79c1\u6709LLM\u63a8\u7406\u65b9\u6848\uff0c\u652f\u6301\u6570\u5343\u8f93\u5165\u4ee4\u724c\uff0c\u4ec5\u90e8\u5206\u52a0\u5bc6\uff0c\u91c7\u7528\u975e\u5e73\u8861\u5206\u5757\u9884\u586b\u5145\u6846\u67b6\uff0c\u4f18\u5316\u77e9\u9635\u4e58\u6cd5\u548c\u591a\u9879\u5f0f\u8bc4\u4f30\uff0c\u51cf\u5c11\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u5b9e\u73b0Llama-2-7B\u7aef\u5230\u7aef\u79c1\u6709\u63a8\u7406\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u666e\u53ca\uff0c\u63a8\u7406\u8f93\u5165\u7684\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u73b0\u6709FHE\u65b9\u6848\u5bf9\u8f93\u5165\u4ee4\u724c\u957f\u5ea6\u6269\u5c55\u6027\u5dee\uff0c\u4ec5\u9002\u7528\u4e8e\u5c0f\u6a21\u578b\u6216\u5c11\u91cf\u8f93\u5165\u4ee4\u724c\uff0c\u4e14\u5b58\u5728\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u5bfc\u81f4\u975e\u7ebf\u6027\u5c42\u8bc4\u4f30\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u975e\u5e73\u8861\u5206\u5757\u9884\u586b\u5145\u6846\u67b6\uff0c\u533a\u5206\u5904\u7406\u79c1\u6709\u548c\u516c\u5171\u8f93\u5165\u90e8\u5206\uff0c\u5305\u542b\u660e\u6587-\u660e\u6587\u3001\u660e\u6587-\u5bc6\u6587\u548c\u5bc6\u6587-\u5bc6\u6587\u8ba1\u7b97\u7ec4\u4ef6\u3002\u8bbe\u8ba1\u65b0\u7684\u540c\u6001\u7b97\u6cd5\u7528\u4e8eLLM\u63a8\u7406\u4e2d\u7684\u77e9\u9635\u4e58\u6cd5\u548c\u591a\u9879\u5f0f\u8bc4\u4f30\u4efb\u52a1\uff0c\u91c7\u7528\u4ee4\u724c\u524d\u7f6e\u548c\u65cb\u8f6c\u7b49\u673a\u5668\u5b66\u4e60\u7b56\u7565\u51cf\u5c11\u5f02\u5e38\u503c\u8303\u56f4\u3002", "result": "\u57fa\u4e8eCKKS\u65b9\u6848\u5b9e\u73b0Llama-2-7B\u7aef\u5230\u7aef\u79c1\u6709\u63a8\u7406\uff0c\u652f\u6301\u6700\u591a4096\u4e2a\u8f93\u5165\u4ee4\u724c\uff0c\u5176\u4e2d\u6700\u540e128\u4e2a\u52a0\u5bc6\u3002\u57288\u4e2aNVIDIA RTX-4090 GPU\u96c6\u7fa4\u4e0a\uff0c\u6458\u8981\u63a8\u7406\u8017\u65f685\u79d2\uff0c\u751f\u6210\u6bcf\u4e2a\u8f93\u51fa\u4ee4\u724c\u8017\u65f633\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6848\u89e3\u51b3\u4e86FHE\u5728LLM\u63a8\u7406\u4e2d\u7684\u6269\u5c55\u6027\u548c\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u79c1\u6709LLM\u63a8\u7406\u7cfb\u7edf\uff0c\u4e3a\u654f\u611f\u6570\u636e\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u52a0\u5bc6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17975", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17975", "abs": "https://arxiv.org/abs/2601.17975", "authors": ["Bryan Min", "Peiling Jiang", "Zhicheng Huang", "Haijun Xia"], "title": "Gradual Generation of User Interfaces as a Design Method for Malleable Software", "comment": "5 pages, 3 figures", "summary": "AI is growing increasingly capable of automatically generating user interfaces (GenUI) from user prompts. However, designing GenUI applications that enable users to discover diverse customizations while preserving GenUI's expressiveness remains challenging. Current design methods -- presenting prompt boxes and leveraging context -- lack affordances for customization discovery, while traditional menu-based approaches become overly complex given GenUI's vast customization space. We propose Gradually Generating User Interfaces -- a design method that structures customizations into intermediate UI layers that AI gradually loads during interface generation. These intermediate stages expose different customization features along specific dimensions, making them discoverable to users. Users can wind back the generation process to access customizations. We demonstrate this approach through three prototype websites, showing how designers can support GenUI's expanded customization capabilities while maintaining visual simplicity and discoverability. Our work offers a practical method for integrating customization features into GenUI applications, contributing an approach to designing malleable software.", "AI": {"tldr": "\u63d0\u51fa\"\u9010\u6b65\u751f\u6210\u7528\u6237\u754c\u9762\"\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e2d\u95f4UI\u5c42\u7ed3\u6784\u5316\u548c\u6e10\u8fdb\u5f0f\u52a0\u8f7d\uff0c\u89e3\u51b3GenUI\u5e94\u7528\u4e2d\u5b9a\u5236\u529f\u80fd\u53d1\u73b0\u4e0e\u8868\u8fbe\u6027\u5e73\u8861\u7684\u6311\u6218", "motivation": "AI\u81ea\u52a8\u751f\u6210\u7528\u6237\u754c\u9762\u80fd\u529b\u589e\u5f3a\uff0c\u4f46\u73b0\u6709\u8bbe\u8ba1\u65b9\u6cd5\u96be\u4ee5\u8ba9\u7528\u6237\u53d1\u73b0\u591a\u6837\u5316\u5b9a\u5236\u9009\u9879\uff0c\u540c\u65f6\u4fdd\u6301GenUI\u7684\u8868\u8fbe\u6027\u3002\u5f53\u524d\u65b9\u6cd5\uff08\u63d0\u793a\u6846\u548c\u4e0a\u4e0b\u6587\u5229\u7528\uff09\u7f3a\u4e4f\u5b9a\u5236\u53d1\u73b0\u652f\u6301\uff0c\u4f20\u7edf\u83dc\u5355\u65b9\u6cd5\u5728GenUI\u5e9e\u5927\u5b9a\u5236\u7a7a\u95f4\u4e2d\u8fc7\u4e8e\u590d\u6742", "method": "\u63d0\u51fa\"\u9010\u6b65\u751f\u6210\u7528\u6237\u754c\u9762\"\u8bbe\u8ba1\u65b9\u6cd5\uff1a\u5c06\u5b9a\u5236\u529f\u80fd\u7ed3\u6784\u5316\u5230\u4e2d\u95f4UI\u5c42\uff0cAI\u5728\u754c\u9762\u751f\u6210\u8fc7\u7a0b\u4e2d\u9010\u6b65\u52a0\u8f7d\u8fd9\u4e9b\u5c42\u3002\u4e2d\u95f4\u9636\u6bb5\u6cbf\u7279\u5b9a\u7ef4\u5ea6\u66b4\u9732\u4e0d\u540c\u5b9a\u5236\u529f\u80fd\uff0c\u4f7f\u5176\u5bf9\u7528\u6237\u53ef\u53d1\u73b0\u3002\u7528\u6237\u53ef\u4ee5\u56de\u9000\u751f\u6210\u8fc7\u7a0b\u4ee5\u8bbf\u95ee\u5b9a\u5236\u9009\u9879", "result": "\u901a\u8fc7\u4e09\u4e2a\u539f\u578b\u7f51\u7ad9\u5c55\u793a\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u8bbe\u8ba1\u5e08\u53ef\u4ee5\u5728\u652f\u6301GenUI\u6269\u5c55\u5b9a\u5236\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u89c6\u89c9\u7b80\u6d01\u6027\u548c\u53ef\u53d1\u73b0\u6027", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u5c06\u5b9a\u5236\u529f\u80fd\u96c6\u6210\u5230GenUI\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4e3a\u8bbe\u8ba1\u53ef\u5851\u8f6f\u4ef6\u8d21\u732e\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2601.17261", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17261", "abs": "https://arxiv.org/abs/2601.17261", "authors": ["Wei Lin", "Yining Jiang", "Qingyu Song", "Qiao Xiang", "Hong Xu"], "title": "AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning", "comment": "21 pages in total, including 9 pages of main text, with 4 figures and 3 tables. This manuscript is submitted to arXiv", "summary": "Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.", "AI": {"tldr": "AGZO\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u6fc0\u6d3b\u7ed3\u6784\u4fe1\u606f\uff0c\u5728\u4f4e\u79e9\u5b50\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6270\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u9636\u4f18\u5316\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5404\u5411\u540c\u6027\u6270\u52a8\uff0c\u5ffd\u7565\u4e86\u524d\u5411\u4f20\u64ad\u4e2d\u53ef\u7528\u7684\u4e30\u5bcc\u7ed3\u6784\u4fe1\u606f\u3002\u4f5c\u8005\u53d1\u73b0\u7ebf\u6027\u5c42\u7684\u68af\u5ea6\u88ab\u9650\u5236\u5728\u5176\u8f93\u5165\u6fc0\u6d3b\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u8fd9\u4e3a\u6539\u8fdb\u96f6\u9636\u4f18\u5316\u63d0\u4f9b\u4e86\u5173\u952e\u6d1e\u89c1\u3002", "method": "\u63d0\u51fa\u6fc0\u6d3b\u5f15\u5bfc\u96f6\u9636\u4f18\u5316\uff08AGZO\uff09\uff0c\u5728\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u52a8\u6001\u63d0\u53d6\u7d27\u51d1\u7684\u6fc0\u6d3b\u4fe1\u606f\u5b50\u7a7a\u95f4\uff0c\u5e76\u5c06\u6270\u52a8\u9650\u5236\u5728\u8fd9\u4e2a\u4f4e\u79e9\u5b50\u7a7a\u95f4\u4e2d\u3002\u8be5\u65b9\u6cd5\u4f18\u5316\u4e86\u4e00\u4e2a\u5b50\u7a7a\u95f4\u5e73\u6ed1\u7684\u76ee\u6807\u51fd\u6570\u3002", "result": "AGZO\u5728Qwen3\u548cPangu\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u9636\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u4e00\u9636\u5fae\u8c03\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5176\u4ed6\u96f6\u9636\u65b9\u6cd5\u51e0\u4e4e\u76f8\u540c\u7684\u5cf0\u503c\u5185\u5b58\u5360\u7528\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u6fc0\u6d3b\u7ed3\u6784\u4fe1\u606f\uff0cAGZO\u4e3a\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u7684LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u96f6\u9636\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2601.18107", "categories": ["cs.LG", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18107", "abs": "https://arxiv.org/abs/2601.18107", "authors": ["Pedram Agand", "Mo Chen"], "title": "Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions", "comment": "11 pages, 2 figures, 2 tables", "summary": "Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.", "AI": {"tldr": "MoReBRAC\uff1a\u57fa\u4e8e\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u5faa\u73af\u4e16\u754c\u6a21\u578b\u5408\u6210\u9ad8\u8d28\u91cf\u8f6c\u79fb\u6570\u636e\uff0c\u7ed3\u5408VAE\u6d41\u5f62\u68c0\u6d4b\u3001\u6a21\u578b\u654f\u611f\u6027\u5206\u6790\u548cMC dropout\u7684\u591a\u5c42\u4e0d\u786e\u5b9a\u6027\u7ba1\u9053\u786e\u4fdd\u5408\u6210\u6570\u636e\u53ef\u9760\u6027\uff0c\u5728D4RL\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u9759\u6001\u6570\u636e\u96c6\u4e0e\u5b66\u4e60\u7b56\u7565\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u901a\u5e38\u9700\u8981\u9ad8\u5ea6\u4fdd\u5b88\u7684\u65b9\u6cd5\uff0c\u8fd9\u4f1a\u9650\u5236\u7b56\u7565\u6539\u8fdb\u6f5c\u529b", "method": "MoReBRAC\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u91cd\u5faa\u73af\u4e16\u754c\u6a21\u578b\u5408\u6210\u9ad8\u4fdd\u771f\u8f6c\u79fb\u6570\u636e\u6765\u6269\u5c55\u8bad\u7ec3\u6d41\u5f62\uff1b\u901a\u8fc7\u5206\u5c42\u4e0d\u786e\u5b9a\u6027\u7ba1\u9053\uff08VAE\u6d41\u5f62\u68c0\u6d4b\u3001\u6a21\u578b\u654f\u611f\u6027\u5206\u6790\u3001MC dropout\uff09\u786e\u4fdd\u5408\u6210\u6570\u636e\u53ef\u9760\u6027\uff1b\u4ec5\u4f7f\u7528\u5b66\u4e60\u52a8\u6001\u9ad8\u7f6e\u4fe1\u533a\u57df\u7684\u8f6c\u79fb\u6570\u636e", "result": "\u5728D4RL Gym-MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\"\u968f\u673a\"\u548c\"\u6b21\u4f18\"\u6570\u636e\u673a\u5236\u4e0b\uff1b\u6df1\u5165\u5206\u6790\u4e86VAE\u4f5c\u4e3a\u51e0\u4f55\u951a\u70b9\u7684\u4f5c\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u4ece\u8fd1\u6700\u4f18\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u65f6\u7684\u5206\u5e03\u6743\u8861", "conclusion": "MoReBRAC\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6f5c\u5728\u5408\u6210\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7b56\u7565\u6539\u8fdb\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18033", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18033", "abs": "https://arxiv.org/abs/2601.18033", "authors": ["Ahana Ghosh", "Advait Sarkar", "Si\u00e2n Lindley", "Christian Poelitz"], "title": "An Experimental Comparison of Cognitive Forcing Functions for Execution Plans in AI-Assisted Writing: Effects On Trust, Overreliance, and Perceived Critical Thinking", "comment": null, "summary": "Generative AI (GenAI) tools improve productivity in knowledge workflows such as writing, but also risk overreliance and reduced critical thinking. Cognitive forcing functions (CFFs) mitigate these risks by requiring active engagement with AI output. As GenAI workflows grow more complex, systems increasingly present execution plans for user review. However, these plans are themselves AI-generated and prone to overreliance, and the effectiveness of applying CFFs to AI plans remains underexplored. We conduct a controlled experiment in which participants completed AI-assisted writing tasks while reviewing AI-generated plans under four CFF conditions: Assumption (argument analysis), WhatIf (hypothesis testing), Both, and a no-CFF control. A follow-up think-aloud and interview study qualitatively compared these conditions. Results show that the Assumption CFF most effectively reduced overreliance without increasing cognitive load, while participants perceived the WhatIf CFF as most helpful. These findings highlight the value of plan-focused CFFs for supporting critical reflection in GenAI-assisted knowledge work.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u751f\u6210\u5f0fAI\u8f85\u52a9\u5199\u4f5c\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u8ba4\u77e5\u5f3a\u5236\u51fd\u6570\uff08CFFs\uff09\u51cf\u5c11\u5bf9AI\u751f\u6210\u8ba1\u5212\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u53d1\u73b0\u5047\u8bbe\u5206\u6790CFF\u6700\u6709\u6548\u964d\u4f4e\u8fc7\u5ea6\u4f9d\u8d56\u4e14\u4e0d\u589e\u52a0\u8ba4\u77e5\u8d1f\u8377\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5de5\u5177\u867d\u7136\u63d0\u9ad8\u4e86\u77e5\u8bc6\u5de5\u4f5c\u6d41\u7a0b\uff08\u5982\u5199\u4f5c\uff09\u7684\u751f\u4ea7\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u548c\u6279\u5224\u6027\u601d\u7ef4\u51cf\u5c11\u7684\u98ce\u9669\u3002\u968f\u7740AI\u5de5\u4f5c\u6d41\u7a0b\u65e5\u76ca\u590d\u6742\uff0c\u7cfb\u7edf\u5e38\u63d0\u4f9bAI\u751f\u6210\u7684\u6267\u884c\u8ba1\u5212\u4f9b\u7528\u6237\u5ba1\u67e5\uff0c\u4f46\u8fd9\u4e9b\u8ba1\u5212\u672c\u8eab\u4e5f\u5bb9\u6613\u5f15\u53d1\u8fc7\u5ea6\u4f9d\u8d56\u3002\u76ee\u524d\u5bf9\u4e8e\u5c06\u8ba4\u77e5\u5f3a\u5236\u51fd\u6570\u5e94\u7528\u4e8eAI\u8ba1\u5212\u7684\u6709\u6548\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u53d7\u63a7\u5b9e\u9a8c\uff0c\u53c2\u4e0e\u8005\u5728AI\u8f85\u52a9\u5199\u4f5c\u4efb\u52a1\u4e2d\u5ba1\u67e5AI\u751f\u6210\u8ba1\u5212\uff0c\u5206\u4e3a\u56db\u79cdCFF\u6761\u4ef6\uff1a\u5047\u8bbe\u5206\u6790\uff08\u8bba\u8bc1\u5206\u6790\uff09\u3001\u5047\u8bbe\u6d4b\u8bd5\u3001\u4e24\u8005\u7ed3\u5408\u548c\u65e0CFF\u5bf9\u7167\u7ec4\u3002\u968f\u540e\u8fdb\u884c\u6709\u58f0\u601d\u7ef4\u548c\u8bbf\u8c08\u7814\u7a76\uff0c\u5b9a\u6027\u6bd4\u8f83\u8fd9\u4e9b\u6761\u4ef6\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5047\u8bbe\u5206\u6790CFF\u6700\u6709\u6548\u5730\u51cf\u5c11\u4e86\u8fc7\u5ea6\u4f9d\u8d56\u4e14\u672a\u589e\u52a0\u8ba4\u77e5\u8d1f\u8377\uff0c\u800c\u53c2\u4e0e\u8005\u8ba4\u4e3a\u5047\u8bbe\u6d4b\u8bd5CFF\u6700\u6709\u5e2e\u52a9\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u8ba1\u5212\u5bfc\u5411CFF\u5728\u652f\u6301\u751f\u6210\u5f0fAI\u8f85\u52a9\u77e5\u8bc6\u5de5\u4f5c\u4e2d\u6279\u5224\u6027\u53cd\u601d\u7684\u4ef7\u503c\u3002", "conclusion": "\u8ba1\u5212\u5bfc\u5411\u7684\u8ba4\u77e5\u5f3a\u5236\u51fd\u6570\uff08\u7279\u522b\u662f\u5047\u8bbe\u5206\u6790CFF\uff09\u80fd\u6709\u6548\u51cf\u8f7b\u5bf9AI\u751f\u6210\u8ba1\u5212\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7684\u8ba4\u77e5\u8d1f\u8377\uff0c\u4e3a\u8bbe\u8ba1\u652f\u6301\u6279\u5224\u6027\u53cd\u601d\u7684\u751f\u6210\u5f0fAI\u8f85\u52a9\u77e5\u8bc6\u5de5\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2601.18085", "categories": ["cs.HC", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.18085", "abs": "https://arxiv.org/abs/2601.18085", "authors": ["Brian Gin", "Ahreum Lim", "Fl\u00e1via Silva e Oliveira", "Kuan Xing", "Xiaomei Song", "Gayana Amiyangoda", "Thilanka Seneviratne", "Alison F. Doubleday", "Ananya Gangopadhyaya", "Bob Kiser", "Lukas Shum-Tim", "Dhruva Patel", "Kosala Marambe", "Lauren Maggio", "Ara Tekian", "Yoon Soo Park"], "title": "\"Crash Test Dummies\" for AI-Enabled Clinical Assessment: Validating Virtual Patient Scenarios with Virtual Learners", "comment": null, "summary": "Background: In medical and health professions education (HPE), AI is increasingly used to assess clinical competencies, including via virtual standardized patients. However, most evaluations rely on AI-human interrater reliability and lack a measurement framework for how cases, learners, and raters jointly shape scores. This leaves robustness uncertain and can expose learners to misguidance from unvalidated systems. We address this by using AI \"simulated learners\" to stress-test and psychometrically characterize assessment pipelines before human use.\n  Objective: Develop an open-source AI virtual patient platform and measurement model for robust competency evaluation across cases and rating conditions.\n  Methods: We built a platform with virtual patients, virtual learners with tunable ACGME-aligned competency profiles, and multiple independent AI raters scoring encounters with structured Key-Features items. Transcripts were analyzed with a Bayesian HRM-SDT model that treats ratings as decisions under uncertainty and separates learner ability, case performance, and rater behavior; parameters were estimated with MCMC.\n  Results: The model recovered simulated learners' competencies, with significant correlations to the generating competencies across all ACGME domains despite a non-deterministic pipeline. It estimated case difficulty by competency and showed stable rater detection (sensitivity) and criteria (severity/leniency thresholds) across AI raters using identical models/prompts but different seeds. We also propose a staged \"safety blueprint\" for deploying AI tools with learners, tied to entrustment-based validation milestones.\n  Conclusions: Combining a purpose-built virtual patient platform with a principled psychometric model enables robust, interpretable, generalizable competency estimates and supports validation of AI-assisted assessment prior to use with human learners.", "AI": {"tldr": "\u5f00\u53d1\u5f00\u6e90AI\u865a\u62df\u75c5\u4eba\u5e73\u53f0\u548c\u6d4b\u91cf\u6a21\u578b\uff0c\u7528\u4e8e\u8de8\u75c5\u4f8b\u548c\u8bc4\u5206\u6761\u4ef6\u7684\u7a33\u5065\u80fd\u529b\u8bc4\u4f30\uff0c\u901a\u8fc7AI\"\u6a21\u62df\u5b66\u4e60\u8005\"\u5728\u4eba\u7c7b\u4f7f\u7528\u524d\u5bf9\u8bc4\u4f30\u6d41\u7a0b\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u548c\u5fc3\u7406\u6d4b\u91cf\u7279\u6027\u5206\u6790", "motivation": "\u5f53\u524d\u533b\u5b66\u548c\u536b\u751f\u4e13\u4e1a\u6559\u80b2\u4e2d\uff0cAI\u8bc4\u4f30\u4e34\u5e8a\u80fd\u529b\u4e3b\u8981\u4f9d\u8d56AI-\u4eba\u7c7b\u8bc4\u5206\u8005\u95f4\u4fe1\u5ea6\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u75c5\u4f8b\u3001\u5b66\u4e60\u8005\u548c\u8bc4\u5206\u8005\u5982\u4f55\u5171\u540c\u5f71\u54cd\u5f97\u5206\u7684\u6d4b\u91cf\u6846\u67b6\uff0c\u5bfc\u81f4\u7cfb\u7edf\u7a33\u5065\u6027\u4e0d\u786e\u5b9a\uff0c\u5b66\u4e60\u8005\u53ef\u80fd\u53d7\u5230\u672a\u7ecf\u9a8c\u8bc1\u7cfb\u7edf\u7684\u8bef\u5bfc", "method": "\u6784\u5efa\u5305\u542b\u865a\u62df\u75c5\u4eba\u3001\u53ef\u8c03ACGME\u80fd\u529b\u914d\u7f6e\u6587\u4ef6\u7684\u865a\u62df\u5b66\u4e60\u8005\u5e73\u53f0\uff0c\u591a\u4e2a\u72ec\u7acbAI\u8bc4\u5206\u8005\u4f7f\u7528\u7ed3\u6784\u5316\u5173\u952e\u7279\u5f81\u9879\u76ee\u5bf9\u4e92\u52a8\u8fdb\u884c\u8bc4\u5206\uff0c\u91c7\u7528\u8d1d\u53f6\u65afHRM-SDT\u6a21\u578b\u5206\u6790\u8f6c\u5f55\u6587\u672c\uff0c\u5c06\u8bc4\u5206\u89c6\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\uff0c\u5206\u79bb\u5b66\u4e60\u8005\u80fd\u529b\u3001\u75c5\u4f8b\u8868\u73b0\u548c\u8bc4\u5206\u8005\u884c\u4e3a\uff0c\u4f7f\u7528MCMC\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1", "result": "\u6a21\u578b\u6210\u529f\u6062\u590d\u4e86\u6a21\u62df\u5b66\u4e60\u8005\u7684\u80fd\u529b\uff0c\u5728\u6240\u6709ACGME\u9886\u57df\u4e0e\u751f\u6210\u80fd\u529b\u663e\u8457\u76f8\u5173\uff1b\u4f30\u8ba1\u4e86\u6309\u80fd\u529b\u5212\u5206\u7684\u75c5\u4f8b\u96be\u5ea6\uff1bAI\u8bc4\u5206\u8005\u95f4\u663e\u793a\u51fa\u7a33\u5b9a\u7684\u68c0\u6d4b\u7075\u654f\u5ea6\uff08\u654f\u611f\u6027\uff09\u548c\u6807\u51c6\uff08\u4e25\u683c/\u5bbd\u677e\u9608\u503c\uff09\uff1b\u63d0\u51fa\u4e86\u5206\u9636\u6bb5\"\u5b89\u5168\u84dd\u56fe\"\u7528\u4e8eAI\u5de5\u5177\u90e8\u7f72", "conclusion": "\u7ed3\u5408\u4e13\u95e8\u6784\u5efa\u7684\u865a\u62df\u75c5\u4eba\u5e73\u53f0\u548c\u539f\u5219\u6027\u5fc3\u7406\u6d4b\u91cf\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u63a8\u5e7f\u7684\u80fd\u529b\u4f30\u8ba1\uff0c\u652f\u6301\u5728\u4eba\u7c7b\u5b66\u4e60\u8005\u4f7f\u7528\u524d\u9a8c\u8bc1AI\u8f85\u52a9\u8bc4\u4f30"}}
{"id": "2601.17275", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17275", "abs": "https://arxiv.org/abs/2601.17275", "authors": ["Lianlei Shan", "Han Chen", "Yixuan Wang", "Zhenjie Liu", "Wei Li"], "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning", "comment": "12 pages,", "summary": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.", "AI": {"tldr": "DLR\u63d0\u51fa\u4e86\u4e00\u79cd\u6f5c\u5728\u7a7a\u95f4\u53cc\u5411\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u94fe\u7684\u8bd5\u9519\u6210\u672c\u4ece\u6602\u8d35\u7684token\u7ea7\u5e8f\u5217\u751f\u6210\u8f6c\u79fb\u5230\u8fde\u7eed\u6f5c\u5728\u6d41\u5f62\uff0c\u901a\u8fc7\u51bb\u7ed3\u4e3b\u6a21\u578b\u53c2\u6570\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u3001\u66f4\u957f\u63a8\u7406\u94fe\u7684\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u65f6\u5f80\u5f80\u53ea\u662f\"\u7edf\u8ba1\u62df\u5408\"\u800c\u975e\u7cfb\u7edf\u903b\u8f91\u63a8\u7406\u3002\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u79bb\u6563token\u7a7a\u95f4\u76f4\u63a5\u5e94\u7528\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a\u6837\u672c\u6548\u7387\u4f4e\u7684rollout\u3001\u9ad8\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u548c\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\u3002", "method": "\u63d0\u51faDeepLatent Reasoning\u6846\u67b6\uff1a1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u9ad8\u6548\u91c7\u6837K\u4e2a\u63a8\u7406\u94fe\u7f16\u7801\uff1b2) \u57fa\u4e8e\u6b63\u786e\u6027\u548c\u683c\u5f0f\u7684\u53cc\u5956\u52b1\u673a\u5236\u7b5b\u9009\u9ad8\u4ef7\u503c\u6f5c\u5728\u8f68\u8ff9\uff1b3) \u4ec5\u5c06\u7b5b\u9009\u540e\u7684\u8f68\u8ff9\u8f93\u5165\u51bb\u7ed3\u4e3b\u6a21\u578b\u8fdb\u884c\u5355\u6b21\u89e3\u7801\uff1b4) \u8bbe\u8ba1\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5b9e\u73b0\u6f5c\u5728\u7a7a\u95f4\u7684\u6709\u5411\u63a2\u7d22\u3002", "result": "\u5728\u53ef\u6bd4\u8f83\u7684GPU\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cDLR\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u6536\u655b\uff0c\u652f\u6301\u66f4\u957f\u7684\u63a8\u7406\u94fe\uff0c\u4fc3\u8fdb\u4e86\u63a8\u7406\u80fd\u529b\u7684\u53ef\u6301\u7eed\u79ef\u7d2f\uff0c\u4e3aLLMs\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u8def\u5f84\u3002", "conclusion": "DLR\u901a\u8fc7\u5c06\u5f3a\u5316\u5b66\u4e60\u4ece\u79bb\u6563token\u7a7a\u95f4\u8f6c\u79fb\u5230\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u74f6\u9888\uff0c\u4e3aLLMs\u7684\u53ef\u9760\u3001\u53ef\u6269\u5c55\u63a8\u7406\u80fd\u529b\u5f3a\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.15801", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.15801", "abs": "https://arxiv.org/abs/2601.15801", "authors": ["Fengheng Chu", "Jiahao Chen", "Yuhong Wang", "Jun Wang", "Zhihui Fu", "Shouling Ji", "Songze Li"], "title": "Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models", "comment": null, "summary": "While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \\textbf{G}lobal \\textbf{O}ptimization for \\textbf{S}afety \\textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.", "AI": {"tldr": "GOSV\u6846\u67b6\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u8bc6\u522bLLM\u4e2d\u7684\u5b89\u5168\u5173\u952e\u6ce8\u610f\u529b\u5934\uff0c\u53d1\u73b0\u6076\u610f\u6ce8\u5165\u5411\u91cf\u548c\u5b89\u5168\u6291\u5236\u5411\u91cf\u4e24\u79cd\u7a7a\u95f4\u5206\u79bb\u7684\u5b89\u5168\u5411\u91cf\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u7684\u767d\u76d2\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c40\u90e8\u3001\u8d2a\u5a6a\u7684\u5c5e\u6027\u5206\u6790\uff0c\u5047\u8bbe\u7ec4\u4ef6\u8d21\u732e\u72ec\u7acb\uff0c\u5ffd\u89c6\u4e86LLM\u4e2d\u4e0d\u540c\u7ec4\u4ef6\uff08\u5982\u6ce8\u610f\u529b\u5934\uff09\u4e4b\u95f4\u7684\u534f\u540c\u4ea4\u4e92\u4f5c\u7528\uff0c\u8fd9\u4e9b\u7ec4\u4ef6\u5171\u540c\u6784\u6210\u4e86\u5b89\u5168\u673a\u5236\u3002", "method": "\u63d0\u51faGOSV\uff08\u5168\u5c40\u4f18\u5316\u5b89\u5168\u5411\u91cf\u63d0\u53d6\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u540c\u65f6\u8bc6\u522b\u6240\u6709\u6ce8\u610f\u529b\u5934\u4e2d\u7684\u5b89\u5168\u5173\u952e\u5934\uff1b\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7684\u6fc0\u6d3b\u91cd\u4fee\u8865\u7b56\u7565\uff1a\u6709\u5bb3\u4fee\u8865\u548c\u96f6\u6d88\u878d\uff0c\u8bc6\u522b\u51fa\u7a7a\u95f4\u5206\u79bb\u7684\u5b89\u5168\u5411\u91cf\u96c6\u3002", "result": "\u53d1\u73b0\u5bf9\u9f50\u7684LLM\u4e3a\u5b89\u5168\u76ee\u7684\u7ef4\u6301\u7740\u5206\u79bb\u7684\u529f\u80fd\u901a\u8def\uff08\u6076\u610f\u6ce8\u5165\u5411\u91cf\u548c\u5b89\u5168\u6291\u5236\u5411\u91cf\uff09\uff1b\u5f53\u7ea630%\u7684\u603b\u5934\u88ab\u91cd\u4fee\u8865\u65f6\uff0c\u6240\u6709\u6a21\u578b\u90fd\u4f1a\u51fa\u73b0\u5b8c\u5168\u7684\u5b89\u5168\u5d29\u6e83\uff1b\u57fa\u4e8e\u6b64\u5f00\u53d1\u7684\u767d\u76d2\u8d8a\u72f1\u653b\u51fb\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GOSV\u6846\u67b6\u4e3aLLM\u5b89\u5168\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u8bc6\u522b\u51fa\u7684\u5b89\u5168\u5411\u91cf\u63ed\u793a\u4e86LLM\u5b89\u5168\u673a\u5236\u7684\u5185\u5728\u7ed3\u6784\uff0c\u5e76\u53ef\u7528\u4e8e\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u767d\u76d2\u653b\u51fb\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u7406\u89e3LLM\u5b89\u5168\u7ec4\u4ef6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.18092", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18092", "abs": "https://arxiv.org/abs/2601.18092", "authors": ["Nan Chen", "Jing Lu", "Zilong Wang", "Luna K. Qiu", "Siming Chen", "Yuqing Yang"], "title": "From Struggle to Success: Context-Aware Guidance for Screen Reader Users in Computer Use", "comment": "Conditionally accepted to ACM CHI 2026", "summary": "Equal access to digital technologies is critical for education, employment, and social participation. However, mainstream interfaces are visually oriented, creating steep learning curves and frequent obstacles for screen reader users, and limiting their independence and opportunities. Existing support is inadequate -- tutorials mainly target sighted users, while human assistance lacks real-time availability. We introduce AskEase, an on-demand AI assistant that provides step-by-step, screen reader user-friendly guidance for computer use. AskEase manages multiple sources of context to infer user intent and deliver precise, situation-specific guidance. Its seamless interaction design minimizes disruption and reduces the effort of seeking help. We demonstrated its effectiveness through representative usage scenarios and robustness tests. In a within-subjects study with 12 screen reader users, AskEase significantly improved task success while reducing perceived workload, including physical demand, effort, and frustration. These results demonstrate the potential of LLM-powered assistants to promote accessible computing and expand opportunities for users with visual impairments.", "AI": {"tldr": "AskEase\u662f\u4e00\u4e2a\u4e3a\u5c4f\u5e55\u9605\u8bfb\u5668\u7528\u6237\u8bbe\u8ba1\u7684\u6309\u9700AI\u52a9\u624b\uff0c\u901a\u8fc7\u591a\u6e90\u4e0a\u4e0b\u6587\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u63d0\u4f9b\u9010\u6b65\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u5e76\u964d\u4f4e\u5de5\u4f5c\u8d1f\u8377", "motivation": "\u4e3b\u6d41\u754c\u9762\u4ee5\u89c6\u89c9\u4e3a\u5bfc\u5411\uff0c\u7ed9\u5c4f\u5e55\u9605\u8bfb\u5668\u7528\u6237\u5e26\u6765\u9661\u5ced\u7684\u5b66\u4e60\u66f2\u7ebf\u548c\u9891\u7e41\u969c\u788d\uff0c\u9650\u5236\u5176\u72ec\u7acb\u6027\u548c\u673a\u4f1a\u3002\u73b0\u6709\u652f\u6301\u4e0d\u8db3\uff1a\u6559\u7a0b\u4e3b\u8981\u9762\u5411\u89c6\u529b\u6b63\u5e38\u7528\u6237\uff0c\u4eba\u5de5\u5e2e\u52a9\u7f3a\u4e4f\u5b9e\u65f6\u53ef\u7528\u6027\u3002", "method": "\u5f00\u53d1AskEase\u6309\u9700AI\u52a9\u624b\uff0c\u7ba1\u7406\u591a\u4e2a\u4e0a\u4e0b\u6587\u6765\u6e90\u4ee5\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u63d0\u4f9b\u9010\u6b65\u3001\u9002\u5408\u5c4f\u5e55\u9605\u8bfb\u5668\u7528\u6237\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u6307\u5bfc\u3002\u91c7\u7528\u65e0\u7f1d\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u6700\u5c0f\u5316\u5e72\u6270\u5e76\u51cf\u5c11\u5bfb\u6c42\u5e2e\u52a9\u7684\u7cbe\u529b\u3002", "result": "\u572812\u540d\u5c4f\u5e55\u9605\u8bfb\u5668\u7528\u6237\u7684\u53d7\u8bd5\u8005\u5185\u7814\u7a76\u4e2d\uff0cAskEase\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u611f\u77e5\u5de5\u4f5c\u8d1f\u8377\uff0c\u5305\u62ec\u4f53\u529b\u9700\u6c42\u3001\u52aa\u529b\u548c\u632b\u8d25\u611f\u3002\u4ee3\u8868\u6027\u4f7f\u7528\u573a\u666f\u548c\u9c81\u68d2\u6027\u6d4b\u8bd5\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u52a9\u624b\u5177\u6709\u4fc3\u8fdb\u65e0\u969c\u788d\u8ba1\u7b97\u548c\u6269\u5927\u89c6\u969c\u7528\u6237\u673a\u4f1a\u7684\u6f5c\u529b\u3002AskEase\u5c55\u793a\u4e86\u901a\u8fc7AI\u8f85\u52a9\u89e3\u51b3\u5c4f\u5e55\u9605\u8bfb\u5668\u7528\u6237\u9762\u4e34\u7684\u754c\u9762\u969c\u788d\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2601.18125", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18125", "abs": "https://arxiv.org/abs/2601.18125", "authors": ["Mohammad Hadi Nezhad", "Francisco Enrique Vicente Castro", "Ivon Arroyo"], "title": "Understanding Users' Privacy Reasoning and Behaviors During Chatbot Use to Support Meaningful Agency in Privacy", "comment": "Preprint of a paper under review", "summary": "Conversational agents (CAs) (e.g., chatbots) are increasingly used in settings where users disclose sensitive information, raising significant privacy concerns. Because privacy judgments are highly contextual, supporting users to engage in privacy-protective actions during chatbot interactions is essential. However, enabling meaningful engagement requires a deeper understanding of how users currently reason about and manage sensitive information during realistic chatbot use scenarios. To investigate this, we qualitatively examined computer science (undergraduate and masters) students' in-the-moment disclosure and protection behaviors, as well as the reasoning underlying these behaviors, across a range of realistic chatbot tasks. Participants used a simulated ChatGPT interface with and without a privacy notice panel that intercepts message submissions, highlights potentially sensitive information, and offers privacy protective actions. The panel supports anonymization through retracting, faking, and generalizing, and surfaces two of ChatGPT's built-in privacy controls to improve their discoverability. Drawing on interaction logs, think-alouds, and survey responses, we analyzed how the panel fostered privacy awareness, encouraged protective actions, and supported context-specific reasoning about what information to protect and how. We further discuss design opportunities for tools that provide users greater and more meaningful agency in protecting sensitive information during CA interactions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\uff0c\u7528\u6237\u5982\u4f55\u7ba1\u7406\u654f\u611f\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u9690\u79c1\u901a\u77e5\u9762\u677f\u589e\u5f3a\u7528\u6237\u7684\u9690\u79c1\u4fdd\u62a4\u610f\u8bc6\u548c\u884c\u4e3a\u3002", "motivation": "\u968f\u7740\u804a\u5929\u673a\u5668\u4eba\u5728\u654f\u611f\u4fe1\u606f\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u9690\u79c1\u5224\u65ad\u5177\u6709\u9ad8\u5ea6\u60c5\u5883\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u652f\u6301\u7528\u6237\u5728\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u91c7\u53d6\u9690\u79c1\u4fdd\u62a4\u884c\u52a8\uff0c\u8fd9\u9700\u8981\u6df1\u5165\u4e86\u89e3\u7528\u6237\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5982\u4f55\u63a8\u7406\u548c\u7ba1\u7406\u654f\u611f\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u8ba9\u8ba1\u7b97\u673a\u79d1\u5b66\u4e13\u4e1a\u5b66\u751f\uff08\u672c\u79d1\u548c\u7855\u58eb\uff09\u5728\u591a\u79cd\u73b0\u5b9e\u804a\u5929\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4f7f\u7528\u6a21\u62df\u7684ChatGPT\u754c\u9762\u3002\u5b9e\u9a8c\u8bbe\u7f6e\u5305\u62ec\u6709/\u65e0\u9690\u79c1\u901a\u77e5\u9762\u677f\u4e24\u79cd\u6761\u4ef6\uff0c\u9762\u677f\u4f1a\u62e6\u622a\u6d88\u606f\u63d0\u4ea4\u3001\u9ad8\u4eae\u6f5c\u5728\u654f\u611f\u4fe1\u606f\u5e76\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u9009\u9879\uff08\u64a4\u56de\u3001\u4f2a\u9020\u3001\u6cdb\u5316\uff09\uff0c\u540c\u65f6\u5c55\u793aChatGPT\u5185\u7f6e\u9690\u79c1\u63a7\u5236\u4ee5\u63d0\u9ad8\u5176\u53ef\u53d1\u73b0\u6027\u3002\u901a\u8fc7\u4ea4\u4e92\u65e5\u5fd7\u3001\u6709\u58f0\u601d\u7ef4\u548c\u8c03\u67e5\u95ee\u5377\u6536\u96c6\u6570\u636e\uff0c\u5206\u6790\u9762\u677f\u5982\u4f55\u4fc3\u8fdb\u9690\u79c1\u610f\u8bc6\u3001\u9f13\u52b1\u4fdd\u62a4\u884c\u4e3a\u5e76\u652f\u6301\u60c5\u5883\u7279\u5b9a\u7684\u4fe1\u606f\u4fdd\u62a4\u63a8\u7406\u3002", "result": "\u9690\u79c1\u901a\u77e5\u9762\u677f\u6709\u6548\u589e\u5f3a\u4e86\u7528\u6237\u7684\u9690\u79c1\u610f\u8bc6\uff0c\u4fc3\u8fdb\u4e86\u4fdd\u62a4\u6027\u884c\u4e3a\uff0c\u5e76\u652f\u6301\u4e86\u57fa\u4e8e\u5177\u4f53\u60c5\u5883\u7684\u9690\u79c1\u51b3\u7b56\u3002\u7528\u6237\u80fd\u591f\u66f4\u597d\u5730\u8bc6\u522b\u654f\u611f\u4fe1\u606f\uff0c\u5e76\u9009\u62e9\u9002\u5f53\u7684\u4fdd\u62a4\u7b56\u7565\uff08\u5982\u533f\u540d\u5316\u3001\u64a4\u56de\u6216\u6cdb\u5316\uff09\u3002\u9762\u677f\u8fd8\u63d0\u9ad8\u4e86ChatGPT\u5185\u7f6e\u9690\u79c1\u63a7\u5236\u7684\u53ef\u53d1\u73b0\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5de5\u5177\u589e\u5f3a\u7528\u6237\u5728\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u9690\u79c1\u610f\u8bc6\u548c\u63a7\u5236\u80fd\u529b\u662f\u53ef\u884c\u7684\u3002\u672a\u6765\u8bbe\u8ba1\u5e94\u5173\u6ce8\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u6709\u610f\u4e49\u548c\u66f4\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u654f\u611f\u4fe1\u606f\u4ea4\u4e92\u4e2d\u62e5\u6709\u66f4\u5927\u7684\u81ea\u4e3b\u6743\u3002"}}
{"id": "2601.18154", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18154", "abs": "https://arxiv.org/abs/2601.18154", "authors": ["Haiyi Li", "Yiyang Zhao", "Yutong Li", "Alison Deslandes", "Jodie Avery", "Mary Louise Hull", "Hsiang-Ting Chen"], "title": "EndoExtract: Co-Designing Structured Text Extraction from Endometriosis Ultrasound Reports", "comment": null, "summary": "Endometriosis ultrasound reports are often unstructured free-text documents that require manual abstraction for downstream tasks such as analytics, machine learning model training, and clinical auditing. We present \\textbf{EndoExtract}, an on-premise LLM-powered system that extracts structured data from these reports and surfaces interpretive fields for human review. Through contextual inquiry with research assistants, we identified key workflow pain points: asymmetric trust between numerical and interpretive fields, repetitive manual highlighting, fatigue from sustained comparison, and terminology inconsistency across radiologists. These findings informed an interface that surfaces only interpretive fields for mandatory review, automatically highlights source evidence within PDFs, and separates batch extraction from human-paced verification. A formative workshop revealed that \\textbf{EndoExtract} supports a shift from field-by-field data entry to supervisory validation, though participants noted risks of over-skimming and challenges in managing missing data.", "AI": {"tldr": "EndoExtract\uff1a\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u672c\u5730\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u7684\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u8d85\u58f0\u62a5\u544a\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u754c\u9762\u4f18\u5316\u652f\u6301\u4eba\u5de5\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u7a0b", "motivation": "\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u8d85\u58f0\u62a5\u544a\u901a\u5e38\u662f\u975e\u7ed3\u6784\u5316\u7684\u81ea\u7531\u6587\u672c\u6587\u6863\uff0c\u9700\u8981\u4eba\u5de5\u63d0\u53d6\u6570\u636e\u7528\u4e8e\u5206\u6790\u3001\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u548c\u4e34\u5e8a\u5ba1\u8ba1\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u8017\u65f6\u4e14\u6613\u51fa\u9519", "method": "\u5f00\u53d1EndoExtract\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u8c03\u67e5\u8bc6\u522b\u5de5\u4f5c\u6d41\u7a0b\u75db\u70b9\uff0c\u8bbe\u8ba1\u754c\u9762\u4ec5\u663e\u793a\u9700\u8981\u4eba\u5de5\u9a8c\u8bc1\u7684\u89e3\u91ca\u6027\u5b57\u6bb5\uff0c\u81ea\u52a8\u9ad8\u4eaePDF\u4e2d\u7684\u6e90\u8bc1\u636e\uff0c\u5e76\u5c06\u6279\u91cf\u63d0\u53d6\u4e0e\u4eba\u5de5\u9a8c\u8bc1\u5206\u79bb", "result": "\u7cfb\u7edf\u652f\u6301\u4ece\u9010\u5b57\u6bb5\u6570\u636e\u5f55\u5165\u8f6c\u5411\u76d1\u7763\u5f0f\u9a8c\u8bc1\uff0c\u4f46\u7528\u6237\u6307\u51fa\u5b58\u5728\u8fc7\u5ea6\u6d4f\u89c8\u7684\u98ce\u9669\u548c\u7ba1\u7406\u7f3a\u5931\u6570\u636e\u7684\u6311\u6218", "conclusion": "EndoExtract\u901a\u8fc7LLM\u6280\u672f\u6709\u6548\u63d0\u53d6\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u8d85\u58f0\u62a5\u544a\u7684\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u754c\u9762\u8bbe\u8ba1\u4f18\u5316\u4eba\u5de5\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u63d0\u53d6\u6548\u7387"}}
{"id": "2601.17360", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17360", "abs": "https://arxiv.org/abs/2601.17360", "authors": ["Jiankai Jin", "Xiangzheng Zhang", "Zhao Liu", "Deyue Zhang", "Quanchen Zou"], "title": "Robust Privacy: Inference-Time Privacy through Certified Robustness", "comment": null, "summary": "Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($\u03c3=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9c81\u68d2\u9690\u79c1\uff08RP\uff09\u6982\u5ff5\uff0c\u901a\u8fc7\u4fdd\u8bc1\u6a21\u578b\u5728\u8f93\u5165\u90bb\u57df\u5185\u7684\u9884\u6d4b\u4e0d\u53d8\u6027\u6765\u63d0\u4f9b\u63a8\u7406\u65f6\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5f00\u53d1\u5c5e\u6027\u9690\u79c1\u589e\u5f3a\uff08APE\uff09\u65b9\u6cd5\u5c06\u8f93\u5165\u7ea7\u4e0d\u53d8\u6027\u8f6c\u5316\u4e3a\u5c5e\u6027\u7ea7\u9690\u79c1\u6548\u679c\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u63a8\u7406\u65f6\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u8f93\u5165\u5c5e\u6027\uff0c\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002\u9700\u8981\u4e00\u79cd\u63a8\u7406\u65f6\u7684\u9690\u79c1\u4fdd\u62a4\u6982\u5ff5\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u9690\u79c1\u4fdd\u8bc1\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u9c81\u68d2\u9690\u79c1\uff08RP\uff09\u6982\u5ff5\uff1a\u5982\u679c\u6a21\u578b\u9884\u6d4b\u5728\u8f93\u5165x\u7684\u534a\u5f84R\u90bb\u57df\u5185\u53ef\u8bc1\u660e\u4e0d\u53d8\uff0c\u5219x\u4eab\u6709R-\u9c81\u68d2\u9690\u79c1\u3002\u5f00\u53d1\u5c5e\u6027\u9690\u79c1\u589e\u5f3a\uff08APE\uff09\u65b9\u6cd5\uff0c\u5c06\u8f93\u5165\u7ea7\u4e0d\u53d8\u6027\u8f6c\u5316\u4e3a\u5c5e\u6027\u7ea7\u9690\u79c1\u6548\u679c\u3002\u901a\u8fc7\u6dfb\u52a0\u566a\u58f0\u5b9e\u73b0\u9884\u6d4b\u4e0d\u53d8\u6027\uff0c\u5e76\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u9690\u79c1\u6548\u679c\u3002", "result": "\u5728\u53d7\u63a7\u63a8\u8350\u4efb\u52a1\u4e2d\uff0cRP\u6269\u5c55\u4e86\u4e0e\u6b63\u5411\u63a8\u8350\u517c\u5bb9\u7684\u654f\u611f\u5c5e\u6027\u503c\u96c6\u5408\uff0c\u76f8\u5e94\u6269\u5927\u4e86\u63a8\u7406\u533a\u95f4\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5c0f\u566a\u58f0\u6c34\u5e73\uff08\u03c3=0.1\uff09\u4e0b\uff0cRP\u80fd\u5c06\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u6210\u529f\u7387\u4ece73%\u964d\u81f34%\uff0c\u540c\u65f6\u90e8\u5206\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002RP\u4e5f\u80fd\u5728\u4e0d\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u90e8\u5206\u7f13\u89e3\u653b\u51fb\uff08\u653b\u51fb\u6210\u529f\u7387\u964d\u81f344%\uff09\u3002", "conclusion": "\u9c81\u68d2\u9690\u79c1\uff08RP\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u53ef\u8bc1\u660e\u7684\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u8bc1\u9884\u6d4b\u5728\u8f93\u5165\u90bb\u57df\u5185\u7684\u4e0d\u53d8\u6027\u6765\u9632\u6b62\u654f\u611f\u5c5e\u6027\u63a8\u65ad\u3002\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u5b9e\u7528\u6027\u4e4b\u95f4\u5b9e\u73b0\u5e73\u8861\u3002"}}
{"id": "2601.18177", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18177", "abs": "https://arxiv.org/abs/2601.18177", "authors": ["Ye Tian", "Haohua Du", "Chao Gu", "Junyang Zhang", "Shanyue Wang", "Hao Zhou", "Jiahui Hou", "Xiang-Yang Li"], "title": "Lip-Siri: Contactless Open-Sentence Silent Speech with Wi-Fi Backscatter", "comment": null, "summary": "Silent speech interfaces (SSIs) enable silent interaction in noise-sensitive or privacy-sensitive settings. However, existing SSIs face practical deployment trade-offs among privacy, user experience, and energy consumption, and most remain limited to closed-set recognition over small, pre-defined vocabularies of words or sentences, which restricts real-world expressiveness. In this paper, we present Lip-Siri, to the best of our knowledge, the first Wi-Fi backscatter--based SSI that supports open-vocabulary sentence recognition via lexicon-guided subword decoding. Lip-Siri designs a frequency-shifted backscatter tag to isolate tag-modulated reflections and suppress interference from non-target motions, enabling reliable extraction of lip-motion traces from ubiquitous Wi-Fi signals. We then segment continuous traces into lip-motion units, cluster them, learn robust unit representations via cluster-based self-supervision, and finally propose a lexicon-guided Transformer encoder--decoder with beam search to decode variable-length sentence sequences. We implement an end-to-end prototype and evaluate it with 15 participants on 340 sentences and 3,398 words across multiple scenarios. Lip-Siri achieves 85.61% accuracy on word prediction and a WER of 36.87% on continuous sentence recognition, approaching the performance of representative vision-based lip-reading systems.", "AI": {"tldr": "Lip-Siri\uff1a\u9996\u4e2a\u57fa\u4e8eWi-Fi\u53cd\u5411\u6563\u5c04\u7684\u65e0\u58f0\u8bed\u97f3\u63a5\u53e3\uff0c\u652f\u6301\u901a\u8fc7\u8bcd\u5178\u5f15\u5bfc\u7684\u5b50\u8bcd\u89e3\u7801\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u53e5\u5b50\u8bc6\u522b", "motivation": "\u73b0\u6709\u65e0\u58f0\u8bed\u97f3\u63a5\u53e3\u5728\u9690\u79c1\u3001\u7528\u6237\u4f53\u9a8c\u548c\u80fd\u8017\u4e4b\u95f4\u5b58\u5728\u90e8\u7f72\u6743\u8861\uff0c\u4e14\u5927\u591a\u5c40\u9650\u4e8e\u5c01\u95ed\u8bcd\u6c47\u96c6\uff0c\u9650\u5236\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u8868\u8fbe\u80fd\u529b", "method": "\u8bbe\u8ba1\u9891\u7387\u504f\u79fb\u53cd\u5411\u6563\u5c04\u6807\u7b7e\u9694\u79bb\u6807\u7b7e\u8c03\u5236\u53cd\u5c04\u5e76\u6291\u5236\u975e\u76ee\u6807\u8fd0\u52a8\u5e72\u6270\uff1b\u5c06\u8fde\u7eed\u5507\u52a8\u8f68\u8ff9\u5206\u5272\u4e3a\u5507\u52a8\u5355\u5143\uff0c\u805a\u7c7b\u5e76\u901a\u8fc7\u57fa\u4e8e\u805a\u7c7b\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u9c81\u68d2\u8868\u793a\uff1b\u63d0\u51fa\u8bcd\u5178\u5f15\u5bfc\u7684Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u914d\u5408\u675f\u641c\u7d22\u89e3\u7801\u53d8\u957f\u53e5\u5b50\u5e8f\u5217", "result": "\u572815\u540d\u53c2\u4e0e\u8005\u7684340\u4e2a\u53e5\u5b50\u548c3,398\u4e2a\u5355\u8bcd\u8bc4\u4f30\u4e2d\uff0c\u5355\u8bcd\u9884\u6d4b\u51c6\u786e\u7387\u8fbe85.61%\uff0c\u8fde\u7eed\u53e5\u5b50\u8bc6\u522b\u7684\u8bcd\u9519\u8bef\u7387\u4e3a36.87%\uff0c\u63a5\u8fd1\u4ee3\u8868\u6027\u89c6\u89c9\u5507\u8bfb\u7cfb\u7edf\u6027\u80fd", "conclusion": "Lip-Siri\u662f\u9996\u4e2a\u57fa\u4e8eWi-Fi\u53cd\u5411\u6563\u5c04\u7684\u5f00\u653e\u8bcd\u6c47\u65e0\u58f0\u8bed\u97f3\u63a5\u53e3\uff0c\u901a\u8fc7\u521b\u65b0\u4fe1\u53f7\u5904\u7406\u548c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u7528\u6237\u4f53\u9a8c\u548c\u80fd\u8017\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u89c6\u89c9\u5507\u8bfb\u7cfb\u7edf\u7684\u6027\u80fd"}}
{"id": "2601.17309", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17309", "abs": "https://arxiv.org/abs/2601.17309", "authors": ["Anagha Sabu", "Vidhya S", "Narayanan C Krishnan"], "title": "PAR: Plausibility-aware Amortized Recourse Generation", "comment": null, "summary": "Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.", "AI": {"tldr": "PAR\u662f\u4e00\u79cd\u57fa\u4e8e\u644a\u9500\u8fd1\u4f3c\u63a8\u7406\u7684\u7b97\u6cd5\u8ffd\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u6700\u5927\u540e\u9a8c\u63a8\u65ad\u751f\u6210\u9ad8\u4f3c\u7136\u3001\u73b0\u5b9e\u53ef\u884c\u7684\u53cd\u4e8b\u5b9e\u5efa\u8bae", "motivation": "\u7b97\u6cd5\u8ffd\u7d22\u65e8\u5728\u4e3a\u4e0d\u5229\u6a21\u578b\u51b3\u7b56\u63d0\u4f9b\u53ef\u64cd\u4f5c\u3001\u73b0\u5b9e\u53ef\u884c\u7684\u5c5e\u6027\u6539\u53d8\u5efa\u8bae\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u4f3c\u7136\u53cd\u4e8b\u5b9e\u65b9\u9762\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u5c06\u8ffd\u7d22\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u6700\u5927\u540e\u9a8c\u63a8\u65ad\u95ee\u9898\uff0c\u5728\u53ef\u63a5\u53d7\u7c7b\u6570\u636e\u5206\u5e03\u4e0b\u5bfb\u627e\u9ad8\u4f3c\u7136\u53cd\u4e8b\u5b9e\u3002\u63d0\u51faPAR\uff08\u644a\u9500\u8fd1\u4f3c\u63a8\u7406\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u53ef\u5904\u7406\u6982\u7387\u6a21\u578b\u76f4\u63a5\u4f30\u8ba1\u4f3c\u7136\uff0c\u901a\u8fc7\u68af\u5ea6\u4f20\u64ad\u9ad8\u6548\u8bad\u7ec3\u8ffd\u7d22\u751f\u6210\u5668\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u90bb\u57df\u7684\u8c03\u8282\u673a\u5236\u5b9e\u73b0\u4e2a\u6027\u5316\u8ffd\u7d22\u751f\u6210\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u7b97\u6cd5\u8ffd\u7d22\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1PAR\uff0c\u8bc1\u660e\u5176\u80fd\u9ad8\u6548\u751f\u6210\u6709\u6548\u3001\u4e0e\u4e8b\u5b9e\u76f8\u4f3c\u3001\u7a00\u758f\u4e14\u9ad8\u5ea6\u5408\u7406\u7684\u8ffd\u7d22\u5efa\u8bae\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PAR\u901a\u8fc7\u7ea6\u675f\u6700\u5927\u540e\u9a8c\u63a8\u65ad\u6846\u67b6\u548c\u644a\u9500\u8fd1\u4f3c\u63a8\u7406\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u6548\u751f\u6210\u9ad8\u4f3c\u7136\u3001\u73b0\u5b9e\u53ef\u884c\u8ffd\u7d22\u5efa\u8bae\u7684\u95ee\u9898\uff0c\u4e3a\u7b97\u6cd5\u8ffd\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18179", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18179", "abs": "https://arxiv.org/abs/2601.18179", "authors": ["Yimeng Wang", "Liabette Escamilla", "Yinzhou Wang", "Bianca R. Augustine", "Yixuan Zhang"], "title": "Exploring Customizable Interactive Tools for Therapeutic Homework Support in Mental Health Counseling", "comment": "22 pages, 4 figures", "summary": "Therapeutic homework (i.e., tasks assigned by therapists for clients to complete between sessions) is essential for effective psychotherapy, yet therapists often interpret fragmented client logs, assessments, and reflections within limited preparation time. Our formative study with licensed therapists revealed three critical design requirements: support for interpreting unstructured client self-reports, customization aligned with clinical objectives, and seamless integration across multiple data sources. We then designed and developed TheraTrack, a customizable, therapist-facing tool that integrates multi-dimensional data and leverages large language models to generate traceable summaries and support natural-language queries, to streamline between-session homework tracking. Our pilot study with 14 therapists showed that TheraTrack reduced their cognitive load, enabled verification through direct navigation from AI summaries to original data entries, and was adapted differently for private analysis compared to in-session use, with dependence varying based on therapist experience and usage duration. We also discuss design implications for clinician-centered AI for mental health.", "AI": {"tldr": "TheraTrack\uff1a\u4e00\u6b3e\u9762\u5411\u6cbb\u7597\u5e08\u7684\u53ef\u5b9a\u5236\u5de5\u5177\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6574\u5408\u591a\u7ef4\u6570\u636e\uff0c\u751f\u6210\u53ef\u8ffd\u6eaf\u7684\u603b\u7ed3\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u4ee5\u7b80\u5316\u6cbb\u7597\u4f5c\u4e1a\u8ddf\u8e2a\uff0c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u3002", "motivation": "\u6cbb\u7597\u6027\u4f5c\u4e1a\u5bf9\u6709\u6548\u5fc3\u7406\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6cbb\u7597\u5e08\u5728\u6709\u9650\u51c6\u5907\u65f6\u95f4\u5185\u9700\u8981\u89e3\u8bfb\u96f6\u6563\u7684\u5ba2\u6237\u65e5\u5fd7\u3001\u8bc4\u4f30\u548c\u53cd\u601d\uff0c\u5b58\u5728\u4fe1\u606f\u6574\u5408\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u8fdb\u884c\u5f62\u6210\u6027\u7814\u7a76\u786e\u5b9a\u8bbe\u8ba1\u9700\u6c42\uff0c\u7136\u540e\u8bbe\u8ba1\u5f00\u53d1TheraTrack\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u6574\u5408\u591a\u7ef4\u6570\u636e\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53ef\u8ffd\u6eaf\u603b\u7ed3\u5e76\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u6700\u540e\u901a\u8fc714\u540d\u6cbb\u7597\u5e08\u7684\u8bd5\u70b9\u7814\u7a76\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bd5\u70b9\u7814\u7a76\u8868\u660e\uff1aTheraTrack\u964d\u4f4e\u4e86\u6cbb\u7597\u5e08\u7684\u8ba4\u77e5\u8d1f\u8377\uff0c\u901a\u8fc7\u4eceAI\u603b\u7ed3\u76f4\u63a5\u5bfc\u822a\u5230\u539f\u59cb\u6570\u636e\u6761\u76ee\u5b9e\u73b0\u9a8c\u8bc1\u529f\u80fd\uff0c\u6839\u636e\u6cbb\u7597\u5e08\u7ecf\u9a8c\u548c\u4f7f\u7528\u65f6\u957f\u7684\u4e0d\u540c\uff0c\u5728\u79c1\u4eba\u5206\u6790\u4e0e\u4f1a\u8bdd\u4f7f\u7528\u4e2d\u7684\u9002\u5e94\u65b9\u5f0f\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "TheraTrack\u5c55\u793a\u4e86AI\u8f85\u52a9\u5de5\u5177\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4e3a\u4e34\u5e8a\u533b\u751f\u4e2d\u5fc3\u5316\u7684\u5fc3\u7406\u5065\u5eb7AI\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u5ba2\u6237\u81ea\u6211\u62a5\u544a\u3001\u652f\u6301\u4e34\u5e8a\u76ee\u6807\u5b9a\u5236\u548c\u8de8\u6570\u636e\u6e90\u6574\u5408\u65b9\u9762\u3002"}}
{"id": "2601.17329", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17329", "abs": "https://arxiv.org/abs/2601.17329", "authors": ["Tiejin Chen", "Xiaoou Liu", "Vishnu Nandam", "Kuan-Ru Liou", "Hua Wei"], "title": "Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment", "comment": "Accetped to Findings of EACL", "summary": "Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \\emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \\emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.", "AI": {"tldr": "\u63d0\u51faConformal Feedback Alignment (CFA)\u6846\u67b6\uff0c\u5229\u7528Conformal Prediction\u7684\u7edf\u8ba1\u4fdd\u8bc1\u6765\u91cf\u5316\u7b54\u6848\u53ef\u9760\u6027\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3aDPO\u548cPPO\u8bad\u7ec3\u7684\u6743\u91cd\uff0c\u4ee5\u89e3\u51b3\u504f\u597d\u5bf9\u9f50\u4e2d\u6807\u7b7e\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u504f\u597d\u7684\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\uff09\u9762\u4e34\u6807\u7b7e\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u5f53\u524d\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u65b9\u6cd5\u53ea\u5173\u6ce8\u504f\u597d\u6743\u91cd\uff0c\u4f46\u5ffd\u7565\u4e86\u88ab\u6bd4\u8f83\u7b54\u6848\u7684\u53ef\u9760\u6027\u8fd9\u4e00\u66f4\u57fa\u672c\u56e0\u7d20\u3002", "method": "\u63d0\u51faConformal Feedback Alignment (CFA)\u6846\u67b6\uff1a1) \u4f7f\u7528Conformal Prediction\u6784\u5efa\u5177\u6709\u53ef\u63a7\u8986\u76d6\u7387\u7684\u9884\u6d4b\u96c6\u6765\u91cf\u5316\u7b54\u6848\u7ea7\u53ef\u9760\u6027\uff1b2) \u5c06\u8fd9\u4e9b\u53ef\u9760\u6027\u805a\u5408\u4e3a\u539f\u5219\u6027\u6743\u91cd\uff1b3) \u5c06\u6743\u91cd\u5e94\u7528\u4e8eDPO\u548cPPO\u98ce\u683c\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCFA\u63d0\u9ad8\u4e86\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002\u8bc1\u660e\u5efa\u6a21\u7b54\u6848\u4fa7\u4e0d\u786e\u5b9a\u6027\u53ef\u4ee5\u8865\u5145\u504f\u597d\u7ea7\u52a0\u6743\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u3001\u6570\u636e\u9ad8\u6548\u7684\u5bf9\u9f50\u3002", "conclusion": "CFA\u901a\u8fc7Conformal Prediction\u7684\u7edf\u8ba1\u4fdd\u8bc1\u6765\u91cf\u5316\u7b54\u6848\u53ef\u9760\u6027\uff0c\u4e3a\u504f\u597d\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2601.18193", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18193", "abs": "https://arxiv.org/abs/2601.18193", "authors": ["Shiwei Wu", "Ziyao Gao", "Zhendong He", "Zongtan He", "Zhupeng Huang", "Xia Chen", "Wei Zeng", "Xiaojuan Ma", "Zhenhui Peng"], "title": "InkIdeator: Supporting Chinese-Style Visual Design Ideation via AI-Infused Exploration of Chinese Paintings", "comment": "21 pages, 15 figures, CHI 2026", "summary": "Visual designers often seek inspiration from Chinese paintings when tasked with creating Chinese-style illustrations, posters, etc. Our formative study (N=10) reveals that during ideation, designers learn the cultural symbols, emotions, compositions, and styles in Chinese paintings but face challenges in searching, analyzing, and integrating these dimensions. This paper leverages multi-modal large models to annotate the value of each dimension in 16,315 Chinese paintings, built on which we propose InkIdeator, an ideation support system for Chinese-style visual designs. InkIdeator suggests cultural symbols associated with the task theme, provides dimensional keywords to help analyze Chinese paintings, and generates visual examples integrating user-selected keywords. Our within-subjects study (N=12) using a baseline system without extracted dimensional keywords, along with two extended use cases by Chinese painters, indicates InkIdeator's effectiveness in creative ideation support, helping users efficiently explore cultural dimensions in Chinese paintings and visualize their ideas. We discuss implications for supporting culture-related visual design ideation with generative AI.", "AI": {"tldr": "InkIdeator\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u6a21\u578b\u6807\u6ce8\u4e2d\u56fd\u753b\u6587\u5316\u7ef4\u5ea6\uff0c\u652f\u6301\u4e2d\u5f0f\u89c6\u89c9\u8bbe\u8ba1\u521b\u610f\u6784\u601d\u7684\u7cfb\u7edf", "motivation": "\u89c6\u89c9\u8bbe\u8ba1\u5e08\u5728\u521b\u4f5c\u4e2d\u5f0f\u98ce\u683c\u4f5c\u54c1\u65f6\uff0c\u5e38\u4ece\u4e2d\u56fd\u753b\u4e2d\u5bfb\u627e\u7075\u611f\uff0c\u4f46\u9762\u4e34\u641c\u7d22\u3001\u5206\u6790\u548c\u6574\u5408\u6587\u5316\u7b26\u53f7\u3001\u60c5\u611f\u3001\u6784\u56fe\u3001\u98ce\u683c\u7b49\u591a\u7ef4\u5ea6\u7684\u6311\u6218\u3002\u7814\u7a76\u53d1\u73b0\u8bbe\u8ba1\u5e08\u9700\u8981\u7cfb\u7edf\u652f\u6301\u6765\u6709\u6548\u63a2\u7d22\u548c\u6574\u5408\u8fd9\u4e9b\u6587\u5316\u7ef4\u5ea6\u3002", "method": "1. \u901a\u8fc7\u591a\u6a21\u6001\u5927\u6a21\u578b\u5bf916,315\u5e45\u4e2d\u56fd\u753b\u8fdb\u884c\u6587\u5316\u7ef4\u5ea6\u6807\u6ce8\uff1b2. \u5f00\u53d1InkIdeator\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4efb\u52a1\u4e3b\u9898\u76f8\u5173\u7684\u6587\u5316\u7b26\u53f7\u5efa\u8bae\u3001\u7ef4\u5ea6\u5173\u952e\u8bcd\u5e2e\u52a9\u5206\u6790\u4e2d\u56fd\u753b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7528\u6237\u9009\u62e9\u5173\u952e\u8bcd\u751f\u6210\u89c6\u89c9\u793a\u4f8b\uff1b3. \u91c7\u7528\u57fa\u7ebf\u7cfb\u7edf\u5bf9\u6bd4\u7684\u7ec4\u5185\u7814\u7a76(N=12)\u548c\u4e24\u540d\u4e2d\u56fd\u753b\u5bb6\u7684\u6269\u5c55\u7528\u4f8b\u9a8c\u8bc1\u7cfb\u7edf\u6548\u679c\u3002", "result": "InkIdeator\u80fd\u6709\u6548\u652f\u6301\u521b\u610f\u6784\u601d\uff0c\u5e2e\u52a9\u7528\u6237\u9ad8\u6548\u63a2\u7d22\u4e2d\u56fd\u753b\u4e2d\u7684\u6587\u5316\u7ef4\u5ea6\u5e76\u53ef\u89c6\u5316\u5176\u60f3\u6cd5\u3002\u7814\u7a76\u8868\u660e\u7cfb\u7edf\u5728\u6587\u5316\u76f8\u5173\u89c6\u89c9\u8bbe\u8ba1\u6784\u601d\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u652f\u6301\u6587\u5316\u76f8\u5173\u89c6\u89c9\u8bbe\u8ba1\u6784\u601d\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u6807\u6ce8\u4e2d\u56fd\u753b\u7684\u591a\u7ef4\u5ea6\u6587\u5316\u7279\u5f81\uff0c\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u521b\u610f\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2601.17330", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17330", "abs": "https://arxiv.org/abs/2601.17330", "authors": ["Laurent Caraffa"], "title": "Thermodynamically Optimal Regularization under Information-Geometric Constraints", "comment": "7 pages, 0 figures", "summary": "Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.\n  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.\n  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u70ed\u529b\u5b66\u6700\u4f18\u6027\u3001\u4fe1\u606f\u51e0\u4f55\u548c\u6b63\u5219\u5316\u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\uff0cFisher-Rao\u5ea6\u91cf\u662f\u4fe1\u5ff5\u7a7a\u95f4\u4e0a\u552f\u4e00\u53ef\u5bb9\u8bb8\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u70ed\u529b\u5b66\u6700\u4f18\u6b63\u5219\u5316\u5bf9\u5e94\u4e8e\u6700\u5c0f\u5316\u5230\u53c2\u8003\u72b6\u6001\u7684Fisher-Rao\u8ddd\u79bb\u5e73\u65b9\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4f9d\u8d56\u4e8e\u4e00\u7cfb\u5217\u7ecf\u9a8c\u6210\u529f\u4f46\u7406\u8bba\u4e0a\u5f02\u8d28\u7684\u6b63\u5219\u5316\u6280\u672f\uff08\u5982\u6743\u91cd\u8870\u51cf\u3001dropout\u3001\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff09\uff0c\u540c\u65f6\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u65e5\u76ca\u589e\u52a0\u7684\u80fd\u91cf\u6210\u672c\u5f15\u53d1\u4e86\u5b66\u4e60\u7b97\u6cd5\u662f\u5426\u63a5\u8fd1\u4efb\u4f55\u57fa\u672c\u6548\u7387\u754c\u9650\u7684\u95ee\u9898\u3002\u9700\u8981\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6b63\u5219\u5316\u63d0\u4f9b\u539f\u5219\u6027\u7684\u51e0\u4f55\u548c\u70ed\u529b\u5b66\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u57fa\u4e8e\u4e09\u4e2a\u660e\u786e\u5047\u8bbe\uff1a(A1) \u6700\u4f18\u6027\u9700\u8981\u5185\u5728\u7684\u3001\u53c2\u6570\u5316\u4e0d\u53d8\u7684\u4fe1\u606f\u5ea6\u91cf\uff1b(A2) \u4fe1\u5ff5\u72b6\u6001\u7531\u5df2\u77e5\u7ea6\u675f\u4e0b\u7684\u6700\u5927\u71b5\u5206\u5e03\u5efa\u6a21\uff1b(A3) \u6700\u4f18\u8fc7\u7a0b\u662f\u51c6\u9759\u6001\u7684\u3002\u5728\u6b64\u6846\u67b6\u4e0b\u8bc1\u660e\u4e86\u6761\u4ef6\u6700\u4f18\u6027\u5b9a\u7406\uff0c\u63a8\u5bfc\u4e86\u9ad8\u65af\u548c\u5706\u5f62\u4fe1\u5ff5\u6a21\u578b\u7684\u8bf1\u5bfc\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u4e86\u5b66\u4e60\u7684\u70ed\u529b\u5b66\u6548\u7387\u6982\u5ff5\u3002", "result": "\u8bc1\u660e\u4e86Fisher-Rao\u5ea6\u91cf\u662f\u4fe1\u5ff5\u7a7a\u95f4\u4e0a\u552f\u4e00\u53ef\u5bb9\u8bb8\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u70ed\u529b\u5b66\u6700\u4f18\u6b63\u5219\u5316\u5bf9\u5e94\u4e8e\u6700\u5c0f\u5316\u5230\u53c2\u8003\u72b6\u6001\u7684Fisher-Rao\u8ddd\u79bb\u5e73\u65b9\u3002\u63a8\u5bfc\u51fa\u9ad8\u65af\u4fe1\u5ff5\u6a21\u578b\u5bf9\u5e94\u53cc\u66f2\u6d41\u5f62\uff0c\u5706\u5f62\u4fe1\u5ff5\u6a21\u578b\u5bf9\u5e94von Mises\u6d41\u5f62\u3002\u8868\u660e\u7ecf\u5178\u6b63\u5219\u5316\u65b9\u6848\u5728\u7ed3\u6784\u4e0a\u65e0\u6cd5\u4fdd\u8bc1\u70ed\u529b\u5b66\u6700\u4f18\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u9884\u6d4b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u51e0\u4f55\u548c\u70ed\u529b\u5b66\u57fa\u7840\uff0c\u5c06\u70ed\u529b\u5b66\u6700\u4f18\u6027\u3001\u4fe1\u606f\u51e0\u4f55\u548c\u6b63\u5219\u5316\u7edf\u4e00\u5728\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u4e0b\uff0c\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.18218", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18218", "abs": "https://arxiv.org/abs/2601.18218", "authors": ["Meziah Ruby Cristobal", "Hyeonjeong Byeon", "Tze-Yu Chen", "Ruoxi Shang", "Donghoon Shin", "Ruican Zhong", "Tony Zhou", "Gary Hsieh"], "title": "PaperTok: Exploring the Use of Generative AI for Creating Short-form Videos for Research Communication", "comment": null, "summary": "The dissemination of scholarly research is critical, yet researchers often lack the time and skills to create engaging content for popular media such as short-form videos. To address this gap, we explore the use of generative AI to help researchers transform their academic papers into accessible video content. Informed by a formative study with science communicators and content creators (N=8), we designed PaperTok, an end-to-end system that automates the initial creative labor by generating script options and corresponding audiovisual content from a source paper. Researchers can then refine based on their preferences with further prompting. A mixed-methods user study (N=18) and crowdsourced evaluation (N=100) demonstrate that PaperTok's workflow can help researchers create engaging and informative short-form videos. We also identified the need for more fine-grained controls in the creation process. To this end, we offer implications for future generative tools that support science outreach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPaperTok\u7cfb\u7edf\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u5c06\u5b66\u672f\u8bba\u6587\u81ea\u52a8\u8f6c\u5316\u4e3a\u77ed\u89c6\u9891\u5185\u5bb9\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u8fdb\u884c\u79d1\u5b66\u4f20\u64ad", "motivation": "\u7814\u7a76\u8005\u7f3a\u4e4f\u65f6\u95f4\u548c\u6280\u80fd\u4e3a\u5927\u4f17\u5a92\u4f53\uff08\u5982\u77ed\u89c6\u9891\uff09\u5236\u4f5c\u5438\u5f15\u4eba\u7684\u5185\u5bb9\uff0c\u963b\u788d\u4e86\u5b66\u672f\u7814\u7a76\u7684\u4f20\u64ad", "method": "\u57fa\u4e8e\u5bf9\u79d1\u5b66\u4f20\u64ad\u8005\u548c\u5185\u5bb9\u521b\u4f5c\u8005\uff08N=8\uff09\u7684\u521d\u6b65\u7814\u7a76\uff0c\u8bbe\u8ba1\u4e86\u7aef\u5230\u7aef\u7cfb\u7edfPaperTok\uff0c\u4ece\u6e90\u8bba\u6587\u81ea\u52a8\u751f\u6210\u811a\u672c\u9009\u9879\u548c\u89c6\u542c\u5185\u5bb9\uff0c\u7814\u7a76\u8005\u53ef\u901a\u8fc7\u8fdb\u4e00\u6b65\u63d0\u793a\u8fdb\u884c\u7ec6\u5316", "result": "\u6df7\u5408\u65b9\u6cd5\u7528\u6237\u7814\u7a76\uff08N=18\uff09\u548c\u4f17\u5305\u8bc4\u4f30\uff08N=100\uff09\u8868\u660e\uff0cPaperTok\u5de5\u4f5c\u6d41\u7a0b\u80fd\u5e2e\u52a9\u7814\u7a76\u8005\u521b\u5efa\u5438\u5f15\u4eba\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u77ed\u89c6\u9891\uff0c\u540c\u65f6\u53d1\u73b0\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u521b\u4f5c\u63a7\u5236", "conclusion": "PaperTok\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u652f\u6301\u79d1\u5b66\u4f20\u64ad\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u652f\u6301\u79d1\u5b66\u63a8\u5e7f\u7684\u751f\u6210\u5de5\u5177\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u542f\u793a"}}
{"id": "2601.17334", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17334", "abs": "https://arxiv.org/abs/2601.17334", "authors": ["Yufeng Huang"], "title": "Power-based Partial Attention: Bridging Linear-Complexity and Full Attention", "comment": "12 pages, 3 figures", "summary": "It is widely accepted from transformer research that \"attention is all we need\", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \\leq p \\leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e42\u7684\u90e8\u5206\u6ce8\u610f\u529b\u673a\u5236\uff08PPA\uff09\uff0c\u5176\u590d\u6742\u5ea6\u4e3aO(L^{1+p})\uff0c\u5176\u4e2d0\u2264p\u22641\uff0c\u901a\u8fc7\u8c03\u8282p\u503c\u53ef\u4ee5\u63a2\u7d22\u4ece\u7ebf\u6027\u590d\u6742\u5ea6\u5230\u4e8c\u6b21\u590d\u6742\u5ea6\u6ce8\u610f\u529b\u4e4b\u95f4\u7684\u6027\u80fd\u53d8\u5316\u3002", "motivation": "\u867d\u7136Transformer\u7814\u7a76\u4e2d\u666e\u904d\u8ba4\u4e3a\"\u6ce8\u610f\u529b\u5c31\u662f\u4e00\u5207\"\uff0c\u4f46\u4ece\u672a\u7cfb\u7edf\u91cf\u5316\u8fc7\u9700\u8981\u591a\u5c11\u6ce8\u610f\u529b\u3002\u9700\u8981\u63a2\u7a76\u4e8c\u6b21\u590d\u6742\u5ea6O(L^2)\u7684\u6ce8\u610f\u529b\u662f\u5426\u5fc5\u8981\uff0c\u662f\u5426\u5b58\u5728\u80fd\u8fbe\u5230\u76f8\u5f53\u6027\u80fd\u7684\u6b21\u4e8c\u6b21\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5e42\u7684\u90e8\u5206\u6ce8\u610f\u529b\u673a\u5236\uff08PPA\uff09\uff0c\u5176\u590d\u6742\u5ea6\u4e3aO(L^{1+p})\uff0c\u5176\u4e2dp\u662f\u63a7\u5236\u6ce8\u610f\u529b\u7f29\u653e\u884c\u4e3a\u7684\u53c2\u6570\u3002\u5f53p=0\u65f6\u5bf9\u5e94\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff0cp=1\u65f6\u5bf9\u5e94\u5b8c\u5168\u6ce8\u610f\u529b\u3002\u901a\u8fc7\u8c03\u8282p\u503c\u6765\u7814\u7a76Transformer\u67b6\u6784\u6027\u80fd\u968f\u6ce8\u610f\u529b\u7f29\u653e\u884c\u4e3a\u7684\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u5448\u73b0S\u66f2\u7ebf\u884c\u4e3a\uff1a\u5728p\u503c\u7684\u72ed\u7a84\u7a97\u53e3\u5185\uff0c\u6027\u80fd\u4ece\u6ed1\u52a8\u7a97\u53e3\uff08\u7ebf\u6027\u590d\u6742\u5ea6\uff09\u6ce8\u610f\u529b\u8fc7\u6e21\u5230\u5b8c\u5168\u6ce8\u610f\u529b\uff0c\u5f53p\u63a5\u8fd11\u65f6\u6027\u80fd\u8d8b\u4e8e\u7a33\u5b9a\u3002\u5b58\u57280<p<1\u4f7f\u5f97O(L^{1+p})\u6ce8\u610f\u529b\u8db3\u4ee5\u8fbe\u5230\u4e0eO(L^2)\u5b8c\u5168\u6ce8\u610f\u529b\u76f8\u4f3c\u7684\u7ed3\u679c\u3002", "conclusion": "\u4e8c\u6b21\u590d\u6742\u5ea6\u6ce8\u610f\u529b\u5e76\u975e\u5fc5\u8981\uff0c\u5b58\u5728\u6b21\u4e8c\u6b21\u6ce8\u610f\u529b\u673a\u5236\uff08O(L^{1+p})\uff0c\u5176\u4e2d0<p<1\uff09\u80fd\u591f\u8fbe\u5230\u4e0e\u5b8c\u5168\u6ce8\u610f\u529b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684Transformer\u67b6\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2601.18239", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18239", "abs": "https://arxiv.org/abs/2601.18239", "authors": ["Sizhe Cheng", "Feng Liang", "Yuhan Wen", "Xipei Yu", "Yong Wang"], "title": "Probing the Future of Meta-Analysis: Eliciting Design Principles via an Agentic Research IDE", "comment": null, "summary": "Meta-analyses and systematic reviews demand rigorous abductive reasoning to build, test, and refine hypotheses across vast, heterogeneous literature. While NLP advancements have automated parts of this pipeline, existing tools often detach researchers from the cognitive loop or function merely as retrieval engines, leading to loss of intellectual ownership and frequent context switching. We present Research IDE, a prototype reimagining authoring environments through the \"Research as Code\" metaphor. Research IDE embeds a multi-agent backend into the writing flow, enabling in-situ verification via \"hypothesis breakpoints.\" A one-week field deployment with 8 domain experts, followed by a reflective workshop, as a Research through Design (RtD) probe, reveals that users strongly preferred this verification workflow, actively leveraged prior knowledge for confirmation, and reported that breakpoints sparked insights. Drawing from participant feedback and suggestions, we derive design implications for future AI-assisted research tools that fully preserve researcher autonomy and intellectual ownership while harnessing computational scale.", "AI": {"tldr": "\u7814\u7a76IDE\u539f\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\"\u7814\u7a76\u5373\u4ee3\u7801\"\u9690\u55bb\u548c\u591a\u667a\u80fd\u4f53\u540e\u7aef\uff0c\u5728\u5199\u4f5c\u6d41\u7a0b\u4e2d\u5d4c\u5165\u5047\u8bbe\u65ad\u70b9\u8fdb\u884c\u539f\u4f4d\u9a8c\u8bc1\uff0c\u63d0\u5347\u7814\u7a76\u81ea\u4e3b\u6027\u548c\u8ba4\u77e5\u53c2\u4e0e", "motivation": "\u73b0\u6709NLP\u5de5\u5177\u5728\u5143\u5206\u6790\u548c\u7cfb\u7edf\u7efc\u8ff0\u4e2d\u5f80\u5f80\u5c06\u7814\u7a76\u8005\u4e0e\u8ba4\u77e5\u5faa\u73af\u5206\u79bb\uff0c\u6216\u4ec5\u4f5c\u4e3a\u68c0\u7d22\u5f15\u64ce\uff0c\u5bfc\u81f4\u667a\u529b\u6240\u6709\u6743\u4e27\u5931\u548c\u9891\u7e41\u4e0a\u4e0b\u6587\u5207\u6362\uff0c\u9700\u8981\u65b0\u7684\u5de5\u5177\u8bbe\u8ba1\u6765\u4fdd\u6301\u7814\u7a76\u8005\u81ea\u4e3b\u6027", "method": "\u63d0\u51faResearch IDE\u539f\u578b\uff0c\u91c7\u7528\"\u7814\u7a76\u5373\u4ee3\u7801\"\u9690\u55bb\uff0c\u5728\u5199\u4f5c\u6d41\u7a0b\u4e2d\u5d4c\u5165\u591a\u667a\u80fd\u4f53\u540e\u7aef\uff0c\u652f\u6301\u901a\u8fc7\"\u5047\u8bbe\u65ad\u70b9\"\u8fdb\u884c\u539f\u4f4d\u9a8c\u8bc1\uff1b\u901a\u8fc7\u4e3a\u671f\u4e00\u5468\u76848\u4f4d\u9886\u57df\u4e13\u5bb6\u5b9e\u5730\u90e8\u7f72\u548c\u53cd\u601d\u7814\u8ba8\u4f1a\uff0c\u4f5c\u4e3a\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1(RtD)\u63a2\u9488\u8fdb\u884c\u8bc4\u4f30", "result": "\u7528\u6237\u5f3a\u70c8\u504f\u597d\u8fd9\u79cd\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u79ef\u6781\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u786e\u8ba4\uff0c\u5e76\u62a5\u544a\u65ad\u70b9\u6fc0\u53d1\u4e86\u6d1e\u5bdf\uff1b\u4ece\u53c2\u4e0e\u8005\u53cd\u9988\u4e2d\u5f97\u51fa\u672a\u6765AI\u8f85\u52a9\u7814\u7a76\u5de5\u5177\u7684\u8bbe\u8ba1\u542f\u793a", "conclusion": "Research IDE\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u5d4c\u5165\u667a\u80fd\u4f53\u5230\u5199\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u5728\u5229\u7528\u8ba1\u7b97\u89c4\u6a21\u7684\u540c\u65f6\u5b8c\u5168\u4fdd\u7559\u7814\u7a76\u8005\u81ea\u4e3b\u6027\u548c\u667a\u529b\u6240\u6709\u6743\uff0c\u4e3a\u672a\u6765AI\u8f85\u52a9\u7814\u7a76\u5de5\u5177\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u65b9\u5411"}}
{"id": "2601.17376", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17376", "abs": "https://arxiv.org/abs/2601.17376", "authors": ["Ruijin Hua", "Zichuan Liu", "Kun Zhang", "Yiyuan Yang"], "title": "Diversified Scaling Inference in Time Series Foundation Models", "comment": "23 pages, 16 figures, 9 tables", "summary": "The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u6f5c\u529b\uff0c\u53d1\u73b0\u6807\u51c6\u91c7\u6837\u65b9\u6cd5\u56e0\u63a2\u7d22\u4e0d\u8db3\u800c\u65e0\u6cd5\u9075\u5faa\u7f29\u653e\u5b9a\u5f8b\uff0c\u63d0\u51fa\u901a\u8fc7\u591a\u6837\u5316\u63a8\u7406\u7f29\u653e\uff08\u4f7f\u7528\u5b9a\u5236\u7684\u65f6\u95f4\u5e8f\u5217\u6270\u52a8\uff09\u6765\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u7406\u8bba\u5206\u6790\u4e86\u591a\u6837\u6027-\u4fdd\u771f\u5ea6\u6743\u8861\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u4f46\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1aTSFMs\u5728\u6807\u51c6\u91c7\u6837\u63a8\u7406\u7f29\u653e\u4e0b\u7684\u884c\u4e3a\u8868\u73b0\uff0c\u4ee5\u53ca\u53d7\u63a7\u91c7\u6837\u591a\u6837\u6027\u662f\u5426\u80fd\u63d0\u5347\u6027\u80fd\u3002", "method": "\u9996\u5148\u5206\u6790TSFMs\u5728\u6807\u51c6\u91c7\u6837\u4e0b\u7684\u7279\u6027\uff0c\u53d1\u73b0\u5176\u56e0\u89e3\u7a7a\u95f4\u63a2\u7d22\u4e0d\u8db3\u800c\u65e0\u6cd5\u9075\u5faa\u7f29\u653e\u5b9a\u5f8b\u3002\u7136\u540e\u901a\u8fc7\u5b9a\u5236\u7684\u65f6\u95f4\u5e8f\u5217\u6270\u52a8\u5b9e\u73b0\u591a\u6837\u5316\u63a8\u7406\u7f29\u653e\uff0c\u6269\u5c55\u751f\u6210\u5206\u5e03\u7684\u652f\u6491\u96c6\u3002\u7406\u8bba\u5206\u6790\u4e86\u591a\u6837\u6027-\u4fdd\u771f\u5ea6\u6743\u8861\uff0c\u63a8\u5bfc\u51fa\u591a\u6837\u5316\u91c7\u6837\u4f18\u4e8e\u6807\u51c6\u91c7\u6837\u7684\u4e34\u754c\u6837\u672c\u9608\u503c\u3002", "result": "\u5728\u591a\u79cdTSFMs\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u5f53\u7684\u591a\u6837\u5316\u63a8\u7406\u7f29\u653e\u80fd\u5728\u4e0d\u66f4\u65b0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u63d0\u51fa\u4e86RobustMSE\u6307\u6807\u6765\u91cf\u5316\u56fa\u5b9a\u9884\u7b97\u4e0bTSFM\u7684\u6f5c\u5728\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "\u63a8\u7406\u8bbe\u8ba1\u662fTSFM\u4f18\u5316\u7684\u4e00\u4e2a\u5173\u952e\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u7ef4\u5ea6\u3002\u7814\u7a76\u7ed3\u679c\u9610\u660e\u4e86\u5404\u56e0\u7d20\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4f7f\u5f97\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3TSFMs\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5e76\u884c\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u5927\u89c4\u6a21\u63a8\u7406\u65f6\u95f4\u5e8f\u5217\u5b9e\u73b0\u53ef\u9760\u6027\u80fd\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2601.18424", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18424", "abs": "https://arxiv.org/abs/2601.18424", "authors": ["Tianyi Gong", "Can Han", "Junxi Wu", "Dahong Qian"], "title": "Fusion of Spatio-Temporal and Multi-Scale Frequency Features for Dry Electrodes MI-EEG Decoding", "comment": null, "summary": "Dry-electrode Motor Imagery Electroencephalography (MI-EEG) enables fast, comfortable, real-world Brain Computer Interface by eliminating gels and shortening setup for at-home and wearable use.However, dry recordings pose three main issues: lower Signal-to-Noise Ratio with more baseline drift and sudden transients; weaker and noisier data with poor phase alignment across trials; and bigger variances between sessions. These drawbacks lead to larger data distribution shift, making features less stable for MI-EEG tasks.To address these problems, we introduce STGMFM, a tri-branch framework tailored for dry-electrode MI-EEG, which models complementary spatio-temporal dependencies via dual graph orders, and captures robust envelope dynamics with a multi-scale frequency mixing branch, motivated by the observation that amplitude envelopes are less sensitive to contact variability than instantaneous waveforms. Physiologically meaningful connectivity priors guide learning, and decision-level fusion consolidates a noise-tolerant consensus. On our collected dry-electrode MI-EEG, STGMFM consistently surpasses competitive CNN/Transformer/graph baselines. Codes are available at https://github.com/Tianyi-325/STGMFM.", "code_url": "https://github.com/Tianyi-325/STGMFM", "code_stars": 1, "code_last_update": "2025-09-18", "AI": {"tldr": "STGMFM\uff1a\u4e00\u79cd\u9488\u5bf9\u5e72\u7535\u6781\u8fd0\u52a8\u60f3\u8c61\u8111\u7535\u7684\u4e09\u5206\u652f\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u56fe\u9636\u5efa\u6a21\u65f6\u7a7a\u4f9d\u8d56\u3001\u591a\u5c3a\u5ea6\u9891\u7387\u6df7\u5408\u5206\u652f\u6355\u83b7\u5305\u7edc\u52a8\u6001\uff0c\u89e3\u51b3\u5e72\u7535\u6781\u8bb0\u5f55\u7684\u4fe1\u566a\u6bd4\u4f4e\u3001\u76f8\u4f4d\u5bf9\u9f50\u5dee\u3001\u4f1a\u8bdd\u95f4\u65b9\u5dee\u5927\u7b49\u95ee\u9898", "motivation": "\u5e72\u7535\u6781\u8fd0\u52a8\u60f3\u8c61\u8111\u7535\u867d\u7136\u4fbf\u4e8e\u5feb\u901f\u3001\u8212\u9002\u7684\u73b0\u5b9e\u4e16\u754c\u8111\u673a\u63a5\u53e3\u5e94\u7528\uff0c\u4f46\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4fe1\u566a\u6bd4\u66f4\u4f4e\uff0c\u57fa\u7ebf\u6f02\u79fb\u548c\u77ac\u6001\u5e72\u6270\u66f4\u591a\uff1b2\uff09\u6570\u636e\u66f4\u5f31\u66f4\u5608\u6742\uff0c\u8de8\u8bd5\u6b21\u7684\u76f8\u4f4d\u5bf9\u9f50\u5dee\uff1b3\uff09\u4f1a\u8bdd\u95f4\u65b9\u5dee\u66f4\u5927\u3002\u8fd9\u4e9b\u7f3a\u70b9\u5bfc\u81f4\u6570\u636e\u5206\u5e03\u504f\u79fb\u66f4\u5927\uff0c\u7279\u5f81\u7a33\u5b9a\u6027\u964d\u4f4e", "method": "\u63d0\u51faSTGMFM\u4e09\u5206\u652f\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u53cc\u56fe\u9636\u5efa\u6a21\u4e92\u8865\u7684\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff1b2\uff09\u4f7f\u7528\u591a\u5c3a\u5ea6\u9891\u7387\u6df7\u5408\u5206\u652f\u6355\u83b7\u9c81\u68d2\u7684\u5305\u7edc\u52a8\u6001\uff08\u57fa\u4e8e\u5305\u7edc\u5bf9\u63a5\u89e6\u53d8\u5316\u6bd4\u77ac\u65f6\u6ce2\u5f62\u66f4\u4e0d\u654f\u611f\u7684\u89c2\u5bdf\uff09\uff1b3\uff09\u5229\u7528\u751f\u7406\u5b66\u4e0a\u6709\u610f\u4e49\u7684\u8fde\u63a5\u5148\u9a8c\u6307\u5bfc\u5b66\u4e60\uff1b4\uff09\u51b3\u7b56\u7ea7\u878d\u5408\u5f62\u6210\u566a\u58f0\u5bb9\u5fcd\u5171\u8bc6", "result": "\u5728\u6536\u96c6\u7684\u5e72\u7535\u6781\u8fd0\u52a8\u60f3\u8c61\u8111\u7535\u6570\u636e\u4e0a\uff0cSTGMFM\u6301\u7eed\u8d85\u8d8a\u7ade\u4e89\u6027\u7684CNN/Transformer/\u56fe\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "STGMFM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5e72\u7535\u6781\u8fd0\u52a8\u60f3\u8c61\u8111\u7535\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5efa\u6a21\u65f6\u7a7a\u4f9d\u8d56\u548c\u5305\u7edc\u52a8\u6001\uff0c\u7ed3\u5408\u751f\u7406\u5148\u9a8c\u548c\u878d\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u8111\u673a\u63a5\u53e3\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17396", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17396", "abs": "https://arxiv.org/abs/2601.17396", "authors": ["Vashista Nobaub"], "title": "GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems", "comment": "21 pages, 5 figures. Includes theoretical analysis, ablation studies, and experiments on synthetic and real vibration datasets. Code available", "summary": "Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.", "AI": {"tldr": "GO-OSC\uff1a\u7528\u4e8e\u632f\u8361\u65f6\u95f4\u5e8f\u5217\u7684\u51e0\u4f55\u611f\u77e5\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u8303\u5316\u7684\u6f5c\u5728\u53c2\u6570\u5316\u5b9e\u73b0\u65e9\u671f\u9000\u5316\u68c0\u6d4b\uff0c\u6bd4\u4f20\u7edf\u80fd\u91cf\u57fa\u65b9\u6cd5\u66f4\u654f\u611f", "motivation": "\u632f\u8361\u7cfb\u7edf\u65e9\u671f\u9000\u5316\u8868\u73b0\u4e3a\u51e0\u4f55\u5931\u771f\uff08\u5982\u76f8\u4f4d\u6296\u52a8\u3001\u9891\u7387\u6f02\u79fb\uff09\uff0c\u5728\u4fe1\u53f7\u80fd\u91cf\u53d8\u5316\u53ef\u68c0\u6d4b\u4e4b\u524d\u5c31\u5df2\u53d1\u751f\u3002\u4f20\u7edf\u7684\u80fd\u91cf\u57fa\u8bca\u65ad\u548c\u65e0\u7ea6\u675f\u5b66\u4e60\u8868\u793a\u5bf9\u6b64\u7ed3\u6784\u4e0d\u654f\u611f\uff0c\u5bfc\u81f4\u68c0\u6d4b\u5ef6\u8fdf\u6216\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faGO-OSC\u6846\u67b6\uff1a1\uff09\u5f3a\u5236\u89c4\u8303\u5316\u548c\u53ef\u8bc6\u522b\u7684\u6f5c\u5728\u53c2\u6570\u5316\uff0c\u5b9e\u73b0\u8de8\u77ed\u65f6\u3001\u672a\u6807\u8bb0\u7a97\u53e3\u7684\u7a33\u5b9a\u6bd4\u8f83\u548c\u805a\u5408\uff1b2\uff09\u57fa\u4e8e\u6b64\u8868\u793a\u5b9a\u4e49\u4e0d\u53d8\u7ebf\u6027\u51e0\u4f55\u63a2\u9488\uff0c\u9488\u5bf9\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9000\u5316\u76f8\u5173\u65b9\u5411\uff1b3\uff09\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u8bc1\u660e\u51e0\u4f55\u63a2\u9488\u5728\u65e9\u671f\u76f8\u4f4d\u9000\u5316\u4e0b\u7684\u68c0\u6d4b\u4f18\u52bf\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u5728\u65e9\u671f\u4ec5\u76f8\u4f4d\u9000\u5316\u60c5\u51b5\u4e0b\uff0c\u80fd\u91cf\u57fa\u7edf\u8ba1\u91cf\u7684\u4e00\u9636\u68c0\u6d4b\u80fd\u529b\u4e3a\u96f6\uff0c\u800c\u51e0\u4f55\u63a2\u9488\u5177\u6709\u4e25\u683c\u6b63\u654f\u611f\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u632f\u52a8\u6570\u636e\u96c6\u4e0a\uff0cGO-OSC\u5b9e\u73b0\u66f4\u65e9\u68c0\u6d4b\u3001\u6539\u8fdb\u7684\u6570\u636e\u6548\u7387\u548c\u8fd0\u884c\u6761\u4ef6\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "GO-OSC\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u8868\u793a\u5b66\u4e60\u548c\u89c4\u8303\u5316\u6f5c\u5728\u53c2\u6570\u5316\uff0c\u89e3\u51b3\u4e86\u632f\u8361\u7cfb\u7edf\u65e9\u671f\u9000\u5316\u68c0\u6d4b\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u6709\u4e25\u683c\u4fdd\u8bc1\uff0c\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u80fd\u91cf\u57fa\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u5de5\u4e1a\u76d1\u6d4b\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u654f\u611f\u7684\u68c0\u6d4b\u5de5\u5177\u3002"}}
{"id": "2601.18428", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18428", "abs": "https://arxiv.org/abs/2601.18428", "authors": ["Jiayi Zhou", "Liwenhan Xie", "Jiaju Ma", "Zheng Wei", "Huamin Qu", "Anyi Rao"], "title": "Collaposer: Transforming Photo Collections into Visual Assets for Storytelling with Collages", "comment": "To be published at ACM CHI 2026 Conference on Human Factors in Computing Systems", "summary": "Digital collage is an artistic practice that combines image cutouts to tell stories. However, preparing cutouts from a set of photos remains a tedious and time-consuming task. A formative study identified three main challenges: 1) inefficient search for relevant photos, 2) manual image cutout, and 3) difficulty in organizing large sets of cutouts. To meet these challenges and facilitate asset preparation for collage, we propose Collaposer, a tool that transforms a collection of photos into organized, ready-to-use visual cutouts based on user-provided story descriptions. Collaposer tags, detects, and segments photos, and then uses an LLM to select central and related labels based on the user-provided story description. Collaposer presents the resulting visuals in varying sizes, clustered according to semantic hierarchy. Our evaluation shows that Collaposer effectively automates the preparation process to produce diverse sets of visual cutouts adhering to the storyline, allowing users to focus on collaging these assets for storytelling.\n  Project website: https://jiayzhou.github.io/collaposer-website/", "code_url": "https://jiayzhou.github.io/collaposer-website", "AI": {"tldr": "Collaposer\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6570\u5b57\u62fc\u8d34\u7d20\u6750\u51c6\u5907\u5de5\u5177\uff0c\u901a\u8fc7AI\u6280\u672f\u5c06\u7167\u7247\u96c6\u8f6c\u6362\u4e3a\u57fa\u4e8e\u6545\u4e8b\u63cf\u8ff0\u7684\u7ec4\u7ec7\u5316\u89c6\u89c9\u526a\u88c1\u7d20\u6750", "motivation": "\u6570\u5b57\u62fc\u8d34\u521b\u4f5c\u4e2d\uff0c\u4ece\u7167\u7247\u96c6\u4e2d\u51c6\u5907\u526a\u88c1\u7d20\u6750\u662f\u4e00\u4e2a\u8017\u65f6\u8d39\u529b\u7684\u8fc7\u7a0b\uff0c\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u641c\u7d22\u76f8\u5173\u7167\u7247\u6548\u7387\u4f4e\u4e0b\uff1b2) \u624b\u52a8\u56fe\u50cf\u526a\u88c1\uff1b3) \u5927\u91cf\u526a\u88c1\u7d20\u6750\u96be\u4ee5\u7ec4\u7ec7\u7ba1\u7406", "method": "Collaposer\u901a\u8fc7\u591a\u9636\u6bb5AI\u5904\u7406\u6d41\u7a0b\uff1a\u9996\u5148\u5bf9\u7167\u7247\u8fdb\u884c\u6807\u6ce8\u3001\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u7136\u540e\u4f7f\u7528LLM\u6839\u636e\u7528\u6237\u63d0\u4f9b\u7684\u6545\u4e8b\u63cf\u8ff0\u9009\u62e9\u6838\u5fc3\u548c\u76f8\u5173\u6807\u7b7e\uff0c\u6700\u540e\u5c06\u751f\u6210\u7684\u89c6\u89c9\u7d20\u6750\u6309\u8bed\u4e49\u5c42\u6b21\u805a\u7c7b\u5e76\u4ee5\u4e0d\u540c\u5c3a\u5bf8\u5448\u73b0", "result": "\u8bc4\u4f30\u663e\u793aCollaposer\u80fd\u6709\u6548\u81ea\u52a8\u5316\u7d20\u6750\u51c6\u5907\u8fc7\u7a0b\uff0c\u751f\u6210\u7b26\u5408\u6545\u4e8b\u60c5\u8282\u7684\u591a\u6837\u5316\u89c6\u89c9\u526a\u88c1\u7d20\u6750\u96c6\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u4e13\u6ce8\u4e8e\u4f7f\u7528\u8fd9\u4e9b\u7d20\u6750\u8fdb\u884c\u62fc\u8d34\u521b\u4f5c", "conclusion": "Collaposer\u6210\u529f\u89e3\u51b3\u4e86\u6570\u5b57\u62fc\u8d34\u521b\u4f5c\u4e2d\u7684\u7d20\u6750\u51c6\u5907\u96be\u9898\uff0c\u901a\u8fc7AI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u5c06\u7167\u7247\u96c6\u8f6c\u6362\u4e3a\u7ec4\u7ec7\u826f\u597d\u7684\u89c6\u89c9\u526a\u88c1\u7d20\u6750\uff0c\u663e\u8457\u63d0\u5347\u4e86\u521b\u4f5c\u6548\u7387"}}
{"id": "2601.17407", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17407", "abs": "https://arxiv.org/abs/2601.17407", "authors": ["Prajwal Chauhan", "Salah Eddine Choutri", "Saif Eddin Jabari"], "title": "Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations", "comment": "Accepted to Transactions on Machine Learning Research (TMLR)", "summary": "Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.", "AI": {"tldr": "D-SENO\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u5f20\u5377\u79ef\u548c\u6324\u538b-\u6fc0\u52b1\u6a21\u5757\uff0c\u7528\u4e8e\u9ad8\u6548\u6c42\u89e3\u591a\u79cdPDE\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5feb\u7ea620\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u5176\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u548c\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u91cf\u5927\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u90e8\u7f72\u7f13\u6162\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684PDE\u6c42\u89e3\u5668\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faD-SENO\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u5f20\u5377\u79ef\u5757\uff08\u6355\u83b7\u5bbd\u611f\u53d7\u91ce\u548c\u957f\u7a0b\u7269\u7406\u4f9d\u8d56\uff09\u4e0e\u6324\u538b-\u6fc0\u52b1\u6a21\u5757\uff08\u901a\u8fc7\u901a\u9053\u6ce8\u610f\u529b\u81ea\u9002\u5e94\u91cd\u6821\u51c6\u7279\u5f81\u901a\u9053\uff09\uff0c\u7cbe\u5fc3\u9009\u62e9\u6269\u5f20\u7387\u4ee5\u805a\u7126\u5173\u952e\u533a\u57df\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\uff08\u7ffc\u578b\u52bf\u6d41\u3001\u591a\u5b54\u4ecb\u8d28\u8fbe\u897f\u6d41\u3001\u7ba1\u9053\u6cca\u8083\u53f6\u6d41\u3001\u4e0d\u53ef\u538b\u7f29Navier-Stokes\u6da1\u6d41\u573a\uff09\u4e2d\u8fbe\u5230\u7ea620\u500d\u4e8e\u6807\u51c6Transformer\u6a21\u578b\u548c\u795e\u7ecf\u7b97\u5b50\u7684\u8bad\u7ec3\u901f\u5ea6\uff0c\u540c\u65f6\u7cbe\u5ea6\u76f8\u5f53\u6216\u66f4\u4f18\uff1b\u6d88\u878d\u7814\u7a76\u8868\u660e\u79fb\u9664SE\u6a21\u5757\u4f1a\u5bfc\u81f4\u6027\u80fd\u8f7b\u5fae\u4e0b\u964d\u3002", "conclusion": "D-SENO\u662f\u4e00\u79cd\u9ad8\u6548\u8f7b\u91cf\u7684\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5f20\u5377\u79ef\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7269\u7406\u9a71\u52a8PDE\u7684\u5feb\u901f\u6c42\u89e3\u3002"}}
{"id": "2601.18497", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18497", "abs": "https://arxiv.org/abs/2601.18497", "authors": ["Sizhe Cheng", "Songheng Zhang", "Dong Ma", "Yong Wang"], "title": "BAIT: Visual-illusion-inspired Privacy Preservation for Mobile Data Visualization", "comment": "Accepted by CHI'26", "summary": "With the prevalence of mobile data visualizations, there have been growing concerns about their privacy risks, especially shoulder surfing attacks. Inspired by prior research on visual illusion, we propose BAIT, a novel approach to automatically generate privacy-preserving visualizations by stacking a decoy visualization over a given visualization. It allows visualization owners at proximity to clearly discern the original visualization and makes shoulder surfers at a distance be misled by the decoy visualization, by adjusting different visual channels of a decoy visualization (e.g., shape, position, tilt, size, color and spatial frequency). We explicitly model human perception effect at different viewing distances to optimize the decoy visualization design. Privacy-preserving examples and two in-depth user studies demonstrate the effectiveness of BAIT in both controlled lab study and real-world scenarios.", "AI": {"tldr": "BAIT\u662f\u4e00\u79cd\u901a\u8fc7\u53e0\u52a0\u8bf1\u9975\u53ef\u89c6\u5316\u6765\u4fdd\u62a4\u79fb\u52a8\u6570\u636e\u53ef\u89c6\u5316\u9690\u79c1\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u9519\u89c9\u539f\u7406\uff0c\u4f7f\u8fd1\u8ddd\u79bb\u7528\u6237\u80fd\u770b\u6e05\u539f\u59cb\u53ef\u89c6\u5316\uff0c\u800c\u8fdc\u8ddd\u79bb\u7aa5\u89c6\u8005\u4f1a\u88ab\u8bf1\u9975\u8bef\u5bfc\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u6570\u636e\u53ef\u89c6\u5316\u7684\u666e\u53ca\uff0c\u5176\u9690\u79c1\u98ce\u9669\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u7279\u522b\u662f\u80a9\u7aa5\u653b\u51fb\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5f80\u5f80\u5f71\u54cd\u53ef\u89c6\u5316\u53ef\u7528\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u4e0d\u635f\u5bb3\u7528\u6237\u4f53\u9a8c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faBAIT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u539f\u59cb\u53ef\u89c6\u5316\u4e0a\u53e0\u52a0\u8bf1\u9975\u53ef\u89c6\u5316\u6765\u4fdd\u62a4\u9690\u79c1\u3002\u901a\u8fc7\u8c03\u6574\u8bf1\u9975\u53ef\u89c6\u5316\u7684\u89c6\u89c9\u901a\u9053\uff08\u5982\u5f62\u72b6\u3001\u4f4d\u7f6e\u3001\u503e\u659c\u5ea6\u3001\u5927\u5c0f\u3001\u989c\u8272\u548c\u7a7a\u95f4\u9891\u7387\uff09\uff0c\u5e76\u663e\u5f0f\u5efa\u6a21\u4e0d\u540c\u89c2\u770b\u8ddd\u79bb\u4e0b\u7684\u4eba\u7c7b\u611f\u77e5\u6548\u5e94\uff0c\u4f18\u5316\u8bf1\u9975\u53ef\u89c6\u5316\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u793a\u4f8b\u548c\u4e24\u4e2a\u6df1\u5165\u7684\u7528\u6237\u7814\u7a76\uff0c\u5728\u53d7\u63a7\u5b9e\u9a8c\u5ba4\u7814\u7a76\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u8bc1\u660e\u4e86BAIT\u7684\u6709\u6548\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u62a4\u53ef\u89c6\u5316\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6388\u6743\u7528\u6237\u7684\u53ef\u7528\u6027\u3002", "conclusion": "BAIT\u662f\u4e00\u79cd\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u9519\u89c9\u539f\u7406\u548c\u4eba\u7c7b\u611f\u77e5\u5efa\u6a21\uff0c\u5728\u4fdd\u62a4\u79fb\u52a8\u6570\u636e\u53ef\u89c6\u5316\u514d\u53d7\u80a9\u7aa5\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18641", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.18641", "abs": "https://arxiv.org/abs/2601.18641", "authors": ["Onyedikachi Hope Amaechi-Okorie", "Branislav Radeljic"], "title": "Unheard in the Digital Age: Rethinking AI Bias and Speech Diversity", "comment": null, "summary": "Speech remains one of the most visible yet overlooked vectors of inclusion and exclusion in contemporary society. While fluency is often equated with credibility and competence, individuals with atypical speech patterns are routinely marginalized. Given the current state of the debate, this article focuses on the structural biases that shape perceptions of atypical speech and are now being encoded into artificial intelligence. Automated speech recognition (ASR) systems and voice interfaces, trained predominantly on standardized speech, routinely fail to recognize or respond to diverse voices, compounding digital exclusion. As AI technologies increasingly mediate access to opportunity, the study calls for inclusive technological design, anti-bias training to minimize the impact of discriminatory algorithmic decisions, and enforceable policy reform that explicitly recognize speech diversity as a matter of equity, not merely accessibility. Drawing on interdisciplinary research, the article advocates for a cultural and institutional shift in how we value voice, urging co-created solutions that elevate the rights, representation, and realities of atypical speakers in the digital age. Ultimately, the article reframes speech inclusion as a matter of equity (not accommodation) and advocates for co-created AI systems that reflect the full spectrum of human voices.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u97f3\u8bc6\u522b\u6280\u672f\u4e2d\u7684\u7ed3\u6784\u6027\u504f\u89c1\u95ee\u9898\uff0c\u6307\u51fa\u5f53\u524dASR\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u6807\u51c6\u5316\u8bed\u97f3\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5bf9\u975e\u5178\u578b\u8bed\u97f3\u6a21\u5f0f\u8bc6\u522b\u5931\u8d25\uff0c\u52a0\u5267\u4e86\u6570\u5b57\u6392\u65a5\u3002\u6587\u7ae0\u547c\u5401\u901a\u8fc7\u5305\u5bb9\u6027\u8bbe\u8ba1\u3001\u53cd\u504f\u89c1\u8bad\u7ec3\u548c\u653f\u7b56\u6539\u9769\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u8bed\u97f3\u662f\u793e\u4f1a\u5305\u5bb9\u4e0e\u6392\u65a5\u7684\u91cd\u8981\u4f46\u5e38\u88ab\u5ffd\u89c6\u7684\u8f7d\u4f53\u3002\u975e\u5178\u578b\u8bed\u97f3\u6a21\u5f0f\u8005\u5e38\u56e0\u8bed\u97f3\u5dee\u5f02\u800c\u88ab\u8fb9\u7f18\u5316\uff0c\u800c\u5f53\u524dAI\u6280\u672f\uff08\u7279\u522b\u662fASR\u7cfb\u7edf\uff09\u5c06\u8fd9\u79cd\u504f\u89c1\u7f16\u7801\u5230\u7b97\u6cd5\u4e2d\uff0c\u52a0\u5267\u4e86\u6570\u5b57\u4e0d\u5e73\u7b49\u3002\u968f\u7740AI\u6280\u672f\u65e5\u76ca\u6210\u4e3a\u673a\u4f1a\u83b7\u53d6\u7684\u4e2d\u4ecb\uff0c\u89e3\u51b3\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u53d8\u5f97\u5c24\u4e3a\u7d27\u8feb\u3002", "method": "\u6587\u7ae0\u91c7\u7528\u8de8\u5b66\u79d1\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790\u8bed\u97f3\u8bc6\u522b\u6280\u672f\u4e2d\u7684\u7ed3\u6784\u6027\u504f\u89c1\u95ee\u9898\u3002\u901a\u8fc7\u6279\u5224\u6027\u5206\u6790\u5f53\u524dASR\u7cfb\u7edf\u7684\u8bad\u7ec3\u6570\u636e\u548c\u65b9\u6cd5\uff0c\u63ed\u793a\u5176\u5bf9\u6807\u51c6\u5316\u8bed\u97f3\u7684\u8fc7\u5ea6\u4f9d\u8d56\u3002\u63d0\u51fa\u901a\u8fc7\u5305\u5bb9\u6027\u6280\u672f\u8bbe\u8ba1\u3001\u53cd\u504f\u89c1\u7b97\u6cd5\u8bad\u7ec3\u548c\u653f\u7b56\u6539\u9769\u76f8\u7ed3\u5408\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dASR\u7cfb\u7edf\u548c\u8bed\u97f3\u63a5\u53e3\u4e3b\u8981\u57fa\u4e8e\u6807\u51c6\u5316\u8bed\u97f3\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5bf9\u591a\u6837\u5316\u8bed\u97f3\uff08\u5982\u53e3\u97f3\u3001\u8bed\u901f\u3001\u53d1\u97f3\u6a21\u5f0f\u7b49\u975e\u5178\u578b\u7279\u5f81\uff09\u8bc6\u522b\u5931\u8d25\u3002\u8fd9\u79cd\u6280\u672f\u504f\u89c1\u4e0d\u4ec5\u53cd\u6620\u4e86\u793e\u4f1a\u504f\u89c1\uff0c\u8fd8\u901a\u8fc7\u7b97\u6cd5\u51b3\u7b56\u52a0\u5267\u4e86\u6570\u5b57\u6392\u65a5\uff0c\u9650\u5236\u4e86\u975e\u5178\u578b\u8bed\u97f3\u8005\u7684\u673a\u4f1a\u83b7\u53d6\u3002", "conclusion": "\u8bed\u97f3\u5305\u5bb9\u5e94\u88ab\u89c6\u4e3a\u516c\u5e73\u95ee\u9898\u800c\u975e\u4ec5\u4ec5\u662f\u4fbf\u5229\u6027\u8c03\u6574\u3002\u9700\u8981\u6587\u5316\u3001\u6280\u672f\u548c\u5236\u5ea6\u5c42\u9762\u7684\u7cfb\u7edf\u6027\u53d8\u9769\uff1a1\uff09\u5305\u5bb9\u6027\u6280\u672f\u8bbe\u8ba1\uff1b2\uff09\u53cd\u504f\u89c1\u7b97\u6cd5\u8bad\u7ec3\uff1b3\uff09\u53ef\u6267\u884c\u7684\u653f\u7b56\u6539\u9769\uff1b4\uff09\u5171\u540c\u521b\u9020\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6700\u7ec8\u76ee\u6807\u662f\u5efa\u7acb\u53cd\u6620\u4eba\u7c7b\u8bed\u97f3\u5168\u8c31\u7cfb\u7684AI\u7cfb\u7edf\uff0c\u63d0\u5347\u975e\u5178\u578b\u8bed\u97f3\u8005\u7684\u6743\u5229\u3001\u4ee3\u8868\u6027\u548c\u73b0\u5b9e\u5904\u5883\u3002"}}
{"id": "2601.17441", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17441", "abs": "https://arxiv.org/abs/2601.17441", "authors": ["Ondrej Bohdal", "Taha Ceritli", "Mete Ozay", "Jijoong Moon", "Kyeng-Hun Lee", "Hyeonmok Ko", "Umberto Michieli"], "title": "Data-driven Clustering and Merging of Adapters for On-device Large Language Models", "comment": "Accepted at ICASSP 2026", "summary": "On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.", "AI": {"tldr": "\u63d0\u51faD2C\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u4efb\u52a1\u793a\u4f8b\u8fdb\u884c\u9002\u914d\u5668\u805a\u7c7b\uff0c\u5c06\u540c\u7c07\u9002\u914d\u5668\u5408\u5e76\u4e3a\u591a\u4efb\u52a1\u9002\u914d\u5668\uff0c\u4ee5\u5728\u5b58\u50a8\u53d7\u9650\u8bbe\u5907\u4e0a\u63d0\u5347\u6027\u80fd", "motivation": "\u79fb\u52a8\u8bbe\u5907\u5b58\u50a8\u5bb9\u91cf\u6709\u9650\uff0c\u65e0\u6cd5\u5b58\u50a8\u6240\u6709\u4efb\u52a1\u9002\u914d\u5668\uff0c\u9700\u8981\u9009\u62e9\u4ee3\u8868\u6027\u9002\u914d\u5668\u4ee5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u83b7\u5f97\u826f\u597d\u6cdb\u5316\u6027\u80fd", "method": "D2C\u65b9\u6cd5\u5229\u7528\u5c11\u91cf\u4efb\u52a1\u7279\u5b9a\u793a\u4f8b\uff08\u5982\u6bcf\u4e2a\u4efb\u52a110\u4e2a\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u7cbe\u70bc\u805a\u7c7b\u5206\u914d\uff0c\u5c06\u6bcf\u4e2a\u7c07\u5185\u7684\u9002\u914d\u5668\u5408\u5e76\u4e3a\u591a\u4efb\u52a1\u9002\u914d\u5668", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8003\u8651\u5b58\u50a8\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684D2C\u65b9\u6cd5\u89e3\u51b3\u4e86\u8bbe\u5907\u7aefLLM\u9002\u914d\u5668\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u805a\u7c7b\u5408\u5e76\u7b56\u7565\u5728\u6709\u9650\u5b58\u50a8\u4e0b\u5b9e\u73b0\u4e86\u591a\u4efb\u52a1\u6027\u80fd\u63d0\u5347"}}
{"id": "2601.18697", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18697", "abs": "https://arxiv.org/abs/2601.18697", "authors": ["Junling Wang", "Lahari Goswami", "Gustavo Kreia Umbelino", "Kiara Garcia Chau", "Mrinmaya Sachan", "April Yi Wang"], "title": "Bridging Instead of Replacing Online Coding Communities with AI through Community-Enriched Chatbot Designs", "comment": "Accepted at the ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW 2026). To appear in PACMHCI", "summary": "LLM-based chatbots like ChatGPT have become popular tools for assisting with coding tasks. However, they often produce isolated responses and lack mechanisms for social learning or contextual grounding. In contrast, online coding communities like Kaggle offer socially mediated learning environments that foster critical thinking, engagement, and a sense of belonging. Yet, growing reliance on LLMs risks diminishing participation in these communities and weakening their collaborative value. To address this, we propose Community-Enriched AI, a design paradigm that embeds social learning dynamics into LLM-based chatbots by surfacing user-generated content and social design feature from online coding communities. Using this paradigm, we implemented a RAG-based AI chatbot leveraging resources from Kaggle to validate our design. Across two empirical studies involving 28 and 12 data science learners, respectively, we found that Community-Enriched AI significantly enhances user trust, encourages engagement with community, and effectively supports learners in solving data science tasks. We conclude by discussing design implications for AI assistance systems that bridge -- rather than replace -- online coding communities.", "AI": {"tldr": "\u63d0\u51faCommunity-Enriched AI\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5c06\u5728\u7ebf\u7f16\u7a0b\u793e\u533a\u7684\u793e\u4f1a\u5b66\u4e60\u673a\u5236\u5d4c\u5165LLM\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7Kaggle\u8d44\u6e90\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u4fe1\u4efb\u548c\u793e\u533a\u53c2\u4e0e\u5ea6", "motivation": "\u5f53\u524dLLM\u804a\u5929\u673a\u5668\u4eba\uff08\u5982ChatGPT\uff09\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u63d0\u4f9b\u5b64\u7acb\u54cd\u5e94\uff0c\u7f3a\u4e4f\u793e\u4f1a\u5b66\u4e60\u548c\u60c5\u5883\u57fa\u7840\uff1b\u800c\u5728\u7ebf\u7f16\u7a0b\u793e\u533a\uff08\u5982Kaggle\uff09\u63d0\u4f9b\u793e\u4f1a\u4e2d\u4ecb\u7684\u5b66\u4e60\u73af\u5883\uff0c\u4f46LLM\u7684\u4f9d\u8d56\u53ef\u80fd\u524a\u5f31\u793e\u533a\u53c2\u4e0e\u548c\u534f\u4f5c\u4ef7\u503c", "method": "\u63d0\u51faCommunity-Enriched AI\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5c06\u7528\u6237\u751f\u6210\u5185\u5bb9\u548c\u793e\u4ea4\u8bbe\u8ba1\u7279\u5f81\u4ece\u5728\u7ebf\u7f16\u7a0b\u793e\u533a\u5d4c\u5165LLM\u804a\u5929\u673a\u5668\u4eba\uff1b\u5b9e\u73b0\u57fa\u4e8eRAG\u7684AI\u804a\u5929\u673a\u5668\u4eba\uff0c\u5229\u7528Kaggle\u8d44\u6e90\u9a8c\u8bc1\u8bbe\u8ba1\uff1b\u8fdb\u884c\u4e24\u9879\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u522b\u6d89\u53ca28\u540d\u548c12\u540d\u6570\u636e\u79d1\u5b66\u5b66\u4e60\u8005", "result": "Community-Enriched AI\u663e\u8457\u589e\u5f3a\u7528\u6237\u4fe1\u4efb\uff0c\u9f13\u52b1\u793e\u533a\u53c2\u4e0e\uff0c\u6709\u6548\u652f\u6301\u5b66\u4e60\u8005\u89e3\u51b3\u6570\u636e\u79d1\u5b66\u4efb\u52a1", "conclusion": "\u8ba8\u8bbaAI\u8f85\u52a9\u7cfb\u7edf\u7684\u8bbe\u8ba1\u542f\u793a\uff0c\u5f3a\u8c03\u5e94\u8fde\u63a5\u800c\u975e\u53d6\u4ee3\u5728\u7ebf\u7f16\u7a0b\u793e\u533a\uff0c\u4fc3\u8fdb\u793e\u4f1a\u5b66\u4e60\u52a8\u6001\u7684\u6574\u5408"}}
{"id": "2601.18750", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18750", "abs": "https://arxiv.org/abs/2601.18750", "authors": ["Francesco Chiossi", "Elnur Imamaliyev", "Martin Bleichner", "Sven Mayer"], "title": "Anticipation in Action: Evaluating Stimulus-Preceding Negativity as an Implicit Trigger for Adaptive Mixed Reality", "comment": null, "summary": "Mixed Reality (MR) interfaces increasingly rely on gaze for interaction , yet distinguishing visual attention from intentional action remains difficult, leading to the Midas Touch problem. Existing solutions require explicit confirmations, while brain-computer interfaces may provide an implicit marker of intention using Stimulus-Preceding Negativity (SPN). We investigated how Intention (Select vs. Observe) and Feedback (With vs. Without) modulate SPN during gaze-based MR interactions. During realistic selection tasks, we acquired EEG and eye-tracking data from 28 participants. SPN was robustly elicited and sensitive to both factors: observation without feedback produced the strongest amplitudes, while intention to select and expectation of feedback reduced activity, suggesting SPN reflects anticipatory uncertainty rather than motor preparation. Complementary decoding with deep learning models achieved reliable person-dependent classification of user intention, with accuracies ranging from 75% to 97% across participants. These findings identify SPN as an implicit marker for building intention-aware MR interfaces that mitigate the Midas Touch.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u6df7\u5408\u73b0\u5b9e(MR)\u754c\u9762\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u8111\u7535\u4fe1\u53f7\u4e2d\u7684\u523a\u6fc0\u524d\u8d1f\u6ce2(SPN)\u4f5c\u4e3a\u9690\u5f0f\u610f\u56fe\u6807\u8bb0\uff0c\u4ee5\u89e3\u51b3\"\u8fc8\u8fbe\u65af\u89e6\u6478\"\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0SPN\u5bf9\u7528\u6237\u610f\u56fe(\u9009\u62e9vs\u89c2\u5bdf)\u548c\u53cd\u9988\u9884\u671f\u654f\u611f\uff0c\u5e76\u80fd\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u9760\u5730\u89e3\u7801\u7528\u6237\u610f\u56fe\u3002", "motivation": "\u6df7\u5408\u73b0\u5b9e\u754c\u9762\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u6ce8\u89c6\u8fdb\u884c\u4ea4\u4e92\uff0c\u4f46\u96be\u4ee5\u533a\u5206\u89c6\u89c9\u6ce8\u610f\u548c\u6709\u610f\u884c\u4e3a\uff0c\u5bfc\u81f4\"\u8fc8\u8fbe\u65af\u89e6\u6478\"\u95ee\u9898\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u8981\u663e\u5f0f\u786e\u8ba4\uff0c\u800c\u8111\u673a\u63a5\u53e3\u53ef\u80fd\u901a\u8fc7\u523a\u6fc0\u524d\u8d1f\u6ce2(SPN)\u63d0\u4f9b\u9690\u5f0f\u7684\u610f\u56fe\u6807\u8bb0\u3002", "method": "\u7814\u7a76\u572828\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u73b0\u5b9e\u9009\u62e9\u4efb\u52a1\u65f6\uff0c\u540c\u65f6\u91c7\u96c6\u8111\u7535\u56fe(EEG)\u548c\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u3002\u5b9e\u9a8c\u8bbe\u8ba1\u8003\u5bdf\u4e86\u610f\u56fe(\u9009\u62e9vs\u89c2\u5bdf)\u548c\u53cd\u9988(\u6709vs\u65e0)\u4e24\u4e2a\u56e0\u7d20\u5bf9SPN\u7684\u8c03\u5236\u4f5c\u7528\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u610f\u56fe\u89e3\u7801\u3002", "result": "SPN\u88ab\u7a33\u5065\u5730\u8bf1\u53d1\u4e14\u5bf9\u4e24\u4e2a\u56e0\u7d20\u654f\u611f\uff1a\u65e0\u53cd\u9988\u7684\u89c2\u5bdf\u4ea7\u751f\u6700\u5f3a\u7684\u632f\u5e45\uff0c\u800c\u9009\u62e9\u610f\u56fe\u548c\u53cd\u9988\u9884\u671f\u964d\u4f4e\u4e86\u6d3b\u52a8\uff0c\u8868\u660eSPN\u53cd\u6620\u9884\u671f\u4e0d\u786e\u5b9a\u6027\u800c\u975e\u8fd0\u52a8\u51c6\u5907\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u4e2a\u4f53\u4f9d\u8d56\u610f\u56fe\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u572875%\u523097%\u4e4b\u95f4\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5c06SPN\u786e\u5b9a\u4e3a\u6784\u5efa\u610f\u56fe\u611f\u77e5\u6df7\u5408\u73b0\u5b9e\u754c\u9762\u7684\u9690\u5f0f\u6807\u8bb0\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\"\u8fc8\u8fbe\u65af\u89e6\u6478\"\u95ee\u9898\u3002\u7814\u7a76\u5c55\u793a\u4e86\u8111\u673a\u63a5\u53e3\u5728\u6539\u5584\u6df7\u5408\u73b0\u5b9e\u4ea4\u4e92\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.17467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17467", "abs": "https://arxiv.org/abs/2601.17467", "authors": ["Jianxiong Zhang", "Bing Guo", "Yuming Jiang", "Haobo Wang", "Bo An", "Xuefeng Du"], "title": "Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping", "comment": null, "summary": "Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.", "AI": {"tldr": "ARS\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u7b54\u6848\u4e00\u81f4\u6027\u8868\u5f81\u6765\u68c0\u6d4b\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\uff0c\u901a\u8fc7\u6f5c\u5728\u5e72\u9884\u751f\u6210\u53cd\u4e8b\u5b9e\u7b54\u6848\u5e76\u5b66\u4e60\u8868\u5f81\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u63d0\u5347\u68c0\u6d4b\u6027\u80fd", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7ecf\u5e38\u751f\u6210\u770b\u4f3c\u8fde\u8d2f\u4f46\u7b54\u6848\u9519\u8bef\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u4f7f\u5f97\u5e7b\u89c9\u68c0\u6d4b\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u4f7f\u7528\u8f68\u8ff9\u6587\u672c\u6216\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u68c0\u6d4b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u8868\u9762\u6a21\u5f0f\u800c\u975e\u7b54\u6848\u6709\u6548\u6027", "method": "\u63d0\u51fa\u7b54\u6848\u4e00\u81f4\u6027\u8868\u5f81\u5851\u5f62\uff08ARS\uff09\u65b9\u6cd5\uff1a\u901a\u8fc7\u5c0f\u89c4\u6a21\u6f5c\u5728\u5e72\u9884\uff08\u6270\u52a8\u8f68\u8ff9\u8fb9\u754c\u5d4c\u5165\uff09\u751f\u6210\u53cd\u4e8b\u5b9e\u7b54\u6848\uff0c\u6839\u636e\u6270\u52a8\u540e\u7b54\u6848\u662f\u5426\u4e0e\u539f\u7b54\u6848\u4e00\u81f4\u8fdb\u884c\u6807\u6ce8\uff0c\u7136\u540e\u5b66\u4e60\u5c06\u7b54\u6848\u4e00\u81f4\u7684\u9690\u85cf\u72b6\u6001\u805a\u5408\u3001\u7b54\u6848\u4e0d\u4e00\u81f4\u7684\u72b6\u6001\u5206\u79bb\u7684\u8868\u5f81", "result": "\u5b9e\u9a8c\u8868\u660eARS\u80fd\u6301\u7eed\u6539\u8fdb\u68c0\u6d4b\u6027\u80fd\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002\u5851\u5f62\u540e\u7684\u5d4c\u5165\u53ef\u4e0e\u73b0\u6709\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u6d4b\u5668\u5373\u63d2\u5373\u7528\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u65e0\u9700\u4eba\u5de5\u6807\u6ce8", "conclusion": "ARS\u901a\u8fc7\u663e\u5f0f\u7f16\u7801\u7b54\u6848\u7a33\u5b9a\u6027\u5b66\u4e60\u68c0\u6d4b\u53cb\u597d\u7684\u8f68\u8ff9\u6761\u4ef6\u8868\u5f81\uff0c\u6709\u6548\u66b4\u9732\u6307\u793a\u5e7b\u89c9\u98ce\u9669\u7684\u6f5c\u5728\u4e0d\u7a33\u5b9a\u6027\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18759", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18759", "abs": "https://arxiv.org/abs/2601.18759", "authors": ["Junling Wang", "Hongyi Lan", "Xiaotian Su", "Mustafa Doga Dogan", "April Yi Wang"], "title": "UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing", "comment": "Accepted at the 31st International Conference on Intelligent User Interfaces (IUI 2026)", "summary": "Designing user interfaces (UIs) is a critical step when launching products, building portfolios, or personalizing projects, yet end users without design expertise often struggle to articulate their intent and to trust design choices. Existing example-based tools either promote broad exploration, which can cause overwhelm and design drift, or require adapting a single example, risking design fixation. We present UI Remix, an interactive system that supports mobile UI design through an example-driven design workflow. Powered by a multimodal retrieval-augmented generation (MMRAG) model, UI Remix enables iterative search, selection, and adaptation of examples at both the global (whole interface) and local (component) level. To foster trust, it presents source transparency cues such as ratings, download counts, and developer information. In an empirical study with 24 end users, UI Remix significantly improved participants' ability to achieve their design goals, facilitated effective iteration, and encouraged exploration of alternative designs. Participants also reported that source transparency cues enhanced their confidence in adapting examples. Our findings suggest new directions for AI-assisted, example-driven systems that empower end users to design with greater control, trust, and openness to exploration.", "AI": {"tldr": "UI Remix\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u652f\u6301\u79fb\u52a8UI\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u793a\u4f8b\u7684\u641c\u7d22\u3001\u9009\u62e9\u548c\u8fed\u4ee3\u9002\u5e94\uff0c\u5e2e\u52a9\u975e\u4e13\u4e1a\u7528\u6237\u5b9e\u73b0\u8bbe\u8ba1\u76ee\u6807\u5e76\u589e\u5f3a\u4fe1\u4efb\u3002", "motivation": "\u975e\u4e13\u4e1a\u7528\u6237\u5728UI\u8bbe\u8ba1\u4e2d\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u96be\u4ee5\u6e05\u6670\u8868\u8fbe\u8bbe\u8ba1\u610f\u56fe\uff0c\u4ee5\u53ca\u5bf9\u8bbe\u8ba1\u9009\u62e9\u7f3a\u4e4f\u4fe1\u4efb\u3002\u73b0\u6709\u793a\u4f8b\u5de5\u5177\u8981\u4e48\u5bfc\u81f4\u63a2\u7d22\u8fc7\u5ea6\u548c\u8bbe\u8ba1\u504f\u79bb\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u5355\u4e00\u793a\u4f8b\u5bfc\u81f4\u8bbe\u8ba1\u56fa\u5316\u3002", "method": "\u5f00\u53d1\u4e86UI Remix\u7cfb\u7edf\uff0c\u91c7\u7528\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u5168\u5c40\uff08\u6574\u4e2a\u754c\u9762\uff09\u548c\u5c40\u90e8\uff08\u7ec4\u4ef6\uff09\u5c42\u9762\u7684\u793a\u4f8b\u641c\u7d22\u3001\u9009\u62e9\u548c\u8fed\u4ee3\u9002\u5e94\uff0c\u5e76\u63d0\u4f9b\u6765\u6e90\u900f\u660e\u5ea6\u63d0\u793a\uff08\u8bc4\u5206\u3001\u4e0b\u8f7d\u91cf\u3001\u5f00\u53d1\u8005\u4fe1\u606f\uff09\u4ee5\u589e\u5f3a\u4fe1\u4efb\u3002", "result": "\u572824\u540d\u6700\u7ec8\u7528\u6237\u7684\u5b9e\u8bc1\u7814\u7a76\u4e2d\uff0cUI Remix\u663e\u8457\u63d0\u9ad8\u4e86\u53c2\u4e0e\u8005\u5b9e\u73b0\u8bbe\u8ba1\u76ee\u6807\u7684\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u6709\u6548\u8fed\u4ee3\uff0c\u5e76\u9f13\u52b1\u4e86\u66ff\u4ee3\u8bbe\u8ba1\u7684\u63a2\u7d22\u3002\u53c2\u4e0e\u8005\u62a5\u544a\u6765\u6e90\u900f\u660e\u5ea6\u63d0\u793a\u589e\u5f3a\u4e86\u4ed6\u4eec\u9002\u5e94\u793a\u4f8b\u7684\u4fe1\u5fc3\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u8f85\u52a9\u7684\u793a\u4f8b\u9a71\u52a8\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u80fd\u591f\u8d4b\u4e88\u6700\u7ec8\u7528\u6237\u66f4\u5927\u7684\u63a7\u5236\u6743\u3001\u4fe1\u4efb\u611f\u548c\u63a2\u7d22\u5f00\u653e\u6027\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u8fdb\u884cUI\u8bbe\u8ba1\u3002"}}
{"id": "2601.17473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17473", "abs": "https://arxiv.org/abs/2601.17473", "authors": ["Manooshree Patel", "Rayna Bhattacharyya", "Thomas Lu", "Arnav Mehta", "Niels Voss", "Narges Norouzi", "Gireeja Ranade"], "title": "LeanTutor: Towards a Verified AI Mathematical Proof Tutor", "comment": "arXiv admin note: substantial text overlap with arXiv:2506.08321. substantial text overlap with arXiv:2506.08321. substantial text overlap with arXiv:2506.08321. substantial text overlap with arXiv:2506.08321", "summary": "This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.", "AI": {"tldr": "\u5f00\u53d1\u7ed3\u5408LLM\u4e0e\u5b9a\u7406\u8bc1\u660e\u5668\u7684AI\u6570\u5b66\u8bc1\u660e\u8f85\u5bfc\u7cfb\u7edfLeanTutor\uff0c\u5b9e\u73b0\u53ef\u8bc1\u660e\u6b63\u786e\u6027", "motivation": "LLM\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4f46\u6613\u51fa\u9519\uff0c\u5b9a\u7406\u8bc1\u660e\u5668\uff08\u5982Lean\uff09\u80fd\u4fdd\u8bc1\u6b63\u786e\u6027\u4f46\u5b66\u4e60\u95e8\u69db\u9ad8\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf", "method": "\u6784\u5efaLeanTutor\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u81ea\u52a8\u5f62\u5f0f\u5316/\u8bc1\u660e\u68c0\u67e5\u5668\u3001\u4e0b\u4e00\u6b65\u751f\u6210\u5668\u3001\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u751f\u6210\u5668\uff1b\u521b\u5efaPeanoBench\u6570\u636e\u96c6\uff08371\u4e2a\u76ae\u4e9a\u8bfa\u7b97\u672f\u8bc1\u660e\uff09\u8fdb\u884c\u8bc4\u4f30", "result": "\u63d0\u51fa\u4e86\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edfLeanTutor\uff0c\u7ed3\u5408LLM\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u80fd\u529b\u548c\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u53ef\u8bc1\u660e\u6b63\u786e\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u8bc4\u4f30\u6570\u636e\u96c6", "conclusion": "\u901a\u8fc7\u7ed3\u5408LLM\u4e0e\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u5f00\u53d1\u4e86\u53ef\u8bc1\u660e\u6b63\u786e\u7684\u6570\u5b66\u8bc1\u660e\u8f85\u5bfc\u7cfb\u7edf\uff0c\u4e3a\u89e3\u51b3LLM\u9519\u8bef\u503e\u5411\u548c\u5b9a\u7406\u8bc1\u660e\u5668\u5b66\u4e60\u96be\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2601.18785", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18785", "abs": "https://arxiv.org/abs/2601.18785", "authors": ["Tiffany Wang", "Yuqian Sun", "Yi Wang", "Melissa Roemmele", "John Joon Young Chung", "Max Kreminski"], "title": "Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System", "comment": "Extended abstract presented at the 2025 Wordplay Workshop at EMNLP", "summary": "The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.", "AI": {"tldr": "Dramamancer\u7cfb\u7edf\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u4f5c\u8005\u521b\u5efa\u7684\u6545\u4e8b\u6a21\u5f0f\u8f6c\u5316\u4e3a\u73a9\u5bb6\u9a71\u52a8\u7684\u6e38\u620f\u4f53\u9a8c\uff0c\u63a2\u7d22\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u4e2d\u4f5c\u8005\u610f\u56fe\u4e0e\u73a9\u5bb6\u80fd\u52a8\u6027\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\u4e3a\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u4e2d\u4f5c\u8005\u610f\u56fe\u4e0e\u73a9\u5bb6\u80fd\u52a8\u6027\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5229\u7528LLM\u6280\u672f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u5f00\u53d1\u4e86Dramamancer\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u4f5c\u8005\u521b\u5efa\u7684\u6545\u4e8b\u6a21\u5f0f\uff08story schemas\uff09\u8f6c\u5316\u4e3a\u73a9\u5bb6\u9a71\u52a8\u7684\u6e38\u620f\u8fc7\u7a0b\u3002", "result": "\u8bba\u6587\u6982\u8ff0\u4e86\u4e0e\u8be5\u7cfb\u7edf\u76f8\u5173\u7684\u8bbe\u8ba1\u6280\u672f\u548c\u8bc4\u4f30\u8003\u8651\u56e0\u7d20\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "Dramamancer\u7cfb\u7edf\u5c55\u793a\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u4e2d\u5b9e\u73b0\u4f5c\u8005\u610f\u56fe\u4e0e\u73a9\u5bb6\u80fd\u52a8\u6027\u5e73\u8861\u7684\u65b0\u9014\u5f84\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u7814\u7a76\u3002"}}
{"id": "2601.17480", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17480", "abs": "https://arxiv.org/abs/2601.17480", "authors": ["Marton Szep", "Jorge Marin Ruiz", "Georgios Kaissis", "Paulina Seidl", "R\u00fcdiger von Eisenhart-Rothe", "Florian Hinterwimmer", "Daniel Rueckert"], "title": "Unintended Memorization of Sensitive Information in Fine-Tuned Language Models", "comment": "Accepted to EACL 2026. 20 pages", "summary": "Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u5fae\u8c03\u4e2dPII\u6cc4\u9732\u98ce\u9669\uff0c\u7279\u522b\u662f\u4ec5\u51fa\u73b0\u5728\u8f93\u5165\u800c\u975e\u8bad\u7ec3\u76ee\u6807\u4e2d\u7684PII\u66b4\u9732\u95ee\u9898\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u7684\u6743\u8861", "motivation": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u6570\u636e\u96c6\u4e0a\u5b58\u5728\u610f\u5916\u8bb0\u5fc6\u548cPII\u6cc4\u9732\u7684\u91cd\u5927\u98ce\u9669\uff0c\u53ef\u80fd\u8fdd\u53cd\u9690\u79c1\u6cd5\u89c4\u5e76\u5371\u5bb3\u4e2a\u4eba\u5b89\u5168\u3002\u5f53\u524d\u7814\u7a76\u5bf9\u4ec5\u51fa\u73b0\u5728\u6a21\u578b\u8f93\u5165\u800c\u975e\u8bad\u7ec3\u76ee\u6807\u4e2d\u7684PII\u66b4\u9732\u8fd9\u4e00\u5173\u952e\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6f0f\u6d1e\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u8bbe\u8ba1\u53d7\u63a7\u63d0\u53d6\u63a2\u9488\u6765\u91cf\u5316\u610f\u5916PII\u8bb0\u5fc6\uff0c\u7814\u7a76\u8bed\u8a00\u3001PII\u9891\u7387\u3001\u4efb\u52a1\u7c7b\u578b\u548c\u6a21\u578b\u89c4\u6a21\u7b49\u56e0\u7d20\u5bf9\u8bb0\u5fc6\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u8fdb\u4e00\u6b65\u57fa\u51c6\u6d4b\u8bd5\u56db\u79cd\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff1a\u5dee\u5206\u9690\u79c1\u3001\u673a\u5668\u9057\u5fd8\u3001\u6b63\u5219\u5316\u548c\u504f\u597d\u5bf9\u9f50\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u9690\u79c1\u4e0e\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u540e\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u63d0\u4f9b\u66f4\u4e00\u81f4\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u800c\u5dee\u5206\u9690\u79c1\u5728\u7279\u5b9a\u8bbe\u7f6e\u4e2d\u80fd\u663e\u8457\u51cf\u5c11\u6cc4\u9732\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5fae\u8c03LLM\u4e2d\u8bb0\u5fc6\u95ee\u9898\u7684\u6301\u7eed\u6311\u6218\u3002", "conclusion": "\u5fae\u8c03LLM\u4e2d\u7684PII\u8bb0\u5fc6\u95ee\u9898\u662f\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\uff0c\u9700\u8981\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u3002\u540e\u5904\u7406\u65b9\u6cd5\u5728\u9690\u79c1-\u6548\u7528\u6743\u8861\u65b9\u9762\u8868\u73b0\u66f4\u4e00\u81f4\uff0c\u800c\u5dee\u5206\u9690\u79c1\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u6709\u6548\u4f46\u5b58\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\u3002"}}
{"id": "2601.18792", "categories": ["cs.HC", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18792", "abs": "https://arxiv.org/abs/2601.18792", "authors": ["Brian Liu", "Oiwi Parker Jones"], "title": "MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data", "comment": null, "summary": "Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalography (MEG), while participants listened to audiobooks. Having annotated the text, we employ force-alignment of the text and audio to align our sentiment labels with the brain recordings. It is straightforward then to train Brainto-Sentiment models on these data. Experimental results show an improvement in balanced accuracy for Brain-to-Sentiment compared to baseline, supporting the proposed approach as a proof-of-concept for leveraging existing MEG datasets and learning to decode sentiment directly from the brain.", "AI": {"tldr": "\u5229\u7528\u9884\u8bad\u7ec3\u6587\u672c\u60c5\u611f\u6a21\u578b\u6807\u6ce8MEG\u8111\u7535\u6570\u636e\uff0c\u901a\u8fc7\u6587\u672c-\u97f3\u9891\u5bf9\u9f50\u6784\u5efa\u8111\u7535-\u60c5\u611f\u6570\u636e\u96c6\uff0c\u8bad\u7ec3Brain-to-Sentiment\u6a21\u578b\u89e3\u7801\u60c5\u611f", "motivation": "\u73b0\u6709\u8111\u7535\u6570\u636e\u96c6\u591a\u4e0e\u8bed\u97f3\u53ca\u8f6c\u5f55\u6587\u672c\u5bf9\u9f50\uff0c\u4f46\u7f3a\u4e4f\u60c5\u611f\u6807\u6ce8\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u5229\u7528\u9884\u8bad\u7ec3\u6587\u672c\u60c5\u611f\u6a21\u578b\u6807\u6ce8\u975e\u4fb5\u5165\u6027\u8111\u7535\u6570\u636e\uff08MEG\uff09\uff0c\u5b9e\u73b0\u4ece\u8111\u7535\u4fe1\u53f7\u89e3\u7801\u60c5\u611f", "method": "1. \u4f7f\u7528\u9884\u8bad\u7ec3\u6587\u672c\u60c5\u611f\u6a21\u578b\u6807\u6ce8\u6587\u672c\u60c5\u611f\uff1b2. \u901a\u8fc7\u6587\u672c-\u97f3\u9891\u5f3a\u5236\u5bf9\u9f50\u5c06\u60c5\u611f\u6807\u7b7e\u4e0eMEG\u8111\u7535\u8bb0\u5f55\u5bf9\u9f50\uff1b3. \u57fa\u4e8e\u5bf9\u9f50\u6570\u636e\u8bad\u7ec3Brain-to-Sentiment\u6a21\u578b", "result": "Brain-to-Sentiment\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u5728\u5e73\u8861\u51c6\u786e\u7387\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5229\u7528\u73b0\u6709MEG\u6570\u636e\u96c6\u76f4\u63a5\u4ece\u8111\u7535\u4fe1\u53f7\u89e3\u7801\u60c5\u611f\u7684\u53ef\u884c\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5229\u7528\u73b0\u6709MEG\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6587\u672c\u60c5\u611f\u6a21\u578b\u6784\u5efa\u8111\u7535-\u60c5\u611f\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u60c5\u611f\u89e3\u7801\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4ece\u8111\u7535\u4fe1\u53f7\u89e3\u7801\u60c5\u611f\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2601.17489", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17489", "abs": "https://arxiv.org/abs/2601.17489", "authors": ["Ashutosh Bajpai", "Akshat Bhandari", "Akshay Nambi", "Tanmoy Chakraborty"], "title": "SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving", "comment": null, "summary": "Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.", "AI": {"tldr": "SpatialMath\u6846\u67b6\u901a\u8fc7\u5c06\u7a7a\u95f4\u8868\u793a\u6ce8\u5165\u7b26\u53f7\u63a8\u7406\u94fe\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u4e2d\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u6570\u5b66\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u51e0\u4f55\u95ee\u9898\u4e0a\u96be\u4ee5\u51c6\u786e\u5206\u89e3\u590d\u6742\u89c6\u89c9\u8f93\u5165\u5e76\u5c06\u611f\u77e5\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u76f8\u7ed3\u5408", "method": "\u63d0\u51faSpatialMath\u6846\u67b6\uff0c\u5305\u542b\u4e13\u95e8\u611f\u77e5\u6a21\u5757\u63d0\u53d6\u89c6\u89c9\u56fe\u8868\u4e2d\u7684\u7a7a\u95f4\u57fa\u7840\u8868\u793a\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u8868\u793a\u7cfb\u7edf\u6027\u5730\u6ce8\u5165\u7b26\u53f7\u63a8\u7406\u94fe\uff1b\u540c\u65f6\u6784\u5efaMATHVERSE-PLUS\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ed3\u6784\u5316\u89c6\u89c9\u89e3\u91ca\u548c\u9010\u6b65\u63a8\u7406\u8def\u5f84", "result": "SpatialMath\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001\u57fa\u7ebf\uff0c\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u8bbe\u7f6e\u4e0b\u6bd4\u76d1\u7763\u5fae\u8c03\u52a0\u6570\u636e\u589e\u5f3a\u63d0\u5347\u9ad8\u8fbe10\u4e2a\u767e\u5206\u70b9\uff1b\u9c81\u68d2\u6027\u5206\u6790\u663e\u793a\u589e\u5f3a\u7684\u7a7a\u95f4\u8868\u793a\u76f4\u63a5\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027", "conclusion": "\u7ed3\u6784\u5316\u611f\u77e5\u5230\u63a8\u7406\u7ba1\u9053\u5bf9\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u7a7a\u95f4\u8868\u793a\u4e0e\u7b26\u53f7\u63a8\u7406\u7684\u6574\u5408\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9\u5bc6\u96c6\u578b\u6570\u5b66\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b"}}
{"id": "2601.17495", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17495", "abs": "https://arxiv.org/abs/2601.17495", "authors": ["Ruiyu Zhang", "Lin Nie", "Wai-Fung Lam", "Qihao Wang", "Xin Zhao"], "title": "PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems", "comment": "15 pages, 1 figure", "summary": "In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.", "AI": {"tldr": "PEARL\u662f\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u5d4c\u5165\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u9650\u7684\u76d1\u7763\u5c06\u5d4c\u5165\u8f6f\u5bf9\u9f50\u5230\u7c7b\u522b\u539f\u578b\uff0c\u6539\u5584\u5c40\u90e8\u90bb\u57df\u51e0\u4f55\u7ed3\u6784\uff0c\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5b9e\u9645\u90e8\u7f72\u7cfb\u7edf\u4e2d\uff0c\u57fa\u4e8e\u5d4c\u5165\u7684\u6700\u8fd1\u90bb\u68c0\u7d22\u5e38\u56e0\u5d4c\u5165\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u4e0d\u4f73\u800c\u5931\u8d25\u3002\u7531\u4e8e\u6807\u7b7e\u7a00\u7f3a\u3001\u9886\u57df\u6f02\u79fb\u548c\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u4e0b\u6e38\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u5d4c\u5165\u51e0\u4f55\u7ed3\u6784\uff0c\u4f46\u539f\u59cb\u5d4c\u5165\u5f80\u5f80\u4e0e\u6700\u8fd1\u90bb\u68c0\u7d22\u6240\u9700\u7684\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\u4e0d\u5339\u914d\u3002", "method": "PEARL\uff08\u539f\u578b\u589e\u5f3a\u5bf9\u9f50\u8868\u793a\u5b66\u4e60\uff09\u4f7f\u7528\u6709\u9650\u76d1\u7763\u5c06\u5d4c\u5165\u8f6f\u5bf9\u9f50\u5230\u7c7b\u522b\u539f\u578b\uff0c\u91cd\u5851\u5c40\u90e8\u90bb\u57df\u51e0\u4f55\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u6301\u7ef4\u5ea6\u5e76\u907f\u514d\u6fc0\u8fdb\u6295\u5f71\u6216\u574d\u7f29\u3002\u8be5\u65b9\u6cd5\u4ecb\u4e8e\u65e0\u76d1\u7763\u540e\u5904\u7406\uff08\u589e\u76ca\u6709\u9650\uff09\u548c\u5168\u76d1\u7763\u6295\u5f71\uff08\u9700\u8981\u5927\u91cf\u6807\u7b7e\uff09\u4e4b\u95f4\u3002", "result": "\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\uff0cPEARL\u663e\u8457\u6539\u5584\u5c40\u90e8\u90bb\u57df\u8d28\u91cf\uff1a\u76f8\u6bd4\u539f\u59cb\u5d4c\u5165\u63d0\u534725.7%\uff0c\u76f8\u6bd4\u5f3a\u65e0\u76d1\u7763\u540e\u5904\u7406\u63d0\u5347\u8d85\u8fc721.1%\uff0c\u5728\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u7cfb\u7edf\u6700\u8106\u5f31\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PEARL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u5d4c\u5165\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u6807\u7b7e\u7a00\u7f3a\u7684\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2d\u6709\u6548\u6539\u5584\u5d4c\u5165\u51e0\u4f55\u7ed3\u6784\uff0c\u63d0\u5347\u6700\u8fd1\u90bb\u68c0\u7d22\u548c\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u6027\u80fd\uff0c\u586b\u8865\u4e86\u65e0\u76d1\u7763\u540e\u5904\u7406\u548c\u5168\u76d1\u7763\u6295\u5f71\u4e4b\u95f4\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.17512", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17512", "abs": "https://arxiv.org/abs/2601.17512", "authors": ["Yiqun Zhang", "Shenghong Cai", "Zihua Yang", "Sen Feng", "Yuzhu Ji", "Haijun Zhang"], "title": "One-Shot Federated Clustering of Non-Independent Completely Distributed Data", "comment": "This work has been accepted for publication in IEEE Internet of Things Journal", "summary": "Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGOLD\u6846\u67b6\uff0c\u89e3\u51b3\u8054\u90a6\u805a\u7c7b\u4e2dNon-IID\u6570\u636e\u5bfc\u81f4\u7684\u96c6\u7fa4\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u5bfc\u5411\u7684\u5c40\u90e8\u5206\u5e03\u5b66\u4e60\u63d0\u5347\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u805a\u7c7b\u5728\u65e0\u6807\u7b7e\u7684\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u6570\u636e\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34Non-IID\u6570\u636e\u7684\u6311\u6218\uff1a\u5982\u4f55\u878d\u5408\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5ba2\u6237\u7aef\u7684\u6a21\u5f0f\u77e5\u8bc6\uff08\u96c6\u7fa4\u5206\u5e03\uff09\u3001\u5ba2\u6237\u7aef\u95f4\u96c6\u7fa4\u5206\u5e03\u7684\u5173\u7cfb\u5982\u4f55\u3001\u4ee5\u53ca\u8fd9\u79cd\u5173\u7cfb\u5982\u4f55\u4e0e\u5168\u5c40\u77e5\u8bc6\u878d\u5408\u76f8\u5173\u8054\u3002\u672c\u6587\u63ed\u793a\u4e86Non-IID\u4e2d\u4e00\u4e2a\u88ab\u5ffd\u89c6\u4f46\u66f4\u68d8\u624b\u7684\u95ee\u9898\uff1a\u4e0d\u540c\u5ba2\u6237\u7aef\u53ef\u80fd\u5c06\u4e00\u4e2a\u96c6\u7fa4\u788e\u7247\u5316\uff0c\u5e76\u7531\u6b64\u63d0\u51fa\u4e86\u66f4\u5e7f\u4e49\u7684Non-ICD\u6982\u5ff5\u3002", "method": "\u63d0\u51faGOLD\u6846\u67b6\uff1a1\uff09\u7cbe\u7ec6\u63a2\u7d22\u5ba2\u6237\u7aef\u6f5c\u5728\u7684\u4e0d\u5b8c\u6574\u5c40\u90e8\u96c6\u7fa4\u5206\u5e03\uff1b2\uff09\u5c06\u5206\u5e03\u6458\u8981\u4e0a\u4f20\u5230\u670d\u52a1\u5668\u8fdb\u884c\u5168\u5c40\u878d\u5408\uff1b3\uff09\u5728\u5168\u5c40\u5206\u5e03\u6307\u5bfc\u4e0b\u8fdb\u884c\u5c40\u90e8\u96c6\u7fa4\u589e\u5f3a\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5168\u5c40\u5bfc\u5411\u7684\u5c40\u90e8\u5206\u5e03\u5b66\u4e60\u89e3\u51b3\u96c6\u7fa4\u788e\u7247\u5316\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1GOLD\u7684\u4f18\u8d8a\u6027\uff0c\u5305\u62ec\u663e\u8457\u6027\u68c0\u9a8c\u3001\u6d88\u878d\u7814\u7a76\u3001\u53ef\u6269\u5c55\u6027\u8bc4\u4f30\u3001\u5b9a\u6027\u7ed3\u679c\u7b49\uff0c\u8868\u660eGOLD\u5728\u8054\u90a6\u805a\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "GOLD\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u805a\u7c7b\u4e2dNon-IID\u6570\u636e\u5bfc\u81f4\u7684\u96c6\u7fa4\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u878d\u5408\u548c\u5c40\u90e8\u589e\u5f3a\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u7684\u805a\u7c7b\u6027\u80fd\u3002"}}
{"id": "2601.17563", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17563", "abs": "https://arxiv.org/abs/2601.17563", "authors": ["Nathan Gavenski", "Matteo Leonetti", "Odinaldo Rodrigues"], "title": "Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment", "comment": "The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026)", "summary": "State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.", "AI": {"tldr": "\u63d0\u51faUfO\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65e0\u76d1\u7763\u5b66\u4e60\u4ece\u89c2\u5bdf\u4e2d\u6a21\u4eff\uff0c\u65e0\u9700\u52a8\u4f5c\u76d1\u7763\uff0c\u514b\u670d\u73b0\u6709ILfO\u65b9\u6cd5\u7684\u591a\u91cd\u9650\u5236", "motivation": "\u73b0\u6709\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5(ILfO)\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u9700\u8981\u57fa\u4e8e\u52a8\u4f5c\u7684\u76d1\u7763\u4f18\u5316\u3001\u5047\u8bbe\u72b6\u6001\u6709\u5355\u4e00\u6700\u4f18\u52a8\u4f5c\u3001\u503e\u5411\u4e8e\u673a\u68b0\u5e94\u7528\u6559\u5e08\u52a8\u4f5c\u800c\u4e0d\u5145\u5206\u8003\u8651\u5b9e\u9645\u73af\u5883\u72b6\u6001\u3002\u867d\u7136\u771f\u5b9e\u4fe1\u606f\u5b58\u5728\u4e8e\u89c2\u5bdf\u8f68\u8ff9\u4e2d\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u65e0\u76d1\u7763\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u53d6\u3002", "method": "\u63d0\u51fa\u65e0\u76d1\u7763\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60(UfO)\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\uff1a1) \u4ece\u89c2\u5bdf\u72b6\u6001\u8f6c\u79fb\u4e2d\u8fd1\u4f3c\u6559\u5e08\u771f\u5b9e\u52a8\u4f5c\uff1b2) \u901a\u8fc7\u8c03\u6574\u667a\u80fd\u4f53\u8f68\u8ff9\u4f7f\u5176\u4e0e\u6559\u5e08\u8f68\u8ff9\u7d27\u5bc6\u5bf9\u9f50\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u73af\u5883\u4e2d\u5b9e\u9a8c\u8868\u660e\uff0cUfO\u4e0d\u4ec5\u8d85\u8d8a\u6559\u5e08\u548c\u6240\u6709\u5176\u4ed6ILfO\u65b9\u6cd5\uff0c\u8fd8\u663e\u793a\u51fa\u6700\u5c0f\u7684\u6807\u51c6\u5dee\u3002\u6807\u51c6\u5dee\u51cf\u5c0f\u8868\u660e\u5728\u672a\u89c1\u573a\u666f\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UfO\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709ILfO\u65b9\u6cd5\u7684\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60\uff0c\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.18678", "categories": ["cs.LG", "cs.CV", "cs.HC", "math.DG"], "pdf": "https://arxiv.org/pdf/2601.18678", "abs": "https://arxiv.org/abs/2601.18678", "authors": ["Eslam Zaher", "Maciej Trzaskowski", "Quan Nguyen", "Fred Roosta"], "title": "Counterfactual Explanations on Robust Perceptual Geodesics", "comment": "Accepted at ICLR 2026", "summary": "Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.", "AI": {"tldr": "PCG\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u9ece\u66fc\u5ea6\u91cf\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6d4b\u5730\u7ebf\u751f\u6210\u8bed\u4e49\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u8ddd\u79bb\u5ea6\u91cf\u9009\u62e9\u4e0d\u5f53\u5bfc\u81f4\u7684\u5bf9\u6297\u6027\u6270\u52a8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u8ddd\u79bb\u5ea6\u91cf\u9009\u62e9\u6a21\u7cca\u7684\u95ee\u9898\uff1a\u5e73\u5766\u6216\u672a\u5bf9\u9f50\u7684\u51e0\u4f55\u7ed3\u6784\u4f1a\u5bfc\u81f4\u79bb\u6d41\u5f62\u4f2a\u5f71\u3001\u8bed\u4e49\u6f02\u79fb\u6216\u5bf9\u6297\u6027\u5d29\u6e83\uff0c\u65e0\u6cd5\u751f\u6210\u6709\u610f\u4e49\u7684\u8bed\u4e49\u6270\u52a8\u3002", "method": "\u63d0\u51fa\u611f\u77e5\u53cd\u4e8b\u5b9e\u6d4b\u5730\u7ebf\uff08PCG\uff09\uff0c\u5728\u9c81\u68d2\u89c6\u89c9\u7279\u5f81\u8bf1\u5bfc\u7684\u611f\u77e5\u9ece\u66fc\u5ea6\u91cf\u4e0b\u6784\u9020\u53cd\u4e8b\u5b9e\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6d4b\u5730\u7ebf\u5b9e\u73b0\u5e73\u6ed1\u3001\u5728\u6d41\u5f62\u4e0a\u3001\u8bed\u4e49\u6709\u6548\u7684\u8fc7\u6e21\u3002", "result": "\u5728\u4e09\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPCG\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u6807\u51c6\u5ea6\u91cf\u4e0b\u9690\u85cf\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "PCG\u901a\u8fc7\u611f\u77e5\u5bf9\u9f50\u7684\u9ece\u66fc\u51e0\u4f55\u89e3\u51b3\u4e86\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4e2d\u7684\u8ddd\u79bb\u5ea6\u91cf\u6a21\u7cca\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u8bed\u4e49\u6709\u6548\u4e14\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002"}}
{"id": "2601.17602", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17602", "abs": "https://arxiv.org/abs/2601.17602", "authors": ["Xuanzhou Chen"], "title": "Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout", "comment": null, "summary": "We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u9ad8\u7ef4\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5d4c\u5165\u7684\u89d2\u5ea6\u76f8\u4f3c\u6027\u5206\u6790Transformer\u8fc7\u53c2\u6570\u5316\uff0c\u4f7f\u7528\u4f2f\u52aa\u5229dropout\u8bc6\u522b\u4fdd\u6301Top-1\u9884\u6d4b\u7684\u7a00\u758f\u6027\u9608\u503c\uff0c\u7406\u8bba\u8bc1\u660e\u8db3\u591f\u5927\u7684\u6709\u6548\u7a00\u758f\u6027\u4e0b\u6027\u80fd\u7a33\u5b9a\uff0c\u5b9e\u9a8c\u901a\u8fc7BEC\u589e\u5f3a\u7684Transformer\u5728\u82f1\u6cd5\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6027\u80fd\u5728\u9608\u503c\u5904\u6025\u5267\u4e0b\u964d\u3002", "motivation": "\u7814\u7a76Transformer\u6a21\u578b\u7684\u8fc7\u53c2\u6570\u5316\u95ee\u9898\uff0c\u63a2\u7d22\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5d4c\u5165\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u89d2\u5ea6\u76f8\u4f3c\u6027\uff0c\u65e8\u5728\u7406\u89e3\u6a21\u578b\u5728\u53c2\u6570\u5197\u4f59\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u7a33\u5b9a\u6027\u3002", "method": "1. \u4f7f\u7528\u4f2f\u52aa\u5229dropout\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e4b\u95f4\uff0c\u901a\u8fc7\u6539\u53d8\u4fdd\u7559\u6982\u7387p\u6765\u8bc6\u522b\u7a00\u758f\u6027\u4f9d\u8d56\u7684\u9608\u503c\uff1b2. \u7406\u8bba\u8bc1\u660e\u5f53\u6709\u6548\u7a00\u758f\u6027\u8db3\u591f\u5927\u65f6\uff0c\u5d4c\u5165\u8868\u793a\u548c\u89e3\u7801\u5668\u6027\u80fd\u5728\u9002\u5ea6\u5750\u6807dropout\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff1b3. \u5b9e\u9a8c\u6784\u5efa\u5e26\u6709\u4e8c\u8fdb\u5236\u64e6\u9664\u901a\u9053(BEC)\u7684\u589e\u5f3aTransformer\u6a21\u578b\uff0c\u5728\u82f1\u6cd5\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u6027\u80fd\u3002", "result": "1. \u8bc6\u522b\u51fa\u7a00\u758f\u6027\u4f9d\u8d56\u7684\u9608\u503c\uff0c\u8d85\u8fc7\u8be5\u9608\u503cTop-1\u9884\u6d4b\u5f97\u4ee5\u4fdd\u6301\uff1b2. \u9a8c\u8bc1\u51c6\u786e\u7387\u548cBLEU\u5206\u6570\u5728\u67d0\u4e2a\u9608\u503c\u5904\u6025\u5267\u4e0b\u964d\uff1b3. \u5b9e\u9a8c\u53ef\u89c6\u5316\u663e\u793a\u4e86\u6027\u80fd\u968f\u7a00\u758f\u6027\u53d8\u5316\u7684\u8d8b\u52bf\u3002", "conclusion": "Transformer\u6a21\u578b\u5728\u8fc7\u53c2\u6570\u5316\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u5bf9\u9002\u5ea6dropout\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5b58\u5728\u4e34\u754c\u7a00\u758f\u6027\u9608\u503c\uff0c\u8d85\u8fc7\u8be5\u9608\u503c\u6027\u80fd\u4f1a\u6025\u5267\u4e0b\u964d\uff0c\u8fd9\u4e3a\u7406\u89e3Transformer\u7684\u5197\u4f59\u6027\u548c\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2601.17607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17607", "abs": "https://arxiv.org/abs/2601.17607", "authors": ["Daisuke Okanohara"], "title": "A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs", "comment": "9 pages. Part I of a planned series entitled \"A Thermodynamic Theory of Learning.\"", "summary": "Learning systems acquire structured internal representations from data, yet classical information-theoretic results state that deterministic transformations do not increase information. This raises a fundamental question: how can learning produce abstraction and insight without violating information-theoretic limits?\n  We argue that learning is inherently an irreversible process when performed over finite time, and that the realization of epistemic structure necessarily incurs entropy production. To formalize this perspective, we model learning as a transport process in the space of probability distributions over model configurations and introduce an epistemic free-energy framework.\n  Within this framework, we define the free-energy drop as a bookkeeping quantity that records the total reduction of epistemic free energy along a learning trajectory. This reduction decomposes into a reversible component associated with potential improvement and an irreversible component corresponding to entropy production.\n  We then derive the Epistemic Speed Limit (ESL), a finite-time inequality that lower-bounds the minimal entropy production required by any learning process to realize a given distributional transformation. This bound depends only on the Wasserstein distance between initial and final ensemble distributions and is independent of the specific learning algorithm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba4\u8bc6\u8bba\u81ea\u7531\u80fd\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u5efa\u6a21\u4e3a\u6982\u7387\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u4f20\u8f93\u8fc7\u7a0b\uff0c\u5e76\u63a8\u5bfc\u51fa\u8ba4\u8bc6\u8bba\u901f\u5ea6\u6781\u9650\uff08ESL\uff09\uff0c\u8be5\u6781\u9650\u4e3a\u4efb\u4f55\u5b66\u4e60\u8fc7\u7a0b\u5b9e\u73b0\u7ed9\u5b9a\u5206\u5e03\u53d8\u6362\u6240\u9700\u7684\u6700\u5c0f\u71b5\u4ea7\u751f\u63d0\u4f9b\u4e86\u4e0b\u754c\u3002", "motivation": "\u7ecf\u5178\u4fe1\u606f\u8bba\u7ed3\u679c\u8868\u660e\u786e\u5b9a\u6027\u53d8\u6362\u4e0d\u4f1a\u589e\u52a0\u4fe1\u606f\uff0c\u8fd9\u4e0e\u5b66\u4e60\u7cfb\u7edf\u4ece\u6570\u636e\u4e2d\u83b7\u5f97\u7ed3\u6784\u5316\u5185\u90e8\u8868\u793a\u7684\u73b0\u8c61\u76f8\u77db\u76fe\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u57fa\u672c\u95ee\u9898\uff1a\u5b66\u4e60\u5982\u4f55\u5728\u6709\u9650\u65f6\u95f4\u5185\u4ea7\u751f\u62bd\u8c61\u548c\u6d1e\u5bdf\u800c\u4e0d\u8fdd\u53cd\u4fe1\u606f\u8bba\u9650\u5236\u3002", "method": "\u5c06\u5b66\u4e60\u5efa\u6a21\u4e3a\u6a21\u578b\u914d\u7f6e\u6982\u7387\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u4f20\u8f93\u8fc7\u7a0b\uff0c\u5f15\u5165\u8ba4\u8bc6\u8bba\u81ea\u7531\u80fd\u6846\u67b6\u3002\u5728\u8be5\u6846\u67b6\u4e2d\u5b9a\u4e49\u81ea\u7531\u80fd\u4e0b\u964d\u4f5c\u4e3a\u8bb0\u5f55\u5b66\u4e60\u8f68\u8ff9\u4e0a\u8ba4\u8bc6\u8bba\u81ea\u7531\u80fd\u603b\u51cf\u5c11\u7684\u8bb0\u8d26\u91cf\uff0c\u5e76\u5c06\u5176\u5206\u89e3\u4e3a\u4e0e\u6f5c\u5728\u6539\u8fdb\u76f8\u5173\u7684\u53ef\u9006\u5206\u91cf\u548c\u4e0e\u71b5\u4ea7\u751f\u5bf9\u5e94\u7684\u4e0d\u53ef\u9006\u5206\u91cf\u3002", "result": "\u63a8\u5bfc\u51fa\u8ba4\u8bc6\u8bba\u901f\u5ea6\u6781\u9650\uff08ESL\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u6709\u9650\u65f6\u95f4\u4e0d\u7b49\u5f0f\uff0c\u4e3a\u4efb\u4f55\u5b66\u4e60\u8fc7\u7a0b\u5b9e\u73b0\u7ed9\u5b9a\u5206\u5e03\u53d8\u6362\u6240\u9700\u7684\u6700\u5c0f\u71b5\u4ea7\u751f\u63d0\u4f9b\u4e86\u4e0b\u754c\u3002\u8be5\u754c\u9650\u4ec5\u53d6\u51b3\u4e8e\u521d\u59cb\u548c\u6700\u7ec8\u96c6\u5408\u5206\u5e03\u4e4b\u95f4\u7684Wasserstein\u8ddd\u79bb\uff0c\u4e14\u72ec\u7acb\u4e8e\u5177\u4f53\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u5b66\u4e60\u5728\u6709\u9650\u65f6\u95f4\u5185\u672c\u8d28\u4e0a\u662f\u4e0d\u53ef\u9006\u8fc7\u7a0b\uff0c\u8ba4\u8bc6\u8bba\u7ed3\u6784\u7684\u5b9e\u73b0\u5fc5\u7136\u4f34\u968f\u71b5\u4ea7\u751f\u3002\u8ba4\u8bc6\u8bba\u901f\u5ea6\u6781\u9650\u4e3a\u5b66\u4e60\u8fc7\u7a0b\u7684\u57fa\u672c\u9650\u5236\u63d0\u4f9b\u4e86\u91cf\u5316\u6846\u67b6\uff0c\u8868\u660e\u5b66\u4e60\u6548\u7387\u53d7\u5230\u5206\u5e03\u53d8\u6362\u51e0\u4f55\u8ddd\u79bb\u7684\u7ea6\u675f\u3002"}}
{"id": "2601.17625", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17625", "abs": "https://arxiv.org/abs/2601.17625", "authors": ["Yuhan Xie", "Jinhan Liu", "Xiaoyong Ni", "Fei Tan", "Icare Sakr", "Thibault Collin", "Shiqi Sun", "Alejandro Rodriguez Guajardo", "Demon Fanny", "Charles-francois Vincent Latchoumane", "Henri Lorach", "Jocelyne Bloch", "Gregoire Courtine", "Mahsa Shoaran"], "title": "BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation", "comment": "21 pages,7 figures", "summary": "Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.", "AI": {"tldr": "BrainDistill\uff1a\u4e00\u79cd\u65b0\u578b\u690d\u5165\u5f0f\u8fd0\u52a8\u89e3\u7801\u7ba1\u9053\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u84b8\u998f\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u529f\u7387\u53d7\u9650\u7684\u690d\u5165\u5f0f\u7cfb\u7edf\u3002", "motivation": "Transformer\u57fa\u795e\u7ecf\u89e3\u7801\u5668\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u53c2\u6570\u91cf\u5927\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u529f\u7387\u53d7\u9650\u7684\u690d\u5165\u5f0f\u7cfb\u7edf\u4e2d\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u80fd\u6ee1\u8db3\u690d\u5165\u5f0f\u7cfb\u7edf\u4e25\u683c\u529f\u7387\u7ea6\u675f\u7684\u89e3\u7801\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBrainDistill\u7ba1\u9053\uff0c\u5305\u542b\u690d\u5165\u5f0f\u795e\u7ecf\u89e3\u7801\u5668\uff08IND\uff09\u548c\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u84b8\u998f\uff08TSKD\uff09\u6846\u67b6\u3002TSKD\u901a\u8fc7\u76d1\u7763\u6295\u5f71\u4f18\u5148\u63d0\u53d6\u89e3\u7801\u5173\u952e\u7279\u5f81\uff0c\u800c\u975e\u5b8c\u6574\u4fdd\u7559\u6559\u5e08\u8868\u5f81\u3002\u540c\u65f6\u5f15\u5165\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6848\uff0c\u652f\u6301\u4ec5\u6574\u6570\u63a8\u7406\u5e76\u5b66\u4e60\u6fc0\u6d3b\u88c1\u526a\u8303\u56f4\u3002", "result": "IND\u5728\u591a\u4e2a\u795e\u7ecf\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u795e\u7ecf\u89e3\u7801\u5668\uff1b\u5176TSKD\u84b8\u998f\u53d8\u4f53\u5728\u5c11\u6837\u672c\u6821\u51c6\u8bbe\u7f6e\u4e2d\u8d85\u8d8a\u5176\u4ed6\u84b8\u998f\u65b9\u6cd5\uff1b\u91cf\u5316\u540e\u7684IND\u5728\u4e25\u683c\u529f\u7387\u7ea6\u675f\u4e0b\u90e8\u7f72\u65f6\u6027\u80fd\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "BrainDistill\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u84b8\u998f\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u6210\u529f\u89e3\u51b3\u4e86Transformer\u57fa\u89e3\u7801\u5668\u5728\u690d\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e0e\u4f4e\u529f\u8017\u7684\u5e73\u8861\uff0c\u4e3a\u690d\u5165\u5f0f\u8111\u673a\u63a5\u53e3\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.17641", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17641", "abs": "https://arxiv.org/abs/2601.17641", "authors": ["Hao Fang", "Ryan A. Canfield", "Tomohiro Ouchi", "Beatrice Macagno", "Eli Shlizerman", "Amy L. Orsborn"], "title": "RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding", "comment": null, "summary": "Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.", "AI": {"tldr": "RPNT\uff08\u9c81\u68d2\u9884\u8bad\u7ec3\u795e\u7ecfTransformer\uff09\u662f\u4e00\u79cd\u7528\u4e8e\u8111\u89e3\u7801\u7684\u9884\u8bad\u7ec3Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u7ef4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u3001\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u9c81\u68d2\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\uff0c\u5b9e\u73b0\u5728\u8de8\u4f1a\u8bdd\u3001\u8de8\u7c7b\u578b\u3001\u8de8\u88ab\u8bd5\u548c\u8de8\u8111\u533a\u7684\u884c\u4e3a\u89e3\u7801\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8111\u89e3\u7801\u9700\u8981\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u8111\u533a\u8bb0\u5f55\u3001\u4e0d\u540c\u4f1a\u8bdd\u3001\u4e0d\u540c\u884c\u4e3a\u7c7b\u578b\u548c\u4e0d\u540c\u88ab\u8bd5\u3002\u73b0\u6709\u6a21\u578b\u53ea\u80fd\u90e8\u5206\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u548c\u6cdb\u5316\u7684\u9884\u8bad\u7ec3\u795e\u7ecfTransformer\u6a21\u578b\u3002", "method": "RPNT\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u591a\u7ef4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08MRoPE\uff09\u7528\u4e8e\u805a\u5408\u5b9e\u9a8c\u5143\u6570\u636e\uff08\u5982\u8111\u533a\u5750\u6807\u3001\u4f1a\u8bdd\u540d\u79f0\u548c\u884c\u4e3a\u7c7b\u578b\uff09\uff1b2\uff09\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5377\u79ef\u6838\u5728\u5168\u5c40\u6ce8\u610f\u529b\u4e0a\u64cd\u4f5c\uff0c\u5b66\u4e60\u5c40\u90e8\u65f6\u95f4\u7ed3\u6784\u4ee5\u5904\u7406\u795e\u7ecf\u7fa4\u4f53\u6d3b\u52a8\u7684\u975e\u5e73\u7a33\u6027\uff1b3\uff09\u5177\u6709\u5747\u5300\u56e0\u679c\u63a9\u7801\u7b56\u7565\u548c\u5bf9\u6bd4\u8868\u793a\u7684\u9c81\u68d2\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u3002\u5728\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002", "result": "RPNT\u5728\u8de8\u4f1a\u8bdd\u3001\u8de8\u7c7b\u578b\u3001\u8de8\u88ab\u8bd5\u548c\u8de8\u8111\u533a\u7684\u4e0b\u6e38\u884c\u4e3a\u89e3\u7801\u4efb\u52a1\u4e2d\uff0c\u59cb\u7ec8\u8fbe\u5230\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u89e3\u7801\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "RPNT\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u8111\u89e3\u7801\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u795e\u7ecf\u6d3b\u52a8\u5230\u884c\u4e3a\u7684\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17646", "categories": ["cs.LG", "math.FA", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.17646", "abs": "https://arxiv.org/abs/2601.17646", "authors": ["Karim Bounja", "Lahcen Laayouni", "Abdeljalil Sakat"], "title": "A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization", "comment": null, "summary": "Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlev\u00e9-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u51f8\u975e\u4e25\u683c\u635f\u5931\u51fd\u6570\u4ea7\u751f\u7684\u96c6\u503c\u6781\u5c0f\u5316\u5668\u3002\u4f5c\u8005\u63d0\u51faPainlev\u00e9-Kuratowski\u4e0a\u534a\u8fde\u7eed\u6027\uff08PK-u.s.c.\uff09\u662fERM\u89e3\u5bf9\u5e94\u5173\u7cfb\u7684\u56fa\u6709\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u5e76\u5efa\u7acb\u4e86\u5728Mosco\u4e00\u81f4\u6270\u52a8\u548c\u5c40\u90e8\u6709\u754c\u6781\u5c0f\u5316\u5668\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u7406\u8bba\u3002", "motivation": "\u4f20\u7edfERM\u7a33\u5b9a\u6027\u7814\u7a76\u901a\u5e38\u57fa\u4e8e\u5355\u503c\u8f93\u51fa\uff0c\u4f46\u51f8\u975e\u4e25\u683c\u635f\u5931\u51fd\u6570\u4f1a\u4ea7\u751f\u96c6\u503c\u6781\u5c0f\u5316\u5668\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8fd9\u79cd\u96c6\u503c\u89e3\u5bf9\u5e94\u5173\u7cfb\u7684\u7a33\u5b9a\u6027\u5206\u6790\u6846\u67b6\uff0c\u9700\u8981\u5efa\u7acb\u5185\u5728\u7684\u7a33\u5b9a\u6027\u6982\u5ff5\u6765\u7406\u89e3\u89e3\u96c6\u548c\u9009\u62e9\u51fd\u6570\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u96c6\u503c\u5206\u6790\u6846\u67b6\uff0c\u5c06Painlev\u00e9-Kuratowski\u4e0a\u534a\u8fde\u7eed\u6027\uff08PK-u.s.c.\uff09\u5b9a\u4e49\u4e3aERM\u89e3\u5bf9\u5e94\u5173\u7cfb\u7684\u56fa\u6709\u7a33\u5b9a\u6027\u6982\u5ff5\uff08\u96c6\u6c34\u5e73Hadamard\u9002\u5b9a\u6027\uff09\u3002\u5728\u6700\u5c0f\u975e\u9000\u5316\u5b9a\u6027\u673a\u5236\u4e0b\uff0c\u8bc1\u660e\u4e86Mosco\u4e00\u81f4\u6270\u52a8\u548c\u5c40\u90e8\u6709\u754c\u6781\u5c0f\u5316\u5668\u6761\u4ef6\u8574\u542bPK-u.s.c.\u3001\u6781\u5c0f\u503c\u8fde\u7eed\u6027\u548c\u6d88\u5931\u95f4\u9699\u8fd1\u6781\u5c0f\u5316\u5668\u7684\u4e00\u81f4\u6027\u3002\u5bf9\u4e8e\u4e8c\u6b21\u589e\u957f\u60c5\u51b5\uff0c\u63a8\u5bfc\u4e86\u663e\u5f0f\u7684\u5b9a\u91cf\u504f\u5dee\u754c\u3002", "result": "\u5efa\u7acb\u4e86ERM\u89e3\u5bf9\u5e94\u5173\u7cfb\u7684\u7a33\u5b9a\u6027\u7406\u8bba\uff1aPK-u.s.c.\u662f\u96c6\u503c\u89e3\u5185\u5728\u7684\u7a33\u5b9a\u6027\u6982\u5ff5\uff1b\u5728Mosco\u4e00\u81f4\u6270\u52a8\u548c\u5c40\u90e8\u6709\u754c\u6781\u5c0f\u5316\u5668\u6761\u4ef6\u4e0b\uff0c\u89e3\u5bf9\u5e94\u5173\u7cfb\u5177\u6709PK-u.s.c.\u6027\u8d28\uff0c\u6781\u5c0f\u503c\u8fde\u7eed\uff0c\u4e14\u6d88\u5931\u95f4\u9699\u8fd1\u6781\u5c0f\u5316\u5668\u5177\u6709\u4e00\u81f4\u6027\uff1b\u4e8c\u6b21\u589e\u957f\u6761\u4ef6\u53ef\u5bfc\u51fa\u663e\u5f0f\u7684\u5b9a\u91cf\u504f\u5dee\u754c\u3002", "conclusion": "PK-u.s.c.\u662fERM\u96c6\u503c\u89e3\u5bf9\u5e94\u5173\u7cfb\u7684\u6b63\u786e\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u4e3a\u7406\u89e3\u51f8\u975e\u4e25\u683c\u635f\u5931\u51fd\u6570\u7684ERM\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002\u5728\u9002\u5f53\u7684\u5b9a\u6027\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u83b7\u5f97\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u800c\u4e8c\u6b21\u589e\u957f\u6761\u4ef6\u8fdb\u4e00\u6b65\u63d0\u4f9b\u4e86\u5b9a\u91cf\u63a7\u5236\u3002\u8fd9\u4e00\u6846\u67b6\u4e3a\u5206\u6790\u673a\u5668\u5b66\u4e60\u4e2d\u96c6\u503c\u89e3\u7684\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\u3002"}}
{"id": "2601.17647", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17647", "abs": "https://arxiv.org/abs/2601.17647", "authors": ["Akila Sampath", "Vandana Janeja", "Jianwu Wang"], "title": "Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics", "comment": null, "summary": "Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\\% reduction in estimation error.", "AI": {"tldr": "\u63d0\u51faKGCM-VAE\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u5f15\u5bfc\u7684\u56e0\u679c\u5efa\u6a21\u91cf\u5316\u6d77\u51b0\u539a\u5ea6\u4e0e\u6d77\u8868\u9762\u9ad8\u5ea6\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u548c\u5206\u5e03\u5e73\u8861\u673a\u5236\u63d0\u5347\u5904\u7406\u6548\u5e94\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u91cf\u5316\u51b0\u878d\u5316\u548c\u6de1\u6c34\u5206\u5e03\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u5bf9\u7406\u89e3\u6781\u5730\u6c14\u5019\u53d8\u5316\u548c\u5168\u7403\u6d77\u5e73\u9762\u4e0a\u5347\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u7a7a\u573a\u666f\u4e2d\u56e0\u672a\u89c2\u6d4b\u6df7\u6742\u53d8\u91cf\u548c\u7f3a\u4e4f\u7269\u7406\u7ea6\u675f\u800c\u96be\u4ee5\u53ef\u9760\u4f30\u8ba1\u5904\u7406\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u5f15\u5bfc\u7684\u56e0\u679c\u6a21\u578b\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08KGCM-VAE\uff09\uff0c\u5305\u542b\uff1a1\uff09\u901f\u5ea6\u8c03\u5236\u65b9\u6848\uff0c\u901a\u8fc7SSH\u8f6c\u6362\u63a7\u5236\u7684sigmoid\u51fd\u6570\u52a8\u6001\u653e\u5927\u5e73\u6ed1\u901f\u5ea6\u4fe1\u53f7\u751f\u6210\u7269\u7406\u57fa\u7840\u7684\u5904\u7406\uff1b2\uff09\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u5e73\u8861\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5904\u7406\u548c\u5bf9\u7167\u534f\u53d8\u91cf\u5206\u5e03\uff1b3\uff09\u56e0\u679c\u90bb\u63a5\u7ea6\u675f\u89e3\u7801\u5668\u786e\u4fdd\u4e0e\u5df2\u77e5\u7269\u7406\u7ed3\u6784\u5bf9\u9f50\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5317\u6781\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKGCM-VAE\u5728PEHE\u6307\u6807\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u51c6\u6a21\u578b\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0cMMD\u548c\u56e0\u679c\u90bb\u63a5\u7ea6\u675f\u7684\u8054\u5408\u5e94\u7528\u4f7f\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e1.88%\u3002", "conclusion": "KGCM-VAE\u901a\u8fc7\u6574\u5408\u7269\u7406\u77e5\u8bc6\u548c\u56e0\u679c\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u7a7a\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u6df7\u6742\u53d8\u91cf\u548c\u7269\u7406\u4e00\u81f4\u6027\u6311\u6218\uff0c\u4e3a\u91cf\u5316\u6d77\u51b0\u539a\u5ea6\u4e0e\u6d77\u8868\u9762\u9ad8\u5ea6\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\u3002"}}
{"id": "2601.17654", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17654", "abs": "https://arxiv.org/abs/2601.17654", "authors": ["Ruofan Wu", "Jae-Won Chung", "Mosharaf Chowdhury"], "title": "Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training", "comment": null, "summary": "The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.\n  We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.", "AI": {"tldr": "Kareus\uff1a\u4e00\u79cd\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7ec6\u7c92\u5ea6\u5185\u6838\u8c03\u5ea6\u548c\u9891\u7387\u7f29\u653e\u6765\u63a8\u52a8\u65f6\u95f4-\u80fd\u8017\u6743\u8861\u524d\u6cbf\u7684AI\u8bad\u7ec3\u7cfb\u7edf", "motivation": "AI\u8ba1\u7b97\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u4f46\u80fd\u6e90\u4f9b\u5e94\u8ddf\u4e0d\u4e0a\uff0c\u80fd\u6e90\u5df2\u6210\u4e3a\u6602\u8d35\u4e14\u7ade\u4e89\u6fc0\u70c8\u7684\u8d44\u6e90\u3002\u73b0\u6709\u5de5\u4f5c\u4ec5\u5173\u6ce8\u52a8\u6001\u6216\u9759\u6001\u80fd\u8017\u7684\u5355\u4e00\u65b9\u9762\uff0c\u800c\u7ec6\u7c92\u5ea6\u5185\u6838\u8c03\u5ea6\u548c\u9891\u7387\u7f29\u653e\u4f1a\u8054\u5408\u4e14\u76f8\u4e92\u4f9d\u8d56\u5730\u5f71\u54cd\u8fd9\u4e24\u7c7b\u80fd\u8017\u3002", "method": "\u8bbe\u8ba1Kareus\u8bad\u7ec3\u7cfb\u7edf\uff0c\u5c06\u96be\u5904\u7406\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u57fa\u4e8e\u5206\u533a\u7684\u5c40\u90e8\u5b50\u95ee\u9898\uff0c\u4f7f\u7528\u591a\u901a\u9053\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u5bfb\u627e\u63a8\u52a8\u65f6\u95f4-\u80fd\u8017\u6743\u8861\u524d\u6cbf\u7684\u6267\u884c\u8c03\u5ea6\u65b9\u6848\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0cKareus\u5728\u76f8\u540c\u8bad\u7ec3\u65f6\u95f4\u4e0b\u51cf\u5c11\u8bad\u7ec3\u80fd\u8017\u9ad8\u8fbe28.3%\uff0c\u6216\u5728\u76f8\u540c\u80fd\u8017\u4e0b\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u9ad8\u8fbe27.5%\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5185\u6838\u8c03\u5ea6\u548c\u9891\u7387\u7f29\u653e\u6765\u7ba1\u7406\u52a8\u6001\u548c\u9759\u6001\u80fd\u8017\uff0c\u80fd\u591f\u663e\u8457\u63a8\u8fdbAI\u8bad\u7ec3\u7684\u65f6\u95f4-\u80fd\u8017\u6743\u8861\u524d\u6cbf\uff0c\u4e3a\u80fd\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6548AI\u8bad\u7ec3\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17667", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17667", "abs": "https://arxiv.org/abs/2601.17667", "authors": ["Pedro P. Santos", "Jacopo Silvestrin", "Alberto Sardinha", "Francisco S. Melo"], "title": "Entropic Risk-Aware Monte Carlo Tree Search", "comment": null, "summary": "We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \\textit{risk-aware} Markov decision processes (MDPs) with \\textit{entropic risk measure} (ERM) objectives. We provide a \\textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \\textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \\textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u8bc1\u660e\u6b63\u786e\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u6c42\u89e3\u5177\u6709\u71b5\u98ce\u9669\u5ea6\u91cf\u76ee\u6807\u7684\u98ce\u9669\u611f\u77e5\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u63d0\u4f9b\u975e\u6e10\u8fd1\u5206\u6790\u8bc1\u660e\u5176\u6b63\u786e\u6027\u548c\u591a\u9879\u5f0f\u9057\u61be\u96c6\u4e2d\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u98ce\u9669\u611f\u77e5MDP\u65f6\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff0c\u7279\u522b\u662f\u9488\u5bf9\u71b5\u98ce\u9669\u5ea6\u91cf\u76ee\u6807\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\u3002\u9700\u8981\u5f00\u53d1\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e0a\u7f6e\u4fe1\u754c\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\uff0c\u5229\u7528\u5148\u524d\u5de5\u4f5c\u4e2d\u5f15\u5165\u7684\u52a8\u6001\u89c4\u5212\u516c\u5f0f\u6765\u6c42\u89e3\u5177\u6709\u71b5\u98ce\u9669\u5ea6\u91cf\u76ee\u6807\u7684\u98ce\u9669\u611f\u77e5MDP\u3002\u7b97\u6cd5\u7ed3\u5408\u4e86\u6811\u641c\u7d22\u548c\u98ce\u9669\u611f\u77e5\u7684\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u3002", "result": "\u7b97\u6cd5\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff1a1\uff09\u6b63\u786e\u6027\uff1a\u6839\u8282\u70b9\u7684\u7ecf\u9a8c\u71b5\u98ce\u9669\u5ea6\u91cf\u6536\u655b\u5230\u6700\u4f18\u71b5\u98ce\u9669\u5ea6\u91cf\uff1b2\uff09\u591a\u9879\u5f0f\u9057\u61be\u96c6\u4e2d\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u9996\u4e2a\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\u7528\u4e8e\u98ce\u9669\u611f\u77e5MDP\uff0c\u4e3a\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.17668", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17668", "abs": "https://arxiv.org/abs/2601.17668", "authors": ["Jang-Hyun Kim", "Dongyoon Han", "Sangdoo Yun"], "title": "Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction", "comment": null, "summary": "Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7\u7684KV\u7f13\u5b58\u9a71\u9010\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7sink-attention\u95e8\u63a7\u6a21\u5757\u8bc6\u522b\u548c\u4fdd\u7559\u5173\u952eKV\u5bf9\uff0c\u5728\u51bb\u7ed3\u6743\u91cd\u7684LLMs\u4e2d\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u548c\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u538b\u7f29\u6280\u672f\u901a\u5e38\u5728\u6027\u80fd\u4e0b\u964d\u548c\u8ba1\u7b97\u5f00\u9500\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u53c8\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u65b9\u6cd5\u6765\u4fc3\u8fdbLLMs\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7sink-attention\u95e8\u63a7\u6a21\u5757\u8bc6\u522b\u5173\u952eKV\u5bf9\uff0c\u63d0\u51fa\u4ec5\u4f9d\u8d56LLM\u524d\u5411\u4f20\u64ad\u7684\u95e8\u8bad\u7ec3\u7b97\u6cd5\uff0c\u907f\u514d\u6602\u8d35\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u91c7\u7528\u4efb\u52a1\u65e0\u5173\u7684\u91cd\u5efa\u76ee\u6807\u5b9e\u73b0\u5f3a\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728Qwen2.5-1M\u3001Qwen3\u548cGemma3\u7cfb\u5217\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9a71\u9010\u9ad8\u8fbe70%\u7684KV\u7f13\u5b58\u65f6\u4ecd\u80fd\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u6027\u80fd\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u4ee3\u7801\u7406\u89e3\u548c\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u51bb\u7ed3\u6743\u91cd\u7684LLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684KV\u7f13\u5b58\u7ba1\u7406\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u538b\u7f29\u6bd4\u4e0e\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u826f\u597d\u5e73\u8861\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2601.17689", "categories": ["cs.LG", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17689", "abs": "https://arxiv.org/abs/2601.17689", "authors": ["Shanu Saklani", "Tushar M. Athawale", "Nairita Pal", "David Pugmire", "Christopher R. Johnson", "Soumya Dutta"], "title": "REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization", "comment": null, "summary": "Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.", "AI": {"tldr": "REV-INR\u662f\u4e00\u79cd\u6b63\u5219\u5316\u8bc1\u636e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u540c\u65f6\u9884\u6d4b\u6570\u636e\u503c\u548c\u5750\u6807\u7ea7\u522b\u7684\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u53ca\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u786e\u5b9a\u6027INR\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u786e\u5b9a\u6027\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INR)\u53ea\u80fd\u9884\u6d4b\u6570\u636e\u503c\uff0c\u65e0\u6cd5\u63d0\u4f9b\u6a21\u578b\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6216\u6570\u636e\u56fa\u6709\u566a\u58f0\u7684\u5f71\u54cd\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u91cd\u5efa\u4f53\u79ef\u4e2d\u7684\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u9020\u6210\u4e0d\u53ef\u9760\u7684\u6570\u636e\u89e3\u91ca\u548c\u53ef\u89c6\u5316\u3002\u7531\u4e8e\u539f\u59cb\u6570\u636e\u53ef\u80fd\u56e0\u4f53\u79ef\u8fc7\u5927\u800c\u4e0d\u53ef\u7528\uff0c\u4ece\u6a21\u578b\u9884\u6d4b\u6570\u636e\u4e2d\u8bc6\u522b\u9519\u8bef\u7ed3\u679c\u53ef\u80fd\u4e0d\u53ef\u884c\u3002", "method": "\u63d0\u51faREV-INR\uff08\u6b63\u5219\u5316\u8bc1\u636e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff09\uff0c\u8be5\u65b9\u6cd5\u5b66\u4e60\u51c6\u786e\u9884\u6d4b\u6570\u636e\u503c\uff0c\u540c\u65f6\u901a\u8fc7\u8bad\u7ec3\u540e\u7684REV-INR\u5728\u63a8\u7406\u65f6\u7684\u5355\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u5173\u8054\u5750\u6807\u7ea7\u522b\u7684\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "REV-INR\u4e0e\u73b0\u6709\u6210\u719f\u7684\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u6bd4\u8f83\u540e\uff0c\u663e\u793a\u51fa\u6700\u4f73\u7684\u4f53\u79ef\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u9c81\u68d2\u7684\u6570\u636e\uff08\u5076\u7136\u6027\uff09\u548c\u6a21\u578b\uff08\u8ba4\u77e5\u6027\uff09\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e14\u63a8\u7406\u65f6\u95f4\u6700\u5feb\u3002", "conclusion": "REV-INR\u80fd\u591f\u8bc4\u4f30\u63d0\u53d6\u7684\u7b49\u503c\u9762\u548c\u4f53\u79ef\u53ef\u89c6\u5316\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4f7f\u5f97\u5206\u6790\u53ef\u4ee5\u5b8c\u5168\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u7684\u6570\u636e\u8fdb\u884c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfINR\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.17713", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17713", "abs": "https://arxiv.org/abs/2601.17713", "authors": ["Kaile Wang", "Jiannong Cao", "Yu Yang", "Xiaoyin Li", "Yinfeng Cao"], "title": "FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices", "comment": "Accepted by IEEE Annual Congress on Artificial Intelligence of Things (IEEE AIoT) 2025", "summary": "With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.", "AI": {"tldr": "FedCCA\u662f\u4e00\u79cd\u9488\u5bf9\u7269\u8054\u7f51\u6570\u636e\u5f02\u6784\u6027\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u4e2d\u5fc3\u5316\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5229\u7528\u5ba2\u6237\u7aef\u7279\u5b9a\u77e5\u8bc6\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5b66\u4e60\u72ec\u7279\u6a21\u578b", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u7684\u6570\u636e\u5f02\u6784\u6027\u4e25\u91cd\u5f71\u54cd\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u56fa\u5b9a\u5ba2\u6237\u7aef\u9009\u62e9\u548c\u4e91\u7aef\u805a\u5408\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u5728\u672c\u5730\u8bad\u7ec3\u4e2d\u9690\u79c1\u4fdd\u62a4\u5730\u63d0\u53d6\u5ba2\u6237\u7aef\u7279\u5b9a\u4fe1\u606f", "method": "\u63d0\u51fa\u5ba2\u6237\u7aef\u4e2d\u5fc3\u5316\u81ea\u9002\u5e94\u8054\u90a6\u5b66\u4e60(FedCCA)\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u81ea\u9002\u5e94\u673a\u5236\u5229\u7528\u5ba2\u6237\u7aef\u7279\u5b9a\u77e5\u8bc6\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5b66\u4e60\u72ec\u7279\u6a21\u578b\uff0c\u91c7\u7528\u52a8\u6001\u5ba2\u6237\u7aef\u9009\u62e9\u548c\u57fa\u4e8e\u989d\u5916\u5ba2\u6237\u7aef\u7279\u5b9a\u7f16\u7801\u5668\u7684\u81ea\u9002\u5e94\u805a\u5408\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5168\u5c40\u805a\u5408\u7b56\u7565\u589e\u5f3a\u591a\u6e90\u77e5\u8bc6\u8f6c\u79fb", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFedCCA\u5728\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\u65f6\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf", "conclusion": "FedCCA\u901a\u8fc7\u5ba2\u6237\u7aef\u4e2d\u5fc3\u5316\u81ea\u9002\u5e94\u673a\u5236\u6709\u6548\u7f13\u89e3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6"}}
{"id": "2601.17716", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17716", "abs": "https://arxiv.org/abs/2601.17716", "authors": ["Daniel M. Pedrozo", "Telma W. de L. Soares", "Bryan L. M. de Oliveira"], "title": "Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games", "comment": "Presented at the NeusymBridge Workshop at AAAI 2026", "summary": "Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u91cf\u8bc4\u4f30LLMs\u901a\u8fc7\u662f/\u5426\u95ee\u9898\u5728\u5206\u5c42\u77e5\u8bc6\u56fe\u8c31\u73af\u5883\u4e2d\u6536\u96c6\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u91c7\u7528\u4fe1\u606f\u589e\u76ca\u4f5c\u4e3a\u6838\u5fc3\u6307\u6807\uff0c\u5e76\u5728\u5730\u7406\u731c\u57ce\u5e02\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u5177\u6709\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728LLM\u667a\u80fd\u4f53\u6240\u9700\u7684\u5173\u952e\u80fd\u529b\u2014\u2014\u4e3a\u6d88\u9664\u7528\u6237\u8bf7\u6c42\u4e2d\u7684\u6b67\u4e49\u800c\u63d0\u51fa\u597d\u95ee\u9898\u2014\u2014\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u4e14\u5f88\u5c11\u7cfb\u7edf\u6bd4\u8f83\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4e0e\u4e0d\u4f7f\u7528\u8be5\u63a8\u7406\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u4e2a\u4ea4\u4e92\u7684LLM\u667a\u80fd\u4f53\uff1a\u63d0\u95ee\u8005\u3001\u56de\u7b54\u8005\u548c\u5047\u8bbe\u7a7a\u95f4\u66f4\u65b0\u8005\u3002\u4f7f\u7528\u57fa\u4e8e\u9999\u519c\u71b5\u7684\u4fe1\u606f\u589e\u76ca\u4f5c\u4e3a\u6838\u5fc3\u6307\u6807\uff0c\u8bc4\u4f30\u6bcf\u8f6e\u67e5\u8be2\u6548\u679c\u548c\u7d2f\u79ef\u6548\u679c\u3002\u5728\u4e94\u7ea7\u5206\u7c7b\u7684\u5730\u7406\u731c\u57ce\u5e02\u6e38\u620f\u73af\u5883\u4e2d\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u53d8\u4f53\u5728\u5b8c\u5168\u53ef\u89c2\u5bdf\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u6761\u4ef6\u4e0b\uff0c\u6709/\u65e0\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8bc4\u4f30\u7684\u6a21\u578b\u4e2d\uff0c\u5177\u6709\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u6bcf\u8f6e\u83b7\u5f97\u66f4\u9ad8\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u4e14\u4ee5\u66f4\u5c11\u7684\u6b65\u9aa4\u8fbe\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u8bbe\u7f6e\u4e2d\u3002\u63a8\u7406\u8f68\u8ff9\u5206\u6790\u663e\u793a\uff0c\u8f83\u5c0f\u6a21\u578b\u901a\u8fc7\u66f4\u79ef\u6781\u5730\u63a2\u7d22\u5019\u9009\u95ee\u9898\u6765\u5f25\u8865\u80fd\u529b\u9650\u5236\uff0c\u800c\u8f83\u5927\u6a21\u578b\u5728\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u81ea\u4fe1\u5ea6\uff0c\u751f\u6210\u7684\u5019\u9009\u95ee\u9898\u5177\u6709\u66f4\u5927\u7684\u6f5c\u5728\u4fe1\u606f\u589e\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30LLMs\u4fe1\u606f\u6536\u96c6\u80fd\u529b\u7684\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u5bf9\u6709\u6548\u63d0\u95ee\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u5728\u4fe1\u606f\u6536\u96c6\u7b56\u7565\u4e0a\u7684\u5dee\u5f02\uff0c\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u7684LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.17761", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17761", "abs": "https://arxiv.org/abs/2601.17761", "authors": ["Dongjie Cheng", "Ruifeng Yuan", "Yongqi Li", "Runyang You", "Wenjie Wang", "Liqiang Nie", "Lei Zhang", "Wenjie Li"], "title": "AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation", "comment": null, "summary": "Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of \"Omni\" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.", "AI": {"tldr": "AR-Omni\uff1a\u57fa\u4e8e\u81ea\u56de\u5f52\u8303\u5f0f\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u548c\u8bed\u97f3\u7684\u4efb\u610f\u5230\u4efb\u610f\u751f\u6210\uff0c\u65e0\u9700\u4e13\u5bb6\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u5b9e\u65f6\u591a\u6a21\u6001\u751f\u6210\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u611f\u77e5\u548c\u4ea4\u4e92\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u7684\uff0c\u6db5\u76d6\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u8bed\u97f3\u3002\u73b0\u6709\"Omni\"\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5927\u591a\u4f9d\u8d56\u989d\u5916\u7684\u4e13\u5bb6\u7ec4\u4ef6\u5b9e\u73b0\u591a\u6a21\u6001\u751f\u6210\uff0c\u9650\u5236\u4e86\u7edf\u4e00\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u7b80\u6d01\u6027\u3002\u81ea\u56de\u5f52\u5efa\u6a21\u5728\u6587\u672c\u9886\u57df\u5df2\u88ab\u8bc1\u660e\u662f\u4f18\u96c5\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u56e0\u6b64\u7814\u7a76\u8005\u5e0c\u671b\u5c06\u5176\u6269\u5c55\u5230\u591a\u6a21\u6001\u9886\u57df\u3002", "method": "\u63d0\u51faAR-Omni\u6a21\u578b\uff0c\u91c7\u7528\u5355\u4e00Transformer\u89e3\u7801\u5668\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u548c\u6d41\u5f0f\u8bed\u97f3\u7684\u81ea\u56de\u5f52\u751f\u6210\u3002\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6280\u672f\u89e3\u51b3\u7edf\u4e00\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u5b9e\u8df5\u95ee\u9898\uff1a1\uff09\u4efb\u52a1\u611f\u77e5\u635f\u5931\u91cd\u52a0\u6743\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861\uff1b2\uff09\u8f7b\u91cf\u7ea7token\u7ea7\u611f\u77e5\u5bf9\u9f50\u635f\u5931\u63d0\u5347\u89c6\u89c9\u4fdd\u771f\u5ea6\uff1b3\uff09\u6709\u9650\u72b6\u6001\u89e3\u7801\u673a\u5236\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u521b\u9020\u6027\u3002", "result": "AR-Omni\u5728\u4e09\u4e2a\u6a21\u6001\u4e0a\u90fd\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u3002\u8bed\u97f3\u751f\u6210\u7684\u5b9e\u65f6\u56e0\u5b50\u8fbe\u52300.88\uff0c\u8868\u660e\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u3002", "conclusion": "AR-Omni\u8bc1\u660e\u4e86\u81ea\u56de\u5f52\u8303\u5f0f\u53ef\u4ee5\u6709\u6548\u5730\u6269\u5c55\u5230\u591a\u6a21\u6001\u9886\u57df\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u4efb\u610f\u5230\u4efb\u610f\u751f\u6210\uff0c\u65e0\u9700\u4e13\u5bb6\u89e3\u7801\u5668\u3002\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u7b80\u6d01\u3001\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2601.17768", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17768", "abs": "https://arxiv.org/abs/2601.17768", "authors": ["Raja Gond", "Aditya K Kamath", "Arkaprava Basu", "Ramachandran Ramjee", "Ashish Panwar"], "title": "LLM-42: Enabling Determinism in LLM Inference with Verified Speculation", "comment": "https://github.com/microsoft/llm-42", "summary": "In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.\n  Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.", "AI": {"tldr": "LLM-42\uff1a\u57fa\u4e8e\u8c03\u5ea6\u7684\u786e\u5b9a\u6027LLM\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9a8c\u8bc1-\u56de\u6eda\u5faa\u73af\u5728\u4fdd\u6301\u52a8\u6001\u6279\u5904\u7406\u541e\u5410\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u786e\u5b9a\u6027\u8f93\u51fa", "motivation": "LLM\u63a8\u7406\u4e2d\u7684\u975e\u786e\u5b9a\u6027\u6e90\u4e8e\u6d6e\u70b9\u975e\u7ed3\u5408\u6027\u4e0e\u52a8\u6001\u6279\u5904\u7406\u53caGPU\u6838\u7684\u4ea4\u4e92\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u727a\u7272\u541e\u5410\u91cf\uff08\u7981\u7528\u52a8\u6001\u6279\u5904\u7406\uff09\uff0c\u8981\u4e48\u4e0e\u6838\u8bbe\u8ba1\u7d27\u8026\u5408\u4e14\u5e26\u6765\u56fa\u5b9a\u5f00\u9500", "method": "\u53d7\u63a8\u6d4b\u89e3\u7801\u542f\u53d1\uff0c\u5229\u7528\u5e8f\u5217\u5728\u4e00\u81f4\u72b6\u6001\u4e0b\u5927\u6982\u7387\u4ea7\u751f\u4e00\u81f4token\u7684\u89c2\u5bdf\uff0c\u4ee5\u53caGPU\u6838\u4f7f\u7528\u5f62\u72b6\u4e00\u81f4\u5f52\u7ea6\u7684\u7279\u6027\uff0c\u8bbe\u8ba1\u975e\u786e\u5b9a\u6027\u5feb\u901f\u8def\u5f84\u89e3\u7801\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9a8c\u8bc1-\u56de\u6eda\u5faa\u73af\u786e\u4fdd\u786e\u5b9a\u6027\uff1a\u9a8c\u8bc1\u5668\u5728\u56fa\u5b9a\u5f62\u72b6\u5f52\u7ea6\u8c03\u5ea6\u4e0b\u91cd\u653e\u5019\u9009token\uff0c\u63d0\u4ea4\u4fdd\u8bc1\u8de8\u8fd0\u884c\u4e00\u81f4\u7684token\uff0c\u56de\u6eda\u8fdd\u53cd\u786e\u5b9a\u6027\u7684token", "result": "LLM-42\u4e3b\u8981\u590d\u7528\u73b0\u6709\u6838\u5b9e\u73b0\uff0c\u5f00\u9500\u4ec5\u4e0e\u9700\u8981\u786e\u5b9a\u6027\u7684\u6d41\u91cf\u6210\u6bd4\u4f8b\uff0c\u5728\u4fdd\u6301\u52a8\u6001\u6279\u5904\u7406\u541e\u5410\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u786e\u5b9a\u6027\u63a8\u7406", "conclusion": "\u8c03\u5ea6\u65b9\u6cd5\u80fd\u591f\u89e3\u8026\u786e\u5b9a\u6027\u4e0e\u6838\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u6309\u9700\u786e\u5b9a\u6027\u4fdd\u969c\uff0c\u907f\u514d\u56fa\u5b9a\u5f00\u9500\uff0c\u4e3aLLM\u63a8\u7406\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17883", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17883", "abs": "https://arxiv.org/abs/2601.17883", "authors": ["Dingkun Liu", "Yuheng Chen", "Zhu Chen", "Zhenyao Cui", "Yaozhi Wen", "Jiayu An", "Jingwei Luo", "Dongrui Wu"], "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems", "comment": null, "summary": "Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9EEG\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u6bd4\u8f83\u7814\u7a76\uff0c\u8bc4\u4f30\u4e8612\u4e2a\u5f00\u6e90\u6a21\u578b\u572813\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7ebf\u6027\u63a2\u6d4b\u901a\u5e38\u4e0d\u8db3\u3001\u4e13\u7528\u6a21\u578b\u4ecd\u6709\u7ade\u4e89\u529b\u3001\u6a21\u578b\u89c4\u6a21\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u597d\u6027\u80fd\u3002", "motivation": "EEG\u57fa\u7840\u6a21\u578b\u5728\u8111\u673a\u63a5\u53e3\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u7531\u4e8e\u9884\u8bad\u7ec3\u76ee\u6807\u3001\u9884\u5904\u7406\u65b9\u6cd5\u548c\u4e0b\u6e38\u8bc4\u4f30\u534f\u8bae\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u516c\u5e73\u5168\u9762\u7684\u6bd4\u8f83\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aEEG\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u9996\u5148\u56de\u987e50\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u5c06\u5176\u8bbe\u8ba1\u9009\u62e9\u7ec4\u7ec7\u4e3a\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\uff08\u6570\u636e\u6807\u51c6\u5316\u3001\u6a21\u578b\u67b6\u6784\u3001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff09\u3002\u7136\u540e\u8bc4\u4f3012\u4e2a\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u548c\u7ade\u4e89\u6027\u4e13\u7528\u57fa\u7ebf\u6a21\u578b\uff0c\u572813\u4e2aEEG\u6570\u636e\u96c6\u4e0a\u6db5\u76d69\u79cdBCI\u8303\u5f0f\u3002\u91c7\u7528\u7559\u4e00\u88ab\u8bd5\u4ea4\u53c9\u9a8c\u8bc1\u548c\u5c11\u6837\u672c\u5feb\u901f\u6821\u51c6\u4e24\u79cd\u8bbe\u7f6e\uff0c\u6bd4\u8f83\u5168\u53c2\u6570\u5fae\u8c03\u4e0e\u7ebf\u6027\u63a2\u6d4b\uff0c\u5e76\u5206\u6790\u6a21\u578b\u89c4\u6a21\u4e0e\u4e0b\u6e38\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u7ebf\u6027\u63a2\u6d4b\u901a\u5e38\u4e0d\u8db3\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff1b2\uff09\u4ece\u5934\u8bad\u7ec3\u7684\u4e13\u7528\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u4ecd\u5177\u6709\u7ade\u4e89\u529b\uff1b3\uff09\u5728\u5f53\u524d\u6570\u636e\u89c4\u6a21\u548c\u8bad\u7ec3\u5b9e\u8df5\u4e0b\uff0c\u66f4\u5927\u7684\u57fa\u7840\u6a21\u578b\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aEEG\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\uff1a\u9700\u8981\u66f4\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3001\u91cd\u65b0\u601d\u8003\u6a21\u578b\u89c4\u6a21\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u3002"}}
{"id": "2601.17910", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17910", "abs": "https://arxiv.org/abs/2601.17910", "authors": ["Aaron R. Flouro", "Shawn P. Chadwick"], "title": "Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization", "comment": "12 pages, 1 figure, 1 table", "summary": "Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e0e\u5177\u4f53\u7b97\u5b50\u65e0\u5173\u7684\u516c\u7406\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\uff0c\u6db5\u76d6token\u3001task\u548ccontext\u4e09\u4e2a\u4e92\u8865\u5c3a\u5ea6\u3002", "motivation": "\u73b0\u6709\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u5b9e\u73b0\u7279\u5b9a\u7684\u6743\u91cd\u65b9\u6848\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u4fdd\u8bc1\u5728\u4e0d\u540c\u5c3a\u5ea6\uff08token\u3001task\u3001context\uff09\u4e0a\u7684\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u7684\u5408\u7406\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b97\u5b50\u65e0\u5173\u7684\u516c\u7406\u5316\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u4e86\u81ea\u9002\u5e94\u6743\u91cd\u7b97\u5b50\u7684\u7ed3\u6784\u6761\u4ef6\uff0c\u5305\u62ec\u826f\u597d\u5b9a\u4e49\u6027\u3001\u591a\u91cd\u975e\u7b49\u4ef7\u5b9e\u73b0\u53ef\u80fd\u6027\uff0c\u4ee5\u53ca\u901a\u8fc7\u4e58\u79ef\u7ed3\u6784\u5f52\u4e00\u5316\u7684\u5c42\u6b21\u7ec4\u5408\u3002\u5728\u8be5\u6846\u67b6\u5185\u5efa\u7acb\u4e86\u7b26\u5408\u6761\u4ef6\u7b97\u5b50\u7684\u5b58\u5728\u6027\u548c\u975e\u552f\u4e00\u6027\uff0c\u5206\u6790\u4e86\u68af\u5ea6\u4f18\u5316\u6536\u655b\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6270\u52a8\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b89\u5168\u7ea6\u675f\u84b8\u998f\u7684\u62bd\u8c61\u8868\u8ff0\u3002", "result": "\u8be5\u6846\u67b6\u5c06\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5177\u4f53\u6743\u91cd\u516c\u5f0f\u89e3\u8026\uff0c\u4f7f\u5f97\u80fd\u591f\u5728\u5f02\u8d28\u6027\u3001\u5206\u5e03\u504f\u79fb\u548c\u5b89\u5168\u7ea6\u675f\u4e0b\u5bf9\u81ea\u9002\u5e94\u84b8\u998f\u65b9\u6cd5\u8fdb\u884c\u539f\u5219\u6027\u5206\u6790\uff0c\u4e3a\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u63d0\u51fa\u7684\u516c\u7406\u5316\u6846\u67b6\u4e3a\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u5f97\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u7684\u60c5\u51b5\u4e0b\u5206\u6790\u65b9\u6cd5\u7684\u7406\u8bba\u6027\u8d28\uff0c\u4e3a\u5904\u7406\u5f02\u8d28\u6027\u3001\u5206\u5e03\u504f\u79fb\u548c\u5b89\u5168\u7ea6\u675f\u7b49\u95ee\u9898\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\u3002"}}
{"id": "2601.17912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17912", "abs": "https://arxiv.org/abs/2601.17912", "authors": ["Qinyi Liu", "Mohammad Khalil", "Naman Goel"], "title": "Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN", "comment": null, "summary": "Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, remain underexplored. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying TabPFN (and similar) models in practice and the need for further fairness interventions.", "AI": {"tldr": "TabPFN\u7b49\u8868\u683c\u6570\u636e\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u56e0\u679c\u9884\u8bad\u7ec3\u83b7\u5f97\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u5176\u516c\u5e73\u6027\u8868\u73b0\u6709\u9650\u4e14\u4e0d\u4e00\u81f4\uff0c\u7279\u522b\u662f\u5728MNAR\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\uff0c\u9700\u8981\u989d\u5916\u7684\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\u3002", "motivation": "TabPFN\u7b49\u8868\u683c\u6570\u636e\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u56e0\u679c\u9884\u8bad\u7ec3\uff0c\u5177\u6709\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u5176\u516c\u5e73\u6027\u7279\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5bf9TabPFN\u53ca\u5176\u5fae\u8c03\u53d8\u4f53\u8fdb\u884c\u5168\u9762\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5305\u62ec\u9884\u6d4b\u6027\u80fd\u3001\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8003\u5bdf\u4e0d\u540c\u6570\u636e\u96c6\u89c4\u6a21\u548c\u5206\u5e03\u504f\u79fb\uff08\u7279\u522b\u662fMNAR\u534f\u53d8\u91cf\u504f\u79fb\uff09\u4e0b\u7684\u8868\u73b0\u3002", "result": "TabPFN\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5bf9\u865a\u5047\u76f8\u5173\u6027\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u516c\u5e73\u6027\u65b9\u9762\u7684\u6539\u8fdb\u6709\u9650\u4e14\u4e0d\u4e00\u81f4\uff0c\u7279\u522b\u662f\u5728MNAR\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "TabPFN\u7684\u56e0\u679c\u9884\u8bad\u7ec3\u5bf9\u9884\u6d4b\u6709\u5e2e\u52a9\u4f46\u4e0d\u8db3\u4ee5\u786e\u4fdd\u7b97\u6cd5\u516c\u5e73\u6027\uff0c\u8fd9\u5bf9\u5176\u5b9e\u9645\u90e8\u7f72\u5177\u6709\u91cd\u8981\u542f\u793a\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2601.17916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17916", "abs": "https://arxiv.org/abs/2601.17916", "authors": ["Jialu Tang", "Tong Xia", "Yuan Lu", "Aaqib Saeed"], "title": "UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR", "comment": "Accepted to IEEE ICASSP 2026", "summary": "Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.", "AI": {"tldr": "UniPACT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4e34\u5e8a\u9884\u540e\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u5c06\u6570\u503c\u578bEHR\u6570\u636e\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\uff0c\u5e76\u4e0e\u539f\u59cbECG\u6ce2\u5f62\u8868\u793a\u878d\u5408\uff0c\u4f7fLLM\u80fd\u591f\u5bf9\u4e24\u79cd\u6a21\u6001\u8fdb\u884c\u6574\u4f53\u63a8\u7406\u3002", "motivation": "\u51c6\u786e\u7684\u4e34\u5e8a\u9884\u540e\u9700\u8981\u7efc\u5408\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u548c\u5b9e\u65f6\u751f\u7406\u4fe1\u53f7\uff08\u5982\u5fc3\u7535\u56feECG\uff09\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u6b64\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u5f15\u64ce\uff0c\u4f46\u96be\u4ee5\u539f\u751f\u5904\u7406\u8fd9\u4e9b\u5f02\u6784\u7684\u975e\u6587\u672c\u6570\u636e\u7c7b\u578b\u3002", "method": "\u63d0\u51faUniPACT\u6846\u67b6\uff0c\u6838\u5fc3\u8d21\u732e\u662f\u7ed3\u6784\u5316\u63d0\u793a\u673a\u5236\uff0c\u5c06\u6570\u503cEHR\u6570\u636e\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\u3002\u7136\u540e\u5c06\u8fd9\u79cd\u6587\u672c\u5316\u7684\u60a3\u8005\u4e0a\u4e0b\u6587\u4e0e\u76f4\u63a5\u4ece\u539f\u59cbECG\u6ce2\u5f62\u5b66\u4e60\u7684\u8868\u793a\u878d\u5408\uff0c\u4f7fLLM\u80fd\u591f\u5bf9\u4e24\u79cd\u6a21\u6001\u8fdb\u884c\u6574\u4f53\u63a8\u7406\u3002", "result": "\u5728\u7efc\u5408MDS-ED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniPACT\u5728\u5305\u62ec\u8bca\u65ad\u3001\u6076\u5316\u3001ICU\u5165\u9662\u548c\u6b7b\u4ea1\u7387\u5728\u5185\u7684\u591a\u6837\u5316\u9884\u540e\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e8689.37%\u7684\u5e73\u5747AUROC\uff0c\u4f18\u4e8e\u4e13\u95e8\u57fa\u7ebf\u3002\u591a\u6a21\u6001\u591a\u4efb\u52a1\u65b9\u6cd5\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u5728\u7f3a\u5931\u6570\u636e\u573a\u666f\u4e2d\u63d0\u4f9b\u9c81\u68d2\u6027\u3002", "conclusion": "UniPACT\u901a\u8fc7\u5f25\u5408\u6a21\u6001\u5dee\u8ddd\uff0c\u4f7fLLM\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784\u4e34\u5e8a\u6570\u636e\uff0c\u4e3a\u51c6\u786e\u7684\u591a\u6a21\u6001\u4e34\u5e8a\u9884\u540e\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u591a\u79cd\u9884\u540e\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2601.17917", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17917", "abs": "https://arxiv.org/abs/2601.17917", "authors": ["Zhongyu Xiao", "Zhiwei Hao", "Jianyuan Guo", "Yong Luo", "Jia Liu", "Jie Xu", "Han Hu"], "title": "treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding", "comment": "Tech report. Code is available at https://github.com/xiaoshideta/Streaming-dLLM", "summary": "Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.", "code_url": "https://github.com/xiaoshideta/Streaming-dLLM", "code_stars": 3, "code_last_update": "2026-01-27", "AI": {"tldr": "Streaming-dLLM\u662f\u4e00\u4e2a\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u7ef4\u5ea6\u7684\u8870\u51cf\u5f15\u5bfc\u540e\u7f00\u5efa\u6a21\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u52a8\u6001\u7f6e\u4fe1\u611f\u77e5\u7b56\u7565\uff0c\u663e\u8457\u52a0\u901f\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u6700\u9ad8\u53ef\u8fbe68.2\u500d\u52a0\u901f", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\uff08\u5982KV\u7f13\u5b58\u91cd\u7528\u6216\u542f\u53d1\u5f0f\u89e3\u7801\uff09\u5ffd\u89c6\u4e86\u5757\u7ea7\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u5185\u5728\u4f4e\u6548\u6027\uff1a\u7a7a\u95f4\u5197\u4f59\uff08\u5bf9\u4fe1\u606f\u7a00\u758f\u7684\u540e\u7f00\u533a\u57df\u8fdb\u884c\u5747\u5300\u5efa\u6a21\uff09\u548c\u65f6\u95f4\u4f4e\u6548\uff08\u5728\u6574\u4e2a\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5e94\u7528\u56fa\u5b9a\u7684\u53bb\u566a\u8c03\u5ea6\uff09", "method": "\u63d0\u51faStreaming-dLLM\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u7a7a\u95f4\u7ef4\u5ea6\uff1a\u8870\u51cf\u5f15\u5bfc\u540e\u7f00\u5efa\u6a21\uff0c\u901a\u8fc7\u526a\u679d\u5197\u4f59\u63a9\u7801\u6807\u8bb0\u6765\u8fd1\u4f3c\u5b8c\u6574\u4e0a\u4e0b\u6587\uff1b2\uff09\u65f6\u95f4\u7ef4\u5ea6\uff1a\u52a8\u6001\u7f6e\u4fe1\u611f\u77e5\u7b56\u7565\u4e0e\u63d0\u524d\u9000\u51fa\u673a\u5236\uff0c\u5141\u8bb8\u6a21\u578b\u5bf9\u5df2\u6536\u655b\u7684\u6807\u8bb0\u8df3\u8fc7\u4e0d\u5fc5\u8981\u7684\u8fed\u4ee3", "result": "\u5b9e\u9a8c\u8868\u660eStreaming-dLLM\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe68.2\u500d\u7684\u52a0\u901f\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6269\u6563\u89e3\u7801\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "Streaming-dLLM\u901a\u8fc7\u89e3\u51b3\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u7a7a\u95f4\u5197\u4f59\u548c\u65f6\u95f4\u4f4e\u6548\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u7528\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17933", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17933", "abs": "https://arxiv.org/abs/2601.17933", "authors": ["Laurent Caraffa"], "title": "Dissipative Learning: A Framework for Viable Adaptive Systems", "comment": "68 pages, 14 figures", "summary": "We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.\n  A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.\n  Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.", "AI": {"tldr": "\u5b66\u4e60\u88ab\u91cd\u6784\u4e3a\u5185\u5728\u8017\u6563\u8fc7\u7a0b\uff0c\u9057\u5fd8\u548c\u6b63\u5219\u5316\u662f\u81ea\u9002\u5e94\u7cfb\u7edf\u7684\u7ed3\u6784\u8981\u6c42\u800c\u975e\u542f\u53d1\u5f0f\u9644\u52a0\u9879\u3002BEDS\u6846\u67b6\u5c06\u5b66\u4e60\u5efa\u6a21\u4e3a\u538b\u7f29\u4fe1\u5ff5\u72b6\u6001\u5728\u8017\u6563\u7ea6\u675f\u4e0b\u7684\u6f14\u5316\uff0c\u8bc1\u660e\u4e86Fisher-Rao\u6b63\u5219\u5316\u662f\u70ed\u529b\u5b66\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5c06\u9057\u5fd8\u548c\u6b63\u5219\u5316\u89c6\u4e3a\u542f\u53d1\u5f0f\u9644\u52a0\u9879\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002\u672c\u6587\u65e8\u5728\u4ece\u4fe1\u606f\u8bba\u3001\u70ed\u529b\u5b66\u548c\u4fe1\u606f\u51e0\u4f55\u89d2\u5ea6\uff0c\u4e3a\u5b66\u4e60\u8fc7\u7a0b\u63d0\u4f9b\u66f4\u6839\u672c\u7684\u7269\u7406\u89e3\u91ca\uff0c\u5c06\u5b66\u4e60\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5185\u5728\u8017\u6563\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faBEDS\uff08\u8d1d\u53f6\u65af\u6d8c\u73b0\u8017\u6563\u7ed3\u6784\uff09\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u5efa\u6a21\u4e3a\u538b\u7f29\u4fe1\u5ff5\u72b6\u6001\u5728\u8017\u6563\u7ea6\u675f\u4e0b\u7684\u6f14\u5316\u3002\u5f15\u5165\u6761\u4ef6\u6700\u4f18\u6027\u5b9a\u7406\uff0c\u8bc1\u660eFisher-Rao\u6b63\u5219\u5316\uff08\u57fa\u4e8e\u4fe1\u606f\u6563\u5ea6\u800c\u975e\u6b27\u6c0f\u8ddd\u79bb\uff09\u662f\u552f\u4e00\u70ed\u529b\u5b66\u6700\u4f18\u7684\u6b63\u5219\u5316\u7b56\u7565\u3002\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u73b0\u6709\u65b9\u6cd5\uff08Ridge\u3001SIGReg\u3001EMA\u3001SAC\uff09\u4f5c\u4e3a\u5355\u4e00\u63a7\u5236\u65b9\u7a0b\u7684\u7279\u4f8b\u3002", "result": "\u8bc1\u660e\u4e86Fisher-Rao\u6b63\u5219\u5316\u5728\u70ed\u529b\u5b66\u610f\u4e49\u4e0a\u662f\u6700\u4f18\u7684\uff0c\u800c\u6b27\u6c0f\u6b63\u5219\u5316\u662f\u7ed3\u6784\u4e0a\u6b21\u4f18\u7684\u3002\u5c06\u8fc7\u62df\u5408\u89e3\u91ca\u4e3a\"\u8fc7\u5ea6\u7ed3\u6676\"\uff0c\u707e\u96be\u6027\u9057\u5fd8\u89e3\u91ca\u4e3a\u8017\u6563\u63a7\u5236\u4e0d\u8db3\u3002\u533a\u5206\u4e86BEDS\u53ef\u7ed3\u6676\u95ee\u9898\uff08\u4fe1\u5ff5\u6536\u655b\u5230\u7a33\u5b9a\u5e73\u8861\uff09\u548cBEDS\u53ef\u7ef4\u6301\u95ee\u9898\uff08\u9700\u8981\u6301\u7eed\u9002\u5e94\uff09\u3002\u6846\u67b6\u81ea\u7136\u6269\u5c55\u5230\u6301\u7eed\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "\u5b66\u4e60\u5e94\u88ab\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u8017\u6563\u7ea6\u675f\u4e0b\u7ef4\u6301\u53ef\u884c\u4fe1\u5ff5\u72b6\u6001\u7684\u8fc7\u7a0b\u3002\u8be5\u6846\u67b6\u4e3a\u9057\u5fd8\u3001\u6b63\u5219\u5316\u548c\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89c6\u89d2\uff0c\u5c06\u5b66\u4e60\u7cfb\u7edf\u7684\u8bc4\u4f30\u6807\u51c6\u4ece\u6e10\u8fd1\u6700\u4f18\u6027\u8f6c\u5411\u5728\u9002\u5e94\u548c\u6709\u9650\u8d44\u6e90\u4e0b\u7684\u53ef\u884c\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.17986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17986", "abs": "https://arxiv.org/abs/2601.17986", "authors": ["Anders Eklund"], "title": "Federated learning for unpaired multimodal data through a homogeneous transformer model", "comment": null, "summary": "Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.", "AI": {"tldr": "\u63d0\u51fa\u8054\u90a6\u65e0\u914d\u5bf9\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u516c\u5171\u951a\u70b9\u96c6\u5bf9\u9f50\u5206\u6563\u7684\u79c1\u6709\u6570\u636e\u6d41\u5f62\uff0c\u65e0\u9700\u4f20\u8f93\u79c1\u6709\u6837\u672c\u6216\u5171\u4eab\u539f\u59cb\u7279\u5f81\u5d4c\u5165", "motivation": "\u73b0\u5b9e\u8054\u90a6\u73af\u5883\u4e2d\u6570\u636e\u901a\u5e38\u662f\u672a\u914d\u5bf9\u4e14\u5206\u6563\u7684\uff0c\u4e0d\u540c\u8282\u70b9\u6301\u6709\u4e0d\u540c\u6a21\u6001\u7684\u79c1\u6709\u6570\u636e\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\uff0c\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u6a21\u6001\u4e0d\u5339\u914d\u4e14\u9700\u8981\u4fdd\u62a4\u6570\u636e\u4e3b\u6743\u7684\u60c5\u51b5", "method": "1) \u4f7f\u7528\u5c0f\u578b\u516c\u5171\u951a\u70b9\u96c6\u5bf9\u9f50\u4e0d\u540c\u6a21\u6001\u7684\u79c1\u6709\u6d41\u5f62\uff1b2) \u901a\u8fc7Gram\u77e9\u9635\u548c\u4e2d\u5fc3\u6838\u5bf9\u9f50\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff1b3) \u63d0\u51fa\u5b50\u7a7a\u95f4\u7a33\u5b9a\u5fae\u8c03\u65b9\u6cd5\u5904\u7406\u5927Transformer\u6a21\u578b\uff1b4) \u5f15\u5165\u7cbe\u5ea6\u52a0\u6743\u5e73\u5747\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u964d\u4f4e\u4e0d\u786e\u5b9a\u8282\u70b9\u7684\u6743\u91cd", "result": "\u5efa\u7acb\u4e86\u8054\u90a6\u65e0\u914d\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u6570\u5b66\u57fa\u7840\uff0c\u80fd\u591f\u5728\u5206\u6563\u3001\u672a\u914d\u5bf9\u3001\u79c1\u6709\u7684\u6570\u636e\u5b64\u5c9b\u4e0a\u8bad\u7ec3\u5168\u5c40\u591a\u6a21\u6001Transformer\uff0c\u65e0\u9700\u96c6\u4e2d\u5b58\u50a8\u6216\u914d\u5bf9\u6837\u672c\uff0c\u63d0\u4f9b\u6bd4\u539f\u578b\u5171\u4eab\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u8bc1", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u8054\u90a6\u73af\u5883\u4e2d\u8bad\u7ec3\u5168\u5c40\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6a21\u6001\u4e0d\u5339\u914d\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4ece\u5206\u6563\u7684\u79c1\u6709\u6570\u636e\u6e90\u5b66\u4e60\u7edf\u4e00\u4e16\u754c\u8868\u793a\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2601.17995", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17995", "abs": "https://arxiv.org/abs/2601.17995", "authors": ["Shudi Weng", "Ming Xiao", "Mikael Skoglund"], "title": "Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning", "comment": null, "summary": "Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees", "AI": {"tldr": "\u63d0\u51faH-SecCoGC\u65b9\u6848\uff0c\u901a\u8fc7\u7f16\u7801\u7b56\u7565\u5b9e\u73b0\u7ed3\u6784\u5316\u805a\u5408\uff0c\u89e3\u51b3\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u4e2d\u9690\u79c1\u4fdd\u62a4\u4e0e\u901a\u4fe1\u4e0d\u53ef\u9760\u7684\u534f\u8c03\u95ee\u9898", "motivation": "\u5206\u5c42\u8054\u90a6\u5b66\u4e60(HFL)\u867d\u7136\u6539\u5584\u4e86\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u5668\u95f4\u7684\u94fe\u8def\u8d28\u91cf\uff0c\u4f46\u5728\u4e0d\u53ef\u9760\u901a\u4fe1\u4e0b\u540c\u65f6\u4fdd\u8bc1\u6a21\u578b\u7cbe\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9690\u79c1\u566a\u58f0\u7684\u534f\u8c03\u53ef\u80fd\u88ab\u968f\u673a\u7834\u574f", "method": "\u63d0\u51fa\u9c81\u68d2\u5206\u5c42\u5b89\u5168\u805a\u5408\u65b9\u6848H-SecCoGC\uff0c\u96c6\u6210\u7f16\u7801\u7b56\u7565\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u5316\u805a\u5408\uff0c\u907f\u514d\u90e8\u5206\u53c2\u4e0e\u95ee\u9898", "result": "\u65b9\u6848\u5728\u4e0d\u540c\u9690\u79c1\u7ea7\u522b\u4e0b\u786e\u4fdd\u51c6\u786e\u7684\u5168\u5c40\u6a21\u578b\u6784\u5efa\uff0c\u663e\u8457\u63d0\u9ad8\u9c81\u68d2\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u5b66\u4e60\u6548\u7387\uff0c\u7406\u8bba\u548c\u5b9e\u9a8c\u7ed3\u679c\u5747\u8bc1\u660e\u5176\u5728\u4efb\u610f\u5f3a\u9690\u79c1\u4fdd\u8bc1\u4e0b\u7684\u4f18\u8d8a\u6027", "conclusion": "H-SecCoGC\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u4e2d\u9690\u79c1\u4fdd\u62a4\u4e0e\u901a\u4fe1\u4e0d\u53ef\u9760\u7684\u534f\u8c03\u95ee\u9898\uff0c\u4e3a\u4e0d\u53ef\u9760\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u5b89\u5168\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18030", "abs": "https://arxiv.org/abs/2601.18030", "authors": ["Markus N. Rabe", "Judith Clymo", "Zheren Dong"], "title": "Spelling Bee Embeddings for Language Modeling", "comment": null, "summary": "We introduce a simple modification to the embedding layer. The key change is to infuse token embeddings with information about their spelling. Models trained with these embeddings improve not only on spelling, but also across standard benchmarks. We conduct scaling studies for models with 40M to 800M parameters, which suggest that the improvements are equivalent to needing about 8% less compute and data to achieve the same test loss.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u62fc\u5199\u4fe1\u606f\u878d\u5165\u8bcd\u5d4c\u5165\u5c42\uff0c\u6a21\u578b\u5728\u62fc\u5199\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u76f8\u5f53\u4e8e\u51cf\u5c11\u7ea68%\u7684\u8ba1\u7b97\u548c\u6570\u636e\u9700\u6c42", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7684\u8bcd\u5d4c\u5165\u5c42\u4e3b\u8981\u57fa\u4e8e\u8bed\u4e49\u4fe1\u606f\uff0c\u7f3a\u4e4f\u5bf9\u5355\u8bcd\u62fc\u5199\u7ed3\u6784\u7684\u663e\u5f0f\u7f16\u7801\u3002\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u62fc\u5199\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e5f\u53ef\u80fd\u5f71\u54cd\u6574\u4f53\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5bf9\u5d4c\u5165\u5c42\u7684\u7b80\u5355\u4fee\u6539\uff1a\u5c06\u8bcd\u7b26\u7684\u62fc\u5199\u4fe1\u606f\u878d\u5165\u5176\u5d4c\u5165\u8868\u793a\u4e2d\u3002\u5177\u4f53\u901a\u8fc7\u5c06\u62fc\u5199\u7279\u5f81\uff08\u5982\u5b57\u7b26\u5e8f\u5217\u3001\u5f62\u6001\u4fe1\u606f\uff09\u4e0e\u539f\u59cb\u8bcd\u5d4c\u5165\u76f8\u7ed3\u5408\u6765\u5b9e\u73b0\u3002", "result": "\u4f7f\u7528\u8be5\u65b9\u6cd5\u7684\u6a21\u578b\u5728\u62fc\u5199\u4efb\u52a1\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u572840M\u5230800M\u53c2\u6570\u89c4\u6a21\u7684\u7f29\u653e\u7814\u7a76\u4e2d\uff0c\u6539\u8fdb\u6548\u679c\u76f8\u5f53\u4e8e\u9700\u8981\u7ea68%\u66f4\u5c11\u7684\u8ba1\u7b97\u548c\u6570\u636e\u6765\u8fbe\u5230\u76f8\u540c\u7684\u6d4b\u8bd5\u635f\u5931\u3002", "conclusion": "\u5c06\u62fc\u5199\u4fe1\u606f\u878d\u5165\u8bcd\u5d4c\u5165\u5c42\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u80fd\u63d0\u5347\u62fc\u5199\u80fd\u529b\uff0c\u8fd8\u80fd\u5168\u9762\u6539\u5584\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u9ad8\u8ba1\u7b97\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2601.18032", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.18032", "abs": "https://arxiv.org/abs/2601.18032", "authors": ["Brijesh FNU", "Viet Thanh Duy Nguyen", "Ashima Sharma", "Md Harun Rashid Molla", "Chengyi Xu", "Truong-Son Hy"], "title": "Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity", "comment": null, "summary": "Dielectric materials are critical building blocks for modern electronics such as sensors, actuators, and transistors. With the rapid recent advance in soft and stretchable electronics for emerging human- and robot-interfacing applications, there is a surging need for high-performance dielectric elastomers. However, it remains a grand challenge to develop soft elastomers that simultaneously possess high dielectric constants (k, related to energy storage capacity) and low Young's moduli (E, related to mechanical flexibility). While some new elastomer designs have been reported in individual (mostly one-off) studies, almost no structured dataset is currently available for dielectric elastomers that systematically encompasses their molecular sequence, dielectric, and mechanical properties. Within this context, we curate a compact, high-quality dataset of acrylate-based dielectric elastomers, one of the most widely explored elastomer backbones due to its versatile chemistry and molecular design flexibility, by screening and aggregating experimental results from the literature over the past 10 years. Building on this dataset, we propose a multimodal learning framework that leverages large-scale pretrained polymer representations from graph- and sequence-based encoders. These pretrained embeddings transfer rich chemical and structural knowledge from vast polymer corpora, enabling accurate few-shot prediction of both dielectric and mechanical properties from molecular sequences. Our results represent a new paradigm for transferring knowledge from pretrained multimodal models to overcome severe data scarcity, which can be readily translated to other polymer backbones (e.g., silicones, urethanes) and thus accelerate data-efficient discovery of soft high-k dielectric elastomers. Our source code and dataset are publicly available at https://github.com/HySonLab/Polymers", "code_url": "https://github.com/HySonLab/Polymers", "code_stars": 1, "code_last_update": "2026-01-27", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e19\u70ef\u9178\u916f\u57fa\u4ecb\u7535\u5f39\u6027\u4f53\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u805a\u5408\u7269\u8868\u793a\u8fdb\u884c\u5c11\u6837\u672c\u9884\u6d4b\uff0c\u4ee5\u514b\u670d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u5e76\u52a0\u901f\u9ad8\u6027\u80fd\u4ecb\u7535\u5f39\u6027\u4f53\u7684\u53d1\u73b0\u3002", "motivation": "\u968f\u7740\u8f6f\u6027\u548c\u53ef\u62c9\u4f38\u7535\u5b50\u5b66\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u9ad8\u6027\u80fd\u4ecb\u7535\u5f39\u6027\u4f53\u7684\u9700\u6c42\u6fc0\u589e\u3002\u7136\u800c\uff0c\u540c\u65f6\u5177\u5907\u9ad8\u4ecb\u7535\u5e38\u6570\u548c\u4f4e\u6768\u6c0f\u6a21\u91cf\u7684\u8f6f\u5f39\u6027\u4f53\u5f00\u53d1\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6db5\u76d6\u5206\u5b50\u5e8f\u5217\u3001\u4ecb\u7535\u548c\u673a\u68b0\u6027\u80fd\u7684\u7ed3\u6784\u5316\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u7b5b\u9009\u548c\u6c47\u603b\u8fc7\u53bb10\u5e74\u6587\u732e\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6784\u5efa\u4e86\u4e19\u70ef\u9178\u916f\u57fa\u4ecb\u7535\u5f39\u6027\u4f53\u7684\u7d27\u51d1\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002\u63d0\u51fa\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u56fe\u548c\u5e8f\u5217\u7f16\u7801\u5668\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u805a\u5408\u7269\u8868\u793a\uff0c\u4ece\u5206\u5b50\u5e8f\u5217\u51c6\u786e\u9884\u6d4b\u4ecb\u7535\u548c\u673a\u68b0\u6027\u80fd\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u4ece\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u6a21\u578b\u8f6c\u79fb\u4e30\u5bcc\u7684\u5316\u5b66\u548c\u7ed3\u6784\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4ece\u5206\u5b50\u5e8f\u5217\u5bf9\u4ecb\u7535\u548c\u673a\u68b0\u6027\u80fd\u7684\u51c6\u786e\u5c11\u6837\u672c\u9884\u6d4b\uff0c\u514b\u670d\u4e86\u4e25\u91cd\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u8f6c\u79fb\u77e5\u8bc6\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u8f7b\u677e\u63a8\u5e7f\u5230\u5176\u4ed6\u805a\u5408\u7269\u9aa8\u67b6\uff08\u5982\u7845\u916e\u3001\u805a\u6c28\u916f\uff09\uff0c\u4ece\u800c\u52a0\u901f\u8f6f\u6027\u9ad8\u4ecb\u7535\u5e38\u6570\u5f39\u6027\u4f53\u7684\u6570\u636e\u9ad8\u6548\u53d1\u73b0\u3002"}}
{"id": "2601.18076", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18076", "abs": "https://arxiv.org/abs/2601.18076", "authors": ["Alexandra Chouldechova", "A. Feder Cooper", "Solon Barocas", "Abhinav Palia", "Dan Vann", "Hanna Wallach"], "title": "Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming", "comment": null, "summary": "We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u57fa\u4e8e\u653b\u51fb\u6210\u529f\u7387\u6bd4\u8f83\u6765\u8bc4\u4f30\u7cfb\u7edf\u5b89\u5168\u6027\u6216\u653b\u51fb\u65b9\u6cd5\u6709\u6548\u6027\u7684\u7ed3\u8bba\u5f80\u5f80\u7f3a\u4e4f\u8bc1\u636e\u652f\u6301\uff0c\u5b58\u5728\u82f9\u679c\u4e0e\u6a59\u5b50\u6bd4\u8f83\u548c\u6d4b\u91cf\u6548\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dAI\u7ea2\u961f\u6d4b\u8bd5\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u7ecf\u5e38\u901a\u8fc7\u653b\u51fb\u6210\u529f\u7387\u6bd4\u8f83\u6765\u5f97\u51fa\u5173\u4e8e\u7cfb\u7edf\u76f8\u5bf9\u5b89\u5168\u6027\u6216\u653b\u51fb\u65b9\u6cd5\u6709\u6548\u6027\u7684\u7ed3\u8bba\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u8bba\u5f80\u5f80\u5efa\u7acb\u5728\u65e0\u6548\u7684\u6d4b\u91cf\u548c\u4e0d\u5f53\u7684\u6bd4\u8f83\u57fa\u7840\u4e0a\uff0c\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u3002", "method": "\u91c7\u7528\u793e\u4f1a\u79d1\u5b66\u6d4b\u91cf\u7406\u8bba\u548c\u63a8\u65ad\u7edf\u8ba1\u5b66\u6846\u67b6\uff0c\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u57fa\u7840\u6765\u7406\u89e3\u4f55\u65f6\u53ef\u4ee5\u901a\u8fc7\u91cf\u5316\u7cfb\u7edf\u5c5e\u6027\u83b7\u5f97\u7684\u6570\u503c\u8fdb\u884c\u6709\u610f\u4e49\u7684\u6bd4\u8f83\u3002\u4ee5\u8d8a\u72f1\u653b\u51fb\u4e3a\u4f8b\uff0c\u8be6\u7ec6\u5206\u6790ASR\u6bd4\u8f83\u4e2d\u7684\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u6982\u5ff5\u3001\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86ASR\u6bd4\u8f83\u4e2d\u5b58\u5728\u7684\u82f9\u679c\u4e0e\u6a59\u5b50\u6bd4\u8f83\u95ee\u9898\uff0c\u9610\u660e\u4e86\u653b\u51fb\u6210\u529f\u7387\u53ef\u4ee5\u548c\u4e0d\u53ef\u4ee5\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u8fdb\u884c\u6709\u610f\u4e49\u7684\u6bd4\u8f83\u3002", "conclusion": "\u57fa\u4e8e\u653b\u51fb\u6210\u529f\u7387\u6bd4\u8f83\u5f97\u51fa\u7684\u5173\u4e8e\u7cfb\u7edf\u5b89\u5168\u6027\u6216\u653b\u51fb\u65b9\u6cd5\u6709\u6548\u6027\u7684\u7ed3\u8bba\u5f80\u5f80\u7f3a\u4e4f\u8bc1\u636e\u652f\u6301\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u4e25\u8c28\u7684\u6d4b\u91cf\u548c\u6bd4\u8f83\u6846\u67b6\u6765\u786e\u4fdd\u7ed3\u8bba\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.18081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18081", "abs": "https://arxiv.org/abs/2601.18081", "authors": ["Peixuan Han", "Yingjie Yu", "Jingjun Xu", "Jiaxuan You"], "title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal", "comment": null, "summary": "Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.", "code_url": "https://github.com/ulab-uiuc/DRPG-RebuttalAgen", "AI": {"tldr": "DRPG\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b66\u672f\u53cd\u9a73\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u5ba1\u7a3f\u610f\u89c1\u3001\u68c0\u7d22\u8bba\u6587\u8bc1\u636e\u3001\u89c4\u5212\u53cd\u9a73\u7b56\u7565\u548c\u751f\u6210\u56de\u5e94\u56db\u4e2a\u6b65\u9aa4\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f7f\u75288B\u6a21\u578b\u5373\u53ef\u8fbe\u5230\u8d85\u8d8a\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u7814\u7a76\u5de5\u4f5c\u6d41\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5b66\u672f\u53cd\u9a73\u8fd9\u4e00\u5b66\u672f\u4ea4\u6d41\u548c\u540c\u884c\u8bc4\u5ba1\u7684\u5173\u952e\u73af\u8282\u7684\u81ea\u52a8\u5316\u652f\u6301\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u73b0\u6210\u5927\u8bed\u8a00\u6a21\u578b\u6216\u7b80\u5355\u6d41\u6c34\u7ebf\uff0c\u96be\u4ee5\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4e14\u5f80\u5f80\u65e0\u6cd5\u751f\u6210\u6709\u9488\u5bf9\u6027\u548c\u8bf4\u670d\u529b\u7684\u56de\u5e94\u3002", "method": "\u63d0\u51faDRPG\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6b65\u9aa4\uff1a1) \u5c06\u5ba1\u7a3f\u610f\u89c1\u5206\u89e3\u4e3a\u539f\u5b50\u5316\u5173\u6ce8\u70b9\uff1b2) \u4ece\u8bba\u6587\u4e2d\u68c0\u7d22\u76f8\u5173\u8bc1\u636e\uff1b3) \u89c4\u5212\u53cd\u9a73\u7b56\u7565\uff1b4) \u636e\u6b64\u751f\u6210\u56de\u5e94\u3002\u7279\u522b\u5730\uff0c\u89c4\u5212\u5668\u5728\u8bc6\u522b\u6700\u53ef\u884c\u53cd\u9a73\u65b9\u5411\u65b9\u9762\u51c6\u786e\u7387\u8d85\u8fc798%\u3002", "result": "\u5728\u9876\u7ea7\u4f1a\u8bae\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDRPG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53cd\u9a73\u6d41\u6c34\u7ebf\uff0c\u4ec5\u4f7f\u75288B\u6a21\u578b\u5373\u53ef\u8fbe\u5230\u8d85\u8d8a\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u7684\u6027\u80fd\u3002\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u89c4\u5212\u5668\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u63d0\u4f9b\u591a\u89c6\u89d2\u548c\u53ef\u89e3\u91ca\u5efa\u8bae\u65b9\u9762\u7684\u4ef7\u503c\u3002DRPG\u5728\u591a\u8f6e\u590d\u6742\u573a\u666f\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "DRPG\u6846\u67b6\u6709\u6548\u5c55\u793a\u4e86\u5176\u5728\u63d0\u4f9b\u9ad8\u8d28\u91cf\u53cd\u9a73\u5185\u5bb9\u548c\u652f\u6301\u5b66\u672f\u8ba8\u8bba\u89c4\u6a21\u5316\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8be5\u5de5\u4f5c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.18089", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18089", "abs": "https://arxiv.org/abs/2601.18089", "authors": ["Venmugil Elango", "Nidhi Bhatia", "Roger Waleffe", "Rasoul Shafipour", "Tomer Asida", "Abhinav Khattar", "Nave Assaf", "Maximilian Golub", "Joey Guman", "Tiyasa Mitra", "Ritchie Zhao", "Ritika Borkar", "Ran Zilberstein", "Mostofa Patwary", "Mohammad Shoeybi", "Bita Rouhani"], "title": "LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts", "comment": null, "summary": "Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).", "AI": {"tldr": "LatentMoE\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u4f18\u5316\u63a8\u7406\u6210\u672c\uff0c\u5728\u51c6\u786e\u7387/FLOP\u548c\u51c6\u786e\u7387/\u53c2\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u6807\u51c6MoE\u67b6\u6784\u3002", "motivation": "\u73b0\u6709MoE\u67b6\u6784\u5728\u63a8\u7406\u6210\u672c\uff08\u4ee5\u51c6\u786e\u7387/FLOP\u548c\u51c6\u786e\u7387/\u53c2\u6570\u8861\u91cf\uff09\u65b9\u9762\u7684\u6700\u4f18\u6027\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4ece\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6MoE\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u4e0b\u7684\u6027\u80fd\u74f6\u9888\uff0c\u901a\u8fc7\u7ecf\u9a8c\u6027\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff08\u6700\u9ad895B\u53c2\u6570\u30011T token\u8bad\u7ec3\uff09\u548c\u7406\u8bba\u5206\u6790\uff0c\u5f00\u53d1LatentMoE\u67b6\u6784\u3002", "result": "LatentMoE\u5728\u51c6\u786e\u7387/FLOP\u548c\u51c6\u786e\u7387/\u53c2\u6570\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u6807\u51c6MoE\u67b6\u6784\uff0c\u5df2\u88abNemotron-3 Super\u548cUltra\u6a21\u578b\u91c7\u7528\uff0c\u5e76\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\u3002", "conclusion": "LatentMoE\u901a\u8fc7\u7cfb\u7edf\u5316\u8bbe\u8ba1\u63a2\u7d22\u548c\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684MoE\u67b6\u6784\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u63a8\u7406\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18091", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18091", "abs": "https://arxiv.org/abs/2601.18091", "authors": ["Longwei Ding", "Anhao Zhao", "Fanghua Ye", "Ziyang Chen", "Xiaoyu Shen"], "title": "From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models", "comment": "18 pages, 7 figures", "summary": "Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\\textbf{LLM-instruct}$) and reasoning-augmented ($\\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.", "code_url": "https://github.com/EIT-NLP/LRM-Prunin", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u6bd4\u4e86\u6307\u4ee4\u9075\u5faa\u578bLLM\u548c\u63a8\u7406\u589e\u5f3a\u578bLLM\u7684\u526a\u679d\u7b56\u7565\u6548\u679c\uff0c\u53d1\u73b0\u4e0d\u540c\u8303\u5f0f\u4e0b\u526a\u679d\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u9488\u5bf9\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u8bbe\u8ba1\u4e13\u95e8\u7684\u526a\u679d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6307\u4ee4\u9075\u5faa\u578bLLM\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u63a8\u7406\u589e\u5f3a\u578b\u6a21\u578b\uff08\u751f\u6210\u957f\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\uff09\u7684\u526a\u679d\u6548\u679c\u4e86\u89e3\u3002\u9700\u8981\u660e\u786e\u5df2\u6709\u526a\u679d\u7b56\u7565\u662f\u5426\u9002\u7528\u4e8e\u8fd9\u7c7b\u6a21\u578b\u3002", "method": "\u91c7\u7528\u63a7\u5236\u53d8\u91cf\u7814\u7a76\uff0c\u5bf9\u9f50\u526a\u679d\u6821\u51c6\u548c\u540e\u526a\u679d\u6062\u590d\u6570\u636e\u4e0e\u539f\u59cb\u8bad\u7ec3\u5206\u5e03\u3002\u8bc4\u4f30\u9759\u6001\u6df1\u5ea6\u526a\u679d\u3001\u9759\u6001\u5bbd\u5ea6\u526a\u679d\u548c\u52a8\u6001\u526a\u679d\u4e09\u79cd\u7b56\u7565\uff0c\u572817\u4e2a\u6db5\u76d6\u5206\u7c7b\u3001\u751f\u6210\u548c\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u8303\u5f0f\u4f9d\u8d56\u7684\u660e\u663e\u5dee\u5f02\uff1a\u6df1\u5ea6\u526a\u679d\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5bbd\u5ea6\u526a\u679d\u5728\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u66f4\u7a33\u5065\uff1b\u9759\u6001\u526a\u679d\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u63a8\u7406\u6027\u80fd\uff0c\u52a8\u6001\u526a\u679d\u5728\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u4f46\u5728\u957f\u94fe\u63a8\u7406\u4e0a\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "\u63a8\u7406\u589e\u5f3a\u578bLLM\u9700\u8981\u4e13\u95e8\u8003\u8651\u5176\u7279\u6027\u7684\u526a\u679d\u7b56\u7565\uff0c\u4e0d\u80fd\u7b80\u5355\u5957\u7528\u6307\u4ee4\u9075\u5faa\u578b\u6a21\u578b\u7684\u526a\u679d\u65b9\u6cd5\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u9488\u5bf9\u4e0d\u540c\u6a21\u578b\u8303\u5f0f\u8bbe\u8ba1\u5b9a\u5236\u5316\u526a\u679d\u65b9\u6848\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.18110", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18110", "abs": "https://arxiv.org/abs/2601.18110", "authors": ["Pedram Zaree", "Md Abdullah Al Mamun", "Yue Dong", "Ihsen Alouani", "Nael Abu-Ghazaleh"], "title": "AttenMIA: LLM Membership Inference Attack through Attention Signals", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.", "AI": {"tldr": "AttenMIA\uff1a\u4e00\u79cd\u5229\u7528Transformer\u81ea\u6ce8\u610f\u529b\u6a21\u5f0f\u8fdb\u884c\u6210\u5458\u63a8\u7406\u653b\u51fb\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u5934\u5728\u4e0d\u540c\u5c42\u7684\u6a21\u5f0f\u6765\u8bc6\u522b\u8bad\u7ec3\u6570\u636e\u6210\u5458\uff0c\u5728\u4f4e\u8bef\u62a5\u7387\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5e9e\u5927\u6027\u5bfc\u81f4\u5176\u503e\u5411\u4e8e\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u4e25\u91cd\u7684\u9690\u79c1\u548c\u77e5\u8bc6\u4ea7\u6743\u95ee\u9898\u3002\u73b0\u6709\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u4e3b\u8981\u4f9d\u8d56\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u6216\u5d4c\u5165\u7279\u5f81\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u53f7\u901a\u5e38\u8106\u5f31\uff0c\u653b\u51fb\u6210\u529f\u7387\u6709\u9650\u3002", "method": "\u63d0\u51faAttenMIA\u6846\u67b6\uff0c\u5229\u7528Transformer\u5185\u90e8\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5f0f\u63a8\u65ad\u6210\u5458\u8eab\u4efd\u3002\u6ce8\u610f\u529b\u63a7\u5236Transformer\u5185\u7684\u4fe1\u606f\u6d41\uff0c\u66b4\u9732\u4e0d\u540c\u7684\u8bb0\u5fc6\u6a21\u5f0f\u3002\u65b9\u6cd5\u4f7f\u7528\u8de8\u5c42\u6ce8\u610f\u529b\u5934\u7684\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u6270\u52a8\u7684\u6563\u5ea6\u5ea6\u91cf\u6765\u8bad\u7ec3\u6709\u6548\u7684MIA\u5206\u7c7b\u5668\u3002", "result": "\u5728LLaMA-2\u3001Pythia\u548cOpt\u7b49\u5f00\u6e90\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u91cd\u8981\u7684\u4f4e\u8bef\u62a5\u7387\u6307\u6807\u4e0b\uff08\u5728WikiMIA-32\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLlama2-13b\u8fbe\u52300.996 ROC AUC\u548c87.9% TPR@1%FPR\uff09\u3002\u6ce8\u610f\u529b\u4fe1\u53f7\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u67b6\u6784\u95f4\u5177\u6709\u6cdb\u5316\u6027\uff0c\u5c42\u548c\u5934\u7ea7\u5206\u6790\u663e\u793a\u6210\u5458\u6cc4\u9732\u6700\u660e\u663e\u7684\u533a\u57df\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u539f\u672c\u65e8\u5728\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u5374\u65e0\u610f\u4e2d\u653e\u5927\u4e86LLMs\u7684\u9690\u79c1\u98ce\u9669\u3002AttenMIA\u5728\u6570\u636e\u63d0\u53d6\u6846\u67b6\u4e2d\u66ff\u4ee3\u5176\u4ed6\u6210\u5458\u63a8\u7406\u653b\u51fb\u65f6\uff0c\u80fd\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u653b\u51fb\uff0c\u51f8\u663e\u4e86\u5f00\u53d1\u65b0\u9632\u5fa1\u63aa\u65bd\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2601.18111", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18111", "abs": "https://arxiv.org/abs/2601.18111", "authors": ["Jean Kossaifi", "Nikola Kovachki", "Morteza Mardani", "Daniel Leibovici", "Suman Ravuri", "Ira Shokar", "Edoardo Calvello", "Mohammad Shoaib Abbas", "Peter Harrington", "Ashay Subramaniam", "Noah Brenowitz", "Boris Bonev", "Wonmin Byeon", "Karsten Kreis", "Dale Durran", "Arash Vahdat", "Mike Pritchard", "Jan Kautz"], "title": "Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting", "comment": null, "summary": "The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u5c3a\u5ea6\u5927\u6c14\u52a8\u529b\u5b66\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u4e0b\u91c7\u6837\u6f5c\u5728\u7a7a\u95f4\u548c\u5386\u53f2\u6761\u4ef6\u5c40\u90e8\u6295\u5f71\u5668\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7269\u7406\u6a21\u62df\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u6216\u4e13\u95e8\u8bad\u7ec3\u7b56\u7565\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6982\u7387\u9884\u62a5\u6280\u80fd\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u9a71\u52a8\u7684\u5929\u6c14\u9884\u62a5\u65b9\u6cd5\u5b58\u5728\u67b6\u6784\u590d\u6742\u3001\u8bad\u7ec3\u7b56\u7565\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u8fd9\u63a9\u76d6\u4e86\u9884\u62a5\u51c6\u786e\u6027\u7684\u6839\u672c\u9a71\u52a8\u56e0\u7d20\u3002\u7814\u7a76\u65e8\u5728\u8bc1\u660e\u6700\u5148\u8fdb\u7684\u6982\u7387\u9884\u62a5\u6280\u80fd\u4e0d\u9700\u8981\u590d\u6742\u7684\u67b6\u6784\u7ea6\u675f\u6216\u4e13\u95e8\u7684\u8bad\u7ec3\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u53ef\u6269\u5c55\u7684\u591a\u5c3a\u5ea6\u5927\u6c14\u52a8\u529b\u5b66\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u76f4\u63a5\u4e0b\u91c7\u6837\u7684\u6f5c\u5728\u7a7a\u95f4\u548c\u5386\u53f2\u6761\u4ef6\u5c40\u90e8\u6295\u5f71\u5668\u6765\u89e3\u6790\u9ad8\u5206\u8fa8\u7387\u7269\u7406\u3002\u8be5\u6846\u67b6\u8bbe\u8ba1\u5bf9\u6982\u7387\u4f30\u8ba1\u5668\u9009\u62e9\u5177\u6709\u9c81\u68d2\u6027\uff0c\u65e0\u7f1d\u652f\u6301\u968f\u673a\u63d2\u503c\u3001\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8eCRPS\u7684\u96c6\u6210\u8bad\u7ec3\u3002", "result": "\u4e0e\u96c6\u6210\u9884\u62a5\u7cfb\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u6982\u7387\u6a21\u578bGenCast\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u5927\u591a\u6570\u53d8\u91cf\u4e0a\u5b9e\u73b0\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u6539\u8fdb\u3002\u9a8c\u8bc1\u8868\u660e\u8be5\u6846\u67b6\u5728\u4e2d\u7b49\u8303\u56f4\u9884\u62a5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u6269\u5c55\u901a\u7528\u6a21\u578b\u8db3\u4ee5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u4e2d\u671f\u9884\u62a5\uff0c\u65e0\u9700\u5b9a\u5236\u8bad\u7ec3\u65b9\u6848\uff0c\u4e14\u5728\u5b8c\u6574\u7684\u6982\u7387\u6846\u67b6\u8c31\u7cfb\u4e2d\u90fd\u6709\u6548\u3002\u8fd9\u7b80\u5316\u4e86\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5f00\u53d1\uff0c\u7a81\u51fa\u4e86\u53ef\u6269\u5c55\u6027\u800c\u975e\u4e13\u95e8\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.18115", "categories": ["cs.LG", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.18115", "abs": "https://arxiv.org/abs/2601.18115", "authors": ["Guyang Cao", "Shuyao Li", "Sushrut Karmalkar", "Jelena Diakonikolas"], "title": "Robust Learning of a Group DRO Neuron", "comment": null, "summary": "We study the problem of learning a single neuron under standard squared loss in the presence of arbitrary label noise and group-level distributional shifts, for a broad family of covariate distributions. Our goal is to identify a ''best-fit'' neuron parameterized by $\\mathbf{w}_*$ that performs well under the most challenging reweighting of the groups. Specifically, we address a Group Distributionally Robust Optimization problem: given sample access to $K$ distinct distributions $\\mathcal p_{[1]},\\dots,\\mathcal p_{[K]}$, we seek to approximate $\\mathbf{w}_*$ that minimizes the worst-case objective over convex combinations of group distributions $\\boldsymbol\u03bb \\in \u0394_K$, where the objective is $\\sum_{i \\in [K]}\u03bb_{[i]}\\,\\mathbb E_{(\\mathbf x,y)\\sim\\mathcal p_{[i]}}(\u03c3(\\mathbf w\\cdot\\mathbf x)-y)^2 - \u03bdd_f(\\boldsymbol\u03bb,\\frac{1}{K}\\mathbf1)$ and $d_f$ is an $f$-divergence that imposes (optional) penalty on deviations from uniform group weights, scaled by a parameter $\u03bd\\geq 0$. We develop a computationally efficient primal-dual algorithm that outputs a vector $\\widehat{\\mathbf w}$ that is constant-factor competitive with $\\mathbf{w}_*$ under the worst-case group weighting. Our analytical framework directly confronts the inherent nonconvexity of the loss function, providing robust learning guarantees in the face of arbitrary label corruptions and group-specific distributional shifts. The implementation of the dual extrapolation update motivated by our algorithmic framework shows promise on LLM pre-training benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5728\u4efb\u610f\u6807\u7b7e\u566a\u58f0\u548c\u7ec4\u7ea7\u5206\u5e03\u504f\u79fb\u4e0b\u5b66\u4e60\u5355\u4e2a\u795e\u7ecf\u5143\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u539f\u59cb-\u5bf9\u5076\u7b97\u6cd5\uff0c\u80fd\u5728\u6700\u574f\u60c5\u51b5\u7ec4\u6743\u91cd\u4e0b\u83b7\u5f97\u4e0e\u6700\u4f18\u53c2\u6570\u5e38\u6570\u500d\u7ade\u4e89\u7684\u8fd1\u4f3c\u89e3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u5b66\u4e60\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6807\u7b7e\u566a\u58f0\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u6570\u636e\u4e0e\u6d4b\u8bd5\u6570\u636e\u540c\u5206\u5e03\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u9762\u4e34\u7ec4\u7ea7\u5206\u5e03\u504f\u79fb\u548c\u6807\u7b7e\u566a\u58f0\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u6700\u574f\u60c5\u51b5\u7ec4\u52a0\u6743\u7684\u9c81\u68d2\u5b66\u4e60\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u539f\u59cb-\u5bf9\u5076\u7b97\u6cd5\u6765\u89e3\u51b3\u7ec4\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u5904\u7406\u635f\u5931\u51fd\u6570\u7684\u975e\u51f8\u6027\uff0c\u901a\u8fc7f-\u6563\u5ea6\u60e9\u7f5a\u504f\u79bb\u5747\u5300\u7ec4\u6743\u91cd\u7684\u7a0b\u5ea6\uff0c\u4f7f\u7528\u5bf9\u5076\u5916\u63a8\u66f4\u65b0\u673a\u5236\uff0c\u8f93\u51fa\u4e0e\u6700\u4f18\u53c2\u6570\u5e38\u6570\u500d\u7ade\u4e89\u7684\u8fd1\u4f3c\u89e3\u3002", "result": "\u7b97\u6cd5\u8f93\u51fa\u5411\u91cf\u0175\u5728\u6700\u574f\u60c5\u51b5\u7ec4\u6743\u91cd\u4e0b\u4e0e\u6700\u4f18\u53c2\u6570w*\u4fdd\u6301\u5e38\u6570\u500d\u7ade\u4e89\u5173\u7cfb\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4efb\u610f\u6807\u7b7e\u566a\u58f0\u548c\u7ec4\u7279\u5b9a\u5206\u5e03\u504f\u79fb\uff0c\u5bf9\u5076\u5916\u63a8\u66f4\u65b0\u5728LLM\u9884\u8bad\u7ec3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u826f\u597d\u524d\u666f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u6807\u7b7e\u566a\u58f0\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u5b66\u4e60\u5355\u4e2a\u795e\u7ecf\u5143\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6846\u67b6\u76f4\u63a5\u5904\u7406\u975e\u51f8\u635f\u5931\u51fd\u6570\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18142", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18142", "abs": "https://arxiv.org/abs/2601.18142", "authors": ["Mingxu Zhang", "Huicheng Zhang", "Jiaming Ji", "Yaodong Yang", "Ying Sun"], "title": "Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods", "comment": null, "summary": "Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\\%, establishing superior effectiveness for Safe RL in complex environments.", "AI": {"tldr": "\u63d0\u51faADRC-Lagrangian\u65b9\u6cd5\uff0c\u5c06\u4e3b\u52a8\u6297\u6270\u63a7\u5236\u4e0e\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u6027\u80fd", "motivation": "\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5305\u62ecPID\u548c\u7ecf\u5178\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\uff09\u5b58\u5728\u632f\u8361\u548c\u9891\u7e41\u5b89\u5168\u8fdd\u89c4\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u53c2\u6570\u654f\u611f\u6027\u548c\u56fa\u6709\u7684\u76f8\u4f4d\u6ede\u540e\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u63a7\u5236\u65b9\u6cd5", "method": "\u63d0\u51faADRC-Lagrangian\u65b9\u6cd5\uff0c\u5c06\u4e3b\u52a8\u6297\u6270\u63a7\u5236\uff08ADRC\uff09\u96c6\u6210\u5230\u62c9\u683c\u6717\u65e5\u6846\u67b6\u4e2d\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u632f\u8361\uff0c\u8be5\u7edf\u4e00\u6846\u67b6\u5305\u542b\u7ecf\u5178\u548cPID\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u4f5c\u4e3a\u7279\u4f8b", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u5b89\u5168\u8fdd\u89c4\u51cf\u5c1174%\uff0c\u7ea6\u675f\u8fdd\u89c4\u5e45\u5ea6\u964d\u4f4e89%\uff0c\u5e73\u5747\u6210\u672c\u51cf\u5c1167%\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u6548\u679c", "conclusion": "ADRC-Lagrangian\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u4e3b\u52a8\u6297\u6270\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u632f\u8361\u548c\u5b89\u5168\u8fdd\u89c4\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18150", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18150", "abs": "https://arxiv.org/abs/2601.18150", "authors": ["Zhaopeng Qiu", "Shuang Yu", "Jingqi Zhang", "Shuai Zhang", "Xue Huang", "Jingyi Yang", "Junjie Lai"], "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684FP8\u63a8\u7406\u6808\uff0c\u7528\u4e8e\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u901a\u8fc7W8A8\u7ebf\u6027\u5c42\u91cf\u5316\u3001KV\u7f13\u5b58FP8\u5316\u548c\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad844%\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\uff0c\u957f\u5e8f\u5217\u751f\u6210\u5bfc\u81f4\u6ce8\u610f\u529b\u673a\u5236\u548cKV\u7f13\u5b58\u5185\u5b58\u6210\u4e3a\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u7684\u4e3b\u8981\u74f6\u9888\u3002FP8\u91cf\u5316\u867d\u7136\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u6d41\u91cf\uff0c\u4f46\u5728RL\u573a\u666f\u4e2d\u9762\u4e34\u6743\u91cd\u9891\u7e41\u53d8\u5316\u548c\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "1) \u4f7f\u7528\u5757\u7ea7FP8\u91cf\u5316\u5b9e\u73b0W8A8\u7ebf\u6027\u5c42\u63a8\u7406\uff1b2) \u901a\u8fc7\u6bcf\u6b65QKV\u5c3a\u5ea6\u91cd\u65b0\u6821\u51c6\u5c06FP8\u6269\u5c55\u5230KV\u7f13\u5b58\uff0c\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5185\u5b58\u74f6\u9888\uff1b3) \u91c7\u7528\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u7684\u63a8\u7406\u6821\u6b63\uff08token\u7ea7TIS/MIS\u53d8\u4f53\uff09\u7f13\u89e3\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u5bc6\u96c6\u6a21\u578b\u548cMoE\u6a21\u578b\u4e0a\uff0c\u8be5\u6280\u672f\u5b9e\u73b0\u4e86\u6700\u9ad844%\u7684\u63a8\u7406\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0eBF16\u57fa\u7ebf\u76f8\u5f53\u7684\u5b66\u4e60\u884c\u4e3a\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684FP8\u63a8\u7406\u6808\uff0c\u89e3\u51b3\u4e86RL\u573a\u666f\u4e2dFP8\u91cf\u5316\u7684\u5de5\u7a0b\u548c\u7b97\u6cd5\u6311\u6218\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4e3a\u9ad8\u6548RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18171", "abs": "https://arxiv.org/abs/2601.18171", "authors": ["Yuguang Zhang", "Lijun Sheng", "Jian Liang", "Ran He"], "title": "Learning Fair Domain Adaptation with Virtual Label Distribution", "comment": "ICASSP 2026", "summary": "Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.", "AI": {"tldr": "\u63d0\u51faVILL\u6846\u67b6\u89e3\u51b3\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u7c7b\u522b\u516c\u5e73\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u548cKL\u6563\u5ea6\u91cd\u5e73\u8861\u7b56\u7565\u63d0\u5347\u6700\u5dee\u7c7b\u522b\u6027\u80fd", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4f46\u5ffd\u89c6\u4e86\u4e0d\u540c\u7c7b\u522b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff08\u7c7b\u522b\u516c\u5e73\u6027\u95ee\u9898\uff09\u3002\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0UDA\u5206\u7c7b\u5668\u503e\u5411\u4e8e\u504f\u597d\u5bb9\u6613\u7c7b\u522b\u800c\u5ffd\u89c6\u56f0\u96be\u7c7b\u522b\u3002", "method": "\u63d0\u51fa\u865a\u62df\u6807\u7b7e\u5206\u5e03\u611f\u77e5\u5b66\u4e60\uff08VILL\uff09\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u7b56\u7565\uff08\u653e\u5927\u56f0\u96be\u7c7b\u522b\u5f71\u54cd\uff09\u548c\u57fa\u4e8eKL\u6563\u5ea6\u7684\u91cd\u5e73\u8861\u7b56\u7565\uff08\u663e\u5f0f\u8c03\u6574\u51b3\u7b56\u8fb9\u754c\u4ee5\u589e\u5f3a\u7c7b\u522b\u516c\u5e73\u6027\uff09\u3002", "result": "\u5728\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVILL\u53ef\u4ee5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709UDA\u65b9\u6cd5\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u7c7b\u522b\u516c\u5e73\u6027\u3002", "conclusion": "VILL\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u6574\u4f53\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u6709\u6548\u6539\u5584\u4e86\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u7c7b\u522b\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6700\u5dee\u7c7b\u522b\u6027\u80fd\u3002"}}
{"id": "2601.18189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18189", "abs": "https://arxiv.org/abs/2601.18189", "authors": ["Rui Wu", "Yongjun Li"], "title": "Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients", "comment": "20 pages, 8 figures", "summary": "Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.", "AI": {"tldr": "\u63d0\u51faAHOC\u6df7\u5408\u9636\u65e0\u73af\u7ea6\u675f\u548cSPG-AHOC\u5e73\u6ed1\u8fd1\u7aef\u68af\u5ea6\u4f18\u5316\uff0c\u8bc1\u660e\u6709\u9650\u65f6\u95f4oracle\u6027\u8d28\uff1a\u5728\u6807\u51c6\u53ef\u8bc6\u522b\u6027\u5047\u8bbe\u4e0b\uff0c\u6709\u9650\u8fed\u4ee3\u5185\u7cbe\u786e\u6062\u590dDAG\u7ed3\u6784\uff0c\u65e0\u9700\u542f\u53d1\u5f0f\u622a\u65ad\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5\uff08\u5982NOTEARS\uff09\u4ec5\u4fdd\u8bc1\u6e10\u8fd1\u6536\u655b\u5230\u5e73\u7a33\u70b9\uff0c\u901a\u5e38\u4ea7\u751f\u7a20\u5bc6\u6743\u91cd\u77e9\u9635\uff0c\u9700\u8981\u4efb\u610f\u540e\u5904\u7406\u9608\u503c\u5316\u6765\u6062\u590dDAG\u3002\u8fde\u7eed\u4f18\u5316\u4e0e\u79bb\u6563\u56fe\u7ed3\u6784\u4e4b\u95f4\u7684\u5dee\u8ddd\u4ecd\u662f\u6839\u672c\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u9636\u65e0\u73af\u7ea6\u675f(AHOC)\uff0c\u901a\u8fc7\u5e73\u6ed1\u8fd1\u7aef\u68af\u5ea6(SPG-AHOC)\u4f18\u5316\u3002\u5229\u7528\u8fd1\u7aef\u7b97\u6cd5\u7684\u6d41\u5f62\u8bc6\u522b\u6027\u8d28\uff0c\u5728\u6807\u51c6\u53ef\u8bc6\u522b\u6027\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u6709\u9650\u65f6\u95f4oracle\u6027\u8d28\uff1a\u6709\u9650\u8fed\u4ee3\u5185\u7cbe\u786e\u6062\u590dDAG\u652f\u6491\uff08\u7ed3\u6784\uff09\u3002", "result": "\u7406\u8bba\u8bc1\u660eSPG-AHOC\u5728\u6709\u9650\u8fed\u4ee3\u5185\u6062\u590d\u7cbe\u786eDAG\u7ed3\u6784\uff0c\u65e0\u9700\u542f\u53d1\u5f0f\u622a\u65ad\u3002\u5b9e\u8bc1\u663e\u793a\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u5f3a\u70c8\u652f\u6301\u6709\u9650\u65f6\u95f4\u8bc6\u522b\u7406\u8bba\u3002", "conclusion": "\u901a\u8fc7AHOC\u7ea6\u675f\u548cSPG-AHOC\u4f18\u5316\uff0c\u5f25\u5408\u4e86\u8fde\u7eed\u4f18\u5316\u4e0e\u79bb\u6563\u56fe\u7ed3\u6784\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u6709\u9650\u65f6\u95f4\u7cbe\u786e\u7ed3\u6784\u6062\u590d\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u6d88\u9664\u7ed3\u6784\u6a21\u7cca\u6027\u3002"}}
{"id": "2601.18231", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18231", "abs": "https://arxiv.org/abs/2601.18231", "authors": ["Trong Khiem Tran", "Manh Cuong Dao", "Phi Le Nguyen", "Thao Nguyen Truong", "Trong Nghia Hoang"], "title": "Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting", "comment": "Accepted AISTATS 20226. Preprint version", "summary": "Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u65b0\u7279\u5f81\u6a21\u6001\u65f6\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u5fae\u8c03\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u7279\u5f81-\u6807\u7b7e\u626d\u66f2\u6982\u5ff5\u5efa\u7acb\u4e86\u53ef\u8bc1\u660e\u7684\u6cdb\u5316\u8fb9\u754c\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u6539\u8fdb\u7684\u7b97\u6cd5\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u672a\u89c1\u7279\u5f81\u6a21\u6001\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u5fae\u8c03\u4e4b\u95f4\u5173\u952e\u4ea4\u4e92\u4f5c\u7528\u7684\u7406\u8bba\u7406\u89e3\u3002\u672a\u6821\u51c6\u7684\u7ec4\u5408\u4f1a\u52a0\u5267\u6e90\u57df\u548c\u76ee\u6807\u57df\u7279\u5f81-\u6807\u7b7e\u7ed3\u6784\u4e4b\u95f4\u7684\u9519\u4f4d\uff0c\u964d\u4f4e\u76ee\u6807\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u7279\u5f81-\u6807\u7b7e\u626d\u66f2\u6982\u5ff5\u5efa\u7acb\u4e86\u76ee\u6807\u8bef\u5dee\u7684\u53ef\u8bc1\u660e\u6cdb\u5316\u8fb9\u754c\u3002\u8be5\u8fb9\u754c\u89e3\u91ca\u4e86\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u62df\u5408\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u4e3a\u5b9e\u9645\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u4f18\u5316\u89c1\u89e3\u3002", "result": "\u57fa\u4e8e\u7406\u8bba\u6846\u67b6\u8bbe\u8ba1\u7684\u7b97\u6cd5\u5728\u5e7f\u6cdb\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u5fae\u8c03\u4ea4\u4e92\u4f5c\u7528\u7406\u8bba\u7406\u89e3\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u4e0d\u4ec5\u89e3\u91ca\u4e86\u8fd9\u4e00\u5173\u952e\u4ea4\u4e92\uff0c\u8fd8\u4e3a\u5b9e\u9645\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u539f\u5219\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u7684\u6539\u8fdb\u6027\u80fd\u3002"}}
{"id": "2601.18245", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18245", "abs": "https://arxiv.org/abs/2601.18245", "authors": ["Santanu Das", "Jatin Batra"], "title": "Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity", "comment": null, "summary": "Phase retrieval is the classical problem of recovering a signal $x^* \\in \\mathbb{R}^n$ from its noisy phaseless measurements $y_i = \\langle a_i, x^* \\rangle^2 + \u03b6_i$ (where $\u03b6_i$ denotes noise, and $a_i$ is the sensing vector) for $i \\in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \\log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u89e3\u51b3\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u95ee\u9898\uff0c\u5728\u5b58\u5728\u91cd\u5c3e\u566a\u58f0\u548c\u5bf9\u6297\u6027\u635f\u574f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8fd1\u7ebf\u6027\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u76f8\u4f4d\u6062\u590d\u5728\u5149\u5b66\u3001\u6676\u4f53\u5b66\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u5bf9\u6d4b\u91cf\u8bef\u5dee\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u867d\u7136\u6700\u8fd1\u5728\u9c81\u68d2\u7edf\u8ba1\u65b9\u9762\u6709\u7a81\u7834\uff0c\u4f46\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u95ee\u9898\u4ecd\u7f3a\u4e4f\u9ad8\u6548\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u9700\u8981\u9c81\u68d2\u8c31\u521d\u59cb\u5316\u8fd9\u4e00\u5173\u952e\u6b65\u9aa4\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u9c81\u68d2\u8c31\u521d\u59cb\u5316\u4e0e\u9c81\u68d2PCA\u4e4b\u95f4\u7684\u8fde\u63a5\uff0c\u5229\u7528\u9c81\u68d2PCA\u7684\u6700\u65b0\u7b97\u6cd5\u8fdb\u5c55\uff0c\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u7b97\u6cd5\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u7b97\u6cd5\uff0c\u5728\u5b58\u5728\u91cd\u5c3e\u566a\u58f0\u548c\u5bf9\u6297\u6027\u635f\u574f\u7684\u60c5\u51b5\u4e0b\uff0c\u6837\u672c\u590d\u6742\u5ea6\u8fbe\u5230\u8fd1\u7ebf\u6027\uff08O(n log n)\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6307\u6570\u65f6\u95f4\u7b97\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u8fde\u63a5\u9c81\u68d2\u8c31\u521d\u59cb\u5316\u548c\u9c81\u68d2PCA\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u7684\u9ad8\u6548\u7b97\u6cd5\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18255", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18255", "abs": "https://arxiv.org/abs/2601.18255", "authors": ["Fei Meng"], "title": "Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs", "comment": null, "summary": "Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \\textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief \"wake-up\" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded \"safety guarantee\" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u7ecf\u9a8c\u56de\u653e\u5728\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u4e8c\u5206\u73b0\u8c61\uff1a\u5bf9\u975e\u7ed3\u6784\u5316\u4efb\u52a1\u4ea7\u751f\u6b63\u5411\u8fc1\u79fb\uff0c\u4f46\u5bf9\u7ed3\u6784\u5316\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u751f\u6210\uff09\u9020\u6210\u8d1f\u5411\u8fc1\u79fb\uff0c\u5e76\u63d0\u51fa\u4e86\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5524\u9192\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u56f0\u5883\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u7a33\u5b9a\u6027\uff08\u4fdd\u7559\u65e7\u77e5\u8bc6\uff09\u4e0e\u53ef\u5851\u6027\uff08\u5b66\u4e60\u65b0\u4efb\u52a1\uff09\u7684\u5e73\u8861\u6311\u6218\u3002\u867d\u7136\u7ecf\u9a8c\u56de\u653e\u662f\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u5176\u5bf9\u4e0d\u540c\u80fd\u529b\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u7ecf\u9a8c\u56de\u653e\u5728\u4e0d\u540c\u7c7b\u578b\u4efb\u52a1\u4e0a\u7684\u5dee\u5f02\u5316\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5524\u9192\uff08OSW\uff09\u65b9\u6cd5\uff1a\u901a\u8fc7\u7b80\u77ed\u7684\"\u5524\u9192\"\u9636\u6bb5\u8bc6\u522b\u5148\u524d\u4efb\u52a1\u7684\u5173\u952e\u53c2\u6570\u5b50\u7a7a\u95f4\uff0c\u5e76\u4e3a\u65b0\u4efb\u52a1\u5f3a\u5236\u6267\u884c\u6b63\u4ea4\u66f4\u65b0\uff0c\u4e3a\u5df2\u5efa\u7acb\u7684\u77e5\u8bc6\u7ed3\u6784\u63d0\u4f9b\u6570\u5b66\u57fa\u7840\u7684\"\u5b89\u5168\u4fdd\u8bc1\"\u3002", "result": "\u7ecf\u9a8c\u56de\u653e\u8868\u73b0\u51fa\u5173\u952e\u4e8c\u5206\u73b0\u8c61\uff1a\u5bf9\u9c81\u68d2\u7684\u975e\u7ed3\u6784\u5316\u4efb\u52a1\uff08\u5982NLP\u5206\u7c7b\uff09\u4ea7\u751f\u6b63\u5411\u540e\u5411\u8fc1\u79fb\uff0c\u4f46\u5bf9\u8106\u5f31\u7684\u7ed3\u6784\u5316\u9886\u57df\uff08\u5982\u4ee3\u7801\u751f\u6210\uff09\u9020\u6210\u4e25\u91cd\u7684\u8d1f\u5411\u8fc1\u79fb\uff08\u7f16\u7801\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff09\u3002OSW\u5728\u56db\u4efb\u52a1\u5e8f\u5217\u5b9e\u9a8c\u4e2d\u6210\u529f\u4fdd\u7559\u4e86\u7ecf\u9a8c\u56de\u653e\u5931\u8d25\u7684\u8106\u5f31\u7f16\u7801\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u65b0\u4efb\u52a1\u7684\u9ad8\u53ef\u5851\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u8bc4\u4f30\u7ed3\u6784\u5b89\u5168\u6027\u4e0e\u5e73\u5747\u4fdd\u7559\u7387\u540c\u7b49\u91cd\u8981\u3002OSW\u65b9\u6cd5\u901a\u8fc7\u6570\u5b66\u4fdd\u8bc1\u4fdd\u62a4\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u4e3a\u89e3\u51b3\u7ecf\u9a8c\u56de\u653e\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.18261", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18261", "abs": "https://arxiv.org/abs/2601.18261", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Yukun Ma", "Chong Zhang", "Chong Deng", "Qinglin Zhang", "Xiangang Li", "Jieping Ye"], "title": "FGGM: Fisher-Guided Gradient Masking for Continual Learning", "comment": "Accepted by ICASSP 2026", "summary": "Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.", "AI": {"tldr": "FGGM\u901a\u8fc7\u57fa\u4e8eFisher\u4fe1\u606f\u7684\u68af\u5ea6\u63a9\u7801\u7b56\u7565\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u76f8\u6bd4SFT\u548cMIGU\u5728TRACE\u57fa\u51c6\u4e0a\u5206\u522b\u53d6\u5f979.6%\u548c4.4%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u53c2\u6570\u5e45\u5ea6\u7684\u63a9\u7801\u65b9\u6cd5\uff08\u5982MIGU\uff09\u7f3a\u4e4f\u6570\u5b66\u539f\u7406\u652f\u6491\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7406\u8bba\u5316\u7684\u53c2\u6570\u91cd\u8981\u6027\u8bc4\u4f30\u65b9\u6cd5\u6765\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u3002", "method": "\u63d0\u51faFisher\u5f15\u5bfc\u7684\u68af\u5ea6\u63a9\u7801\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u89d2Fisher\u4fe1\u606f\u77e9\u9635\u52a8\u6001\u751f\u6210\u4e8c\u5143\u63a9\u7801\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u7b56\u7565\u6027\u5730\u9009\u62e9\u9700\u8981\u66f4\u65b0\u7684\u53c2\u6570\uff0c\u4fdd\u7559\u5173\u952e\u53c2\u6570\u4ee5\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\uff0c\u4e14\u65e0\u9700\u5386\u53f2\u6570\u636e\u3002", "result": "\u5728TRACE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFGGM\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03\u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u65b9\u9762\u83b7\u5f979.6%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u76f8\u6bd4MIGU\u5728TRACE\u4efb\u52a1\u4e0a\u83b7\u5f974.4%\u7684\u6539\u8fdb\u3002\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u989d\u5916\u5206\u6790\u8bc1\u5b9e\u4e86FGGM\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u51cf\u5c11\u7684\u9057\u5fd8\u73b0\u8c61\u3002", "conclusion": "FGGM\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b66\u539f\u7406\u7684\u53c2\u6570\u91cd\u8981\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u5e73\u8861\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18278", "abs": "https://arxiv.org/abs/2601.18278", "authors": ["Indr\u0117 \u017dliobait\u0117"], "title": "What Do Learned Models Measure?", "comment": null, "summary": "In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u79d1\u5b66\u5e94\u7528\u4e2d\u4f5c\u4e3a\u6d4b\u91cf\u5de5\u5177\u4f7f\u7528\u65f6\uff0c\u6807\u51c6\u9884\u6d4b\u8bc4\u4f30\u6807\u51c6\u65e0\u6cd5\u4fdd\u8bc1\u6d4b\u91cf\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u7ef4\u5ea6\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u79d1\u5b66\u548c\u6570\u636e\u9a71\u52a8\u5e94\u7528\u4e2d\u7684\u6d4b\u91cf\u5de5\u5177\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u9884\u6d4b\u9884\u5b9a\u4e49\u6807\u7b7e\u3002\u5f53\u6d4b\u91cf\u51fd\u6570\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u65f6\uff0c\u4ece\u89c2\u5bdf\u5230\u6570\u91cf\u7684\u6620\u5c04\u7531\u8bad\u7ec3\u5206\u5e03\u548c\u5f52\u7eb3\u504f\u7f6e\u9690\u5f0f\u51b3\u5b9a\uff0c\u5bfc\u81f4\u591a\u4e2a\u4e0d\u7b49\u4ef7\u7684\u6620\u5c04\u90fd\u80fd\u6ee1\u8db3\u6807\u51c6\u9884\u6d4b\u8bc4\u4f30\u6807\u51c6\u3002\u8fd9\u51f8\u663e\u4e86\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u5728\u5c06\u6a21\u578b\u8f93\u51fa\u89c6\u4e3a\u6d4b\u91cf\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u5c06\u5b66\u4e60\u5230\u7684\u6d4b\u91cf\u51fd\u6570\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u8bc4\u4f30\u7126\u70b9\uff0c\u5f15\u5165\u6d4b\u91cf\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u8be5\u5c5e\u6027\u6355\u6349\u4e86\u5b66\u4e60\u8fc7\u7a0b\u7684\u53ef\u5bb9\u8bb8\u5b9e\u73b0\u548c\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u6d4b\u91cf\u91cf\u7684\u4e0d\u53d8\u6027\u3002\u901a\u8fc7\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u5177\u6709\u53ef\u6bd4\u9884\u6d4b\u6027\u80fd\u7684\u6a21\u578b\u53ef\u80fd\u5b9e\u73b0\u7cfb\u7edf\u4e0a\u4e0d\u7b49\u4ef7\u7684\u6d4b\u91cf\u51fd\u6570\u3002", "result": "\u7814\u7a76\u8868\u660e\u6807\u51c6\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u6807\u51c6\uff08\u5305\u62ec\u6cdb\u5316\u8bef\u5dee\u3001\u6821\u51c6\u548c\u9c81\u68d2\u6027\uff09\u4e0d\u80fd\u4fdd\u8bc1\u6d4b\u91cf\u7a33\u5b9a\u6027\u3002\u5206\u5e03\u504f\u79fb\u4e3a\u8fd9\u79cd\u5931\u8d25\u63d0\u4f9b\u4e86\u5177\u4f53\u4f8b\u8bc1\uff0c\u5177\u6709\u53ef\u6bd4\u9884\u6d4b\u6027\u80fd\u7684\u6a21\u578b\u53ef\u80fd\u5b9e\u73b0\u7cfb\u7edf\u4e0a\u4e0d\u7b49\u4ef7\u7684\u6d4b\u91cf\u51fd\u6570\u3002", "conclusion": "\u5728\u5c06\u5b66\u4e60\u5230\u7684\u6a21\u578b\u8f93\u51fa\u8bc6\u522b\u4e3a\u6d4b\u91cf\u7684\u573a\u666f\u4e2d\uff0c\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u989d\u5916\u7684\u8bc4\u4f30\u7ef4\u5ea6\u6765\u786e\u4fdd\u6d4b\u91cf\u7a33\u5b9a\u6027\uff0c\u8fd9\u5bf9\u79d1\u5b66\u5e94\u7528\u4e2d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u6d4b\u91cf\u5de5\u5177\u7684\u4f7f\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.18292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18292", "abs": "https://arxiv.org/abs/2601.18292", "authors": ["Zhewen Tan", "Wenhan Yu", "Jianfeng Si", "Tongxin Liu", "Kaiqi Guan", "Huiyan Jin", "Jiawen Tao", "Xiaokun Yuan", "Duohe Ma", "Xiangzheng Zhang", "Tong Yang", "Lin Sun"], "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment", "comment": null, "summary": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.", "AI": {"tldr": "TriPlay-RL\uff1a\u4e00\u4e2a\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u653b\u51fb\u8005\u3001\u9632\u5fa1\u8005\u548c\u8bc4\u4f30\u8005\u4e09\u4e2a\u89d2\u8272\u7684\u534f\u540c\u8fdb\u5316\uff0c\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u7684\u81ea\u52a8\u5316\u548c\u6301\u7eed\u6539\u8fdb\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u6709\u6548\u7f13\u89e3\u6709\u6bd2\u6709\u5bb3\u5185\u5bb9\u7684\u751f\u6210\u3002\u4f20\u7edf\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\uff0c\u4e14\u96be\u4ee5\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5b89\u5168\u5bf9\u9f50\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e09\u4e2a\u89d2\u8272\u7684\u534f\u540c\u8fdb\u5316\u3002", "method": "\u63d0\u51faTriPlay-RL\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u89d2\u8272\uff1a\u653b\u51fb\u8005\uff08\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\uff09\u3001\u9632\u5fa1\u8005\uff08\u5b89\u5168\u9632\u5fa1\uff09\u3001\u8bc4\u4f30\u8005\uff08\u54cd\u5e94\u8bc4\u4f30\uff09\u3002\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u548c\u534f\u540c\u6539\u8fdb\u5b9e\u73b0\u4e09\u4e2a\u89d2\u8272\u7684\u5171\u540c\u8fdb\u5316\uff0c\u51e0\u4e4e\u4e0d\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u653b\u51fb\u8005\u5728\u4fdd\u6301\u9ad8\u8f93\u51fa\u591a\u6837\u6027\u7684\u540c\u65f6\uff0c\u5bf9\u6297\u6548\u679c\u63d0\u534720%-50%\uff1b\u9632\u5fa1\u8005\u5b89\u5168\u6027\u80fd\u63d0\u534710%-30%\u4e14\u4e0d\u635f\u5bb3\u4e00\u822c\u63a8\u7406\u80fd\u529b\uff1b\u8bc4\u4f30\u8005\u901a\u8fc7\u8fed\u4ee3\u6301\u7eed\u4f18\u5316\u7ec6\u7c92\u5ea6\u5224\u65ad\u80fd\u529b\uff0c\u80fd\u51c6\u786e\u533a\u5206\u4e0d\u5b89\u5168\u54cd\u5e94\u3001\u7b80\u5355\u62d2\u7edd\u548c\u6709\u7528\u6307\u5bfc\u3002", "conclusion": "TriPlay-RL\u5efa\u7acb\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u8303\u5f0f\uff0c\u5728\u7edf\u4e00\u5b66\u4e60\u5faa\u73af\u4e2d\u5b9e\u73b0\u4e86\u4e09\u4e2a\u89d2\u8272\u7684\u6301\u7eed\u534f\u540c\u8fdb\u5316\uff0c\u4e3aLLM\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18326", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18326", "abs": "https://arxiv.org/abs/2601.18326", "authors": ["Jie Li", "Jing Li", "Lu Lv", "Zhanyu Ju", "Fengkui Gong"], "title": "Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals", "comment": null, "summary": "We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eZC\u5e8f\u5217\u548c\u65f6\u9891\u56fe\u50cf\u8ba4\u77e5\u878d\u5408\u7684\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u5904\u7406\u63d0\u5347\u65e0\u4eba\u673a\u8fdc\u7a0b\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u8fdc\u7a0b\u8bc6\u522b(RID)\u9700\u8981\u68c0\u6d4b\u672a\u77e5\u6216\u975e\u6807\u51c6\u901a\u4fe1\u534f\u8bae\u7684\u65e0\u4eba\u673a\u4fe1\u53f7\uff08\u5206\u5e03\u5916\u68c0\u6d4b\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u4fe1\u53f7\u7279\u5f81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408ZC\u5e8f\u5217\uff08\u9488\u5bf9DJI\u65e0\u4eba\u673a\u534f\u8bae\uff09\u548c\u65f6\u9891\u56fe\u50cf\uff08\u6355\u83b7\u672a\u77e5\u534f\u8bae\u4fe1\u53f7\u7279\u5f81\uff09\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u3001\u591a\u6a21\u6001\u7279\u5f81\u4ea4\u4e92\u3001\u5355\u6a21\u6001/\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u751f\u6210\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6743\u91cd\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728RID\u548cOODD\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53471.7%\u548c7.5%\uff0c\u5728\u4e0d\u540c\u98de\u884c\u6761\u4ef6\u548c\u65e0\u4eba\u673a\u7c7b\u578b\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u8ba4\u77e5\u878d\u5408\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u65e0\u4eba\u673a\u8fdc\u7a0b\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18329", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18329", "abs": "https://arxiv.org/abs/2601.18329", "authors": ["Chuhan Feng", "Jing Li", "Jie Li", "Lu Lv", "Fengkui Gong"], "title": "Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection", "comment": null, "summary": "We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.", "AI": {"tldr": "\u57fa\u4e8e\u53ef\u533a\u5206\u6027\u9a71\u52a8\u7684\u7a7a\u95f4-\u901a\u9053\u9009\u62e9\u548c\u68af\u5ea6\u8303\u6570\u7684\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u534f\u8bae\u7279\u5b9a\u7684\u65f6\u9891\u7279\u5f81\u91cf\u5316\u7c7b\u95f4\u76f8\u4f3c\u6027\u548c\u65b9\u5dee\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u65f6\u7a7a\u548c\u901a\u9053\u7ef4\u5ea6\u7279\u5f81\uff0c\u7ed3\u5408\u68af\u5ea6\u8303\u6570\u5ea6\u91cf\u6270\u52a8\u654f\u611f\u6027\uff0c\u4e0e\u57fa\u4e8e\u80fd\u91cf\u7684\u5206\u6570\u878d\u5408\u8fdb\u884c\u8054\u5408\u63a8\u65ad\u3002", "motivation": "\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u533a\u5206\u5df2\u77e5\u534f\u8bae\u4fe1\u53f7\u548c\u672a\u77e5/\u5f02\u5e38\u4fe1\u53f7\u3002\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u5229\u7528\u65f6\u9891\u7279\u5f81\u7684\u5224\u522b\u4fe1\u606f\uff0c\u4e14\u5bf9OOD\u6837\u672c\u7684\u56fa\u6709\u4e0d\u7a33\u5b9a\u6027\u6355\u6349\u4e0d\u8db3\u3002", "method": "1. \u57fa\u4e8e\u534f\u8bae\u7279\u5b9a\u65f6\u9891\u7279\u5f81\u91cf\u5316\u7c7b\u95f4\u76f8\u4f3c\u6027\u548c\u65b9\u5dee\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u65f6\u7a7a\u548c\u901a\u9053\u7ef4\u5ea6\u7279\u5f81\uff1b2. \u5f15\u5165\u68af\u5ea6\u8303\u6570\u5ea6\u91cf\u6270\u52a8\u654f\u611f\u6027\uff0c\u6355\u6349OOD\u6837\u672c\u7684\u56fa\u6709\u4e0d\u7a33\u5b9a\u6027\uff1b3. \u5c06\u68af\u5ea6\u8303\u6570\u5ea6\u91cf\u4e0e\u57fa\u4e8e\u80fd\u91cf\u7684\u5206\u6570\u878d\u5408\u8fdb\u884c\u8054\u5408\u63a8\u65ad\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u4fe1\u566a\u6bd4\u548c\u591a\u79cd\u65e0\u4eba\u673a\u7c7b\u578b\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5224\u522b\u80fd\u529b\u548c\u9c81\u68d2\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u53ef\u533a\u5206\u6027\u9a71\u52a8\u7684\u7a7a\u95f4-\u901a\u9053\u9009\u62e9\u548c\u68af\u5ea6\u8303\u6570\u7684OOD\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u52a0\u6743\u548c\u6270\u52a8\u654f\u611f\u6027\u5ea6\u91cf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.18356", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18356", "abs": "https://arxiv.org/abs/2601.18356", "authors": ["Weiqin Yang", "Haowen Xue", "Qingyi Peng", "Hexuan Hu", "Qian Huang", "Tingbo Zhang"], "title": "Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning", "comment": null, "summary": "Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u56e0\u679c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u5c06\u56e0\u679c\u63a8\u7406\u4e0e\u591a\u6a21\u6001\u68c0\u7d22\u7ed3\u5408\uff0c\u901a\u8fc7\u68c0\u7d22\u4e34\u5e8a\u76f8\u5173\u793a\u4f8b\u548c\u56e0\u679c\u56fe\uff0c\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u548c\u5e72\u9884\u8bc1\u636e\u800c\u975e\u4ec5\u76f8\u5173\u6027\u8fdb\u884c\u63a8\u7406\uff0c\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u76f8\u5173\u6027\u63a8\u7406\u800c\u975e\u56e0\u679c\u673a\u5236\uff0c\u5bfc\u81f4\u6a21\u578b\u8106\u5f31\u3001\u6613\u4ea7\u751f\u5e7b\u89c9\u4e14\u5bf9\u6570\u636e\u96c6\u504f\u5dee\u654f\u611f\u3002\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4f9d\u8d56\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u53ef\u80fd\u5f15\u5165\u65b0\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e34\u5e8a\u51b3\u7b56\u5bf9\u56e0\u679c\u63a8\u7406\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u56e0\u679c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u4ece\u5916\u90e8\u6e90\u68c0\u7d22\u4e34\u5e8a\u76f8\u5173\u793a\u4f8b\u548c\u56e0\u679c\u56fe\uff0c\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u548c\u5e72\u9884\u8bc1\u636e\u800c\u975e\u4ec5\u76f8\u5173\u6027\u8fdb\u884c\u6a21\u578b\u63a8\u7406\u3002\u5e94\u7528\u4e8e\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u3001\u8bca\u65ad\u9884\u6d4b\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u3002", "result": "\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u3001\u8bca\u65ad\u9884\u6d4b\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u56e0\u679c\u68c0\u7d22\u4e3a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u4f7f\u5176\u8d85\u8d8a\u6a21\u5f0f\u5339\u914d\uff0c\u5b9e\u73b0\u9ad8\u98ce\u9669\u4e34\u5e8a\u73af\u5883\u4e2d\u53ef\u4fe1\u8d56\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002"}}
{"id": "2601.18401", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18401", "abs": "https://arxiv.org/abs/2601.18401", "authors": ["Yufeng Huang"], "title": "Superlinear Multi-Step Attention", "comment": "30 pages, 6 figures", "summary": "In this paper, we propose \\textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \\textbf{random context access} (a.k.a.\\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.", "AI": {"tldr": "\u63d0\u51faSuperlinear attention\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u6b65\u641c\u7d22\u5b9e\u73b0\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u968f\u673a\u4e0a\u4e0b\u6587\u8bbf\u95ee\u80fd\u529b\uff0c\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u5b9e\u73b0\u9ad8\u6548\u89e3\u7801\u3002", "motivation": "\u6807\u51c6\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u968f\u673a\u4e0a\u4e0b\u6587\u8bbf\u95ee\u80fd\u529b\uff08\u7ed3\u6784\u6027\u6392\u9664\u67d0\u4e9b\u4f4d\u7f6e\uff09\uff0c\u8981\u4e48\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u6548\u7387\u548c\u7075\u6d3b\u6027\u9700\u6c42\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u968f\u673a\u8bbf\u95ee\u53c8\u80fd\u5b9e\u73b0\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u6ce8\u610f\u529b\u67b6\u6784\u3002", "method": "\u5c06\u6807\u51c6\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u91cd\u65b0\u8868\u8ff0\u4e3aN\u6b65\u641c\u7d22\u95ee\u9898\uff0c\u63d0\u51faSuperlinear attention\u67b6\u6784\u3002\u4ee5N=2\u4e3a\u4f8b\uff1a\u7b2c\u4e00\u6b65\u8fdb\u884cO(L^{3/2})\u7684\u8de8\u5ea6\u641c\u7d22\u9009\u62e9\u76f8\u5173\u5e8f\u5217\u6bb5\uff0c\u7b2c\u4e8c\u6b65\u5728\u9009\u5b9a\u6bb5\u4e0a\u5e94\u7528O(L^{3/2})\u7684\u8de8\u5ea6\u6ce8\u610f\u529b\u3002\u6574\u4f53\u590d\u6742\u5ea6\u4e3aO(L^{1+1/N})\uff0c\u5b9e\u73b0\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u3002", "result": "\u5728O(L^{1.54})\u914d\u7f6e\u4e0b\uff0c\u5355B200 GPU\u4e0a\u5b9e\u73b0\uff1a1M\u4e0a\u4e0b\u6587\u957f\u5ea6\u5e73\u5747\u89e3\u7801\u541e\u5410\u91cf114 tokens/sec\uff0c10M\u4e0a\u4e0b\u6587\u957f\u5ea680 tokens/sec\u3002\u5728NIAH\u4efb\u52a1\u4e0a\u8fbe\u5230256K\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5f3a\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u8def\u7531\u8de8\u5ea6\u9009\u62e9\u53ef\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "conclusion": "Superlinear attention\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8bad\u7ec3\u7684\u591a\u6b65\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u968f\u673a\u4e0a\u4e0b\u6587\u8bbf\u95ee\u7684\u540c\u65f6\u5b9e\u73b0\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u4e3a\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8bba\u6587\u4fa7\u91cd\u4e8e\u67b6\u6784\u8868\u8ff0\u3001\u6269\u5c55\u5206\u6790\u548c\u7cfb\u7edf\u53ef\u884c\u6027\u9a8c\u8bc1\uff0c\u5168\u9762\u8d28\u91cf\u8bc4\u4f30\u7559\u5f85\u672a\u6765\u5de5\u4f5c\u3002"}}
{"id": "2601.18409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18409", "abs": "https://arxiv.org/abs/2601.18409", "authors": ["Aniket Sanyal", "Baraah A. M. Sidahmed", "Rebekka Burkholz", "Tatjana Chavdarova"], "title": "Frequency-Based Hyperparameter Selection in Games", "comment": null, "summary": "Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \\emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51faMoLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u4f30\u8ba1\u81ea\u9002\u5e94\u9009\u62e9LookAhead\u8d85\u53c2\u6570\uff0c\u89e3\u51b3\u535a\u5f08\u5b66\u4e60\u4e2d\u65cb\u8f6c\u52a8\u529b\u5b66\u5bfc\u81f4\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u96be\u9898", "motivation": "\u5e73\u6ed1\u535a\u5f08\u4e2d\u7684\u5b66\u4e60\u4e0e\u6807\u51c6\u6700\u5c0f\u5316\u95ee\u9898\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u65cb\u8f6c\u52a8\u529b\u5b66\u4f7f\u7ecf\u5178\u8d85\u53c2\u6570\u8c03\u4f18\u7b56\u7565\u5931\u6548\u3002\u5c3d\u7ba1LookAhead\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5f15\u5165\u4e86\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u7684\u989d\u5916\u53c2\u6570\uff0c\u800c\u535a\u5f08\u4e2d\u7684\u6709\u6548\u8c03\u4f18\u65b9\u6cd5\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8fde\u7eed\u65f6\u95f4\u8f68\u8ff9\u4e2d\u7684\u632f\u8361\u548c\u79bb\u6563\u52a8\u529b\u5b66\u5728\u9891\u7387\u7a7a\u95f4\u7684\u8c31\uff0c\u63d0\u51fa\u6a21\u6001LookAhead(MoLA)\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u632f\u8361\u52a8\u529b\u5b66\u7684\u9891\u7387\u4f30\u8ba1\uff0c\u81ea\u9002\u5e94\u5730\u4e3a\u7ed9\u5b9a\u95ee\u9898\u9009\u62e9\u8d85\u53c2\u6570\uff0c\u662fLookAhead\u7684\u6269\u5c55\u3002", "result": "MoLA\u5728\u7eaf\u65cb\u8f6c\u535a\u5f08\u548c\u6df7\u5408\u673a\u5236\u4e2d\u90fd\u80fd\u52a0\u901f\u8bad\u7ec3\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u9891\u7387\u4f30\u8ba1\u7684\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u9009\u62e9\u4e3a\u535a\u5f08\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0cMoLA\u901a\u8fc7\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u65cb\u8f6c\u52a8\u529b\u5b66\u5e26\u6765\u7684\u8c03\u4f18\u6311\u6218\u3002"}}
{"id": "2601.18420", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18420", "abs": "https://arxiv.org/abs/2601.18420", "authors": ["Satya Prakash Dash", "Hossein Abdi", "Wei Pan", "Samuel Kaski", "Mingfei Sun"], "title": "Gradient Regularized Natural Gradients", "comment": null, "summary": "Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86GRNG\uff08\u68af\u5ea6\u6b63\u5219\u5316\u81ea\u7136\u68af\u5ea6\uff09\u4f18\u5316\u5668\uff0c\u5c06\u663e\u5f0f\u68af\u5ea6\u6b63\u5219\u5316\u4e0e\u81ea\u7136\u68af\u5ea6\u66f4\u65b0\u76f8\u7ed3\u5408\uff0c\u5305\u542b\u9891\u57df\u548c\u8d1d\u53f6\u65af\u4e24\u79cd\u53d8\u4f53\uff0c\u5728\u4f18\u5316\u901f\u5ea6\u548c\u6cdb\u5316\u6027\u80fd\u4e0a\u4f18\u4e8e\u4e00\u9636\u548c\u4e8c\u9636\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u68af\u5ea6\u6b63\u5219\u5316\u5df2\u88ab\u8bc1\u660e\u80fd\u63d0\u9ad8\u8bad\u7ec3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u5728\u8bad\u7ec3\u521d\u671f\u80fd\u52a0\u901f\u4f18\u5316\uff0c\u4f46\u4e8c\u9636\u4f18\u5316\u5668\u7684\u8bad\u7ec3\u52a8\u6001\u5982\u4f55\u4ece\u68af\u5ea6\u6b63\u5219\u5316\u4e2d\u83b7\u76ca\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86GRNG\u4f18\u5316\u5668\u5bb6\u65cf\uff0c\u5305\u542b\u4e24\u79cd\u7b97\u6cd5\uff1a1\uff09\u9891\u57df\u53d8\u4f53\u901a\u8fc7\u7ed3\u6784\u5316\u8fd1\u4f3c\u907f\u514d\u663e\u5f0fFisher\u4fe1\u606f\u77e9\u9635\u6c42\u9006\uff1b2\uff09\u8d1d\u53f6\u65af\u53d8\u4f53\u57fa\u4e8e\u6b63\u5219\u5316-\u5361\u5c14\u66fc\u516c\u5f0f\u5b8c\u5168\u6d88\u9664FIM\u6c42\u9006\u9700\u6c42\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86GRNG\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u663e\u793a\u68af\u5ea6\u6b63\u5219\u5316\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u5e76\u786e\u4fdd\u6536\u655b\u5230\u5168\u5c40\u6700\u5c0f\u503c\u3002\u5b9e\u9a8c\u8868\u660eGRNG\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u4e00\u9636\u65b9\u6cd5\uff08SGD\u3001AdamW\uff09\u548c\u4e8c\u9636\u57fa\u7ebf\uff08K-FAC\u3001Sophia\uff09\u3002", "conclusion": "\u68af\u5ea6\u6b63\u5219\u5316\u662f\u89e3\u9501\u81ea\u7136\u68af\u5ea6\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u9c81\u68d2\u6027\u7684\u539f\u5219\u6027\u548c\u5b9e\u7528\u5de5\u5177\uff0cGRNG\u5728\u4f18\u5316\u901f\u5ea6\u548c\u6cdb\u5316\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2601.18479", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18479", "abs": "https://arxiv.org/abs/2601.18479", "authors": ["Kyoleen Kwak", "Hyoseok Hwang"], "title": "Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States", "comment": "Accepted at AAAI-26. 7 pages (excluding references), 3 figures", "summary": "Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.", "AI": {"tldr": "\u63d0\u51faASAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u6e21\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\u548c\u5bf9\u9f50\u52a8\u4f5c\u6765\u5e73\u6ed1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9ad8\u9891\u632f\u8361\uff0c\u63d0\u9ad8\u63a7\u5236\u7a33\u5b9a\u6027", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u56fa\u6709\u7684\u9ad8\u9891\u632f\u8361\u7279\u6027\u9650\u5236\u4e86\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u5408\u6210\u7684\u72b6\u6001\u76f8\u4f3c\u6027\u5b9a\u4e49\u6765\u4fc3\u8fdb\u52a8\u4f5c\u4e00\u81f4\u6027\uff0c\u4f46\u8fd9\u4e9b\u5b9a\u4e49\u5f80\u5f80\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5e95\u5c42\u7cfb\u7edf\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u8fc7\u6e21\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\u7684\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e3a\u4ece\u524d\u4e00\u72b6\u6001\u8f6c\u79fb\u5f97\u5230\u7684\u4e0b\u4e00\u72b6\u6001\u5206\u5e03\u3002\u57fa\u4e8e\u6b64\u63d0\u51faASAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u79cd\u673a\u5236\u5e73\u6ed1\u52a8\u4f5c\uff1a1) \u5c06\u5f53\u524d\u52a8\u4f5c\u4e0e\u8fc7\u6e21\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\u4e2d\u91c7\u53d6\u7684\u52a8\u4f5c\u5bf9\u9f50\uff1b2) \u60e9\u7f5a\u4e8c\u9636\u5dee\u5206\u6765\u6291\u5236\u9ad8\u9891\u632f\u8361\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u73af\u5883\u53cd\u9988\u548c\u5b9e\u9645\u6536\u96c6\u7684\u6570\u636e\uff0c\u80fd\u66f4\u597d\u5730\u6355\u6349\u7cfb\u7edf\u52a8\u6001\u3002", "result": "\u5728Gymnasium\u548cIsaac-Lab\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cASAP\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u63a7\u5236\u548c\u66f4\u597d\u7684\u7b56\u7565\u6027\u80fd\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u52a8\u4f5c\u632f\u8361\u95ee\u9898\u3002", "conclusion": "ASAP\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u8fc7\u6e21\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\u548c\u5bf9\u9f50\u52a8\u4f5c\u7684\u673a\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u52a8\u4f5c\u5e73\u6ed1\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u7cfb\u7edf\u52a8\u6001\uff0c\u51cf\u5c11\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9ad8\u9891\u632f\u8361\uff0c\u63d0\u9ad8\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u63a7\u5236\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.18500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18500", "abs": "https://arxiv.org/abs/2601.18500", "authors": ["Chen Liang", "Donghua Yang", "Yutong Wang", "Tianle Zhang", "Shenghe Zhou", "Zhiyu Liang", "Hengtong Zhang", "Hongzhi Wang", "Ziqi Li", "Xiyang Zhang", "Zheng Liang", "Yifei Li"], "title": "Nearly Optimal Bayesian Inference for Structural Missingness", "comment": null, "summary": "Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u6846\u67b6\u5904\u7406\u7ed3\u6784\u5316\u7f3a\u5931\u6570\u636e\uff0c\u901a\u8fc7\u89e3\u8026\u7f3a\u5931\u503c\u540e\u9a8c\u5b66\u4e60\u548c\u6807\u7b7e\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u548c\u6700\u4f18\u9884\u6d4b\u6027\u80fd", "motivation": "\u7ed3\u6784\u5316\u7f3a\u5931\u6570\u636e\u5b58\u5728\u56e0\u679c\u5faa\u73af\u56f0\u5883\uff1a\u9884\u6d4b\u9700\u8981\u7f3a\u5931\u7279\u5f81\uff0c\u4f46\u63a8\u65ad\u5b83\u4eec\u53c8\u4f9d\u8d56\u4e8e\u7f3a\u5931\u673a\u5236\uff1bMNAR\u4e0b\u7f3a\u5931\u90e8\u5206\u53ef\u80fd\u6765\u81ea\u5206\u5e03\u504f\u79fb\uff1b\u5355\u70b9\u63d2\u503c\u4f1a\u9501\u5b9a\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u7684\u504f\u5dee\u51b3\u7b56", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u89c6\u89d2\uff0c\u901a\u8fc7\u540e\u9a8c\u9884\u6d4b\u5206\u5e03\u8fdb\u884c\u9884\u6d4b\uff0c\u89e3\u8026\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a(1)\u5b66\u4e60\u6a21\u578b\u5185\u7f3a\u5931\u503c\u7684\u540e\u9a8c\u5206\u5e03\uff0c(2)\u901a\u8fc7\u4f18\u5316\u9884\u6d4b\u540e\u9a8c\u5206\u5e03\u8fdb\u884c\u6807\u7b7e\u9884\u6d4b\uff0c\u5b9e\u73b0\u540e\u9a8c\u79ef\u5206", "result": "\u572843\u4e2a\u5206\u7c7b\u548c15\u4e2a\u63d2\u503c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728SCM\u5148\u9a8c\u4e0b\u5177\u6709\u6709\u9650\u6837\u672c\u8fd1\u8d1d\u53f6\u65af\u6700\u4f18\u6027\u4fdd\u8bc1", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\"\u51e0\u4e4e\u514d\u8d39\u5348\u9910\"\uff1a\u4e00\u65e6\u5b66\u4e60\u5230\u540e\u9a8c\uff0c\u9884\u6d4b\u5373\u63d2\u5373\u7528\u540c\u65f6\u4fdd\u6301\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ed3\u6784\u5316\u7f3a\u5931\u6570\u636e\u7684\u56e0\u679c\u5faa\u73af\u548cMNAR\u95ee\u9898"}}
{"id": "2601.18509", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18509", "abs": "https://arxiv.org/abs/2601.18509", "authors": ["Andro Sabashvili"], "title": "Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark", "comment": null, "summary": "Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\u7684\u5e94\u7528\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5982\u4f55\u514b\u670d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u975e\u4ea4\u6362\u6027\u5bf9\u4f20\u7edf\u4fdd\u5f62\u9884\u6d4b\u7406\u8bba\u4fdd\u8bc1\u7684\u5f71\u54cd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9650\u5236\u6027\u5206\u5e03\u5047\u8bbe\u3002\u4fdd\u5f62\u9884\u6d4b\u4f5c\u4e3a\u65e0\u5206\u5e03\u6846\u67b6\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u7684\u65f6\u5e8f\u4f9d\u8d56\u6027\u8fdd\u53cd\u4e86\u4fdd\u5f62\u9884\u6d4b\u7684\u6838\u5fc3\u4ea4\u6362\u6027\u5047\u8bbe\uff0c\u9700\u8981\u4e13\u95e8\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u51b2\u7a81\u3002", "method": "\u7efc\u8ff0\u4e86\u56db\u7c7b\u4e3b\u8981\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\uff1a1) \u653e\u677e\u4ea4\u6362\u6027\u5047\u8bbe\u7684\u65b9\u6cd5\uff1b2) \u91cd\u65b0\u5b9a\u4e49\u6570\u636e\u5355\u5143\u4e3a\u72ec\u7acb\u65f6\u95f4\u5e8f\u5217\u96c6\u5408\u7684\u65b9\u6cd5\uff1b3) \u663e\u5f0f\u5efa\u6a21\u9884\u6d4b\u6b8b\u5dee\u52a8\u6001\u7684\u65b9\u6cd5\uff1b4) \u9002\u5e94\u5206\u5e03\u6f02\u79fb\u4ee5\u7ef4\u6301\u957f\u671f\u8986\u76d6\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u3002\u901a\u8fc7\u7efc\u5408\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u9645\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5bf9\u5404\u7c7b\u65b9\u6cd5\u7684\u7cfb\u7edf\u8c03\u67e5\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u65f6\u95f4\u5e8f\u5217\u4fdd\u5f62\u9884\u6d4b\u7684\u5168\u9762\u6280\u672f\u8def\u7ebf\u56fe\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u514b\u670d\u975e\u4ea4\u6362\u6027\u6311\u6218\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u65f6\u95f4\u5e8f\u5217\u4fdd\u5f62\u9884\u6d4b\u9700\u8981\u4e13\u95e8\u65b9\u6cd5\u5904\u7406\u65f6\u5e8f\u4f9d\u8d56\u6027\uff0c\u5404\u7c7b\u7b97\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5404\u6709\u4f18\u52a3\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u7ee7\u7eed\u5173\u6ce8\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u9645\u5e94\u7528\u6027\u80fd\uff0c\u63a8\u52a8\u53ef\u9760\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.18510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18510", "abs": "https://arxiv.org/abs/2601.18510", "authors": ["Yibo Li", "Zijie Lin", "Ailin Deng", "Xuan Zhang", "Yufei He", "Shuo Ji", "Tri Cao", "Bryan Hooi"], "title": "Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates", "comment": null, "summary": "While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.", "code_url": "https://github.com/liushiliushi/JitRL", "code_stars": 0, "code_last_update": "2026-01-26", "AI": {"tldr": "JitRL\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u5373\u65f6\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u52a8\u6001\u68c0\u7d22\u7ecf\u9a8c\u8f68\u8ff9\u6765\u4f30\u8ba1\u52a8\u4f5c\u4f18\u52bf\uff0c\u76f4\u63a5\u8c03\u5236LLM\u8f93\u51falogits\uff0c\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\u800c\u65e0\u9700\u68af\u5ea6\u66f4\u65b0\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u90e8\u7f72\u540e\u6743\u91cd\u56fa\u5b9a\uff0c\u96be\u4ee5\u6301\u7eed\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u4e14\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "JitRL\u7ef4\u62a4\u52a8\u6001\u975e\u53c2\u6570\u5316\u8bb0\u5fc6\u5e93\uff0c\u5728\u6d4b\u8bd5\u65f6\u68c0\u7d22\u76f8\u5173\u7ecf\u9a8c\u8f68\u8ff9\uff0c\u5b9e\u65f6\u4f30\u8ba1\u52a8\u4f5c\u4f18\u52bf\u503c\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u4f30\u8ba1\u503c\u4f5c\u4e3a\u52a0\u6027\u66f4\u65b0\u76f4\u63a5\u8c03\u5236LLM\u7684\u8f93\u51falogits\u3002\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u88ab\u8bc1\u660e\u662fKL\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u76ee\u6807\u7684\u7cbe\u786e\u95ed\u5f0f\u89e3\u3002", "result": "\u5728WebArena\u548cJericho\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJitRL\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u8ba1\u7b97\u6602\u8d35\u7684\u5fae\u8c03\u65b9\u6cd5\uff08\u5982WebRL\uff09\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e8630\u500d\u4ee5\u4e0a\u3002", "conclusion": "JitRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u4f18\u5316\u7b56\u7565\u800c\u4e0d\u9700\u8981\u68af\u5ea6\u66f4\u65b0\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u6301\u7eed\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18524", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18524", "abs": "https://arxiv.org/abs/2601.18524", "authors": ["Yongqi Jin", "Yecheng Wang", "Jun-jie Wang", "Rong Zhu", "Guolin Ke", "Weinan E"], "title": "From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale", "comment": null, "summary": "Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.", "AI": {"tldr": "\u63d0\u51fa\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u6587\u732e\u63d0\u53d6\u7684\u767e\u4e07\u7ea7\u672a\u6807\u8bb0NMR\u8c31\u56fe\u8bad\u7ec3\u5316\u5b66\u4f4d\u79fb\u9884\u6d4b\u6a21\u578b\uff0c\u65e0\u9700\u539f\u5b50\u7ea7\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709NMR\u5316\u5b66\u4f4d\u79fb\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u6570\u636e\u89c4\u6a21\u53d7\u9650\uff0c\u9700\u8981\u5229\u7528\u5927\u89c4\u6a21\u6587\u732e\u4e2d\u63d0\u53d6\u7684\u672a\u6807\u8bb0\u8c31\u56fe\u6570\u636e", "method": "\u63d0\u51fa\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5c06\u6587\u732e\u8c31\u56fe\u5316\u5b66\u4f4d\u79fb\u9884\u6d4b\u5efa\u6a21\u4e3a\u7f6e\u6362\u4e0d\u53d8\u96c6\u5408\u76d1\u7763\u95ee\u9898\uff0c\u5728\u635f\u5931\u51fd\u6570\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u6700\u4f18\u4e8c\u5206\u5339\u914d\u7b80\u5316\u4e3a\u57fa\u4e8e\u6392\u5e8f\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u5927\u89c4\u6a21\u534a\u76d1\u7763\u8bad\u7ec3", "result": "\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u66f4\u5927\u66f4\u591a\u6837\u7684\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9996\u6b21\u5728\u89c4\u6a21\u4e0a\u6355\u6349\u5e38\u89c1NMR\u6eb6\u5242\u7684\u7cfb\u7edf\u6027\u6eb6\u5242\u6548\u5e94", "conclusion": "\u5927\u89c4\u6a21\u6587\u732e\u63d0\u53d6\u7684\u672a\u6807\u8bb0\u8c31\u56fe\u53ef\u4f5c\u4e3a\u8bad\u7ec3NMR\u4f4d\u79fb\u6a21\u578b\u7684\u5b9e\u7528\u6709\u6548\u6570\u636e\u6e90\uff0c\u8868\u660e\u6587\u732e\u884d\u751f\u7684\u5f31\u7ed3\u6784\u5316\u6570\u636e\u5728\u79d1\u5b66\u9886\u57df\u6570\u636e\u9a71\u52a8AI\u4e2d\u5177\u6709\u66f4\u5e7f\u6cdb\u4f5c\u7528"}}
{"id": "2601.18525", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18525", "abs": "https://arxiv.org/abs/2601.18525", "authors": ["Eleonora Grassucci", "Giordano Cicchetti", "Emanuele Frasca", "Aurelio Uncini", "Danilo Comminiello"], "title": "Closing the Modality Gap Aligns Group-Wise Semantics", "comment": "ICLR 2026", "summary": "In multimodal learning, CLIP has been recognized as the \\textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6a21\u6001\u95f4\u9699\uff08modality gap\uff09\u5bf9\u7fa4\u4f53\u7ea7\u4efb\u52a1\uff08\u5982\u805a\u7c7b\uff09\u5f71\u54cd\u663e\u8457\uff0c\u800c\u4f20\u7edf\u5b9e\u4f8b\u7ea7\u4efb\u52a1\uff08\u5982\u68c0\u7d22\uff09\u5f71\u54cd\u6709\u9650\uff0c\u5e76\u5f15\u5165\u65b0\u65b9\u6cd5\u51cf\u5c11\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u8fd9\u79cd\u7ed3\u6784\u4e0d\u5339\u914d\u3002", "motivation": "CLIP\u7b49\u8de8\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u5728\u8bed\u4e49\u5c42\u9762\u5bf9\u9f50\u4e0d\u540c\u6a21\u6001\uff0c\u4f46\u4ea7\u751f\u7684\u6f5c\u5728\u7a7a\u95f4\u5f80\u5f80\u53ea\u662f\u90e8\u5206\u5171\u4eab\uff0c\u5b58\u5728\u7ed3\u6784\u4e0d\u5339\u914d\u95ee\u9898\uff08\u6a21\u6001\u95f4\u9699\uff09\u3002\u73b0\u6709\u7814\u7a76\u5bf9\u8fd9\u79cd\u95f4\u9699\u7684\u5fc5\u8981\u6027\u5b58\u5728\u4e89\u8bae\uff0c\u7279\u522b\u662f\u5176\u5bf9\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u5f71\u54cd\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u6a21\u6001\u95f4\u9699\u5bf9\u7fa4\u4f53\u7ea7\u4efb\u52a1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u5728\u53cc\u6a21\u6001\u8bbe\u7f6e\u4e2d\u6301\u7eed\u51cf\u5c11\u6a21\u6001\u95f4\u9699\uff0c\u5e76\u53ef\u4ee5\u7b80\u5355\u6269\u5c55\u5230\u4e00\u822c\u7684n\u6a21\u6001\u60c5\u51b5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u4e0d\u540c\u6a21\u6001\u8868\u793a\u4e4b\u95f4\u7684\u7ed3\u6784\u4e0d\u5339\u914d\u6765\u6539\u5584\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\u53d1\u73b0\uff1a\u51cf\u5c11\u6a21\u6001\u95f4\u9699\u5728\u4f20\u7edf\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u4e2d\u4ec5\u5e26\u6765\u8fb9\u9645\u6216\u4e0d\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u4f46\u5728\u7fa4\u4f53\u7ea7\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u8fd9\u8bc1\u660e\u4e86\u6a21\u6001\u95f4\u9699\u5bf9\u9700\u8981\u8bed\u4e49\u5206\u7ec4\u7684\u4efb\u52a1\u5177\u6709\u5173\u952e\u5f71\u54cd\u3002", "conclusion": "\u6a21\u6001\u95f4\u9699\u5728\u7fa4\u4f53\u7ea7\u4efb\u52a1\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u51cf\u5c11\u8fd9\u79cd\u7ed3\u6784\u4e0d\u5339\u914d\u80fd\u663e\u8457\u63d0\u5347\u8bed\u4e49\u5206\u7ec4\u4efb\u52a1\u7684\u6027\u80fd\u3002\u8fd9\u4e00\u53d1\u73b0\u53ef\u80fd\u91cd\u5851\u5bf9\u6a21\u6001\u95f4\u9699\u7684\u7406\u89e3\uff0c\u5f3a\u8c03\u5176\u5728\u6539\u5584\u9700\u8981\u8bed\u4e49\u5206\u7ec4\u7684\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.18546", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18546", "abs": "https://arxiv.org/abs/2601.18546", "authors": ["Arash Jamshidi", "Katsiaryna Haitsiukevich", "Kai Puolam\u00e4ki"], "title": "Information Hidden in Gradients of Regression with Target Noise", "comment": null, "summary": "Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $\u03a3$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $\u03a9(1)$ factor. The proposed method is practical (a \"set target-noise variance to $n$\" rule) and robust (variance $\\mathcal{O}(n)$ suffices to recover $\u03a3$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.", "AI": {"tldr": "\u901a\u8fc7\u65b9\u5dee\u6821\u51c6\uff08\u5c06\u76ee\u6807\u566a\u58f0\u65b9\u5dee\u8bbe\u7f6e\u4e3a\u6279\u91cf\u5927\u5c0f\uff09\uff0c\u4ec5\u4f7f\u7528\u68af\u5ea6\u5373\u53ef\u6062\u590dHessian\u77e9\u9635\uff0c\u5728\u7ebf\u6027\u56de\u5f52\u4e2d\u7b49\u4e8e\u6570\u636e\u534f\u65b9\u5dee\u03a3", "motivation": "\u5728\u8bb8\u591a\u73b0\u4ee3\u8bbe\u7f6e\u4e2d\uff0c\u53ea\u80fd\u89c2\u5bdf\u5230\u68af\u5ea6\u800c\u65e0\u6cd5\u76f4\u63a5\u83b7\u53d6\u4e8c\u9636\u4fe1\u606f\uff08\u5982Hessian\u77e9\u9635\uff09\uff0c\u4f46\u4e8c\u9636\u4fe1\u606f\u5bf9\u4e8e\u4f18\u5316\u3001\u8bca\u65ad\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u627e\u5230\u4ec5\u4f7f\u7528\u68af\u5ea6\u5c31\u80fd\u6062\u590d\u4e8c\u9636\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7b80\u5355\u7684\u65b9\u5dee\u6821\u51c6\u65b9\u6cd5\uff1a\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\uff0c\u4f7f\u603b\u76ee\u6807\u566a\u58f0\u65b9\u5dee\u7b49\u4e8e\u6279\u91cf\u5927\u5c0fn\u3002\u8fd9\u79cd\u6821\u51c6\u786e\u4fdd\u7ecf\u9a8c\u68af\u5ea6\u534f\u65b9\u5dee\u5373\u4f7f\u5728\u8fdc\u79bb\u6700\u4f18\u89e3\u65f6\u4e5f\u80fd\u7d27\u5bc6\u903c\u8fd1Hessian\u77e9\u9635\u3002\u63d0\u4f9b\u4e86\u6b21\u9ad8\u65af\u8f93\u5165\u4e0b\u7684\u975e\u6e10\u8fd1\u7b97\u5b50\u8303\u6570\u4fdd\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u901a\u8fc7\u65b9\u5dee\u6821\u51c6\uff0c\u68af\u5ea6\u534f\u65b9\u5dee\u53ef\u4ee5\u7cbe\u786e\u6062\u590dHessian\u77e9\u9635\uff08\u5728\u7ebf\u6027\u56de\u5f52\u4e2d\u7b49\u4e8e\u6570\u636e\u534f\u65b9\u5dee\u03a3\uff09\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\u3002\u540c\u65f6\u8bc1\u660e\uff0c\u6ca1\u6709\u8fd9\u79cd\u6821\u51c6\uff0c\u6062\u590d\u53ef\u80fd\u5931\u8d25\u03a9(1)\u56e0\u5b50\u3002", "conclusion": "\u4ec5\u4f7f\u7528\u68af\u5ea6\u5373\u53ef\u6062\u590d\u4e8c\u9636\u4fe1\u606f\uff0c\u901a\u8fc7\u7b80\u5355\u7684\"\u5c06\u76ee\u6807\u566a\u58f0\u65b9\u5dee\u8bbe\u7f6e\u4e3an\"\u89c4\u5219\u3002\u8be5\u65b9\u6cd5\u5b9e\u7528\u4e14\u9c81\u68d2\uff0c\u65b9\u5deeO(n)\u8db3\u4ee5\u6062\u590d\u03a3\uff08\u81f3\u5c3a\u5ea6\uff09\u3002\u5e94\u7528\u5305\u62ec\u66f4\u5feb\u7684\u4f18\u5316\u9884\u6761\u4ef6\u3001\u5bf9\u6297\u98ce\u9669\u4f30\u8ba1\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u4ec5\u68af\u5ea6\u8bad\u7ec3\u3002"}}
{"id": "2601.18564", "categories": ["cs.LG", "cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.18564", "abs": "https://arxiv.org/abs/2601.18564", "authors": ["Chong Hyun Lee", "Kibae Lee", "Hyun Hee Yim"], "title": "An Unsupervised Tensor-Based Domain Alignment", "comment": "5 pages, 5 figures", "summary": "We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u7684\u57df\u5bf9\u9f50\u7b97\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u77e9\u9635\u5728\u4e0d\u53d8\u5b50\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6e90\u548c\u76ee\u6807\u5f20\u91cf\uff0c\u4f7f\u7528\u659c\u6d41\u5f62\u7ea6\u675f\u63d0\u4f9b\u6bd4\u4f20\u7edfStiefel\u6d41\u5f62\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u901a\u8fc7\u6b63\u5219\u5316\u9879\u4fdd\u6301\u65b9\u5dee\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u7cbe\u5ea6\u548c\u8f6c\u6362\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u57df\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528Stiefel\u6d41\u5f62\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002\u9700\u8981\u5f00\u53d1\u66f4\u7075\u6d3b\u7684\u57df\u5bf9\u9f50\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u57df\u9002\u5e94\u4efb\u52a1\uff0c\u540c\u65f6\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u7684\u57df\u5bf9\u9f50\u7b97\u6cd5\uff1a1\uff09\u4f7f\u7528\u5bf9\u9f50\u77e9\u9635\u5728\u4e0d\u53d8\u5b50\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6e90\u548c\u76ee\u6807\u5f20\u91cf\uff1b2\uff09\u91c7\u7528\u659c\u6d41\u5f62\u7ea6\u675f\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u76f8\u6bd4\u4f20\u7edfStiefel\u6d41\u5f62\u66f4\u5177\u7075\u6d3b\u6027\uff1b3\uff09\u5f15\u5165\u6b63\u5219\u5316\u9879\u4fdd\u6301\u6e90\u548c\u76ee\u6807\u5f20\u91cf\u7684\u65b9\u5dee\uff1b4\uff09\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5c06\u73b0\u6709\u5f20\u91cf\u57df\u5bf9\u9f50\u65b9\u6cd5\u4f5c\u4e3a\u7279\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u663e\u8457\u63d0\u5347\u57df\u5bf9\u9f50\u8f6c\u6362\u901f\u5ea6\uff1b2\uff09\u5927\u5e45\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff1b3\uff09\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff1b4\uff09\u9002\u7528\u4e8e\u590d\u6742\u57df\u9002\u5e94\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f20\u91cf\u57df\u5bf9\u9f50\u7b97\u6cd5\u901a\u8fc7\u659c\u6d41\u5f62\u7ea6\u675f\u548c\u65b9\u5dee\u4fdd\u6301\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u57df\u5bf9\u9f50\uff0c\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u662f\u590d\u6742\u57df\u9002\u5e94\u4efb\u52a1\u7684\u4f18\u9009\u65b9\u6848\u3002"}}
{"id": "2601.18580", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18580", "abs": "https://arxiv.org/abs/2601.18580", "authors": ["Vincenzo De Paola", "Mirco Mutti", "Riccardo Zamboni", "Marcello Restelli"], "title": "K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents", "comment": null, "summary": "Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.", "AI": {"tldr": "K-Myriad\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5e76\u884c\u7b56\u7565\u7fa4\u4f53\u8bf1\u5bfc\u7684\u96c6\u4f53\u72b6\u6001\u71b5\uff0c\u57f9\u517b\u4e13\u95e8\u7684\u63a2\u7d22\u7b56\u7565\u7ec4\u5408\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u9c81\u68d2\u521d\u59cb\u5316\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u5e76\u53d1\u73b0\u5f02\u6784\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5e76\u884c\u5316\u901a\u5e38\u4f7f\u7528\u591a\u4e2a\u5de5\u4f5c\u5668\u4ece\u76f8\u540c\u7684\u91c7\u6837\u5206\u5e03\u6536\u96c6\u7ecf\u9a8c\uff0c\u4ec5\u7528\u4e8e\u52a0\u901f\u5355\u4e2a\u7b56\u7565\u7684\u8bad\u7ec3\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5ffd\u7565\u4e86\u591a\u6837\u5316\u63a2\u7d22\u7b56\u7565\u7684\u4f18\u52bf\uff0c\u9650\u5236\u4e86\u5e76\u884c\u5316\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faK-Myriad\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5e76\u884c\u7b56\u7565\u7fa4\u4f53\u8bf1\u5bfc\u7684\u96c6\u4f53\u72b6\u6001\u71b5\uff0c\u57f9\u517b\u4e13\u95e8\u7684\u63a2\u7d22\u7b56\u7565\u7ec4\u5408\u3002\u8be5\u65b9\u6cd5\u662f\u65e0\u76d1\u7763\u7684\u3001\u53ef\u6269\u5c55\u7684\uff0c\u80fd\u591f\u5b9e\u73b0\u96c6\u4f53\u63a2\u7d22\u3002", "result": "\u5728\u9ad8\u7ef4\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5e76\u884c\u5316\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eK-Myriad\u80fd\u591f\u5b66\u4e60\u5230\u5e7f\u6cdb\u7684\u3001\u4e0d\u540c\u7684\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u96c6\u4f53\u63a2\u7d22\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "K-Myriad\u901a\u8fc7\u6700\u5927\u5316\u96c6\u4f53\u72b6\u6001\u71b5\u548c\u57f9\u517b\u4e13\u95e8\u7684\u63a2\u7d22\u7b56\u7565\u7ec4\u5408\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u9c81\u68d2\u521d\u59cb\u5316\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u5e76\u80fd\u591f\u53d1\u73b0\u5f02\u6784\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u65b0\u578b\u5e76\u884c\u5316\u7b56\u7565\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2601.18615", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18615", "abs": "https://arxiv.org/abs/2601.18615", "authors": ["Ramiro Valdes Jara", "Adam Meyers"], "title": "Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem", "comment": null, "summary": "This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684ECGI\u9006\u95ee\u9898\u6570\u636e\u9a71\u52a8\u6c42\u89e3\u65b9\u6cd5\uff0c\u65e0\u9700\u51e0\u4f55\u7f51\u683c\uff0c\u53ef\u6982\u7387\u6027\u91c7\u6837\u591a\u4e2a\u91cd\u5efa\u7ed3\u679c", "motivation": "\u89e3\u51b3\u5fc3\u7535\u56fe\u6210\u50cf\uff08ECGI\uff09\u9006\u95ee\u9898\u7684\u975e\u552f\u4e00\u6027\u548c\u6b20\u5b9a\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u60a3\u8005\u7279\u5b9a\u7f51\u683c\u6784\u5efa\u4e14\u53ea\u80fd\u63d0\u4f9b\u786e\u5b9a\u6027\u4f30\u8ba1", "method": "\u91c7\u7528\u6761\u4ef6\u6269\u6563\u6846\u67b6\u5b66\u4e60\u4ece\u566a\u58f0\u4f53\u8868\u4fe1\u53f7\u5230\u5fc3\u8868\u7535\u4f4d\u7684\u6982\u7387\u6620\u5c04\uff0c\u51e0\u4f55\u65e0\u5173\u7684\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5", "result": "\u5728\u771f\u5b9eECGI\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4CNN\u3001LSTM\u548cTransformer\u7b49\u786e\u5b9a\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6269\u6563\u6a21\u578b\u83b7\u5f97\u66f4\u597d\u7684\u91cd\u5efa\u7cbe\u5ea6", "conclusion": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3aECGI\u9006\u95ee\u9898\u7684\u7a33\u5065\u5de5\u5177\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6982\u7387\u6027\u91c7\u6837\u591a\u4e2a\u91cd\u5efa\u7ed3\u679c\u800c\u975e\u5355\u4e00\u786e\u5b9a\u6027\u4f30\u8ba1"}}
{"id": "2601.18620", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18620", "abs": "https://arxiv.org/abs/2601.18620", "authors": ["Panagiotis Lymperopoulos", "Abhiramon Rajasekharan", "Ian Berlot-Attwell", "St\u00e9phane Aroca-Ouellette", "Kaheer Suleman"], "title": "CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling", "comment": "28 pages, 2 figures", "summary": "Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.", "AI": {"tldr": "CASSANDRA\uff1a\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u5efa\u6a21\u65b9\u6cd5\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u77e5\u8bc6\u5148\u9a8c\u6784\u5efa\u8f7b\u91cf\u7ea7\u8f6c\u79fb\u6a21\u578b\u7528\u4e8e\u89c4\u5212", "motivation": "\u73b0\u5b9e\u4e16\u754c\u9886\u57df\uff08\u5982\u5546\u4e1a\uff09\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\uff0c\u53ef\u4ee5\u5229\u7528\u4e16\u754c\u77e5\u8bc6\u4ece\u6709\u9650\u6570\u636e\u4e2d\u6709\u6548\u5efa\u6a21\u590d\u6742\u7684\u884c\u52a8\u6548\u679c\u548c\u56e0\u679c\u5173\u7cfb", "method": "CASSANDRA\u6574\u5408\u4e24\u4e2a\u7ec4\u4ef6\uff1a(1) LLM\u5408\u6210\u7684\u4ee3\u7801\u5efa\u6a21\u786e\u5b9a\u6027\u7279\u5f81\uff1b(2) LLM\u5f15\u5bfc\u7684\u6982\u7387\u56fe\u6a21\u578b\u7ed3\u6784\u5b66\u4e60\u4ee5\u6355\u6349\u968f\u673a\u53d8\u91cf\u95f4\u7684\u56e0\u679c\u5173\u7cfb", "result": "\u5728\u5c0f\u578b\u5496\u5561\u5e97\u6a21\u62df\u5668\u548c\u590d\u6742\u4e3b\u9898\u516c\u56ed\u5546\u4e1a\u6a21\u62df\u5668\u4e2d\u8bc4\u4f30\uff0c\u5728\u8f6c\u79fb\u9884\u6d4b\u548c\u89c4\u5212\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb", "conclusion": "CASSANDRA\u901a\u8fc7\u7ed3\u5408LLM\u7684\u77e5\u8bc6\u5148\u9a8c\u548c\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6784\u5efa\u4e16\u754c\u6a21\u578b\u4ee5\u652f\u6301\u590d\u6742\u73b0\u5b9e\u9886\u57df\u7684\u89c4\u5212\u4efb\u52a1"}}
{"id": "2601.18626", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18626", "abs": "https://arxiv.org/abs/2601.18626", "authors": ["Yingxiao Huo", "Satya Prakash Dash", "Radu Stoican", "Samuel Kaski", "Mingfei Sun"], "title": "Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning", "comment": null, "summary": "Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79e9-1\u8fd1\u4f3c\u7684\u81ea\u7136\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u9ad8\u6548\u8ba1\u7b97Fisher\u4fe1\u606f\u77e9\u9635\u7684\u9006\uff0c\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e", "motivation": "\u81ea\u7136\u68af\u5ea6\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u5feb\u901f\u6536\u655b\u7279\u6027\uff0c\u4f46\u8ba1\u7b97Fisher\u4fe1\u606f\u77e9\u9635\u7684\u9006\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6cd5", "method": "\u4f7f\u7528\u79e9-1\u8fd1\u4f3c\u65b9\u6cd5\u903c\u8fd1\u5b8c\u6574\u7684\u9006Fisher\u4fe1\u606f\u77e9\u9635\uff0c\u5f00\u53d1\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u81ea\u7136\u7b56\u7565\u4f18\u5316\u6280\u672f", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u79e9-1\u8fd1\u4f3c\u6bd4\u7b56\u7565\u68af\u5ea6\u6536\u655b\u66f4\u5feb\uff0c\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u4e0e\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5177\u6709\u76f8\u540c\u7684\u6837\u672c\u590d\u6742\u5ea6\uff1b\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u4f18\u4e8e\u6807\u51c6actor-critic\u548c\u4fe1\u4efb\u57df\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u79e9-1\u8fd1\u4f3c\u81ea\u7136\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u81ea\u7136\u68af\u5ea6\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316"}}
{"id": "2601.18640", "categories": ["cs.LG", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2601.18640", "abs": "https://arxiv.org/abs/2601.18640", "authors": ["Zhiwei Zheng", "Kevin Bryson"], "title": "TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning", "comment": null, "summary": "Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.\n  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as \"background\" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.\n  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.", "AI": {"tldr": "TwinPurify\u662f\u4e00\u4e2a\u57fa\u4e8eBarlow Twins\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u540c\u4e00\u961f\u5217\u4e2d\u76f8\u90bb\u6b63\u5e38\u7ec4\u7ec7\u4f5c\u4e3a\"\u80cc\u666f\"\u6307\u5bfc\uff0c\u4ece\u6279\u91cf\u8f6c\u5f55\u7ec4\u6570\u636e\u4e2d\u5b66\u4e60\u8fde\u7eed\u7684\u9ad8\u7ef4\u80bf\u7624\u5d4c\u5165\uff0c\u4ece\u800c\u89e3\u8026\u80bf\u7624\u7279\u5f02\u6027\u4fe1\u53f7\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u53c2\u8003\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u60a3\u8005\u961f\u5217\u7814\u7a76\u4ecd\u4f9d\u8d56\u6279\u91cf\u8f6c\u5f55\u7ec4\u6570\u636e\uff0c\u4f46\u80bf\u7624\u7eaf\u5ea6\u53d8\u5f02\u4f1a\u63a9\u76d6\u80bf\u7624\u5185\u5728\u8f6c\u5f55\u4fe1\u53f7\u5e76\u9650\u5236\u4e0b\u6e38\u53d1\u73b0\u3002\u8bb8\u591a\u53cd\u5377\u79ef\u65b9\u6cd5\u5728\u5408\u6210\u6279\u91cf\u6df7\u5408\u7269\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u672a\u5efa\u6a21\u7684\u751f\u7269\u548c\u6280\u672f\u53d8\u5f02\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u771f\u5b9e\u60a3\u8005\u961f\u5217\u3002", "method": "TwinPurify\u91c7\u7528Barlow Twins\u81ea\u76d1\u7763\u76ee\u6807\uff0c\u901a\u8fc7\u5229\u7528\u540c\u4e00\u961f\u5217\u4e2d\u76f8\u90bb\u6b63\u5e38\u7ec4\u7ec7\u4f5c\u4e3a\"\u80cc\u666f\"\u6307\u5bfc\uff0c\u5b66\u4e60\u8fde\u7eed\u7684\u9ad8\u7ef4\u80bf\u7624\u5d4c\u5165\uff0c\u800c\u4e0d\u662f\u5c06\u6279\u91cf\u6df7\u5408\u7269\u89e3\u6790\u4e3a\u79bb\u6563\u7ec6\u80de\u7c7b\u578b\u5206\u6570\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4f9d\u8d56\u4efb\u4f55\u5916\u90e8\u53c2\u8003\u3002", "result": "\u5728\u591a\u4e2a\u5927\u578b\u764c\u75c7\u961f\u5217\uff08RNA-seq\u548c\u5fae\u9635\u5217\u5e73\u53f0\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTwinPurify\u5728\u6062\u590d\u80bf\u7624\u5185\u5728\u548c\u514d\u75ab\u4fe1\u53f7\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u81ea\u7f16\u7801\u5668\uff09\u3002\u7eaf\u5316\u7684\u5d4c\u5165\u6539\u8fdb\u4e86\u5206\u5b50\u4e9a\u578b\u548c\u5206\u7ea7\u5206\u7c7b\uff0c\u589e\u5f3a\u4e86\u751f\u5b58\u6a21\u578b\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u6bd4\u539f\u59cb\u6279\u91cf\u8c31\u66f4\u5177\u751f\u7269\u5b66\u610f\u4e49\u7684\u901a\u8def\u6d3b\u6027\u3002", "conclusion": "TwinPurify\u901a\u8fc7\u63d0\u4f9b\u53ef\u8f6c\u79fb\u7684\u6279\u91cf\u8f6c\u5f55\u7ec4\u53bb\u6c61\u67d3\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u4e34\u5e8a\u6570\u636e\u96c6\u5728\u5206\u5b50\u53d1\u73b0\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u80bf\u7624\u751f\u6001\u7cfb\u7edf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.18650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18650", "abs": "https://arxiv.org/abs/2601.18650", "authors": ["Liheng Yu", "Zhe Zhao", "Yuxuan Wang", "Pengkun Wang", "Binwu Wang", "Yang Wang"], "title": "FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning", "comment": "camera-ready for iclr2026", "summary": "Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten\". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \\textit{Heterogeneous Unlearning Deviation} and \\textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \\textbf{Supplementary Material}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7814\u7a76\u4e86\u957f\u5c3e\u5206\u5e03\u4e0b\u673a\u5668\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86FaLW\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u52a8\u6001\u635f\u5931\u91cd\u52a0\u6743\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u6570\u636e\u9057\u5fd8\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u504f\u659c\u9057\u5fd8\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u7814\u7a76\u4e3b\u8981\u8bc4\u4f30\u76f8\u5bf9\u5e73\u8861\u7684\u9057\u5fd8\u96c6\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u6570\u636e\uff08\u5982\u7528\u6237\u6d3b\u52a8\u8bb0\u5f55\uff09\u901a\u5e38\u9075\u5faa\u957f\u5c3e\u5206\u5e03\u7684\u5e38\u89c1\u573a\u666f\u3002\u8fd9\u79cd\u7814\u7a76\u7a7a\u767d\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u8bbe\u7f6e\u4e0b\u5b58\u5728\u4e25\u91cd\u95ee\u9898\u3002", "method": "\u63d0\u51faFaLW\u65b9\u6cd5\uff1a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u3001\u5b9e\u4f8b\u7ea7\u52a8\u6001\u635f\u5931\u91cd\u52a0\u6743\u65b9\u6cd5\u3002\u901a\u8fc7\u6bd4\u8f83\u6bcf\u4e2a\u6837\u672c\u7684\u9884\u6d4b\u6982\u7387\u4e0e\u540c\u7c7b\u522b\u672a\u89c1\u6570\u636e\u7684\u5206\u5e03\u6765\u8bc4\u4f30\u5176\u9057\u5fd8\u72b6\u6001\uff0c\u7136\u540e\u4f7f\u7528\u7531\u5e73\u8861\u56e0\u5b50\u8c03\u5236\u7684\u9057\u5fd8\u611f\u77e5\u91cd\u52a0\u6743\u65b9\u6848\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u4e2a\u6837\u672c\u7684\u9057\u5fd8\u5f3a\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eFaLW\u5728\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u673a\u5668\u9057\u5fd8\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u8d28\u6027\u9057\u5fd8\u504f\u5dee\u548c\u504f\u659c\u9057\u5fd8\u504f\u5dee\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u957f\u5c3e\u5206\u5e03\u4e0b\u673a\u5668\u9057\u5fd8\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u63d0\u51fa\u7684FaLW\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u635f\u5931\u91cd\u52a0\u6743\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u73b0\u5b9e\u957f\u5c3e\u6570\u636e\u573a\u666f\u4e0b\u7684\u9057\u5fd8\u6548\u679c\uff0c\u4e3a\u6570\u636e\u9690\u79c1\u6cd5\u89c4\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18675", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18675", "abs": "https://arxiv.org/abs/2601.18675", "authors": ["Aditya Kumar", "Mario A. Cypko", "Oliver Amft"], "title": "Learning temporal embeddings from electronic health records of chronic kidney disease patients", "comment": "7 pages, 3 figures, 3 tables. The paper has been submitted to IEEE EMBC 2026 and copyright might be transferred without notice", "summary": "We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.", "AI": {"tldr": "\u7814\u7a76\u65f6\u95f4\u5d4c\u5165\u6a21\u578b\u5728\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u5b66\u4e60\u4e34\u5e8a\u6709\u610f\u4e49\u8868\u793a\u7684\u80fd\u529b\uff0c\u6bd4\u8f83\u4e09\u79cd\u5faa\u73af\u67b6\u6784\u5728\u6162\u6027\u80be\u75c5\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u65f6\u95f4\u611f\u77e5LSTM\u4ea7\u751f\u66f4\u7ed3\u6784\u5316\u5d4c\u5165\uff0c\u4e14\u5d4c\u5165\u6a21\u578b\u5728\u6b7b\u4ea1\u7387\u9884\u6d4b\u4e0a\u4f18\u4e8e\u7aef\u5230\u7aef\u6a21\u578b\u3002", "motivation": "\u6a21\u578b\u5f15\u5bfc\u533b\u5b66\u9700\u8981\u80fd\u591f\u6355\u6349\u75be\u75c5\u52a8\u6001\u4e14\u4fdd\u6301\u900f\u660e\u3001\u4efb\u52a1\u65e0\u5173\u7684\u8868\u793a\uff0c\u800c\u5927\u591a\u6570\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u4ec5\u9488\u5bf9\u5355\u4e00\u4efb\u52a1\u4f18\u5316\u3002\u8868\u793a\u5b66\u4e60\u6709\u52a9\u4e8e\u5b66\u4e60\u53ef\u6cdb\u5316\u5230\u4e0b\u6e38\u4efb\u52a1\u7684\u5d4c\u5165\uff0c\u5faa\u73af\u67b6\u6784\u9002\u5408\u5efa\u6a21\u4e34\u5e8a\u6570\u636e\u7684\u65f6\u95f4\u7ed3\u6784\u3002", "method": "\u4f7f\u7528MIMIC-IV\u6570\u636e\u96c6\u7814\u7a76\u6162\u6027\u80be\u75c5\u60a3\u8005\uff0c\u6bd4\u8f83\u4e09\u79cd\u5faa\u73af\u67b6\u6784\uff1a\u666e\u901aLSTM\u3001\u6ce8\u610f\u529b\u589e\u5f3aLSTM\u548c\u65f6\u95f4\u611f\u77e5LSTM(T-LSTM)\u3002\u6240\u6709\u6a21\u578b\u65e2\u4f5c\u4e3a\u5d4c\u5165\u6a21\u578b\u8bad\u7ec3\uff0c\u4e5f\u4f5c\u4e3a\u76f4\u63a5\u7aef\u5230\u7aef\u9884\u6d4b\u5668\u8bad\u7ec3\u3002\u901a\u8fc7CKD\u9636\u6bb5\u805a\u7c7b\u548cICU\u5185\u6b7b\u4ea1\u7387\u9884\u6d4b\u8bc4\u4f30\u5d4c\u5165\u8d28\u91cf\u3002", "result": "T-LSTM\u4ea7\u751f\u66f4\u7ed3\u6784\u5316\u7684\u5d4c\u5165\uff0c\u83b7\u5f97\u66f4\u4f4e\u7684Davies-Bouldin\u6307\u6570(DBI=9.91)\u548c\u66f4\u9ad8\u7684CKD\u9636\u6bb5\u5206\u7c7b\u51c6\u786e\u7387(0.74)\uff0c\u4f18\u4e8e\u666e\u901aLSTM(DBI=15.85,\u51c6\u786e\u7387=0.63)\u548c\u6ce8\u610f\u529b\u589e\u5f3aLSTM(DBI=20.72,\u51c6\u786e\u7387=0.67)\u3002\u5728ICU\u5185\u6b7b\u4ea1\u7387\u9884\u6d4b\u4e2d\uff0c\u5d4c\u5165\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u7aef\u5230\u7aef\u9884\u6d4b\u5668\uff0c\u51c6\u786e\u7387\u4ece0.72-0.75\u63d0\u5347\u52300.82-0.83\u3002", "conclusion": "\u65f6\u95f4\u611f\u77e5LSTM\u67b6\u6784\u80fd\u591f\u5b66\u4e60\u5230\u66f4\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u4e14\u5b66\u4e60\u5d4c\u5165\u4f5c\u4e3a\u4e2d\u95f4\u6b65\u9aa4\u6bd4\u76f4\u63a5\u7aef\u5230\u7aef\u5b66\u4e60\u66f4\u6709\u6548\uff0c\u8868\u660e\u8868\u793a\u5b66\u4e60\u5728\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.18676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18676", "abs": "https://arxiv.org/abs/2601.18676", "authors": ["Miles Martinez", "Alex H. Williams"], "title": "Quasi Monte Carlo methods enable extremely low-dimensional deep generative models", "comment": null, "summary": "This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.", "AI": {"tldr": "QLVMs\u662f\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u5bfb\u627e\u9ad8\u7ef4\u6570\u636e\u96c6\u6781\u4f4e\u7ef4\u53ef\u89e3\u91ca\u5d4c\u5165\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u62df\u8499\u7279\u5361\u6d1b\u79ef\u5206\u76f4\u63a5\u8fd1\u4f3c\u8fb9\u9645\u4f3c\u7136\uff0c\u5728\u4e00\u7ef4\u3001\u4e8c\u7ef4\u548c\u4e09\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5b66\u4e60\u7f16\u7801\u5668\u548c\u53d8\u5206\u4e0b\u754c\uff0c\u96be\u4ee5\u83b7\u5f97\u6781\u4f4e\u7ef4\u4e14\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u5d4c\u5165\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u4f18\u5316\u8fb9\u9645\u4f3c\u7136\u3001\u4e13\u95e8\u9488\u5bf9\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u900f\u660e\u7684\u53ef\u89c6\u5316\u548c\u540e\u9a8c\u5206\u6790\u3002", "method": "\u63d0\u51fa\u62df\u8499\u7279\u5361\u6d1b\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff08QLVMs\uff09\uff0c\u901a\u8fc7\u968f\u673a\u62df\u8499\u7279\u5361\u6d1b\u79ef\u5206\u76f4\u63a5\u8fd1\u4f3c\u8fb9\u9645\u4f3c\u7136\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u53d8\u5206\u4e0b\u754c\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u4e00\u7ef4\u3001\u4e8c\u7ef4\u548c\u4e09\u7ef4\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cQLVMs\u5728\u5339\u914d\u6f5c\u5728\u7ef4\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAEs\uff09\u548c\u91cd\u8981\u6027\u52a0\u6743\u81ea\u7f16\u7801\u5668\uff08IWAEs\uff09\u3002\u751f\u6210\u7684\u5d4c\u5165\u652f\u6301\u900f\u660e\u53ef\u89c6\u5316\u3001\u975e\u53c2\u6570\u5bc6\u5ea6\u4f30\u8ba1\u3001\u805a\u7c7b\u548c\u6d4b\u5730\u7ebf\u8def\u5f84\u8ba1\u7b97\u7b49\u540e\u9a8c\u5206\u6790\u3002", "conclusion": "QLVMs\u4e3a\u4f18\u5148\u8003\u8651\u53ef\u89e3\u91ca\u6027\u548c\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u5438\u5f15\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u5bc6\u96c6\u4e14\u5728\u590d\u6742\u6570\u636e\u96c6\u4e2d\u96be\u4ee5\u751f\u6210\u7cbe\u7ec6\u7ec6\u8282\uff0c\u4f46\u5728\u4f4e\u7ef4\u5d4c\u5165\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.18696", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18696", "abs": "https://arxiv.org/abs/2601.18696", "authors": ["Paul Whitten", "Francis Wolff", "Chris Papachristou"], "title": "Explainability Methods for Hardware Trojan Detection: A Systematic Comparison", "comment": null, "summary": "Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).\n  Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like \"high fanin complexity near outputs indicates potential triggers.\" Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.\n  XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.\n  This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.", "AI": {"tldr": "\u6bd4\u8f83\u4e09\u79cd\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5c5e\u6027\u7684\u7535\u8def\u5206\u6790\u3001\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u548c\u6a21\u578b\u65e0\u5173\u7279\u5f81\u5f52\u56e0\uff0c\u663e\u793a\u524d\u4e24\u8005\u5728\u9886\u57df\u5bf9\u9f50\u548c\u57fa\u4e8e\u5148\u4f8b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u7279\u5f81\u6392\u540d", "motivation": "\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\u9700\u8981\u51c6\u786e\u8bc6\u522b\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff0c\u4ee5\u4fbf\u5b89\u5168\u5de5\u7a0b\u5e08\u80fd\u591f\u9a8c\u8bc1\u5e76\u91c7\u53d6\u884c\u52a8\u3002\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u95e8\u7ea7\u6728\u9a6c\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5728Trust-Hub\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6bd4\u8f83\u4e09\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u5c5e\u6027\u7684\u5206\u6790\uff1a\u4f7f\u752831\u4e2a\u7535\u8def\u7279\u5b9a\u7279\u5f81\uff08\u95e8\u6247\u5165\u6a21\u5f0f\u3001\u89e6\u53d1\u5668\u8ddd\u79bb\u3001I/O\u8fde\u63a5\u6027\uff09\uff1b2) \u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\uff1a\u4f7f\u7528k\u8fd1\u90bb\u7b97\u6cd5\u8fdb\u884c\u57fa\u4e8e\u5148\u4f8b\u7684\u89e3\u91ca\uff1b3) \u6a21\u578b\u65e0\u5173\u7279\u5f81\u5f52\u56e0\uff1a\u4f7f\u7528LIME\u3001SHAP\u548c\u68af\u5ea6\u65b9\u6cd5\u3002\u4f7f\u7528XGBoost\u5206\u7c7b\u5668\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "XGBoost\u572811,392\u4e2a\u6d4b\u8bd5\u6837\u672c\u4e0a\u8fbe\u523046.15%\u7cbe\u5ea6\u548c52.17%\u53ec\u56de\u7387\uff0c\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\uff08Hasegawa\u7b49\uff1a5.13%\uff09\u7cbe\u5ea6\u63d0\u53479\u500d\uff0c\u8bef\u62a5\u7387\u4ece5.6%\u964d\u81f30.25%\u3002\u57fa\u4e8e\u5c5e\u6027\u7684\u5206\u6790\u63d0\u4f9b\u7535\u8def\u6982\u5ff5\u89e3\u91ca\uff0c\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u8fbe\u523097.4%\u9884\u6d4b\u4e0e\u8bad\u7ec3\u6837\u672c\u5bf9\u5e94\u6027\uff0cLIME\u548cSHAP\u7279\u5f81\u5f52\u56e0\u76f8\u5173\u6027\u9ad8\uff08r=0.94\uff09\u4f46\u7f3a\u4e4f\u7535\u8def\u7ea7\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u57fa\u4e8e\u5c5e\u6027\u548c\u57fa\u4e8e\u6848\u4f8b\u7684\u65b9\u6cd5\u5728\u9886\u57df\u5bf9\u9f50\u548c\u57fa\u4e8e\u5148\u4f8b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u7279\u5f81\u6392\u540d\u65b9\u6cd5\uff0c\u5bf9\u4e8e\u9700\u8981\u9a8c\u8bc1ML\u9884\u6d4b\u7684\u5b9e\u9645\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u68af\u5ea6\u5f52\u56e0\u6bd4SHAP\u5feb481\u500d\u4f46\u63d0\u4f9b\u7c7b\u4f3c\u7684\u9886\u57df\u4e0d\u900f\u660e\u89c1\u89e3\u3002"}}
{"id": "2601.18702", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18702", "abs": "https://arxiv.org/abs/2601.18702", "authors": ["Hansheng Ren"], "title": "From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic", "comment": "8 pages, 6 figures. Submitted to UAI 2026", "summary": "Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the \"hallucinations\" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u6df1\u5ea6\u5b66\u4e60\u4f9d\u8d56\u6d6e\u70b9\u8fd1\u4f3c\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u7cbe\u786e\u7b97\u672f\u662f\u901a\u7528\u667a\u80fd\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u6709\u7406\u6570\u8fd0\u7b97\u7684Halo\u67b6\u6784", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\u8fc7\u4e8e\u5f3a\u8c03\u8ba1\u7b97\u541e\u5410\u91cf\u800c\u5ffd\u89c6\u6570\u503c\u7cbe\u5ea6\uff0c\u4f5c\u8005\u8ba4\u4e3a\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u7684\"\u5e7b\u89c9\"\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\u6e90\u4e8eIEEE 754\u6d6e\u70b9\u8fd1\u4f3c\u8bef\u5dee\u5728\u6df1\u5ea6\u7ec4\u5408\u51fd\u6570\u4e2d\u7684\u7d2f\u79ef\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u6570\u503c\u7cbe\u5ea6\u95ee\u9898\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u901a\u7528\u667a\u80fd", "method": "\u63d0\u51fa\u7cbe\u786e\u6027\u5047\u8bf4\uff0c\u8ba4\u4e3a\u901a\u7528\u667a\u80fd\u9700\u8981\u4efb\u610f\u7cbe\u5ea6\u7b97\u672f\u8ba1\u7b97\u57fa\u677f\uff1b\u5f15\u5165Halo\u67b6\u6784\uff0c\u91c7\u7528\u6709\u7406\u6570\u7b97\u672f(\u211a)\u548c\u65b0\u578b\u7cbe\u786e\u63a8\u7406\u5355\u5143(EIU)\u6765\u6d88\u9664\u6570\u503c\u8fd1\u4f3c\u8bef\u5dee", "result": "\u5728Huginn-0125\u539f\u578b\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u663e\u793a\uff0c600B\u53c2\u6570\u7684BF16\u57fa\u7ebf\u5728\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u5d29\u6e83\uff0c\u800cHalo\u67b6\u6784\u80fd\u591f\u65e0\u9650\u671f\u4fdd\u6301\u96f6\u6570\u503c\u53d1\u6563", "conclusion": "\u7cbe\u786e\u7b97\u672f\u662f\u51cf\u5c11System 2 AGI\u4e2d\u903b\u8f91\u4e0d\u786e\u5b9a\u6027\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u4e3a\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f"}}
{"id": "2601.18728", "categories": ["cs.LG", "math.DG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.18728", "abs": "https://arxiv.org/abs/2601.18728", "authors": ["Willem Diepeveen", "Oscar Leong"], "title": "Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data", "comment": null, "summary": "Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.", "AI": {"tldr": "Riemannian AmbientFlow\uff1a\u4e00\u79cd\u4ece\u566a\u58f0/\u635f\u574f\u89c2\u6d4b\u4e2d\u540c\u65f6\u5b66\u4e60\u6982\u7387\u751f\u6210\u6a21\u578b\u548c\u5e95\u5c42\u975e\u7ebf\u6027\u6570\u636e\u6d41\u5f62\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u53d8\u5206\u63a8\u65ad\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u9ece\u66fc\u51e0\u4f55", "motivation": "\u5728\u79d1\u5b66\u548c\u6210\u50cf\u5e94\u7528\u4e2d\uff0c\u901a\u5e38\u65e0\u6cd5\u83b7\u5f97\u5e72\u51c0\u6837\u672c\uff0c\u53ea\u80fd\u89c2\u6d4b\u5230\u566a\u58f0\u6216\u7ebf\u6027\u635f\u574f\u7684\u6d4b\u91cf\u503c\u3002\u540c\u65f6\uff0c\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u7ed3\u6784\uff08\u5982\u6d41\u5f62\u51e0\u4f55\uff09\u5bf9\u4e0b\u6e38\u79d1\u5b66\u5206\u6790\u5f88\u91cd\u8981\uff0c\u9700\u8981\u63d0\u53d6", "method": "\u57fa\u4e8eAmbientFlow\u7684\u53d8\u5206\u63a8\u65ad\u6846\u67b6\uff0c\u7ed3\u5408\u7531\u5f52\u4e00\u5316\u6d41\u8bf1\u5bfc\u7684\u6570\u636e\u9a71\u52a8\u9ece\u66fc\u51e0\u4f55\uff0c\u901a\u8fc7\u62c9\u56de\u5ea6\u91cf\u548c\u9ece\u66fc\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u6d41\u5f62\u7ed3\u6784", "result": "\u5728\u9002\u5f53\u7684\u51e0\u4f55\u6b63\u5219\u5316\u548c\u6d4b\u91cf\u6761\u4ef6\u4e0b\uff0c\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u4ee5\u53ef\u63a7\u8bef\u5dee\u6062\u590d\u5e95\u5c42\u6570\u636e\u5206\u5e03\uff0c\u5e76\u83b7\u5f97\u5149\u6ed1\u7684\u53cc\u5229\u666e\u5e0c\u8328\u6d41\u5f62\u53c2\u6570\u5316\uff1b\u5149\u6ed1\u89e3\u7801\u5668\u53ef\u4f5c\u4e3a\u9006\u95ee\u9898\u7684\u539f\u5219\u6027\u751f\u6210\u5148\u9a8c", "conclusion": "Riemannian AmbientFlow\u4e3a\u4ece\u635f\u574f\u89c2\u6d4b\u4e2d\u540c\u65f6\u5b66\u4e60\u751f\u6210\u6a21\u578b\u548c\u5e95\u5c42\u6d41\u5f62\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u6846\u67b6\uff0c\u5728\u4f4e\u7ef4\u5408\u6210\u6d41\u5f62\u548cMNIST\u4e0a\u5f97\u5230\u9a8c\u8bc1"}}
{"id": "2601.18734", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18734", "abs": "https://arxiv.org/abs/2601.18734", "authors": ["Siyan Zhao", "Zhihui Xie", "Mengchen Liu", "Jing Huang", "Guan Pang", "Feiyu Chen", "Aditya Grover"], "title": "Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models", "comment": "13 pages", "summary": "Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.", "AI": {"tldr": "\u63d0\u51faOn-Policy Self-Distillation (OPSD)\u6846\u67b6\uff0c\u8ba9\u5355\u4e2a\u6a21\u578b\u540c\u65f6\u4f5c\u4e3a\u6559\u5e08\u548c\u5b66\u751f\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u9ad8\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5728\u7ebf\u84b8\u998f\u9700\u8981\u72ec\u7acb\u4e14\u901a\u5e38\u66f4\u5927\u7684\u6559\u5e08\u6a21\u578b\uff1b2\uff09\u672a\u80fd\u5145\u5206\u5229\u7528\u63a8\u7406\u6570\u636e\u96c6\u4e2d\u53ef\u7528\u7684\u771f\u5b9e\u89e3\u51b3\u65b9\u6848\u3002\u53d7\u5230\"\u8db3\u591f\u5f3a\u5927\u7684LLM\u80fd\u591f\u63a8\u7406\u5916\u90e8\u7279\u6743\u4fe1\u606f\u5e76\u6559\u5bfc\u5176\u8f83\u5f31\u7248\u672c\"\u8fd9\u4e00\u76f4\u89c9\u542f\u53d1\uff0c\u63d0\u51fa\u81ea\u84b8\u998f\u6846\u67b6\u3002", "method": "OPSD\u6846\u67b6\u8ba9\u5355\u4e2a\u6a21\u578b\u540c\u65f6\u626e\u6f14\u6559\u5e08\u548c\u5b66\u751f\u89d2\u8272\u3002\u6559\u5e08\u7b56\u7565\u57fa\u4e8e\u7279\u6743\u4fe1\u606f\uff08\u5982\u5df2\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9\uff09\uff0c\u5b66\u751f\u7b56\u7565\u4ec5\u770b\u5230\u95ee\u9898\u3002\u8bad\u7ec3\u65f6\u6700\u5c0f\u5316\u8fd9\u4e24\u4e2a\u5206\u5e03\u5728\u5b66\u751f\u81ea\u8eab\u8f68\u8ff9\u4e0a\u7684\u6bcf\u4ee4\u724c\u5206\u6b67\uff0c\u5b9e\u73b0\u81ea\u6211\u84b8\u998f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOPSD\u76f8\u6bd4GRPO\u7b49\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e864-8\u500d\u7684\u4ee4\u724c\u6548\u7387\u63d0\u5347\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u79bb\u7ebf\u84b8\u998f\u65b9\u6cd5\u3002", "conclusion": "OPSD\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u84b8\u998f\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e3aLLM\u63a8\u7406\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18751", "abs": "https://arxiv.org/abs/2601.18751", "authors": ["Seyed Amir Hosseini", "Maryam Abdolali", "Amirhosein Tavakkoli", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback", "comment": "Equal contribution: Seyed Amir Hosseini and Maryam Abdolali. Corresponding author: Maryam Abdolali (maryam.abdolali@kntu.ac.ir)", "summary": "Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.", "AI": {"tldr": "TriTrust-PBRL (TTP) \u662f\u4e00\u4e2a\u5904\u7406\u5f02\u6784\u504f\u597d\u6807\u6ce8\u8005\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u5171\u4eab\u5956\u52b1\u6a21\u578b\u548c\u4e13\u5bb6\u7279\u5b9a\u4fe1\u4efb\u53c2\u6570\uff0c\u80fd\u591f\u81ea\u52a8\u5904\u7406\u53ef\u9760\u3001\u566a\u58f0\u548c\u5bf9\u6297\u6027\u6807\u6ce8\u8005\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8fc7\u6ee4\u4e0d\u53ef\u9760\u53cd\u9988\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u504f\u597d\u6570\u636e\u901a\u5e38\u6765\u81ea\u5f02\u6784\u7684\u6807\u6ce8\u8005\uff0c\u5305\u62ec\u53ef\u9760\u7684\u3001\u6709\u566a\u58f0\u7684\u548c\u7cfb\u7edf\u5bf9\u6297\u6027\u7684\u3002\u73b0\u6709\u7684PBRL\u65b9\u6cd5\u8981\u4e48\u5e73\u7b49\u5bf9\u5f85\u6240\u6709\u53cd\u9988\uff0c\u8981\u4e48\u5c1d\u8bd5\u8fc7\u6ee4\u4e0d\u53ef\u9760\u6765\u6e90\uff0c\u4f46\u5728\u9762\u5bf9\u7cfb\u7edf\u6027\u63d0\u4f9b\u9519\u8bef\u504f\u597d\u7684\u5bf9\u6297\u6027\u6807\u6ce8\u8005\u65f6\u90fd\u4f1a\u5931\u8d25\u3002", "method": "TTP\u6846\u67b6\u8054\u5408\u5b66\u4e60\u5171\u4eab\u5956\u52b1\u6a21\u578b\u548c\u4e13\u5bb6\u7279\u5b9a\u7684\u4fe1\u4efb\u53c2\u6570\u3002\u5173\u952e\u6d1e\u5bdf\u662f\u4fe1\u4efb\u53c2\u6570\u5728\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u8fc7\u7a0b\u4e2d\u81ea\u7136\u6f14\u53d8\u4e3a\u6b63\u503c\uff08\u4fe1\u4efb\uff09\u3001\u63a5\u8fd1\u96f6\uff08\u5ffd\u7565\uff09\u6216\u8d1f\u503c\uff08\u7ffb\u8f6c\uff09\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u52a8\u53cd\u8f6c\u5bf9\u6297\u6027\u504f\u597d\u5e76\u6062\u590d\u6709\u7528\u4fe1\u53f7\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u4e22\u5f03\u635f\u574f\u7684\u53cd\u9988\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\uff08MetaWorld\u64cd\u4f5c\u4efb\u52a1\u548cDM Control\u8fd0\u52a8\u4efb\u52a1\uff09\u7684\u5404\u79cd\u635f\u574f\u573a\u666f\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002TTP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\uff0c\u5728\u5bf9\u6297\u6027\u635f\u574f\u4e0b\u4fdd\u6301\u63a5\u8fd1oracle\u7684\u6027\u80fd\uff0c\u800c\u6807\u51c6PBRL\u65b9\u6cd5\u5219\u5b8c\u5168\u5931\u8d25\u3002TTP\u6210\u529f\u5730\u4ece\u5305\u542b\u53ef\u9760\u548c\u5bf9\u6297\u6027\u6807\u6ce8\u8005\u7684\u6df7\u5408\u4e13\u5bb6\u6c60\u4e2d\u5b66\u4e60\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TTP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784\u504f\u597d\u6807\u6ce8\u8005\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u6807\u6ce8\u8005\uff0c\u901a\u8fc7\u81ea\u52a8\u5b66\u4e60\u4fe1\u4efb\u53c2\u6570\u6765\u5229\u7528\u6240\u6709\u53cd\u9988\u4fe1\u53f7\uff0c\u65e0\u9700\u8d85\u51fa\u8bc6\u522b\u7d22\u5f15\u7684\u4e13\u5bb6\u7279\u5f81\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709PBRL\u6d41\u7a0b\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2601.18760", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18760", "abs": "https://arxiv.org/abs/2601.18760", "authors": ["Henry Bell", "Lara Neubauer da Costa Schertel", "Bochu Ding", "Brandon Fain"], "title": "Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values", "comment": null, "summary": "A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \\textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \\textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.", "AI": {"tldr": "\u63d0\u51faGrounded Constitutional AI\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u5bf9AI\u7684\u666e\u904d\u671f\u671b\u548c\u4ea4\u4e92\u65f6\u504f\u597d\uff0c\u751f\u6210\u66f4\u5177\u4ee3\u8868\u6027\u3001\u9053\u5fb7\u57fa\u7840\u3001\u8fde\u8d2f\u6027\u548c\u591a\u5143\u6027\u7684\u5baa\u6cd5\u539f\u5219", "motivation": "\u5f53\u524d\u5baa\u6cd5AI\u6846\u67b6\u4e2d\uff0c\u5982\u4f55\u516c\u5e73\u786e\u5b9a\u4ee3\u8868\u5e7f\u6cdb\u5229\u76ca\u76f8\u5173\u8005\u610f\u89c1\u7684\u5baa\u6cd5\u539f\u5219\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u53cd\u6620\u7528\u6237\u666e\u904d\u671f\u671b\u548c\u5177\u4f53\u4ea4\u4e92\u504f\u597d\u7684\u5baa\u6cd5\u751f\u6210\u65b9\u6cd5", "method": "\u6269\u5c55Inverse Constitutional AI\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\u6570\u636e\u4e2d\u63d0\u4f9b\u7684\u7406\u7531\u751f\u6210\u60c5\u5883\u6027\u539f\u5219\uff0c\u540c\u65f6\u4ece\u7528\u6237\u5173\u4e8eAI\u4ef7\u503c\u7684\u9648\u8ff0\u4e2d\u63d0\u53d6\u666e\u904d\u6027\u539f\u5219\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u5baa\u6cd5\u751f\u6210\u6846\u67b6", "result": "\u4eba\u7c7b\u53c2\u4e0e\u8005\u66f4\u504f\u597dGCAI\u751f\u6210\u7684\u5baa\u6cd5\uff0c\u8ba4\u4e3a\u5176\u65e2\u9002\u5408\u4e2a\u4eba\u4f7f\u7528\u4e5f\u9002\u5408\u5e7f\u6cdb\u6cbb\u7406AI\u884c\u4e3a\uff0c\u4e14\u66f4\u5177\u9053\u5fb7\u57fa\u7840\u3001\u8fde\u8d2f\u6027\u548c\u591a\u5143\u6027", "conclusion": "Grounded Constitutional AI\u6846\u67b6\u80fd\u591f\u751f\u6210\u66f4\u5168\u9762\u4ee3\u8868\u7528\u6237\u4ef7\u503c\u89c2\u7684\u5baa\u6cd5\u539f\u5219\uff0c\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u516c\u5e73\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u5baa\u6cd5\u5236\u5b9a\u65b9\u6cd5"}}
{"id": "2601.18777", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18777", "abs": "https://arxiv.org/abs/2601.18777", "authors": ["Abhishek Divekar", "Anirban Majumder"], "title": "PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation", "comment": "Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)", "summary": "Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.", "AI": {"tldr": "\u63d0\u51faPRECISE\u6846\u67b6\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u4e0eLLM\u5224\u65ad\u6765\u4f30\u8ba1\u641c\u7d22/\u6392\u5e8f/RAG\u7cfb\u7edf\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\uff0c\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6", "motivation": "\u4f20\u7edf\u641c\u7d22\u3001\u6392\u5e8f\u548cRAG\u7cfb\u7edf\u8bc4\u4f30\u9700\u8981\u5927\u91cf\u4eba\u5de5\u76f8\u5173\u6027\u6807\u6ce8\uff0c\u800cLLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u5224\u5b58\u5728\u56fa\u6709\u504f\u5dee\uff0c\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u6307\u6807\u4f30\u8ba1", "method": "\u6269\u5c55\u9884\u6d4b\u9a71\u52a8\u63a8\u7406(PPI)\u6846\u67b6\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u67e5\u8be2(100\u4e2a)\u548c\u5927\u91cf\u672a\u6807\u6ce8\u6837\u672c(10,000\u4e2a)\uff0c\u5c06LLM\u5224\u65ad\u4e0e\u4eba\u5de5\u6807\u6ce8\u878d\u5408\uff0c\u901a\u8fc7\u91cd\u6784\u6307\u6807\u96c6\u6210\u7a7a\u95f4\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(2^|C|)\u964d\u81f3O(2^K)", "result": "\u5728\u591a\u4e2a\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5173\u952e\u4e1a\u52a1\u6307\u6807Precision@K\u7684\u4f30\u8ba1\u65b9\u5dee\uff0c\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u6709\u6548\u6821\u6b63LLM\u504f\u5dee", "conclusion": "PRECISE\u6846\u67b6\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u7ed3\u5408LLM\u5224\u65ad\u4e0e\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\uff0c\u663e\u8457\u51cf\u5c11\u8bc4\u4f30\u6210\u672c\uff0c\u4e3a\u641c\u7d22\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18779", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18779", "abs": "https://arxiv.org/abs/2601.18779", "authors": ["Yuxiao Qu", "Amrith Setlur", "Virginia Smith", "Ruslan Salakhutdinov", "Aviral Kumar"], "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration", "comment": null, "summary": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.", "AI": {"tldr": "\u63d0\u51faPOPE\u65b9\u6cd5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u786c\u95ee\u9898\u7684\u63a2\u7d22\u96be\u9898\uff0c\u901a\u8fc7\u4f7f\u7528\u7279\u6743\u4fe1\u606f\uff08\u5982\u4eba\u7c7b\u89e3\u51b3\u65b9\u6848\uff09\u4f5c\u4e3a\u524d\u7f00\u5f15\u5bfc\u63a2\u7d22\uff0c\u4f7fRL\u5728\u786c\u95ee\u9898\u4e0a\u83b7\u5f97\u975e\u96f6\u5956\u52b1\uff0c\u5e76\u5c06\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8fc1\u79fb\u56de\u539f\u59cb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u5728\u786c\u63a8\u7406\u95ee\u9898\u4e0a\u7ecf\u5e38\u65e0\u6cd5\u63a2\u7d22\u5230\u6b63\u786e\u8f68\u8ff9\uff0c\u5bfc\u81f4\u96f6\u5956\u52b1\u548c\u7f3a\u4e4f\u5b66\u4e60\u4fe1\u53f7\u3002\u4f20\u7edfRL\u7684\u63a2\u7d22\u6539\u8fdb\u65b9\u6cd5\uff08\u5982\u71b5\u5956\u52b1\u3001\u91cd\u8981\u6027\u6bd4\u7387\u88c1\u526a\u3001pass@k\u4f18\u5316\uff09\u65e0\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u800c\u6df7\u5408\u96be\u6613\u95ee\u9898\u8bad\u7ec3\u4f1a\u4ea7\u751f\u5c04\u7ebf\u5e72\u6270\uff0c\u4f7f\u4f18\u5316\u96c6\u4e2d\u5728\u5df2\u89e3\u51b3\u7684\u95ee\u9898\u4e0a\u800c\u963b\u788d\u786c\u95ee\u9898\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u7279\u6743\u5728\u7ebf\u7b56\u7565\u63a2\u7d22\uff08POPE\uff09\u65b9\u6cd5\uff1a1\uff09\u5728\u786c\u95ee\u9898\u524d\u6dfb\u52a0\u7279\u6743\u4fe1\u606f\uff08\u5982\u4eba\u7c7b\u89e3\u51b3\u65b9\u6848\uff09\u4f5c\u4e3a\u524d\u7f00\uff1b2\uff09RL\u5728\u5f15\u5bfc\u8f68\u8ff9\u4e0a\u83b7\u5f97\u975e\u96f6\u5956\u52b1\uff1b3\uff09\u901a\u8fc7\u6307\u4ee4\u8ddf\u968f\u548c\u63a8\u7406\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5c06\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8fc1\u79fb\u56de\u539f\u59cb\u65e0\u5f15\u5bfc\u95ee\u9898\u3002\u4e0e\u4f7f\u7528\u7279\u6743\u4fe1\u606f\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cPOPE\u4ec5\u5c06\u5176\u7528\u4e8e\u5f15\u5bfc\u63a2\u7d22\u3002", "result": "POPE\u663e\u8457\u6269\u5c55\u4e86\u53ef\u89e3\u51b3\u95ee\u9898\u7684\u96c6\u5408\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u57fa\u51c6\u4e0a\u5927\u5e45\u63d0\u5347\u4e86\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u786c\u95ee\u9898\u63a2\u7d22\u96be\u9898\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5982\u71b5\u5956\u52b1\u3001\u91cd\u8981\u6027\u6bd4\u7387\u88c1\u526a\u6216pass@k\u4f18\u5316\u90fd\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u3002", "conclusion": "POPE\u901a\u8fc7\u7279\u6743\u4fe1\u606f\u5f15\u5bfc\u63a2\u7d22\u6709\u6548\u89e3\u51b3\u4e86RL\u5728\u786c\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u63a2\u7d22\u96be\u9898\uff0c\u907f\u514d\u4e86\u6df7\u5408\u8bad\u7ec3\u4e2d\u7684\u5c04\u7ebf\u5e72\u6270\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ece\u5f15\u5bfc\u95ee\u9898\u5230\u539f\u59cb\u95ee\u9898\u7684\u884c\u4e3a\u8fc1\u79fb\uff0c\u4e3aLLM\u7684RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3002"}}
{"id": "2601.18795", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18795", "abs": "https://arxiv.org/abs/2601.18795", "authors": ["Amrith Setlur", "Zijian Wang", "Andrew Cohen", "Paria Rashidinejad", "Sang Michael Xie"], "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes", "comment": null, "summary": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.", "AI": {"tldr": "PrefixRL\uff1a\u4e00\u79cd\u901a\u8fc7\u91cd\u7528\u79bb\u7b56\u7565\u8f68\u8ff9\u524d\u7f00\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u7684\u65b0\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u79bb\u7b56\u7565\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5728\u89e3\u51b3\u56f0\u96be\u63a8\u7406\u95ee\u9898\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u6b63\u786e\u7684\u7b56\u7565\u8f68\u8ff9\u7a00\u5c11\uff0c\u7b56\u7565\u68af\u5ea6\u6d88\u5931\uff0c\u5b66\u4e60\u505c\u6ede\u3002\u9700\u8981\u66f4\u6709\u6548\u5730\u91cd\u7528\u5148\u524d\u63a8\u7406\u6216\u8bad\u7ec3\u4e2d\u4ea7\u751f\u7684\u8ba1\u7b97\u8d44\u6e90\uff08FLOPs\uff09", "method": "PrefixRL\uff1a\u57fa\u4e8e\u6210\u529f\u79bb\u7b56\u7565\u8f68\u8ff9\u7684\u524d\u7f00\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u7136\u540e\u8fd0\u884c\u7b56\u7565RL\u6765\u5b8c\u6210\u5269\u4f59\u90e8\u5206\u3002\u901a\u8fc7\u8c03\u8282\u524d\u7f00\u957f\u5ea6\u6765\u63a7\u5236\u95ee\u9898\u96be\u5ea6\uff0c\u907f\u514d\u79bb\u7b56\u7565\u4e0d\u7a33\u5b9a\u6027\u3002\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u83b7\u53d6\u79bb\u7b56\u7565\u8f68\u8ff9\uff0c\u5f62\u6210\u81ea\u6211\u6539\u8fdb\u5faa\u73af", "result": "\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\uff0cPrefixRL\u8fbe\u5230\u76f8\u540c\u8bad\u7ec3\u5956\u52b1\u7684\u901f\u5ea6\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff08\u5728\u79bb\u7b56\u7565\u6570\u636e\u4e0a\u8fdb\u884cSFT\u7136\u540eRL\uff09\u5feb2\u500d\uff0c\u5373\u4f7f\u8003\u8651\u4e86\u521d\u59cb\u62d2\u7edd\u91c7\u6837\u7684\u8ba1\u7b97\u6210\u672c\u3002\u6700\u7ec8\u5956\u52b1\u63d0\u9ad83\u500d\uff0c\u6548\u679c\u53ef\u8fc1\u79fb\u5230\u4fdd\u7559\u57fa\u51c6\u6d4b\u8bd5\u4e2d", "conclusion": "PrefixRL\u901a\u8fc7\u91cd\u7528\u79bb\u7b56\u7565\u8f68\u8ff9\u524d\u7f00\u6709\u6548\u63d0\u5347RL\u5b66\u4e60\u6548\u7387\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u79bb\u7b56\u7565\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4e0e\u6807\u51c6RL\u76ee\u6807\u4e00\u81f4\uff0c\u800c\u4e14\u66f4\u5177\u6837\u672c\u6548\u7387\uff0c\u53d1\u73b0\u4e86\"\u53cd\u5411\u6cdb\u5316\"\u73b0\u8c61\uff0c\u5728\u5b9e\u9645\u8bbe\u7f6e\u4e2d\u5177\u6709\u7075\u6d3b\u6027"}}
