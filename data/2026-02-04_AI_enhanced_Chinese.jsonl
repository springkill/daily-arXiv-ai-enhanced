{"id": "2602.01720", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.01720", "abs": "https://arxiv.org/abs/2602.01720", "authors": ["Peisen Yao", "Zinan Gu", "Qingkai Shi"], "title": "Phoenix: A Modular and Versatile Framework for C/C++ Pointer Analysis", "comment": null, "summary": "We present Phoenix, a modular pointer analysis framework for C/C++ that unifies multiple state-of-the-art alias analysis algorithms behind a single, stable interface. Phoenix addresses the fragmentation of today's C/C++ pointer analysis ecosystem by cleanly separating IR construction, constraint generation, solver backends, and client-facing queries, making analyses easy to compare, swap, and compose while exposing explicit precision-performance trade-offs. We evaluate Phoenix against SVF under two representative configurations: a flow- and context-insensitive setting and a more precise flow- and context-sensitive setting, on 28 GNU coreutils programs. Phoenix delivers robust speedups in the baseline configuration (up to 2.88x) and remains competitive, and often faster, even in the stronger precision regime (up to 2.91x), without a systematic runtime penalty. In production, Phoenix serves as the analysis substrate for static analysis and fuzzing tools that have uncovered hundreds of new bugs and enabled deployments reporting more than 1000 bugs found in an industrial toolchain.", "AI": {"tldr": "Phoenix\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684C/C++\u6307\u9488\u5206\u6790\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u6700\u5148\u8fdb\u7684\u522b\u540d\u5206\u6790\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u79bbIR\u6784\u5efa\u3001\u7ea6\u675f\u751f\u6210\u3001\u6c42\u89e3\u5668\u540e\u7aef\u548c\u5ba2\u6237\u7aef\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u6307\u9488\u5206\u6790\u751f\u6001\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dC/C++\u6307\u9488\u5206\u6790\u751f\u6001\u7cfb\u7edf\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6bd4\u8f83\u3001\u4ea4\u6362\u548c\u7ec4\u5408\u4e0d\u540c\u7684\u5206\u6790\u7b97\u6cd5\uff0c\u540c\u65f6\u660e\u786e\u5c55\u793a\u7cbe\u5ea6\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u8bbe\u8ba1\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5c06IR\u6784\u5efa\u3001\u7ea6\u675f\u751f\u6210\u3001\u6c42\u89e3\u5668\u540e\u7aef\u548c\u5ba2\u6237\u7aef\u67e5\u8be2\u5206\u79bb\uff1b\u5b9e\u73b0\u591a\u79cd\u6700\u5148\u8fdb\u7684\u522b\u540d\u5206\u6790\u7b97\u6cd5\uff1b\u63d0\u4f9b\u7a33\u5b9a\u7684\u63a5\u53e3\u4f9b\u4e0d\u540c\u5206\u6790\u914d\u7f6e\u4f7f\u7528\u3002", "result": "\u572828\u4e2aGNU coreutils\u7a0b\u5e8f\u4e0a\u8bc4\u4f30\uff0c\u4e0eSVF\u76f8\u6bd4\uff1a\u5728\u6d41\u4e0d\u654f\u611f\u548c\u4e0a\u4e0b\u6587\u4e0d\u654f\u611f\u914d\u7f6e\u4e0b\u83b7\u5f97\u6700\u9ad82.88\u500d\u52a0\u901f\uff1b\u5728\u66f4\u7cbe\u786e\u7684\u6d41\u654f\u611f\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u914d\u7f6e\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\u4e14\u901a\u5e38\u66f4\u5feb\uff08\u6700\u9ad82.91\u500d\uff09\uff0c\u6ca1\u6709\u7cfb\u7edf\u6027\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "conclusion": "Phoenix\u6210\u529f\u89e3\u51b3\u4e86\u6307\u9488\u5206\u6790\u751f\u6001\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u4e14\u53ef\u914d\u7f6e\u7684\u5206\u6790\u6846\u67b6\uff0c\u5df2\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f5c\u4e3a\u9759\u6001\u5206\u6790\u548c\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u7684\u57fa\u7840\uff0c\u53d1\u73b0\u4e86\u6570\u767e\u4e2a\u65b0\u6f0f\u6d1e\u5e76\u62a5\u544a\u4e86\u8d85\u8fc71000\u4e2a\u5de5\u4e1a\u5de5\u5177\u94fe\u4e2d\u7684\u6f0f\u6d1e\u3002"}}
{"id": "2602.00087", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.00087", "abs": "https://arxiv.org/abs/2602.00087", "authors": ["Haolin Pan", "Lianghong Huang", "Jinyuan Dong", "Mingjie Xing", "Yanjun Wu"], "title": "ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization", "comment": null, "summary": "Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.", "AI": {"tldr": "ECCO\u6846\u67b6\u7ed3\u5408\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7ec4\u5408\u641c\u7d22\uff0c\u901a\u8fc7\u53cd\u5de5\u7a0b\u65b9\u6cd5\u6784\u5efa\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u8ba9LLM\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u7684\u56e0\u679c\u903b\u8f91\u800c\u975e\u7b80\u5355\u6a21\u4eff\uff0c\u7136\u540e\u4f5c\u4e3a\u7b56\u7565\u5e08\u6307\u5bfc\u9057\u4f20\u7b97\u6cd5\u8fdb\u884c\u7f16\u8bd1\u5668\u4f18\u5316\u3002", "motivation": "\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u9762\u4e34\u4f20\u7edf\u9ed1\u76d2\u641c\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u6307\u5bfc\u4e0eLLM\u65b9\u6cd5\u5b58\u5728\u8868\u9762\u6a21\u5f0f\u5339\u914d\u548c\u56e0\u679c\u4e0d\u900f\u660e\u6027\u7684\u4e24\u96be\u56f0\u5883\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7ec4\u5408\u641c\u7d22\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cd\u5411\u5de5\u7a0b\u65b9\u6cd5\u6784\u5efa\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u5c06\u9759\u6001\u4ee3\u7801\u7279\u5f81\u6620\u5c04\u5230\u53ef\u9a8c\u8bc1\u7684\u6027\u80fd\u8bc1\u636e\uff1b\u8bbe\u8ba1\u534f\u4f5c\u63a8\u7406\u673a\u5236\uff0c\u8ba9LLM\u4f5c\u4e3a\u7b56\u7565\u5e08\u5b9a\u4e49\u4f18\u5316\u610f\u56fe\uff0c\u52a8\u6001\u6307\u5bfc\u9057\u4f20\u7b97\u6cd5\u7684\u53d8\u5f02\u64cd\u4f5c\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cECCO\u663e\u8457\u4f18\u4e8eLLVM opt -O3\u57fa\u7ebf\uff0c\u5e73\u5747\u51cf\u5c1124.44%\u7684\u5468\u671f\u6570\u3002", "conclusion": "ECCO\u6846\u67b6\u6210\u529f\u5730\u5c06\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7ec4\u5408\u641c\u7d22\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u8ba9LLM\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u7684\u56e0\u679c\u903b\u8f91\u5e76\u4f5c\u4e3a\u7b56\u7565\u5e08\u6307\u5bfc\u9057\u4f20\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u7684\u7f16\u8bd1\u5668\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2602.00303", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.00303", "abs": "https://arxiv.org/abs/2602.00303", "authors": ["Jyoti Prakash", "Abhishek Tiwari", "Mikkel Baun Kj\u00e6rgaard"], "title": "Towards Analyzing N-language Polyglot Programs", "comment": null, "summary": "Polyglot programming is gaining popularity as developers integrate multiple programming languages to harness their individual strengths. With the recent popularity of platforms like GraalVM and other multi-language runtimes, creating and managing these systems has become much more feasible. However, current research on analyzing multilingual programs mainly focuses on two languages, leaving out the increasing complexity of systems that use three or more. For example, modern web systems often link JavaScript, WebAssembly, and Rust within the same execution chain. This paper envisions the landscape of software systems with three-language polyglot communication. We identify fundamental challenges in analyzing them and propose a conceptual roadmap to advance static analysis techniques to address them. Our vision aims to stimulate discussion and inspire new research directions toward scalable, language-agnostic analysis frameworks for next-generation polyglot systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00\u7f16\u7a0b\u7cfb\u7edf\u4e2d\u4e09\u79cd\u53ca\u4ee5\u4e0a\u8bed\u8a00\u4ea4\u4e92\u7684\u9759\u6001\u5206\u6790\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u6982\u5ff5\u8def\u7ebf\u56fe\u3002", "motivation": "\u968f\u7740GraalVM\u7b49\u591a\u8bed\u8a00\u8fd0\u884c\u65f6\u7684\u6d41\u884c\uff0c\u5f00\u53d1\u8005\u5728\u7cfb\u7edf\u4e2d\u96c6\u6210\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u4ee5\u5229\u7528\u5404\u81ea\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e24\u79cd\u8bed\u8a00\u7684\u4ea4\u4e92\u5206\u6790\uff0c\u5ffd\u89c6\u4e86\u4f7f\u7528\u4e09\u79cd\u53ca\u4ee5\u4e0a\u8bed\u8a00\u7684\u7cfb\u7edf\u65e5\u76ca\u589e\u957f\u7684\u590d\u6742\u6027\u3002\u73b0\u4ee3Web\u7cfb\u7edf\u7ecf\u5e38\u5728\u540c\u4e00\u6267\u884c\u94fe\u4e2d\u8fde\u63a5JavaScript\u3001WebAssembly\u548cRust\u7b49\u8bed\u8a00\uff0c\u9700\u8981\u65b0\u7684\u5206\u6790\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u79cd\u590d\u6742\u6027\u3002", "method": "\u8bba\u6587\u9996\u5148\u8bc6\u522b\u4e86\u5206\u6790\u4e09\u79cd\u8bed\u8a00\u591a\u8bed\u8a00\u901a\u4fe1\u7cfb\u7edf\u7684\u57fa\u672c\u6311\u6218\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u63a8\u8fdb\u9759\u6001\u5206\u6790\u6280\u672f\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u6784\u5efa\u53ef\u6269\u5c55\u3001\u8bed\u8a00\u65e0\u5173\u7684\u5206\u6790\u6846\u67b6\u3002", "result": "\u8bba\u6587\u63cf\u7ed8\u4e86\u4e09\u79cd\u8bed\u8a00\u591a\u8bed\u8a00\u901a\u4fe1\u7cfb\u7edf\u7684\u524d\u666f\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u5206\u6790\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u8def\u7ebf\u56fe\u6765\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002\u8fd9\u4e9b\u53d1\u73b0\u65e8\u5728\u6fc0\u53d1\u8ba8\u8bba\u5e76\u542f\u53d1\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u4e0b\u4e00\u4ee3\u591a\u8bed\u8a00\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u3001\u8bed\u8a00\u65e0\u5173\u5206\u6790\u6846\u67b6\u63d0\u4f9b\u4e86\u613f\u666f\u548c\u8def\u7ebf\u56fe\uff0c\u547c\u5401\u7814\u7a76\u793e\u533a\u5173\u6ce8\u4e09\u79cd\u53ca\u4ee5\u4e0a\u8bed\u8a00\u4ea4\u4e92\u7684\u590d\u6742\u5206\u6790\u95ee\u9898\uff0c\u63a8\u52a8\u591a\u8bed\u8a00\u7f16\u7a0b\u5206\u6790\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.00755", "categories": ["cs.MA", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.00755", "abs": "https://arxiv.org/abs/2602.00755", "authors": ["Ujwal Kumar", "Alice Saito", "Hershraj Niranjani", "Rayan Yessou", "Phan Xuan Tan"], "title": "Evolving Interpretable Constitutions for Multi-Agent Simulation", "comment": "23 pages, 4 figures", "summary": "Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles (\"be helpful, harmless, honest\") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\"\u5baa\u6cd5\u6f14\u5316\"\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u81ea\u52a8\u53d1\u73b0\u884c\u4e3a\u89c4\u8303\uff0c\u901a\u8fc7\u9057\u4f20\u7f16\u7a0b\u6f14\u5316\u51fa\u7684\u5baa\u6cd5\u6bd4\u4eba\u5de5\u8bbe\u8ba1\u7684\u57fa\u51c6\u8868\u73b0\u66f4\u597d\uff0c\u53d1\u73b0\u51cf\u5c11\u901a\u4fe1\u6bd4\u5197\u957f\u534f\u8c03\u66f4\u6709\u6548\u3002", "motivation": "\u4f20\u7edf\u5baa\u6cd5AI\u4e13\u6ce8\u4e8e\u4f7f\u7528\u56fa\u5b9a\u539f\u5219\u8fdb\u884c\u5355\u6a21\u578b\u5bf9\u9f50\uff0c\u4f46\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u8fc7\u6d8c\u73b0\u7684\u793e\u4f1a\u52a8\u6001\u521b\u9020\u4e86\u65b0\u7684\u5bf9\u9f50\u6311\u6218\u3002\u9700\u8981\u7814\u7a76\u5982\u4f55\u5728\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u81ea\u52a8\u53d1\u73b0\u884c\u4e3a\u89c4\u8303\uff0c\u89e3\u51b3\u4e2a\u4f53\u4e0e\u96c6\u4f53\u798f\u5229\u4e4b\u95f4\u7684\u5f20\u529b\u3002", "method": "\u4f7f\u7528\u7f51\u683c\u4e16\u754c\u6a21\u62df\u73af\u5883\u65bd\u52a0\u751f\u5b58\u538b\u529b\uff0c\u7814\u7a76\u4e2a\u4f53\u4e0e\u96c6\u4f53\u798f\u5229\u7684\u5f20\u529b\u3002\u901a\u8fc7LLM\u9a71\u52a8\u7684\u9057\u4f20\u7f16\u7a0b\u8fdb\u884c\u591a\u5c9b\u5c7f\u6f14\u5316\uff0c\u6f14\u5316\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u7684\u5baa\u6cd5\uff0c\u65e0\u9700\u660e\u786e\u6307\u5bfc\u5408\u4f5c\u3002\u91cf\u5316\u6307\u6807\u91c7\u7528\u793e\u4f1a\u7a33\u5b9a\u6027\u5206\u6570S\u2208[0,1]\uff0c\u7ed3\u5408\u751f\u4ea7\u529b\u3001\u751f\u5b58\u548c\u51b2\u7a81\u6307\u6807\u3002", "result": "\u6f14\u5316\u51fa\u7684\u6700\u4f18\u5baa\u6cd5C*\u8fbe\u5230S=0.556\u00b10.008\uff0c\u6bd4\u4eba\u5de5\u8bbe\u8ba1\u7684\u57fa\u51c6\u9ad8123%\uff0c\u5b8c\u5168\u6d88\u9664\u51b2\u7a81\u3002\u53d1\u73b0\u6700\u5c0f\u5316\u901a\u4fe1\uff080.9% vs 62.2%\u7684\u793e\u4f1a\u884c\u4e3a\uff09\u6bd4\u5197\u957f\u534f\u8c03\u8868\u73b0\u66f4\u597d\u3002\u5bf9\u6297\u6027\u5baa\u6cd5\u5bfc\u81f4\u793e\u4f1a\u5d29\u6e83\uff08S=0\uff09\uff0c\u6a21\u7cca\u7684\u4eb2\u793e\u4f1a\u539f\u5219\u4ea7\u751f\u4e0d\u4e00\u81f4\u534f\u8c03\uff08S=0.249\uff09\uff0cClaude 4.5 Opus\u8bbe\u8ba1\u7684\u5baa\u6cd5\u4ec5\u8fbe\u4e2d\u7b49\u6027\u80fd\uff08S=0.332\uff09\u3002", "conclusion": "\u5408\u4f5c\u89c4\u8303\u53ef\u4ee5\u901a\u8fc7\u6f14\u5316\u81ea\u52a8\u53d1\u73b0\u800c\u975e\u4eba\u5de5\u89c4\u5b9a\uff0c\u51cf\u5c11\u901a\u4fe1\u6bd4\u5197\u957f\u534f\u8c03\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u66f4\u6709\u6548\u3002\u8fd9\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6f14\u5316\u65b9\u6cd5\u5728\u53d1\u73b0\u590d\u6742\u793e\u4f1a\u89c4\u8303\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.00563", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.00563", "abs": "https://arxiv.org/abs/2602.00563", "authors": ["Yuhui Lai", "Shixun Huang", "Sheng Wang"], "title": "Updatable Balanced Index for Stable Streaming Similarity Search over Large-Scale Fresh Vectors", "comment": "Accepted for publication in the 13th IEEE International Conference on Big Data (BigData 2025). To appear", "summary": "As artificial intelligence gains more and more popularity, vectors are one of the most widely used data structures for services such as information retrieval and recommendation. Approximate Nearest Neighbor Search (ANNS), which generally relies on indices optimized for fast search to organize large datasets, has played a core role in these popular services. As the frequency of data shift grows, it is crucial for indices to accommodate new data and support real-time updates. Existing researches adopting two different approaches hold the following drawbacks: 1) approaches using additional buffers to temporarily store new data are resource-intensive and inefficient due to the global rebuilding processes; 2) approaches upgrading the internal index structure suffer from performance degradation because of update congestion and imbalanced distribution in streaming workloads. In this paper, we propose UBIS, an Updatable Balanced Index for stable streaming similarity Search, to resolve conflicts by scheduling concurrent updates and maintain good index quality by reducing imbalanced update cases, when the update frequency grows. Experimental results in the real-world datasets demonstrate that UBIS achieves up to 77% higher search accuracy and 45% higher update throughput on average compared to the state-of-the-art indices in streaming workloads.", "AI": {"tldr": "UBIS\u662f\u4e00\u79cd\u53ef\u66f4\u65b0\u7684\u5e73\u8861\u7d22\u5f15\uff0c\u7528\u4e8e\u5904\u7406\u6d41\u5f0f\u76f8\u4f3c\u6027\u641c\u7d22\u4e2d\u7684\u5e76\u53d1\u66f4\u65b0\u548c\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u7cbe\u5ea6\u548c\u66f4\u65b0\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740AI\u670d\u52a1\u7684\u666e\u53ca\uff0c\u5411\u91cf\u6570\u636e\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\u3002\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08ANNS\uff09\u4f9d\u8d56\u7d22\u5f15\u6765\u7ec4\u7ec7\u5927\u6570\u636e\u96c6\uff0c\u4f46\u5728\u6570\u636e\u9891\u7e41\u66f4\u65b0\u7684\u6d41\u5f0f\u573a\u666f\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f7f\u7528\u989d\u5916\u7f13\u51b2\u533a\u7684\u65b9\u6cd5\u56e0\u5168\u5c40\u91cd\u5efa\u8fc7\u7a0b\u800c\u8d44\u6e90\u5bc6\u96c6\u4e14\u4f4e\u6548\uff1b2\uff09\u5347\u7ea7\u5185\u90e8\u7d22\u5f15\u7ed3\u6784\u7684\u65b9\u6cd5\u56e0\u66f4\u65b0\u62e5\u585e\u548c\u6d41\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u4e0d\u5e73\u8861\u5206\u5e03\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faUBIS\uff08\u53ef\u66f4\u65b0\u7684\u5e73\u8861\u7d22\u5f15\uff09\uff0c\u901a\u8fc7\u8c03\u5ea6\u5e76\u53d1\u66f4\u65b0\u6765\u89e3\u51b3\u51b2\u7a81\uff0c\u5e76\u901a\u8fc7\u51cf\u5c11\u4e0d\u5e73\u8861\u66f4\u65b0\u60c5\u51b5\u6765\u7ef4\u6301\u826f\u597d\u7684\u7d22\u5f15\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u66f4\u65b0\u9891\u7387\u589e\u957f\u7684\u60c5\u51b5\u8bbe\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUBIS\u5728\u6d41\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u7d22\u5f15\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe77%\u7684\u641c\u7d22\u7cbe\u5ea6\u63d0\u5347\u548c\u5e73\u574745%\u7684\u66f4\u65b0\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "UBIS\u901a\u8fc7\u6709\u6548\u5904\u7406\u5e76\u53d1\u66f4\u65b0\u548c\u51cf\u5c11\u4e0d\u5e73\u8861\u5206\u5e03\uff0c\u4e3a\u6d41\u5f0f\u76f8\u4f3c\u6027\u641c\u7d22\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u53ef\u66f4\u65b0\u7d22\u5f15\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.00134", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2602.00134", "abs": "https://arxiv.org/abs/2602.00134", "authors": ["Ioannis Tsiokos"], "title": "Six Birds: Foundations of Emergence Calculus", "comment": null, "summary": "We develop a discipline-agnostic emergence calculus that treats theories as fixed points of idempotent operators acting on descriptions. We show that, once processes are composable but access to the underlying system is mediated by a bounded observational interface, a canonical toolkit of six closure-changing primitives (P1--P6) is unavoidable. The framework unifies order-theoretic closure operators with dynamics-induced endomaps $E_{\u03c4,f}$ built from a Markov kernel, a coarse-graining lens, and a time scale $\u03c4$. We introduce a computable total-variation idempotence defect for $E_{\u03c4,f}$; small retention error implies approximate idempotence and yields stable \"objects\" packaged at the chosen $\u03c4$ within a fixed lens. For directionality, we define an arrow-of-time functional as the path-space KL divergence between forward and time-reversed trajectories and prove it is monotone under coarse-graining (data processing); we also formalize a protocol-trap audit showing that protocol holonomy alone cannot sustain asymmetry without a genuine affinity in the lifted dynamics. Finally, we prove a finite forcing-style counting lemma: relative to a partition-based theory, definable predicate extensions are exponentially rare, giving a clean anti-saturation mechanism for strict ladder climbing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u5b66\u79d1\u7684\u6d8c\u73b0\u6f14\u7b97\u6846\u67b6\uff0c\u5c06\u7406\u8bba\u89c6\u4e3a\u4f5c\u7528\u4e8e\u63cf\u8ff0\u4e0a\u7684\u5e42\u7b49\u7b97\u5b50\u7684\u4e0d\u52a8\u70b9\u3002\u901a\u8fc7\u6709\u754c\u89c2\u6d4b\u63a5\u53e3\u548c\u53ef\u7ec4\u5408\u8fc7\u7a0b\uff0c\u63a8\u5bfc\u51fa\u516d\u4e2a\u4e0d\u53ef\u907f\u514d\u7684\u95ed\u5305\u53d8\u5316\u539f\u8bed\uff0c\u7edf\u4e00\u4e86\u5e8f\u8bba\u95ed\u5305\u7b97\u5b50\u548c\u52a8\u529b\u5b66\u8bf1\u5bfc\u7684\u5185\u6620\u5c04\u3002", "motivation": "\u5efa\u7acb\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u6d8c\u73b0\u73b0\u8c61\uff0c\u7279\u522b\u662f\u7406\u89e3\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u548c\u89c2\u6d4b\u7c92\u5ea6\u4e0b\"\u5bf9\u8c61\"\u5982\u4f55\u7a33\u5b9a\u51fa\u73b0\uff0c\u4ee5\u53ca\u65f6\u95f4\u65b9\u5411\u6027\u5982\u4f55\u4ece\u7c97\u7c92\u5316\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u3002", "method": "1. \u5c06\u7406\u8bba\u5efa\u6a21\u4e3a\u5e42\u7b49\u7b97\u5b50\u7684\u4e0d\u52a8\u70b9\uff1b2. \u5f15\u5165\u516d\u4e2a\u95ed\u5305\u53d8\u5316\u539f\u8bed(P1-P6)\uff1b3. \u6784\u5efa\u52a8\u529b\u5b66\u8bf1\u5bfc\u7684\u5185\u6620\u5c04E_{\u03c4,f}\uff0c\u5305\u542b\u9a6c\u5c14\u53ef\u592b\u6838\u3001\u7c97\u7c92\u5316\u900f\u955c\u548c\u65f6\u95f4\u5c3a\u5ea6\u03c4\uff1b4. \u5b9a\u4e49\u603b\u53d8\u5dee\u5e42\u7b49\u7f3a\u9677\u4f5c\u4e3a\u53ef\u8ba1\u7b97\u5ea6\u91cf\uff1b5. \u5f15\u5165\u65f6\u95f4\u7bad\u5934\u6cdb\u51fd\u4f5c\u4e3a\u524d\u5411\u4e0e\u65f6\u95f4\u53cd\u8f6c\u8f68\u8ff9\u7684KL\u6563\u5ea6\uff1b6. \u8bc1\u660e\u6709\u9650\u5f3a\u5236\u8ba1\u6570\u5f15\u7406\u3002", "result": "1. \u5c0f\u4fdd\u7559\u8bef\u5dee\u610f\u5473\u7740\u8fd1\u4f3c\u5e42\u7b49\u6027\uff0c\u5728\u9009\u5b9a\u03c4\u548c\u56fa\u5b9a\u900f\u955c\u5185\u4ea7\u751f\u7a33\u5b9a\"\u5bf9\u8c61\"\uff1b2. \u65f6\u95f4\u7bad\u5934\u6cdb\u51fd\u5728\u7c97\u7c92\u5316\u4e0b\u5355\u8c03\uff08\u6570\u636e\u5904\u7406\u4e0d\u7b49\u5f0f\uff09\uff1b3. \u534f\u8bae\u9677\u9631\u5ba1\u8ba1\u8868\u660e\u4ec5\u534f\u8bae\u5168\u7eaf\u65e0\u6cd5\u7ef4\u6301\u4e0d\u5bf9\u79f0\u6027\uff1b4. \u76f8\u5bf9\u4e8e\u5212\u5206\u7406\u8bba\uff0c\u53ef\u5b9a\u4e49\u8c13\u8bcd\u6269\u5c55\u5448\u6307\u6570\u7ea7\u7a00\u6709\uff0c\u4e3a\u4e25\u683c\u9636\u68af\u722c\u5347\u63d0\u4f9b\u53cd\u9971\u548c\u673a\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6d8c\u73b0\u73b0\u8c61\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u95ed\u5305\u53d8\u5316\u539f\u8bed\u7684\u5fc5\u7136\u6027\u3001\u65f6\u95f4\u65b9\u5411\u6027\u4ece\u7c97\u7c92\u5316\u4e2d\u4ea7\u751f\u7684\u673a\u5236\uff0c\u4ee5\u53ca\u7406\u8bba\u6269\u5c55\u7684\u6307\u6570\u7a00\u6709\u6027\uff0c\u4e3a\u7406\u89e3\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u65f6\u95f4\u4e0d\u5bf9\u79f0\u6027\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u5de5\u5177\u3002"}}
{"id": "2602.00018", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00018", "abs": "https://arxiv.org/abs/2602.00018", "authors": ["Conrad Borchers", "Hannah Deininger", "Zachary A. Pardos"], "title": "Toward Trait-Aware Learning Analytics", "comment": "Full research paper accepted for publication in the Learning Analytics and Knowledge (LAK) 2026 conference proceedings", "summary": "Learning analytics (LA) draws from the learning sciences to interpret learner behavior and inform system design. Yet, past personalization remains largely at the content or performance level (during learner-system interactions), overlooking relatively stable individual differences such as personality (unfolding over long-term learning trajectories such as college degrees). The latter could bring underappreciated benefits to the design, implementation, and impact of LA. In this position paper, we conduct an ad hoc literature review and argue for an expanded framing of LA that centers on learner traits as key to both interpreting and designing close-the-loop experiments in LA. We show that personality traits are relevant to LA's central outcomes (e.g., engagement and achievement) and conducive to action, as their established ties to human-computer interaction (HCI) inform how systems time, frame, and personalize support. Drawing inspiration from HCI, where psychometrics inform personalization strategies, we propose that LA can evolve by treating traits not only as predictive features but as design resources and moderators of analytics efficacy. In line with past position papers published at LAK, we present a research agenda grounded in the LA cycle and discuss methodological and ethical challenges.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u4e3b\u5f20\u5c06\u5b66\u4e60\u8005\u7279\u8d28\uff08\u5982\u4eba\u683c\uff09\u4f5c\u4e3a\u5b66\u4e60\u5206\u6790\u7684\u6838\u5fc3\u8bbe\u8ba1\u8d44\u6e90\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u5185\u5bb9\u548c\u8868\u73b0\u5c42\u9762\u4e2a\u6027\u5316\uff0c\u4ee5\u63d0\u5347\u5b66\u4e60\u5206\u6790\u7684\u957f\u671f\u6548\u679c\u548c\u4f26\u7406\u8003\u91cf\u3002", "motivation": "\u5f53\u524d\u5b66\u4e60\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u5b66\u4e60\u8005\u4e0e\u7cfb\u7edf\u4ea4\u4e92\u4e2d\u7684\u5185\u5bb9\u548c\u8868\u73b0\u5c42\u9762\u4e2a\u6027\u5316\uff0c\u5ffd\u89c6\u4e86\u76f8\u5bf9\u7a33\u5b9a\u7684\u4e2a\u4f53\u5dee\u5f02\uff08\u5982\u4eba\u683c\u7279\u8d28\uff09\uff0c\u8fd9\u4e9b\u7279\u8d28\u5728\u957f\u671f\u5b66\u4e60\u8f68\u8ff9\uff08\u5982\u5927\u5b66\u5b66\u4f4d\uff09\u4e2d\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002\u4f5c\u8005\u8ba4\u4e3a\u5c06\u5b66\u4e60\u8005\u7279\u8d28\u7eb3\u5165\u5b66\u4e60\u5206\u6790\u6846\u67b6\u53ef\u4ee5\u5e26\u6765\u88ab\u4f4e\u4f30\u7684\u8bbe\u8ba1\u3001\u5b9e\u65bd\u548c\u5f71\u54cd\u6548\u76ca\u3002", "method": "\u91c7\u7528\u7279\u8bbe\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u5fc3\u7406\u5b66\u6d4b\u91cf\u65b9\u6cd5\uff0c\u63d0\u51fa\u5c06\u5b66\u4e60\u8005\u7279\u8d28\u4f5c\u4e3a\u8bbe\u8ba1\u8d44\u6e90\u548c\u5206\u6790\u6548\u679c\u8c03\u8282\u56e0\u5b50\u7684\u6269\u5c55\u6846\u67b6\u3002\u57fa\u4e8e\u5b66\u4e60\u5206\u6790\u5468\u671f\u6784\u5efa\u7814\u7a76\u8bae\u7a0b\uff0c\u5e76\u8ba8\u8bba\u65b9\u6cd5\u8bba\u548c\u4f26\u7406\u6311\u6218\u3002", "result": "\u7814\u7a76\u8868\u660e\u4eba\u683c\u7279\u8d28\u4e0e\u5b66\u4e60\u5206\u6790\u7684\u6838\u5fc3\u7ed3\u679c\uff08\u5982\u53c2\u4e0e\u5ea6\u548c\u6210\u5c31\uff09\u76f8\u5173\uff0c\u4e14\u4e0e\u4eba\u673a\u4ea4\u4e92\u7684\u5173\u8054\u6027\u4e3a\u7cfb\u7edf\u652f\u6301\u7684\u65f6\u95f4\u5b89\u6392\u3001\u6846\u67b6\u8bbe\u8ba1\u548c\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002\u4eba\u683c\u7279\u8d28\u4e0d\u4ec5\u662f\u9884\u6d4b\u7279\u5f81\uff0c\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u8bbe\u8ba1\u8d44\u6e90\u548c\u5206\u6790\u6548\u679c\u7684\u8c03\u8282\u56e0\u5b50\u3002", "conclusion": "\u5b66\u4e60\u5206\u6790\u5e94\u901a\u8fc7\u5c06\u5b66\u4e60\u8005\u7279\u8d28\u4f5c\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u5143\u7d20\u6765\u53d1\u5c55\uff0c\u8fd9\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u4e2a\u6027\u5316\u6548\u679c\uff0c\u8fd8\u80fd\u66f4\u597d\u5730\u7406\u89e3\u5206\u6790\u5e72\u9884\u7684\u8fb9\u754c\u6761\u4ef6\u3002\u9700\u8981\u5efa\u7acb\u57fa\u4e8e\u5b66\u4e60\u5206\u6790\u5468\u671f\u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u5e76\u89e3\u51b3\u76f8\u5173\u7684\u4f26\u7406\u548c\u65b9\u6cd5\u8bba\u6311\u6218\u3002"}}
{"id": "2602.00222", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00222", "abs": "https://arxiv.org/abs/2602.00222", "authors": ["Guoxin Lian", "Shuo Wang", "Yucheng Wang", "Yongcai Wang", "Maiyue Chen", "Kaihui Wang", "Bo Zhang", "Zhizhong Su", "Deying Li", "Zhaoxin Fan"], "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation", "comment": null, "summary": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.", "AI": {"tldr": "MapDream\uff1a\u901a\u8fc7\u81ea\u56de\u5f52BEV\u56fe\u50cf\u5408\u6210\u5b66\u4e60\u4efb\u52a1\u9a71\u52a8\u7684\u7d27\u51d1\u5730\u56fe\u8868\u793a\uff0c\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u5730\u56fe\u6784\u5efa\u4e0e\u52a8\u4f5c\u9884\u6d4b\u8054\u5408\u4f18\u5316", "motivation": "\u73b0\u6709VLN\u65b9\u6cd5\u4f9d\u8d56\u4e0e\u5bfc\u822a\u7b56\u7565\u72ec\u7acb\u7684\u624b\u5de5\u6784\u5efa\u5730\u56fe\uff0c\u4f5c\u8005\u8ba4\u4e3a\u5730\u56fe\u5e94\u8be5\u662f\u76f4\u63a5\u7531\u5bfc\u822a\u76ee\u6807\u5851\u9020\u7684\u5b66\u4e60\u8868\u793a\uff0c\u800c\u975e\u8be6\u5c3d\u91cd\u5efa", "method": "\u63d0\u51faMapDream\u6846\u67b6\uff0c\u5c06\u5730\u56fe\u6784\u5efa\u516c\u5f0f\u5316\u4e3a\u81ea\u56de\u5f52\u9e1f\u77b0\u56fe\u56fe\u50cf\u5408\u6210\uff0c\u8054\u5408\u5b66\u4e60\u5730\u56fe\u751f\u6210\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u5c06\u73af\u5883\u4e0a\u4e0b\u6587\u84b8\u998f\u4e3a\u7d27\u51d1\u7684\u4e09\u901a\u9053BEV\u5730\u56fe\uff1b\u901a\u8fc7\u76d1\u7763\u9884\u8bad\u7ec3\u5f15\u5bfc\u6620\u5c04\u5230\u63a7\u5236\u63a5\u53e3\uff0c\u81ea\u56de\u5f52\u8bbe\u8ba1\u652f\u6301\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u8fdb\u884c\u7aef\u5230\u7aef\u8054\u5408\u4f18\u5316", "result": "\u5728R2R-CE\u548cRxR-CE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5355\u76ee\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4efb\u52a1\u9a71\u52a8\u7684\u751f\u6210\u5f0f\u5730\u56fe\u5b66\u4e60\u7684\u6709\u6548\u6027", "conclusion": "\u5730\u56fe\u5e94\u8be5\u4f5c\u4e3a\u76f4\u63a5\u7531\u5bfc\u822a\u76ee\u6807\u5851\u9020\u7684\u5b66\u4e60\u8868\u793a\uff0c\u800c\u975e\u72ec\u7acb\u6784\u5efa\u7684\u8be6\u5c3d\u91cd\u5efa\uff1bMapDream\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u7684\u751f\u6210\u5f0f\u5730\u56fe\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5bfc\u822a\u6027\u80fd"}}
{"id": "2602.00066", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00066", "abs": "https://arxiv.org/abs/2602.00066", "authors": ["Zheng Fang", "Yihong Dong", "Lili Mou", "Dongming Jin", "Zhi Jin", "Ge Li"], "title": "IntentCoding: Amplifying User Intent in Code Generation", "comment": null, "summary": "Large Language Models (LLMs) have shown strong capabilities in code generation, but their adherence to fine-grained user intent with multiple constraints remains a significant challenge. Our empirical analysis reveals two key observations: 1) Model performance deteriorates quickly as the number of constraints in the user intent increases, and 2) While user intent does influence the model's logits, such an influence may not be strong enough to effectively steer the decoding process. To this end, we propose Intent-Amplified Code Generation (IntentCoding), a novel decoding strategy that enhances an LLM's ability to follow user intent. IntentCoding captures the influence of user intent by masking out the intent, and applies a multi-strength ensemble mechanism to amplify the effect of user intent during generation. IntentCoding is model-agnostic, requires no additional training, and integrates seamlessly with existing decoding procedures. To enable systematic evaluation, we also construct CodeConstraints, a benchmark dataset specifically designed to test user intent compliance under varying numbers of constraints. Experiments on our constructed Constraints, as well as popular IFEvalCode, HumanEval and LiveCodeBench datasets, show that our IntentCoding model significantly improves both constraint satisfaction and functional correctness compared to standard decoding approaches. IntentCoding achieves up to 71.0% relative improvement on CodeConstraints, achieves up to 67.3% relative improvement on IFEvalCode and achieves up to 29.3% relative improvement in pass@1 on HumanEval and LiveCodeBench compared with greedy decoding.", "AI": {"tldr": "IntentCoding\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u63a9\u7801\u7528\u6237\u610f\u56fe\u548c\u591a\u5f3a\u5ea6\u96c6\u6210\u673a\u5236\u6765\u589e\u5f3aLLM\u9075\u5faa\u7528\u6237\u610f\u56fe\u7684\u80fd\u529b\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u7ea6\u675f\u6ee1\u8db3\u548c\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u96be\u4ee5\u9075\u5faa\u5305\u542b\u591a\u4e2a\u7ea6\u675f\u7684\u7ec6\u7c92\u5ea6\u7528\u6237\u610f\u56fe\uff0c\u6027\u80fd\u968f\u7ea6\u675f\u6570\u91cf\u589e\u52a0\u800c\u5feb\u901f\u4e0b\u964d\uff0c\u4e14\u7528\u6237\u610f\u56fe\u5bf9\u6a21\u578blogits\u7684\u5f71\u54cd\u4e0d\u8db3\u4ee5\u6709\u6548\u5f15\u5bfc\u89e3\u7801\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faIntentCoding\u89e3\u7801\u7b56\u7565\uff1a1) \u901a\u8fc7\u63a9\u7801\u7528\u6237\u610f\u56fe\u6765\u6355\u6349\u5176\u5f71\u54cd\uff1b2) \u5e94\u7528\u591a\u5f3a\u5ea6\u96c6\u6210\u673a\u5236\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u653e\u5927\u7528\u6237\u610f\u56fe\u7684\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u4e0e\u73b0\u6709\u89e3\u7801\u8fc7\u7a0b\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728CodeConstraints\u3001IFEvalCode\u3001HumanEval\u548cLiveCodeBench\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIntentCoding\u76f8\u6bd4\u6807\u51c6\u89e3\u7801\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u7ea6\u675f\u6ee1\u8db3\u548c\u529f\u80fd\u6b63\u786e\u6027\uff1a\u5728CodeConstraints\u4e0a\u76f8\u5bf9\u63d0\u534771.0%\uff0cIFEvalCode\u4e0a67.3%\uff0cHumanEval\u548cLiveCodeBench\u7684pass@1\u6307\u6807\u4e0a\u63d0\u534729.3%\u3002", "conclusion": "IntentCoding\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u89e3\u7801\u7b56\u7565\uff0c\u80fd\u6709\u6548\u589e\u5f3aLLM\u9075\u5faa\u7528\u6237\u610f\u56fe\u7684\u80fd\u529b\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u7ea6\u675f\u6ee1\u8db3\u548c\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4e3a\u89e3\u51b3LLM\u5728\u7ec6\u7c92\u5ea6\u610f\u56fe\u9075\u5faa\u65b9\u9762\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.01701", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01701", "abs": "https://arxiv.org/abs/2602.01701", "authors": ["Ruyu Li", "Tinghui Zhang", "Haodi Ma", "Daisy Zhe Wang", "Yifan Wang"], "title": "Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems", "comment": null, "summary": "With the increasingly use of multi-modal data, semantic query has become more and more demanded in data management systems, which is an important way to access and analyze multi-modal data. As unstructured data, most information of multi-modal data (text, image, video, etc) hides in the semantics, which cannot be accessed by the traditional database queries like SQL.\n  Given the power of Large Language Model (LLM) in understanding semantics and processing natural language, in recent years several LLM-based semantic query systems have been proposed, to support semantic querying over unstructured data. However, this rapid growth has produced a fragmented ecosystem. Applications face significant integration challenges due to (1) disparate APIs of different semantic query systems and (2) a fundamental trade-off between specialization and generality. Many semantic query systems are highly specialized, offering state-of-the-art performance within a single modality but struggling with multi-modal data. Conversely, some \"all-in-one\" systems handle multiple modalities but often exhibit suboptimal performance compared to their specialized counterparts in specific modalities.\n  This paper introduces Meta Engine, a novel \"query system on query systems\", designed to resolve those aforementioned challenges. Meta Engine is a unified semantic query engine that integrates heterogeneous, specialized LLM-based query systems. Its architecture comprises five key components: (1) a Natural Language (NL) Query Parser, (2) an Operator Generator, (3) a Query Router, (4) a set of Adapters, and (5) a Result Aggregator. In the evaluation, Meta Engine consistently outperforms all baselines, yielding 3-6x higher F1 in most cases and up to 24x on specific datasets.", "AI": {"tldr": "Meta Engine\u662f\u4e00\u4e2a\"\u67e5\u8be2\u7cfb\u7edf\u4e4b\u4e0a\u7684\u67e5\u8be2\u7cfb\u7edf\"\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u96c6\u6210\u5f02\u6784\u7684\u4e13\u7528LLM\u67e5\u8be2\u7cfb\u7edf\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u8bed\u4e49\u67e5\u8be2\u4e2d\u7684API\u788e\u7247\u5316\u548c\u4e13\u4e1a\u5316\u4e0e\u901a\u7528\u6027\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u6570\u636e\u4f7f\u7528\u7684\u589e\u52a0\uff0c\u8bed\u4e49\u67e5\u8be2\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709LLM\u67e5\u8be2\u7cfb\u7edf\u5b58\u5728API\u788e\u7247\u5316\u95ee\u9898\uff0c\u4ee5\u53ca\u4e13\u7528\u7cfb\u7edf\uff08\u5355\u6a21\u6001\u6027\u80fd\u4f18\u4f46\u591a\u6a21\u6001\u80fd\u529b\u5f31\uff09\u4e0e\u901a\u7528\u7cfb\u7edf\uff08\u591a\u6a21\u6001\u652f\u6301\u4f46\u5355\u6a21\u6001\u6027\u80fd\u5dee\uff09\u4e4b\u95f4\u7684\u6839\u672c\u6027\u6743\u8861\u6311\u6218\u3002", "method": "\u63d0\u51faMeta Engine\u7edf\u4e00\u8bed\u4e49\u67e5\u8be2\u5f15\u64ce\uff0c\u5305\u542b\u4e94\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u89e3\u6790\u5668\u3001\u64cd\u4f5c\u7b26\u751f\u6210\u5668\u3001\u67e5\u8be2\u8def\u7531\u5668\u3001\u9002\u914d\u5668\u96c6\u5408\u548c\u7ed3\u679c\u805a\u5408\u5668\uff0c\u901a\u8fc7\u96c6\u6210\u5f02\u6784\u4e13\u7528LLM\u67e5\u8be2\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u591a\u6a21\u6001\u8bed\u4e49\u67e5\u8be2\u3002", "result": "Meta Engine\u5728\u8bc4\u4f30\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u83b7\u5f973-6\u500d\u7684F1\u5206\u6570\u63d0\u5347\uff0c\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u751a\u81f3\u8fbe\u523024\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Meta Engine\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86LLM\u8bed\u4e49\u67e5\u8be2\u7cfb\u7edf\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4e13\u7528\u7cfb\u7edf\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u67e5\u8be2\u7684\u901a\u7528\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bed\u4e49\u67e5\u8be2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00098", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2602.00098", "abs": "https://arxiv.org/abs/2602.00098", "authors": ["Oliver Preu\u00df", "Jeroen Rook", "Jakob Bossek", "Heike Trautmann"], "title": "MO-ELA: Rigorously Expanding Exploratory Landscape Features for Automated Algorithm Selection in Continuous Multi-Objective Optimisation", "comment": null, "summary": "Automated Algorithm Selection (AAS) is a popular meta-algorithmic approach and has demonstrated to work well for single-objective optimisation in combination with exploratory landscape features (ELA), i.e., (numerical) descriptive features derived from sampling the black-box (continuous) optimisation problem. In contrast to the abundance of features that describe single-objective optimisation problems, only a few features have been proposed for multi-objective optimisation so far. Building upon recent work on exploratory landscape features for box-constrained continuous multi-objective optimization problems, we propose a novel and complementary set of additional features (MO-ELA). These features are based on a random sample of points considering both the decision and objective space. The features are divided into 5 feature groups depending on how they are being calculated: non-dominated-sorting, descriptive statistics, principal component analysis, graph structures and gradient information. An AAS study conducted on well-established multi-objective benchmarks demonstrates that the proposed features contribute to successfully distinguishing between algorithm performance and thus adequately capture problem hardness resulting in models that come very close to the virtual best solver. After feature selection, the newly proposed features are frequently among the top contributors, underscoring their value in algorithm selection and problem characterisation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u65b0\u7684\u591a\u76ee\u6807\u4f18\u5316\u63a2\u7d22\u6027\u666f\u89c2\u7279\u5f81\uff08MO-ELA\uff09\uff0c\u901a\u8fc7\u7b97\u6cd5\u9009\u62e9\u7814\u7a76\u8bc1\u660e\u8fd9\u4e9b\u7279\u5f81\u80fd\u6709\u6548\u533a\u5206\u7b97\u6cd5\u6027\u80fd\u5e76\u63a5\u8fd1\u865a\u62df\u6700\u4f73\u6c42\u89e3\u5668\u3002", "motivation": "\u5355\u76ee\u6807\u4f18\u5316\u5df2\u6709\u4e30\u5bcc\u7684\u63a2\u7d22\u6027\u666f\u89c2\u7279\u5f81\uff08ELA\uff09\uff0c\u4f46\u591a\u76ee\u6807\u4f18\u5316\u7684\u7279\u5f81\u76f8\u5bf9\u532e\u4e4f\u3002\u73b0\u6709\u7279\u5f81\u4e0d\u8db3\u4ee5\u5145\u5206\u63cf\u8ff0\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u7279\u5f81\u96c6\u6765\u652f\u6301\u7b97\u6cd5\u9009\u62e9\u548c\u95ee\u9898\u8868\u5f81\u3002", "method": "\u57fa\u4e8e\u968f\u673a\u91c7\u6837\u70b9\uff08\u8003\u8651\u51b3\u7b56\u7a7a\u95f4\u548c\u76ee\u6807\u7a7a\u95f4\uff09\u6784\u5efa\u65b0\u7684MO-ELA\u7279\u5f81\u96c6\uff0c\u5206\u4e3a5\u4e2a\u7279\u5f81\u7ec4\uff1a\u975e\u652f\u914d\u6392\u5e8f\u3001\u63cf\u8ff0\u6027\u7edf\u8ba1\u3001\u4e3b\u6210\u5206\u5206\u6790\u3001\u56fe\u7ed3\u6784\u548c\u68af\u5ea6\u4fe1\u606f\u3002\u5728\u6807\u51c6\u591a\u76ee\u6807\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u81ea\u52a8\u5316\u7b97\u6cd5\u9009\u62e9\u7814\u7a76\u3002", "result": "\u63d0\u51fa\u7684\u7279\u5f81\u80fd\u6210\u529f\u533a\u5206\u7b97\u6cd5\u6027\u80fd\uff0c\u5145\u5206\u6355\u6349\u95ee\u9898\u96be\u5ea6\uff0c\u6784\u5efa\u7684\u6a21\u578b\u63a5\u8fd1\u865a\u62df\u6700\u4f73\u6c42\u89e3\u5668\u3002\u7279\u5f81\u9009\u62e9\u540e\uff0c\u65b0\u7279\u5f81\u7ecf\u5e38\u6210\u4e3a\u4e3b\u8981\u8d21\u732e\u8005\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7b97\u6cd5\u9009\u62e9\u548c\u95ee\u9898\u8868\u5f81\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "MO-ELA\u7279\u5f81\u96c6\u662f\u5bf9\u73b0\u6709\u591a\u76ee\u6807\u4f18\u5316\u7279\u5f81\u7684\u91cd\u8981\u8865\u5145\uff0c\u80fd\u6709\u6548\u652f\u6301\u81ea\u52a8\u5316\u7b97\u6cd5\u9009\u62e9\u5e76\u63d0\u5347\u95ee\u9898\u8868\u5f81\u80fd\u529b\uff0c\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2602.01291", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2602.01291", "abs": "https://arxiv.org/abs/2602.01291", "authors": ["Bowen Yang", "Yi Yuan", "Chenyi Li", "Ziyu Wang", "Liangqi Li", "Bo Zhang", "Zhe Li", "Zaiwen Wen"], "title": "Construction-Verification: A Benchmark for Applied Mathematics in Lean 4", "comment": null, "summary": "Recent advances in large language models have demonstrated impressive capabilities in mathematical formalization. However, existing benchmarks focus on logical verification of declarative propositions, often neglecting the task of explicitly synthesizing solutions. This limitation is particularly acute in applied mathematics domains, where the goal is frequently to derive concrete values or executable algorithms rather than solely proving theorems. To address this, we introduce a Lean 4 framework that enforces a construction-verification workflow, compelling the agent to define explicit solutions before proving their correctness. We curate a comprehensive benchmark AMBER (Applied Mathematics BEnchmark for Reasoning) spanning core domains of applied mathematics, including convex analysis, optimization, numerical algebra, and high-dimensional probability. Aside from theorem proving, our benchmark features complex tasks such as evaluation, algorithm design, and representation transformation. Experiments reveal that current models face significant difficulties with these constructive tasks. Notably, we observe that general-purpose reasoning models consistently outperform specialized theorem provers. We attribute this to a degradation of instruction following capabilities in specialized models. Fine-tuning on proof corpora appears to induce ``tactical overfitting\", compromising the ability to adhere to complex constructive requirements, whereas general models retain the versatility needed for multi-task formal reasoning.", "AI": {"tldr": "\u63d0\u51faAMBER\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5e94\u7528\u6570\u5b66\u4e2d\u7684\u6784\u9020\u6027\u4efb\u52a1\u80fd\u529b\uff0c\u53d1\u73b0\u901a\u7528\u63a8\u7406\u6a21\u578b\u4f18\u4e8e\u4e13\u7528\u5b9a\u7406\u8bc1\u660e\u5668", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u903b\u8f91\u9a8c\u8bc1\u800c\u5ffd\u89c6\u6784\u9020\u6027\u89e3\u51b3\u65b9\u6848\u5408\u6210\uff0c\u8fd9\u5728\u5e94\u7528\u6570\u5b66\u9886\u57df\u5c24\u4e3a\u7a81\u51fa\uff0c\u56e0\u4e3a\u5e94\u7528\u6570\u5b66\u7684\u76ee\u6807\u901a\u5e38\u662f\u63a8\u5bfc\u5177\u4f53\u503c\u6216\u53ef\u6267\u884c\u7b97\u6cd5\u800c\u975e\u5355\u7eaf\u8bc1\u660e\u5b9a\u7406", "method": "\u5f15\u5165Lean 4\u6846\u67b6\u5f3a\u5236\u6267\u884c\u6784\u9020-\u9a8c\u8bc1\u5de5\u4f5c\u6d41\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u5148\u5b9a\u4e49\u663e\u5f0f\u89e3\u518d\u8bc1\u660e\u5176\u6b63\u786e\u6027\uff1b\u6784\u5efaAMBER\u57fa\u51c6\uff0c\u6db5\u76d6\u51f8\u5206\u6790\u3001\u4f18\u5316\u3001\u6570\u503c\u4ee3\u6570\u548c\u9ad8\u7ef4\u6982\u7387\u7b49\u5e94\u7528\u6570\u5b66\u6838\u5fc3\u9886\u57df", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u6784\u9020\u6027\u4efb\u52a1\u65f6\u9762\u4e34\u663e\u8457\u56f0\u96be\uff1b\u901a\u7528\u63a8\u7406\u6a21\u578b\u6301\u7eed\u4f18\u4e8e\u4e13\u7528\u5b9a\u7406\u8bc1\u660e\u5668\uff1b\u5fae\u8c03\u8bc1\u660e\u8bed\u6599\u5e93\u4f1a\u5bfc\u81f4\"\u6218\u672f\u8fc7\u62df\u5408\"\uff0c\u635f\u5bb3\u9075\u5faa\u590d\u6742\u6784\u9020\u8981\u6c42\u7684\u80fd\u529b", "conclusion": "\u5e94\u7528\u6570\u5b66\u9700\u8981\u4e13\u95e8\u7684\u6784\u9020\u6027\u4efb\u52a1\u8bc4\u4f30\u57fa\u51c6\uff1b\u901a\u7528\u6a21\u578b\u5728\u5f62\u5f0f\u63a8\u7406\u7684\u591a\u4efb\u52a1\u9700\u6c42\u4e2d\u4fdd\u6301\u66f4\u597d\u7684\u9002\u5e94\u6027\uff1b\u4e13\u7528\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0a\u5b58\u5728\u9000\u5316\u95ee\u9898"}}
{"id": "2602.00093", "categories": ["cs.HC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00093", "abs": "https://arxiv.org/abs/2602.00093", "authors": ["Anton Malinovskiy"], "title": "Counterfactual Invariant Envelopes for Financial UX: Safety-Lattice Feature-Flag Governance in Crypto-Enabled Streaming", "comment": null, "summary": "Feature flags are the primary mechanism for safely introducing financial capabilities in consumer applications. In crypto-enabled live streaming, however, naive rollouts can create non-obvious risk: users may be exposed to onramps without proper eligibility, external wallets without sufficient fraud controls, or advanced views that alter risk perception and behavior. This paper introduces a novel invention candidate, a Counterfactual Invariant Envelope governor that combines a safety lattice with causal measurement and a shadow cohort for risk estimation. We formalize rollout risk, define invariant constraints across feature combinations, and propose a controller that adapts exposure using leading abuse signals, compliance readiness, and revenue guardrails. We incorporate real-world adoption and fraud data for calibration, provide formulas for rollout safety, and include reproducible policy snippets. The results show that counterfactual, invariant-aware governance reduces risk spillover while preserving conversion and retention, offering a path to patentable governance logic for financial UX.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u52a0\u5bc6\u76f4\u64ad\u4e2d\u91d1\u878d\u529f\u80fd\u5b89\u5168\u53d1\u5e03\u7684Counterfactual Invariant Envelope\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u5b89\u5168\u683c\u3001\u56e0\u679c\u6d4b\u91cf\u548c\u5f71\u5b50\u961f\u5217\u8fdb\u884c\u98ce\u9669\u4f30\u8ba1\uff0c\u51cf\u5c11\u98ce\u9669\u6ea2\u51fa\u540c\u65f6\u4fdd\u6301\u8f6c\u5316\u7387", "motivation": "\u5728\u52a0\u5bc6\u76f4\u64ad\u5e94\u7528\u4e2d\uff0c\u4f20\u7edf\u7684\u529f\u80fd\u6807\u5fd7\u53d1\u5e03\u53ef\u80fd\u5e26\u6765\u975e\u663e\u6027\u98ce\u9669\uff1a\u7528\u6237\u53ef\u80fd\u5728\u4e0d\u5177\u5907\u9002\u5f53\u8d44\u683c\u7684\u60c5\u51b5\u4e0b\u66b4\u9732\u4e8e\u5165\u91d1\u901a\u9053\u3001\u7f3a\u4e4f\u8db3\u591f\u6b3a\u8bc8\u63a7\u5236\u7684\u5916\u90e8\u94b1\u5305\uff0c\u6216\u6539\u53d8\u98ce\u9669\u8ba4\u77e5\u548c\u884c\u4e3a\u7684\u9ad8\u7ea7\u89c6\u56fe\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u91d1\u878d\u529f\u80fd\u53d1\u5e03\u673a\u5236\u3002", "method": "\u63d0\u51faCounterfactual Invariant Envelope\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u5b89\u5168\u683c\u4e0e\u56e0\u679c\u6d4b\u91cf\u548c\u5f71\u5b50\u961f\u5217\u8fdb\u884c\u98ce\u9669\u4f30\u8ba1\u3002\u5f62\u5f0f\u5316\u53d1\u5e03\u98ce\u9669\uff0c\u5b9a\u4e49\u8de8\u529f\u80fd\u7ec4\u5408\u7684\u4e0d\u53d8\u7ea6\u675f\uff0c\u63d0\u51fa\u57fa\u4e8e\u6ee5\u7528\u4fe1\u53f7\u3001\u5408\u89c4\u51c6\u5907\u5ea6\u548c\u6536\u5165\u62a4\u680f\u7684\u81ea\u9002\u5e94\u66b4\u9732\u63a7\u5236\u5668\u3002\u4f7f\u7528\u771f\u5b9e\u91c7\u7528\u548c\u6b3a\u8bc8\u6570\u636e\u8fdb\u884c\u6821\u51c6\uff0c\u63d0\u4f9b\u53d1\u5e03\u5b89\u5168\u516c\u5f0f\u548c\u53ef\u590d\u73b0\u7684\u7b56\u7565\u7247\u6bb5\u3002", "result": "\u53cd\u4e8b\u5b9e\u4e0d\u53d8\u611f\u77e5\u6cbb\u7406\u51cf\u5c11\u4e86\u98ce\u9669\u6ea2\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f6c\u5316\u7387\u548c\u7559\u5b58\u7387\uff0c\u4e3a\u91d1\u878d\u7528\u6237\u4f53\u9a8c\u63d0\u4f9b\u4e86\u53ef\u4e13\u5229\u7684\u6cbb\u7406\u903b\u8f91\u8def\u5f84\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u91d1\u878d\u529f\u80fd\u5b89\u5168\u53d1\u5e03\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u4e0d\u53d8\u4fe1\u5c01\u63a7\u5236\u5668\u5b9e\u73b0\u4e86\u98ce\u9669\u63a7\u5236\u4e0e\u7528\u6237\u4f53\u9a8c\u7684\u5e73\u8861\uff0c\u4e3a\u52a0\u5bc6\u76f4\u64ad\u7b49\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u91d1\u878d\u529f\u80fd\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u4e13\u5229\u7684\u6cbb\u7406\u6846\u67b6\u3002"}}
{"id": "2602.00401", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00401", "abs": "https://arxiv.org/abs/2602.00401", "authors": ["Jean Pierre Sleiman", "He Li", "Alphonsus Adu-Bredu", "Robin Deits", "Arun Kumar", "Kevin Bergamin", "Mohak Bhardwaj", "Scott Biddlestone", "Nicola Burger", "Matthew A. Estrada", "Francesco Iacobelli", "Twan Koolen", "Alexander Lambert", "Erica Lin", "M. Eva Mungai", "Zach Nobles", "Shane Rozen-Levy", "Yuyao Shi", "Jiashun Wang", "Jakob Welner", "Fangzhou Yu", "Mike Zhang", "Alfred Rizzi", "Jessica Hodgins", "Sylvain Bertrand", "Yeuhi Abe", "Scott Kuindersma", "Farbod Farshidian"], "title": "ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control", "comment": null, "summary": "Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.", "AI": {"tldr": "ZEST\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u8fd0\u52a8\u6a21\u4eff\u6846\u67b6\uff0c\u80fd\u4ece\u591a\u79cd\u6570\u636e\u6e90\u8bad\u7ec3\u7b56\u7565\u5e76\u76f4\u63a5\u90e8\u7f72\u5230\u786c\u4ef6\uff0c\u65e0\u9700\u63a5\u89e6\u6807\u7b7e\u3001\u72b6\u6001\u4f30\u8ba1\u5668\u6216\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\uff0c\u5728Atlas\u3001G1\u548cSpot\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u52a8\u6001\u591a\u63a5\u89e6\u6280\u80fd\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5b9e\u73b0\u9c81\u68d2\u3001\u7c7b\u4eba\u5168\u8eab\u63a7\u5236\u7684\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u9488\u5bf9\u6bcf\u4e2a\u6280\u80fd\u7684\u5927\u91cf\u5de5\u7a0b\u8bbe\u8ba1\u548c\u8106\u5f31\u7684\u63a7\u5236\u5668\u8c03\u4f18\u8fc7\u7a0b\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u91c7\u6837\uff08\u4e13\u6ce8\u4e8e\u56f0\u96be\u8fd0\u52a8\u7247\u6bb5\uff09\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u8f85\u52a9\u529b\u77e9\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\uff0c\u4f7f\u7528\u4e2d\u7b49\u7a0b\u5ea6\u7684\u57df\u968f\u673a\u5316\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u7b56\u7565\u3002\u63d0\u4f9b\u4ece\u8fd1\u4f3c\u5206\u6790\u7535\u67a2\u503c\u9009\u62e9\u5173\u8282\u7ea7\u589e\u76ca\u7684\u65b9\u6cd5\u548c\u7cbe\u5236\u7684\u6267\u884c\u5668\u6a21\u578b\u3002", "result": "\u5728Atlas\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u4ece\u52a8\u4f5c\u6355\u6349\u5b66\u4e60\u4e86\u52a8\u6001\u591a\u63a5\u89e6\u6280\u80fd\uff08\u5982\u519b\u961f\u722c\u884c\u3001\u9739\u96f3\u821e\uff09\uff1b\u4ece\u89c6\u9891\u76f4\u63a5\u8fc1\u79fb\u5230Atlas\u548cUnitree G1\u7684\u821e\u8e48\u548c\u573a\u666f\u4ea4\u4e92\u6280\u80fd\uff08\u5982\u7bb1\u5b50\u6500\u722c\uff09\uff1b\u6269\u5c55\u5230Spot\u56db\u8db3\u673a\u5668\u4eba\u5b9e\u73b0\u6742\u6280\uff08\u5982\u8fde\u7eed\u540e\u7a7a\u7ffb\uff09\u3002", "conclusion": "ZEST\u5c55\u793a\u4e86\u8de8\u5f02\u6784\u6570\u636e\u6e90\u548c\u4e0d\u540c\u5f62\u6001\u673a\u5668\u4eba\u7684\u9c81\u68d2\u96f6\u6837\u672c\u90e8\u7f72\u80fd\u529b\uff0c\u5efa\u7acb\u4e86\u751f\u7269\u8fd0\u52a8\u4e0e\u673a\u5668\u4eba\u5bf9\u5e94\u7269\u4e4b\u95f4\u7684\u53ef\u6269\u5c55\u63a5\u53e3\u3002"}}
{"id": "2602.01188", "categories": ["cs.SC"], "pdf": "https://arxiv.org/pdf/2602.01188", "abs": "https://arxiv.org/abs/2602.01188", "authors": ["Shaoshi Chen", "Hanqian Fang", "Joris van der Hoeven"], "title": "A zero-test for D-algebraic transseries", "comment": null, "summary": "Consider formal power series $f_1,\\ldots, f_k\\in\\mathbb{Q}[[z]]$ that are defined as the solutions of a system of polynomial differential equations together with a sufficient number of initial conditions. Given $P\\in \\mathbb{Q}[F_1,\\ldots,F_k]$, several algorithms have been proposed in order to test whether $P(f_1,\\ldots,f_k)=0$. In this paper, we present such an algorithm for the case where $f_1,\\ldots,f_k$ are so-called transseries instead of power series.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u7528\u4e8e\u6d4b\u8bd5\u591a\u9879\u5f0fP\u5728\u8d85\u8d8a\u7ea7\u6570f1,...,fk\u4e0a\u7684\u503c\u662f\u5426\u4e3a\u96f6\uff0c\u8fd9\u4e9b\u8d85\u8d8a\u7ea7\u6570\u7531\u591a\u9879\u5f0f\u5fae\u5206\u65b9\u7a0b\u7ec4\u5b9a\u4e49", "motivation": "\u5df2\u6709\u7b97\u6cd5\u7528\u4e8e\u6d4b\u8bd5\u591a\u9879\u5f0f\u5728\u5f62\u5f0f\u5e42\u7ea7\u6570\u4e0a\u7684\u96f6\u70b9\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8d85\u8d8a\u7ea7\u6570\u60c5\u51b5\u7684\u7b97\u6cd5\u3002\u8d85\u8d8a\u7ea7\u6570\u6bd4\u5e42\u7ea7\u6570\u66f4\u4e00\u822c\u5316\uff0c\u5305\u542b\u6307\u6570\u548c\u5bf9\u6570\u9879\uff0c\u5728\u6e10\u8fd1\u5206\u6790\u548c\u53ef\u8ba1\u7b97\u5206\u6790\u4e2d\u5f88\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u5904\u7406\u7531\u591a\u9879\u5f0f\u5fae\u5206\u65b9\u7a0b\u7ec4\u5b9a\u4e49\u7684\u8d85\u8d8a\u7ea7\u6570f1,...,fk\uff0c\u6d4b\u8bd5\u591a\u9879\u5f0fP(f1,...,fk)\u662f\u5426\u4e3a\u96f6\u3002\u7b97\u6cd5\u57fa\u4e8e\u8d85\u8d8a\u7ea7\u6570\u7684\u4ee3\u6570\u7ed3\u6784\u548c\u5fae\u5206\u6027\u8d28\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b97\u6cd5\uff0c\u80fd\u591f\u5224\u65ad\u591a\u9879\u5f0f\u5728\u8d85\u8d8a\u7ea7\u6570\u4e0a\u7684\u96f6\u70b9\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u5e42\u7ea7\u6570\u7b97\u6cd5\u7684\u9002\u7528\u8303\u56f4\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u586b\u8865\u4e86\u8d85\u8d8a\u7ea7\u6570\u96f6\u70b9\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u4e3a\u8d85\u8d8a\u7ea7\u6570\u7684\u53ef\u8ba1\u7b97\u6027\u7406\u8bba\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5728\u6e10\u8fd1\u5206\u6790\u548c\u7b26\u53f7\u8ba1\u7b97\u4e2d\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00966", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00966", "abs": "https://arxiv.org/abs/2602.00966", "authors": ["Zhaoyang Guan", "Huixi Cao", "Ming Zhong", "Eric Yang", "Lynn Ai", "Yongxin Ni", "Bill Shi"], "title": "Symphony-Coord: Emergent Coordination in Decentralized Agent Systems", "comment": "41 pages,15 figures", "summary": "Multi-agent large language model systems can tackle complex multi-step tasks by decomposing work and coordinating specialized behaviors. However, current coordination mechanisms typically rely on statically assigned roles and centralized controllers. As agent pools and task distributions evolve, these design choices lead to inefficient routing, poor adaptability, and fragile fault recovery capabilities. We introduce Symphony-Coord, a decentralized multi-agent framework that transforms agent selection into an online multi-armed bandit problem, enabling roles to emerge organically through interaction. The framework employs a two-stage dynamic beacon protocol: (i) a lightweight candidate screening mechanism to limit communication and computational overhead; (ii) an adaptive LinUCB selector that routes subtasks based on context features derived from task requirements and agent states, continuously optimized through delayed end-to-end feedback. Under standard linear realizability assumptions, we provide sublinear regret bounds, indicating the system converges toward near-optimal allocation schemes. Validation through simulation experiments and real-world large language model benchmarks demonstrates that Symphony-Coord not only enhances task routing efficiency but also exhibits robust self-healing capabilities in scenarios involving distribution shifts and agent failures, achieving a scalable coordination mechanism without predefined roles.", "AI": {"tldr": "Symphony-Coord\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u9009\u62e9\u8f6c\u5316\u4e3a\u5728\u7ebf\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u4fe1\u6807\u534f\u8bae\u5b9e\u73b0\u6709\u673a\u89d2\u8272\u6d8c\u73b0\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u89d2\u8272\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u673a\u5236\u901a\u5e38\u4f9d\u8d56\u9759\u6001\u5206\u914d\u89d2\u8272\u548c\u96c6\u4e2d\u5f0f\u63a7\u5236\u5668\uff0c\u968f\u7740\u667a\u80fd\u4f53\u6c60\u548c\u4efb\u52a1\u5206\u5e03\u7684\u53d8\u5316\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u5bfc\u81f4\u8def\u7531\u6548\u7387\u4f4e\u4e0b\u3001\u9002\u5e94\u6027\u5dee\u548c\u5bb9\u9519\u6062\u590d\u80fd\u529b\u8106\u5f31\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u52a8\u6001\u4fe1\u6807\u534f\u8bae\uff1a1)\u8f7b\u91cf\u7ea7\u5019\u9009\u7b5b\u9009\u673a\u5236\u9650\u5236\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\uff1b2)\u57fa\u4e8eLinUCB\u7684\u81ea\u9002\u5e94\u9009\u62e9\u5668\uff0c\u5229\u7528\u4efb\u52a1\u9700\u6c42\u548c\u667a\u80fd\u4f53\u72b6\u6001\u7684\u7279\u5f81\u8fdb\u884c\u5b50\u4efb\u52a1\u8def\u7531\uff0c\u901a\u8fc7\u5ef6\u8fdf\u7aef\u5230\u7aef\u53cd\u9988\u6301\u7eed\u4f18\u5316\u3002", "result": "\u5728\u7ebf\u6027\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u4e0b\u63d0\u4f9b\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u754c\uff0c\u8868\u660e\u7cfb\u7edf\u6536\u655b\u4e8e\u63a5\u8fd1\u6700\u4f18\u7684\u5206\u914d\u65b9\u6848\u3002\u4eff\u771f\u5b9e\u9a8c\u548c\u771f\u5b9e\u4e16\u754c\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u4efb\u52a1\u8def\u7531\u6548\u7387\u548c\u5bb9\u9519\u6062\u590d\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "Symphony-Coord\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u5b9a\u4e49\u89d2\u8272\u7684\u53ef\u6269\u5c55\u534f\u8c03\u673a\u5236\uff0c\u5728\u5206\u5e03\u53d8\u5316\u548c\u667a\u80fd\u4f53\u6545\u969c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u81ea\u6108\u80fd\u529b\uff0c\u4e3a\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00793", "categories": ["cs.HC", "cs.CL", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00793", "abs": "https://arxiv.org/abs/2602.00793", "authors": ["Yoonsang Kim", "Devshree Jadeja", "Divyansh Pradhan", "Yalong Yang", "Arie Kaufman"], "title": "SpeechLess: Micro-utterance with Personalized Spatial Memory-aware Assistant in Everyday Augmented Reality", "comment": "11 pages, 9 figures. This is the author's version of the article that will appear at the IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR) 2026", "summary": "Speaking aloud to a wearable AR assistant in public can be socially awkward, and re-articulating the same requests every day creates unnecessary effort. We present SpeechLess, a wearable AR assistant that introduces a speech-based intent granularity control paradigm grounded in personalized spatial memory. SpeechLess helps users \"speak less,\" while still obtaining the information they need, and supports gradual explicitation of intent when more complex expression is required. SpeechLess binds prior interactions to multimodal personal context-space, time, activity, and referents-to form spatial memories, and leverages them to extrapolate missing intent dimensions from under-specified user queries. This enables users to dynamically adjust how explicitly they express their informational needs, from full-utterance to micro/zero-utterance interaction. We motivate our design through a week-long formative study using a commercial smart glasses platform, revealing discomfort with public voice use, frustration with repetitive speech, and hardware constraints. Building on these insights, we design SpeechLess, and evaluate it through controlled lab and in-the-wild studies. Our results indicate that regulated speech-based interaction, can improve everyday information access, reduce articulation effort, and support socially acceptable use without substantially degrading perceived usability or intent resolution accuracy across diverse everyday environments.", "AI": {"tldr": "SpeechLess\u662f\u4e00\u4e2a\u57fa\u4e8e\u7a7a\u95f4\u8bb0\u5fc6\u7684AR\u52a9\u624b\uff0c\u901a\u8fc7\u51cf\u5c11\u8bed\u97f3\u4ea4\u4e92\u9700\u6c42\u6765\u6539\u5584\u516c\u5171\u573a\u5408\u4f7f\u7528\u4f53\u9a8c\uff0c\u652f\u6301\u4ece\u5b8c\u6574\u8bed\u97f3\u5230\u5fae/\u96f6\u8bed\u97f3\u7684\u4ea4\u4e92\u7c92\u5ea6\u63a7\u5236\u3002", "motivation": "\u5728\u516c\u5171\u573a\u5408\u4f7f\u7528\u8bed\u97f3\u4e0eAR\u52a9\u624b\u4ea4\u4e92\u5b58\u5728\u793e\u4ea4\u5c34\u5c2c\u95ee\u9898\uff0c\u4e14\u6bcf\u5929\u91cd\u590d\u8868\u8fbe\u76f8\u540c\u8bf7\u6c42\u9020\u6210\u4e0d\u5fc5\u8981\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002\u73b0\u6709\u5546\u4e1a\u667a\u80fd\u773c\u955c\u5e73\u53f0\u5b58\u5728\u786c\u4ef6\u9650\u5236\u548c\u91cd\u590d\u8bed\u97f3\u4ea4\u4e92\u7684\u632b\u8d25\u611f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7a7a\u95f4\u8bb0\u5fc6\u7684\u8bed\u97f3\u610f\u56fe\u7c92\u5ea6\u63a7\u5236\u8303\u5f0f\uff0c\u5c06\u5148\u524d\u4ea4\u4e92\u4e0e\u591a\u6a21\u6001\u4e2a\u4eba\u4e0a\u4e0b\u6587\uff08\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u6d3b\u52a8\u3001\u6307\u4ee3\u7269\uff09\u7ed1\u5b9a\u5f62\u6210\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u5229\u7528\u8fd9\u4e9b\u8bb0\u5fc6\u4ece\u7528\u6237\u4e0d\u5b8c\u6574\u7684\u67e5\u8be2\u4e2d\u63a8\u65ad\u7f3a\u5931\u7684\u610f\u56fe\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u5ba4\u548c\u91ce\u5916\u7814\u7a76\u8868\u660e\uff0c\u53d7\u8c03\u8282\u7684\u8bed\u97f3\u4ea4\u4e92\u53ef\u4ee5\u6539\u5584\u65e5\u5e38\u4fe1\u606f\u83b7\u53d6\uff0c\u51cf\u5c11\u8868\u8fbe\u52aa\u529b\uff0c\u652f\u6301\u793e\u4ea4\u53ef\u63a5\u53d7\u7684\u4f7f\u7528\uff0c\u4e14\u4e0d\u4f1a\u663e\u8457\u964d\u4f4e\u611f\u77e5\u53ef\u7528\u6027\u6216\u610f\u56fe\u89e3\u6790\u51c6\u786e\u6027\u3002", "conclusion": "SpeechLess\u901a\u8fc7\u7a7a\u95f4\u8bb0\u5fc6\u652f\u6301\u52a8\u6001\u8c03\u6574\u610f\u56fe\u8868\u8fbe\u7c92\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u516c\u5171\u573a\u5408\u8bed\u97f3\u4ea4\u4e92\u7684\u793e\u4ea4\u5c34\u5c2c\u548c\u91cd\u590d\u8868\u8fbe\u95ee\u9898\uff0c\u4e3a\u53ef\u7a7f\u6234AR\u52a9\u624b\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u3001\u9ad8\u6548\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002"}}
{"id": "2602.01822", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.01822", "abs": "https://arxiv.org/abs/2602.01822", "authors": ["Philip Stroemert", "Hendrik Borgelt", "David Linke", "Mark Doerr", "Bhavin Katabathuni", "Oliver Koepler", "Norbert Kockmann"], "title": "ChemDCAT-AP: Enabling Semantic Interoperability with a Contextual Extension of DCAT-AP", "comment": "The peer-reviewed and accepted paper will be published in the proceedings of the 19th International Conference on Metadata and Semantics Research (MTSR 2025), Thessaloniki, Greece, 15 - 19 December 2025", "summary": "Cross-domain data integration drives interdisciplinary data reuse and knowledge transfer across domains. However, each discipline maintains its own metadata schemas and domain ontologies, employing distinct conceptual models and application profiles, which complicates semantic interoperability. The W3C Data Catalog Vocabulary (DCAT) offers a widely adopted RDF vocabulary for describing datasets and their distributions, but its core model is intentionally lightweight. Numerous domain-specific application profiles have emerged to enrich DCAT's expressivity, the most well-known DCAT-AP for public data. To facilitate cross-domain interoperability for research data, we propose DCAT-AP PLUS, a DCAT Application Profile (P)roviding additional (L)inks to (U)se-case (S)pecific context (DCAT-AP+). This generic application profile enables a comprehensive representation of the provenance and context of research data generation. DACT-AP+ introduces an upper-level layer that can be specialized by individual domains without sacrificing compatibility. We demonstrate the application of DCAT-AP+ and a specific profile ChemDCAT-AP to showcase the potential of data integration of the neighboring disciplines chemistry and catalysis. We adopt LinkML, a YAML-based modeling framework, to support schema inheritance, generate domain-specific subschemas, and provide mechanisms for data type harmonization, validation, and format conversion, ensuring smooth integration of DCAT-AP+ and ChemDCAT-AP within existing data infrastructures.", "AI": {"tldr": "\u63d0\u51faDCAT-AP PLUS\u901a\u7528\u5e94\u7528\u914d\u7f6e\u6587\u4ef6\uff0c\u901a\u8fc7\u4e0a\u5c42\u62bd\u8c61\u5c42\u589e\u5f3aDCAT\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\uff0c\u652f\u6301\u8de8\u9886\u57df\u7814\u7a76\u6570\u636e\u4e92\u64cd\u4f5c\uff0c\u5e76\u4ee5\u5316\u5b66\u9886\u57dfChemDCAT-AP\u4e3a\u4f8b\u5c55\u793a\u5e94\u7528", "motivation": "\u8de8\u9886\u57df\u6570\u636e\u96c6\u6210\u9762\u4e34\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u6311\u6218\uff0c\u5404\u5b66\u79d1\u4f7f\u7528\u4e0d\u540c\u7684\u5143\u6570\u636e\u6a21\u5f0f\u548c\u9886\u57df\u672c\u4f53\uff0cDCAT\u6838\u5fc3\u6a21\u578b\u8fc7\u4e8e\u8f7b\u91cf\u7ea7\uff0c\u73b0\u6709\u5e94\u7528\u914d\u7f6e\u6587\u4ef6\u5982DCAT-AP\u4e3b\u8981\u9488\u5bf9\u516c\u5171\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u7814\u7a76\u6570\u636e\u751f\u6210\u80cc\u666f\u548c\u6eaf\u6e90\u7684\u7efc\u5408\u8868\u793a", "method": "\u63d0\u51faDCAT-AP PLUS\u901a\u7528\u5e94\u7528\u914d\u7f6e\u6587\u4ef6\uff0c\u5f15\u5165\u4e0a\u5c42\u62bd\u8c61\u5c42\u652f\u6301\u9886\u57df\u4e13\u4e1a\u5316\u800c\u4e0d\u727a\u7272\u517c\u5bb9\u6027\uff1b\u91c7\u7528LinkML\u5efa\u6a21\u6846\u67b6\u652f\u6301\u6a21\u5f0f\u7ee7\u627f\u3001\u751f\u6210\u9886\u57df\u7279\u5b9a\u5b50\u6a21\u5f0f\uff0c\u63d0\u4f9b\u6570\u636e\u7c7b\u578b\u534f\u8c03\u3001\u9a8c\u8bc1\u548c\u683c\u5f0f\u8f6c\u6362\u673a\u5236\uff1b\u4ee5\u5316\u5b66\u548c\u50ac\u5316\u9886\u57df\u4e3a\u4f8b\u5f00\u53d1ChemDCAT-AP\u5177\u4f53\u914d\u7f6e\u6587\u4ef6", "result": "DCAT-AP PLUS\u80fd\u591f\u5168\u9762\u8868\u793a\u7814\u7a76\u6570\u636e\u751f\u6210\u80cc\u666f\u548c\u6eaf\u6e90\uff0c\u901a\u8fc7ChemDCAT-AP\u5c55\u793a\u5316\u5b66\u4e0e\u50ac\u5316\u9886\u57df\u6570\u636e\u96c6\u6210\u6f5c\u529b\uff0cLinkML\u6846\u67b6\u786e\u4fdd\u4e0e\u73b0\u6709\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u7684\u5e73\u6ed1\u96c6\u6210", "conclusion": "DCAT-AP PLUS\u901a\u8fc7\u901a\u7528\u4e0a\u5c42\u62bd\u8c61\u5c42\u89e3\u51b3\u4e86\u8de8\u9886\u57df\u7814\u7a76\u6570\u636e\u8bed\u4e49\u4e92\u64cd\u4f5c\u95ee\u9898\uff0c\u652f\u6301\u9886\u57df\u7279\u5b9a\u6269\u5c55\u800c\u4e0d\u7834\u574f\u517c\u5bb9\u6027\uff0c\u4e3a\u8de8\u5b66\u79d1\u6570\u636e\u91cd\u7528\u548c\u77e5\u8bc6\u8f6c\u79fb\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.01299", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2602.01299", "abs": "https://arxiv.org/abs/2602.01299", "authors": ["Gianluca Curzi", "Graham E. Leigh"], "title": "Making progress: Reducibility Candidates and Cut Elimination in the Ill-founded Realm", "comment": "45 pages", "summary": "Ill-founded (or non-wellfounded) proof systems have emerged as a natural framework for inductive and coinductive reasoning. In such systems, soundness relies on global correctness criteria, such as the progressivity condition. Ensuring that these criteria are preserved under infinitary cut elimination remains a central technical challenge in ill-founded proof theory.\n  In this paper, we present two cut elimination arguments for ill-founded $\u03bc\\mathsf{MALL}$ - a fragment of linear logic extended with fixed-points - based on the reducibility candidates technique of Tait and Girard. In both arguments, preservation of progressivity follows directly from the defining properties of the reducibility candidates. In particular, the second argument is based on the topological notion of internally closed set developed in previous work by Leigh and Afshari.", "AI": {"tldr": "\u672c\u6587\u4e3a\u5e26\u4e0d\u52a8\u70b9\u7684\u7ebf\u6027\u903b\u8f91\u7247\u6bb5\u03bcMALL\u63d0\u4f9b\u4e86\u4e24\u4e2a\u57fa\u4e8e\u53ef\u5f52\u7ea6\u5019\u9009\u7684\u5207\u5272\u6d88\u9664\u8bba\u8bc1\uff0c\u89e3\u51b3\u4e86\u975e\u826f\u57fa\u8bc1\u660e\u7cfb\u7edf\u4e2d\u4fdd\u6301\u6e10\u8fdb\u6027\u6761\u4ef6\u7684\u6280\u672f\u6311\u6218\u3002", "motivation": "\u975e\u826f\u57fa\u8bc1\u660e\u7cfb\u7edf\u662f\u5f52\u7eb3\u548c\u5171\u5f52\u7eb3\u63a8\u7406\u7684\u81ea\u7136\u6846\u67b6\uff0c\u4f46\u5176\u6b63\u786e\u6027\u4f9d\u8d56\u4e8e\u5168\u5c40\u51c6\u5219\u5982\u6e10\u8fdb\u6027\u6761\u4ef6\u3002\u786e\u4fdd\u8fd9\u4e9b\u51c6\u5219\u5728\u65e0\u7a77\u5207\u5272\u6d88\u9664\u4e0b\u5f97\u4ee5\u4fdd\u6301\u662f\u975e\u826f\u57fa\u8bc1\u660e\u7406\u8bba\u7684\u6838\u5fc3\u6280\u672f\u6311\u6218\u3002", "method": "\u57fa\u4e8eTait\u548cGirard\u7684\u53ef\u5f52\u7ea6\u5019\u9009\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e24\u4e2a\u9488\u5bf9\u03bcMALL\uff08\u5e26\u4e0d\u52a8\u70b9\u7684\u7ebf\u6027\u903b\u8f91\u7247\u6bb5\uff09\u7684\u5207\u5272\u6d88\u9664\u8bba\u8bc1\u3002\u7b2c\u4e8c\u4e2a\u8bba\u8bc1\u5229\u7528\u4e86Leigh\u548cAfshari\u5148\u524d\u5de5\u4f5c\u4e2d\u53d1\u5c55\u7684\u5185\u90e8\u95ed\u96c6\u62d3\u6251\u6982\u5ff5\u3002", "result": "\u4e24\u4e2a\u5207\u5272\u6d88\u9664\u8bba\u8bc1\u90fd\u6210\u529f\u5b9e\u73b0\u4e86\u975e\u826f\u57fa\u8bc1\u660e\u7cfb\u7edf\u4e2d\u7684\u5207\u5272\u6d88\u9664\uff0c\u5176\u4e2d\u6e10\u8fdb\u6027\u6761\u4ef6\u7684\u4fdd\u6301\u76f4\u63a5\u4ece\u53ef\u5f52\u7ea6\u5019\u9009\u7684\u5b9a\u4e49\u6027\u8d28\u5f97\u51fa\u3002", "conclusion": "\u901a\u8fc7\u53ef\u5f52\u7ea6\u5019\u9009\u6280\u672f\uff0c\u672c\u6587\u6210\u529f\u89e3\u51b3\u4e86\u975e\u826f\u57fa\u8bc1\u660e\u7cfb\u7edf\u4e2d\u4fdd\u6301\u6e10\u8fdb\u6027\u6761\u4ef6\u7684\u6280\u672f\u96be\u9898\uff0c\u4e3a\u03bcMALL\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5207\u5272\u6d88\u9664\u8bba\u8bc1\uff0c\u7279\u522b\u662f\u5229\u7528\u62d3\u6251\u6982\u5ff5\u7684\u7b2c\u4e8c\u4e2a\u8bba\u8bc1\u5c55\u73b0\u4e86\u8be5\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.00123", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00123", "abs": "https://arxiv.org/abs/2602.00123", "authors": ["Filip Nowicki", "Hubert Marciniak", "Jakub \u0141\u0105czkowski", "Krzysztof Jassem", "Tomasz G\u00f3recki", "Vimala Balakrishnan", "Desmond C. Ong", "Maciej Behnke"], "title": "Visual Affect Analysis: Predicting Emotions of Image Viewers with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) show promise as tools for inferring affect from visual stimuli at scale; it is not yet clear how closely their outputs align with human affective ratings. We benchmarked nine VLMs, ranging from state-of-the-art proprietary models to open-source models, on three psycho-metrically validated affective image datasets: the International Affective Picture System, the Nencki Affective Picture System, and the Library of AI-Generated Affective Images. The models performed two tasks in the zero-shot setting: (i) top-emotion classification (selecting the strongest discrete emotion elicited by an image) and (ii) continuous prediction of human ratings on 1-7/9 Likert scales for discrete emotion categories and affective dimensions. We also evaluated the impact of rater-conditioned prompting on the LAI-GAI dataset using de-identified participant metadata. The results show good performance in discrete emotion classification, with accuracies typically ranging from 60% to 80% on six-emotion labels and from 60% to 75% on a more challenging 12-category task. The predictions of anger and surprise had the lowest accuracy in all datasets. For continuous rating prediction, models showed moderate to strong alignment with humans (r > 0.75) but also exhibited consistent biases, notably weaker performance on arousal, and a tendency to overestimate response strength. Rater-conditioned prompting resulted in only small, inconsistent changes in predictions. Overall, VLMs capture broad affective trends but lack the nuance found in validated psychological ratings, highlighting their potential and current limitations for affective computing and mental health-related applications.", "AI": {"tldr": "\u8bc4\u4f309\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u4e2a\u60c5\u611f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u79bb\u6563\u60c5\u611f\u5206\u7c7b\u4e0a\u8868\u73b0\u826f\u597d\uff0860-80%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5728\u8fde\u7eed\u60c5\u611f\u8bc4\u5206\u9884\u6d4b\u4e2d\u5b58\u5728\u504f\u5dee\uff0c\u4e14\u65e0\u6cd5\u5b8c\u5168\u5339\u914d\u4eba\u7c7b\u60c5\u611f\u8bc4\u5206\u7684\u7ec6\u5fae\u5dee\u5f02\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4f5c\u4e3a\u5927\u89c4\u6a21\u4ece\u89c6\u89c9\u523a\u6fc0\u63a8\u65ad\u60c5\u611f\u7684\u5de5\u5177\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5176\u8f93\u51fa\u4e0e\u4eba\u7c7b\u60c5\u611f\u8bc4\u5206\u7684\u4e00\u81f4\u6027\u7a0b\u5ea6\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30VLMs\u5728\u5fc3\u7406\u6d4b\u91cf\u5b66\u9a8c\u8bc1\u7684\u60c5\u611f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4ee5\u4e86\u89e3\u5176\u5728\u60c5\u611f\u8ba1\u7b97\u548c\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u5728\u4e09\u4e2a\u5fc3\u7406\u6d4b\u91cf\u5b66\u9a8c\u8bc1\u7684\u60c5\u611f\u56fe\u50cf\u6570\u636e\u96c6\uff08IAPS\u3001NAPS\u3001LAI-GAI\uff09\u4e0a\u5bf99\u4e2aVLMs\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u4ece\u6700\u5148\u8fdb\u7684\u4e13\u6709\u6a21\u578b\u5230\u5f00\u6e90\u6a21\u578b\u3002\u91c7\u7528\u96f6\u6837\u672c\u8bbe\u7f6e\u6267\u884c\u4e24\u4e2a\u4efb\u52a1\uff1a(1) \u9876\u90e8\u60c5\u611f\u5206\u7c7b\uff08\u9009\u62e9\u56fe\u50cf\u5f15\u53d1\u7684\u6700\u5f3a\u79bb\u6563\u60c5\u611f\uff09\uff0c(2) \u57281-7/9\u674e\u514b\u7279\u91cf\u8868\u4e0a\u8fde\u7eed\u9884\u6d4b\u4eba\u7c7b\u5bf9\u79bb\u6563\u60c5\u611f\u7c7b\u522b\u548c\u60c5\u611f\u7ef4\u5ea6\u7684\u8bc4\u5206\u3002\u5728LAI-GAI\u6570\u636e\u96c6\u4e0a\u8fd8\u8bc4\u4f30\u4e86\u8bc4\u5206\u8005\u6761\u4ef6\u63d0\u793a\u7684\u5f71\u54cd\u3002", "result": "\u79bb\u6563\u60c5\u611f\u5206\u7c7b\u8868\u73b0\u826f\u597d\uff1a\u516d\u60c5\u611f\u6807\u7b7e\u4efb\u52a1\u51c6\u786e\u738760-80%\uff0c\u5341\u4e8c\u7c7b\u522b\u4efb\u52a1\u51c6\u786e\u738760-75%\u3002\u6124\u6012\u548c\u60ca\u8bb6\u5728\u6240\u6709\u6570\u636e\u96c6\u4e2d\u9884\u6d4b\u51c6\u786e\u7387\u6700\u4f4e\u3002\u8fde\u7eed\u8bc4\u5206\u9884\u6d4b\u4e0e\u4eba\u7c7b\u6709\u4e2d\u5ea6\u5230\u5f3a\u76f8\u5173\u6027\uff08r>0.75\uff09\uff0c\u4f46\u5b58\u5728\u4e00\u81f4\u504f\u5dee\uff1a\u5524\u9192\u5ea6\u8868\u73b0\u8f83\u5f31\uff0c\u503e\u5411\u4e8e\u9ad8\u4f30\u54cd\u5e94\u5f3a\u5ea6\u3002\u8bc4\u5206\u8005\u6761\u4ef6\u63d0\u793a\u4ec5\u5bfc\u81f4\u9884\u6d4b\u7684\u5c0f\u800c\u4e0d\u4e00\u81f4\u53d8\u5316\u3002", "conclusion": "VLMs\u80fd\u591f\u6355\u6349\u5e7f\u6cdb\u7684\u60c5\u611f\u8d8b\u52bf\uff0c\u4f46\u7f3a\u4e4f\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u5fc3\u7406\u8bc4\u5206\u4e2d\u7684\u7ec6\u5fae\u5dee\u5f02\u3002\u8fd9\u7a81\u663e\u4e86VLMs\u5728\u60c5\u611f\u8ba1\u7b97\u548c\u5fc3\u7406\u5065\u5eb7\u76f8\u5173\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\u6027\uff0c\u8868\u660e\u5b83\u4eec\u53ef\u4ee5\u4f5c\u4e3a\u521d\u6b65\u5de5\u5177\u4f7f\u7528\uff0c\u4f46\u5c1a\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u7c7b\u60c5\u611f\u8bc4\u4f30\u3002"}}
{"id": "2602.00480", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00480", "abs": "https://arxiv.org/abs/2602.00480", "authors": ["Mohini Priya Kolluri", "Ammar Waheed", "Zohaib Hasnain"], "title": "FISC: A Fluid-Inspired Framework for Decentralized and Scalable Swarm Control", "comment": null, "summary": "Achieving scalable coordination in large robotic swarms is often constrained by reliance on inter-agent communication, which introduces latency, bandwidth limitations, and vulnerability to failure. To address this gap, a decentralized approach for outer-loop control of large multi-agent systems based on the paradigm of how a fluid moves through a volume is proposed and evaluated. A relationship between fundamental fluidic element properties and individual robotic agent states is developed such that the corresponding swarm \"flows\" through a space, akin to a fluid when forced via a pressure boundary condition. By ascribing fluid-like properties to subsets of agents, the swarm evolves collectively while maintaining desirable structure and coherence without explicit communication of agent states within or outside of the swarm. The approach is evaluated using simulations involving $O(10^3)$ quadcopter agents and compared against Computational Fluid Dynamics (CFD) solutions for a converging-diverging domain. Quantitative agreement between swarm-derived and CFD fields is assessed using Root-Mean-Square Error (RMSE), yielding normalized errors of 0.15-0.9 for velocity, 0.61-0.98 for density, 0-0.937 for pressure. These results demonstrate the feasibility of treating large robotic swarms as continuum systems that retain the macroscopic structure derived from first principles, providing a basis for scalable and decentralized control.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6d41\u4f53\u52a8\u529b\u5b66\u539f\u7406\u7684\u5927\u89c4\u6a21\u673a\u5668\u4eba\u96c6\u7fa4\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u65b9\u6cd5\uff0c\u5c06\u673a\u5668\u4eba\u89c6\u4e3a\u6d41\u4f53\u5143\u7d20\uff0c\u5b9e\u73b0\u65e0\u663e\u5f0f\u901a\u4fe1\u7684\u534f\u8c03\u8fd0\u52a8", "motivation": "\u4f20\u7edf\u5927\u89c4\u6a21\u673a\u5668\u4eba\u96c6\u7fa4\u4f9d\u8d56\u901a\u4fe1\u8fdb\u884c\u534f\u8c03\uff0c\u5b58\u5728\u5ef6\u8fdf\u3001\u5e26\u5bbd\u9650\u5236\u548c\u6545\u969c\u8106\u5f31\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u663e\u5f0f\u901a\u4fe1\u7684\u53ef\u6269\u5c55\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u65b9\u6cd5", "method": "\u5c06\u673a\u5668\u4eba\u96c6\u7fa4\u89c6\u4e3a\u6d41\u4f53\u7cfb\u7edf\uff0c\u5efa\u7acb\u6d41\u4f53\u5143\u7d20\u5c5e\u6027\u4e0e\u673a\u5668\u4eba\u72b6\u6001\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f7f\u96c6\u7fa4\u50cf\u6d41\u4f53\u4e00\u6837\u5728\u538b\u529b\u8fb9\u754c\u6761\u4ef6\u4e0b\u6d41\u52a8\uff0c\u901a\u8fc7\u8d4b\u4e88\u673a\u5668\u4eba\u5b50\u96c6\u6d41\u4f53\u7279\u6027\u5b9e\u73b0\u96c6\u4f53\u6f14\u5316", "result": "\u5728O(10^3)\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u4e0e\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66(CFD)\u89e3\u6bd4\u8f83\uff0c\u901f\u5ea6\u3001\u5bc6\u5ea6\u3001\u538b\u529b\u7684\u5f52\u4e00\u5316\u5747\u65b9\u6839\u8bef\u5dee\u5206\u522b\u4e3a0.15-0.9\u30010.61-0.98\u30010-0.937", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u53ef\u5c06\u5927\u89c4\u6a21\u673a\u5668\u4eba\u96c6\u7fa4\u89c6\u4e3a\u8fde\u7eed\u4ecb\u8d28\u7cfb\u7edf\uff0c\u4fdd\u7559\u4ece\u57fa\u672c\u539f\u7406\u63a8\u5bfc\u7684\u5b8f\u89c2\u7ed3\u6784\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u63d0\u4f9b\u4e86\u57fa\u7840"}}
{"id": "2602.00180", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00180", "abs": "https://arxiv.org/abs/2602.00180", "authors": ["Deepak Babu Piskala"], "title": "Spec-Driven Development:From Code to Contract in the Age of AI Coding Assistants", "comment": "Submitted to AIWare 2026. 8 pages, 3 figures", "summary": "The rise of AI coding assistants has reignited interest in an old idea: what if specifications-not code-were the primary artifact of software development? Spec-driven development (SDD) inverts the traditional workflow by treating specifications as the source of truth and code as a generated or verified secondary artifact. This paper provides practitioners with a comprehensive guide to SDD, covering its principles, workflow patterns, and supporting tools. We present three levels of specification rigor-spec-first, spec-anchored, and spec-as-source-with clear guidance on when each applies. Through analysis of tools ranging from Behavior-Driven Development frameworks to modern AI-assisted toolkits like GitHub Spec Kit, we demonstrate how the spec-first philosophy maps to real implementations. We present case studies from API development, enterprise systems, and embedded software, illustrating how different domains apply SDD. We conclude with a decision framework helping practitioners determine when SDD provides value and when simpler approaches suffice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u4f9b\u4e86\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\uff08SDD\uff09\u7684\u5b9e\u8df5\u6307\u5357\uff0c\u5c06\u89c4\u8303\u4f5c\u4e3a\u4e3b\u8981\u5de5\u4ef6\u800c\u975e\u4ee3\u7801\uff0c\u6db5\u76d6\u5176\u539f\u5219\u3001\u5de5\u4f5c\u6d41\u6a21\u5f0f\u3001\u5de5\u5177\u652f\u6301\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u51b3\u7b56\u6846\u67b6\u5e2e\u52a9\u5f00\u53d1\u8005\u5224\u65ad\u4f55\u65f6\u91c7\u7528SDD\u3002", "motivation": "AI\u7f16\u7a0b\u52a9\u624b\u7684\u5174\u8d77\u91cd\u65b0\u6fc0\u53d1\u4e86\u5c06\u89c4\u8303\u800c\u975e\u4ee3\u7801\u4f5c\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u4e3b\u8981\u5de5\u4ef6\u7684\u7406\u5ff5\u3002\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u4ee3\u7801\u662f\u4e3b\u8981\u5173\u6ce8\u70b9\uff0c\u800cSDD\u5c06\u89c4\u8303\u89c6\u4e3a\u771f\u76f8\u6765\u6e90\uff0c\u4ee3\u7801\u5219\u4f5c\u4e3a\u751f\u6210\u6216\u9a8c\u8bc1\u7684\u6b21\u8981\u5de5\u4ef6\uff0c\u65e8\u5728\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u5f00\u53d1\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u89c4\u8303\u4e25\u8c28\u6027\u7ea7\u522b\uff1aspec-first\uff08\u89c4\u8303\u4f18\u5148\uff09\u3001spec-anchored\uff08\u89c4\u8303\u951a\u5b9a\uff09\u548cspec-as-source\uff08\u89c4\u8303\u5373\u6e90\u7801\uff09\uff0c\u4e3a\u6bcf\u79cd\u7ea7\u522b\u63d0\u4f9b\u9002\u7528\u573a\u666f\u6307\u5bfc\u3002\u5206\u6790\u4ece\u884c\u4e3a\u9a71\u52a8\u5f00\u53d1\u6846\u67b6\u5230\u73b0\u4ee3AI\u8f85\u52a9\u5de5\u5177\uff08\u5982GitHub Spec Kit\uff09\u7684\u5de5\u5177\u751f\u6001\u3002\u901a\u8fc7API\u5f00\u53d1\u3001\u4f01\u4e1a\u7cfb\u7edf\u548c\u5d4c\u5165\u5f0f\u8f6f\u4ef6\u7b49\u9886\u57df\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793aSDD\u7684\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u5c55\u793a\u4e86\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u5728\u4e0d\u540c\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\uff0c\u8bc1\u660e\u4e86SDD\u65b9\u6cd5\u5728\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf\u3001\u589e\u5f3a\u53ef\u7ef4\u62a4\u6027\u548c\u4fc3\u8fdb\u56e2\u961f\u534f\u4f5c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u5de5\u5177\u5206\u6790\u8868\u660e\uff0cspec-first\u7406\u5ff5\u53ef\u4ee5\u6620\u5c04\u5230\u5b9e\u9645\u5b9e\u73b0\u4e2d\uff0c\u73b0\u4ee3AI\u8f85\u52a9\u5de5\u5177\u8fdb\u4e00\u6b65\u652f\u6301\u4e86\u8fd9\u4e00\u5f00\u53d1\u8303\u5f0f\u3002", "conclusion": "\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4f46\u5e76\u975e\u9002\u7528\u4e8e\u6240\u6709\u573a\u666f\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u51b3\u7b56\u6846\u67b6\uff0c\u5e2e\u52a9\u5b9e\u8df5\u8005\u5224\u65ad\u4f55\u65f6SDD\u80fd\u63d0\u4f9b\u4ef7\u503c\uff0c\u4f55\u65f6\u66f4\u7b80\u5355\u7684\u65b9\u6cd5\u5c31\u8db3\u591f\u3002\u968f\u7740AI\u5de5\u5177\u7684\u53d1\u5c55\uff0cSDD\u7684\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\u5c06\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2602.00636", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.00636", "abs": "https://arxiv.org/abs/2602.00636", "authors": ["Yujie Yang", "Zhilong Zheng", "Shengbo Eben Li"], "title": "Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration", "comment": null, "summary": "Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u63ed\u793a\u4e86\u5b89\u5168\u63a2\u7d22\u7684\u76ee\u6807\u662f\u5728\u53ef\u884c\u533a\u57df\u4e0e\u73af\u5883\u6a21\u578b\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u5e73\u8861\u7684\u5b89\u5168\u63a2\u7d22\u6846\u67b6SEE\uff0c\u901a\u8fc7\u4ea4\u66ff\u5bfb\u627e\u6700\u5927\u53ef\u884c\u533a\u57df\u548c\u6700\u5c0f\u4e0d\u786e\u5b9a\u6a21\u578b\u5b9e\u73b0\u96f6\u7ea6\u675f\u8fdd\u53cd\u7684\u5b89\u5168\u63a2\u7d22\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u63a2\u7d22\u65b9\u6cd5\u901a\u5e38\u5c06\u63a2\u7d22\u9650\u5236\u5728\u53ef\u884c\u533a\u57df\u5185\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u672a\u89e3\u51b3\u95ee\u9898\uff1a\u901a\u8fc7\u63a2\u7d22\u53ef\u83b7\u5f97\u7684\u6700\u5927\u53ef\u884c\u533a\u57df\u662f\u4ec0\u4e48\uff1f\u5982\u4f55\u8bc6\u522b\u8fd9\u4e2a\u533a\u57df\uff1f\u672c\u6587\u65e8\u5728\u9996\u6b21\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\uff0c\u63ed\u793a\u5b89\u5168\u63a2\u7d22\u7684\u672c\u8d28\u662f\u5728\u53ef\u884c\u533a\u57df\u4e0e\u73af\u5883\u6a21\u578b\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u5b89\u5168\u5e73\u8861\u63a2\u7d22\uff08SEE\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4ea4\u66ff\u6267\u884c\u4e24\u4e2a\u6b65\u9aa4\uff1a1\uff09\u5bfb\u627e\u6700\u5927\u53ef\u884c\u533a\u57df\uff1b2\uff09\u5bfb\u627e\u6700\u5c0f\u4e0d\u786e\u5b9a\u6a21\u578b\u3002\u4f7f\u7528\u4e0d\u786e\u5b9a\u6a21\u578b\u7684\u56fe\u8868\u793a\uff0c\u8bc1\u660eSEE\u83b7\u5f97\u7684\u4e0d\u786e\u5b9a\u6a21\u578b\u5355\u8c03\u7ec6\u5316\uff0c\u53ef\u884c\u533a\u57df\u5355\u8c03\u6269\u5c55\uff0c\u4e24\u8005\u90fd\u6536\u655b\u5230\u5b89\u5168\u63a2\u7d22\u7684\u5e73\u8861\u70b9\u3002", "result": "\u5728\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEE\u7b97\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u96f6\u7ea6\u675f\u8fdd\u53cd\u7684\u53ef\u884c\u533a\u57df\u6269\u5c55\uff0c\u5e76\u5728\u51e0\u6b21\u8fed\u4ee3\u5185\u8fbe\u5230\u4e86\u5b89\u5168\u63a2\u7d22\u7684\u5e73\u8861\u3002\u7406\u8bba\u8bc1\u660e\u8868\u660e\u4e0d\u786e\u5b9a\u6a21\u578b\u5355\u8c03\u7ec6\u5316\uff0c\u53ef\u884c\u533a\u57df\u5355\u8c03\u6269\u5c55\uff0c\u4e24\u8005\u90fd\u6536\u655b\u5230\u5e73\u8861\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u63ed\u793a\u4e86\u5b89\u5168\u63a2\u7d22\u7684\u76ee\u6807\u662f\u5728\u53ef\u884c\u533a\u57df\u4e0e\u73af\u5883\u6a21\u578b\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u63d0\u51fa\u4e86SEE\u6846\u67b6\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u53ef\u884c\u533a\u57df\u548c\u73af\u5883\u6a21\u578b\uff0c\u786e\u4fdd\u96f6\u7ea6\u675f\u8fdd\u53cd\u7684\u5b89\u5168\u63a2\u7d22\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u63a2\u7d22\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01011", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01011", "abs": "https://arxiv.org/abs/2602.01011", "authors": ["Aneesh Pappu", "Batu El", "Hancheng Cao", "Carmelo di Nolfo", "Yanchao Sun", "Meng Cao", "James Zou"], "title": "Multi-Agent Teams Hold Experts Back", "comment": "Under review at ICML 2026", "summary": "Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prior work enforces coordination through fixed roles, workflows, or aggregation rules, leaving open the question of how well self-organizing teams perform when coordination is unconstrained. Drawing on organizational psychology, we study whether self-organizing LLM teams achieve strong synergy, where team performance matches or exceeds the best individual member. Across human-inspired and frontier ML benchmarks, we find that -- unlike human teams -- LLM teams consistently fail to match their expert agent's performance, even when explicitly told who the expert is, incurring performance losses of up to 37.6%. Decomposing this failure, we show that expert leveraging, rather than identification, is the primary bottleneck. Conversational analysis reveals a tendency toward integrative compromise -- averaging expert and non-expert views rather than appropriately weighting expertise -- which increases with team size and correlates negatively with performance. Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization. Our findings reveal a significant gap in the ability of self-organizing multi-agent teams to harness the collective expertise of their members.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u81ea\u7ec4\u7ec7\u591a\u667a\u80fd\u4f53LLM\u56e2\u961f\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5373\u4f7f\u660e\u786e\u77e5\u9053\u8c01\u662f\u4e13\u5bb6\uff0c\u56e2\u961f\u8868\u73b0\u4ecd\u6bd4\u6700\u4f73\u4e2a\u4f53\u6210\u5458\u5dee37.6%\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u4e13\u5bb6\u77e5\u8bc6\u5229\u7528\u800c\u975e\u4e13\u5bb6\u8bc6\u522b\u3002", "motivation": "\u968f\u7740\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4f5c\u4e3a\u81ea\u4e3b\u534f\u4f5c\u8005\u90e8\u7f72\uff0c\u9700\u8981\u7814\u7a76\u5728\u65e0\u7ea6\u675f\u534f\u8c03\u6761\u4ef6\u4e0b\u81ea\u7ec4\u7ec7\u56e2\u961f\u80fd\u5426\u5b9e\u73b0\u5f3a\u534f\u540c\u6548\u5e94\uff0c\u5373\u56e2\u961f\u8868\u73b0\u662f\u5426\u5339\u914d\u6216\u8d85\u8d8a\u6700\u4f73\u4e2a\u4f53\u6210\u5458\u3002\u5148\u524d\u7814\u7a76\u5927\u591a\u901a\u8fc7\u56fa\u5b9a\u89d2\u8272\u3001\u5de5\u4f5c\u6d41\u7a0b\u6216\u805a\u5408\u89c4\u5219\u5f3a\u5236\u534f\u8c03\uff0c\u7f3a\u4e4f\u5bf9\u65e0\u7ea6\u675f\u81ea\u7ec4\u7ec7\u56e2\u961f\u8868\u73b0\u7684\u7814\u7a76\u3002", "method": "\u501f\u9274\u7ec4\u7ec7\u5fc3\u7406\u5b66\uff0c\u7814\u7a76\u81ea\u7ec4\u7ec7LLM\u56e2\u961f\u662f\u5426\u5b9e\u73b0\u5f3a\u534f\u540c\u6548\u5e94\u3002\u5728\u4eba\u7c7b\u542f\u53d1\u548c\u524d\u6cbfML\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4\u8f83LLM\u56e2\u961f\u4e0e\u4e13\u5bb6\u4e2a\u4f53\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5bf9\u8bdd\u5206\u6790\u63ed\u793a\u56e2\u961f\u51b3\u7b56\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u4e13\u5bb6\u77e5\u8bc6\u5229\u7528\u4e0e\u6574\u5408\u59a5\u534f\u884c\u4e3a\u3002", "result": "\u4e0e\u4eba\u7c7b\u56e2\u961f\u4e0d\u540c\uff0cLLM\u56e2\u961f\u59cb\u7ec8\u65e0\u6cd5\u5339\u914d\u5176\u4e13\u5bb6\u667a\u80fd\u4f53\u7684\u8868\u73b0\uff0c\u5373\u4f7f\u660e\u786e\u544a\u77e5\u8c01\u662f\u4e13\u5bb6\uff0c\u6027\u80fd\u635f\u5931\u9ad8\u8fbe37.6%\u3002\u4e3b\u8981\u74f6\u9888\u662f\u4e13\u5bb6\u77e5\u8bc6\u5229\u7528\u800c\u975e\u4e13\u5bb6\u8bc6\u522b\u3002\u5bf9\u8bdd\u5206\u6790\u663e\u793a\u56e2\u961f\u503e\u5411\u4e8e\u6574\u5408\u59a5\u534f\u2014\u2014\u5e73\u5747\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u89c2\u70b9\u800c\u975e\u9002\u5f53\u52a0\u6743\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8fd9\u79cd\u884c\u4e3a\u968f\u56e2\u961f\u89c4\u6a21\u589e\u52a0\u4e14\u4e0e\u8868\u73b0\u8d1f\u76f8\u5173\u3002\u6709\u8da3\u7684\u662f\uff0c\u8fd9\u79cd\u5bfb\u6c42\u5171\u8bc6\u7684\u884c\u4e3a\u63d0\u9ad8\u4e86\u5bf9\u6297\u6027\u667a\u80fd\u4f53\u7684\u9c81\u68d2\u6027\uff0c\u8868\u660e\u5bf9\u9f50\u4e0e\u6709\u6548\u4e13\u4e1a\u77e5\u8bc6\u5229\u7528\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "conclusion": "\u81ea\u7ec4\u7ec7\u591a\u667a\u80fd\u4f53\u56e2\u961f\u5728\u5229\u7528\u6210\u5458\u96c6\u4f53\u4e13\u4e1a\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002LLM\u56e2\u961f\u8868\u73b0\u51fa\u8fc7\u5ea6\u5bfb\u6c42\u5171\u8bc6\u7684\u6574\u5408\u59a5\u534f\u884c\u4e3a\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u65e0\u7ea6\u675f\u534f\u8c03\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.00241", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00241", "abs": "https://arxiv.org/abs/2602.00241", "authors": ["Hansol Lee", "AJ Alvero", "Ren\u00e9 F. Kizilcec", "Thorsten Joachims"], "title": "Does Algorithmic Uncertainty Sway Human Experts? Evidence from a Field Experiment in Selective College Admissions", "comment": null, "summary": "Algorithmic predictions are inherently uncertain: even models with similar aggregate accuracy can produce different predictions for the same individual, raising concerns that high-stakes decisions may become sensitive to arbitrary modeling choices. In this paper, we define algorithmic reliance as the extent to which a decision outcome depends on whether a more favorable versus less favorable algorithmic prediction is presented to the decision-maker. We estimate this in a randomized field experiment (n=19,545) embedded in a selective U.S. college admissions cycle, in which admissions officers reviewed each application alongside an algorithmic score while we randomly varied whether the score came from one of two similarly accurate prediction models. Although the two models performed similarly in aggregate, they frequently assigned different scores to the same applicant, creating exogenous variation in the score shown. Surprisingly, we find little evidence of algorithmic reliance: presenting a more favorable score does not meaningfully increase an applicant's probability of admission on average, even when the models disagree substantially. These findings suggest that, in this expert, high-stakes setting, human decision-making is largely invariant to arbitrary variation in algorithmic predictions, underscoring the role of professional discretion and institutional context in mediating the downstream effects of algorithmic uncertainty.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e13\u5bb6\u51b3\u7b56\u7684\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\uff0c\u4eba\u7c7b\u51b3\u7b56\u5bf9\u7b97\u6cd5\u9884\u6d4b\u7684\u4efb\u610f\u53d8\u5316\u5177\u6709\u9ad8\u5ea6\u4e0d\u53d8\u6027\uff0c\u7b97\u6cd5\u4f9d\u8d56\u7a0b\u5ea6\u5f88\u4f4e", "motivation": "\u7b97\u6cd5\u9884\u6d4b\u5b58\u5728\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5373\u4f7f\u603b\u4f53\u51c6\u786e\u7387\u76f8\u4f3c\u7684\u6a21\u578b\u4e5f\u53ef\u80fd\u5bf9\u540c\u4e00\u4e2a\u4f53\u7ed9\u51fa\u4e0d\u540c\u7684\u9884\u6d4b\u3002\u8fd9\u5f15\u53d1\u4e86\u62c5\u5fe7\uff1a\u9ad8\u98ce\u9669\u51b3\u7b56\u53ef\u80fd\u5bf9\u4efb\u610f\u7684\u5efa\u6a21\u9009\u62e9\u53d8\u5f97\u654f\u611f\u3002\u7814\u7a76\u65e8\u5728\u91cf\u5316\u7b97\u6cd5\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u5373\u51b3\u7b56\u7ed3\u679c\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u5411\u51b3\u7b56\u8005\u5448\u73b0\u66f4\u6709\u5229\u8fd8\u662f\u66f4\u4e0d\u5229\u7684\u7b97\u6cd5\u9884\u6d4b\u3002", "method": "\u5728\u7f8e\u56fd\u4e00\u6240\u9009\u62e9\u6027\u5927\u5b66\u7684\u62db\u751f\u5468\u671f\u4e2d\u5d4c\u5165\u968f\u673a\u5316\u73b0\u573a\u5b9e\u9a8c\uff08n=19,545\uff09\u3002\u62db\u751f\u5b98\u5458\u5728\u5ba1\u6838\u6bcf\u4efd\u7533\u8bf7\u65f6\uff0c\u4f1a\u540c\u65f6\u770b\u5230\u4e00\u4e2a\u7b97\u6cd5\u8bc4\u5206\uff0c\u7814\u7a76\u4eba\u5458\u968f\u673a\u5206\u914d\u8be5\u8bc4\u5206\u6765\u81ea\u4e24\u4e2a\u603b\u4f53\u51c6\u786e\u7387\u76f8\u4f3c\u7684\u9884\u6d4b\u6a21\u578b\u4e4b\u4e00\u3002\u867d\u7136\u4e24\u4e2a\u6a21\u578b\u5728\u603b\u4f53\u4e0a\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u5bf9\u540c\u4e00\u7533\u8bf7\u8005\u7ed9\u51fa\u4e0d\u540c\u7684\u8bc4\u5206\uff0c\u4ece\u800c\u521b\u9020\u4e86\u8bc4\u5206\u5448\u73b0\u7684\u5916\u751f\u53d8\u5f02\u3002", "result": "\u5c3d\u7ba1\u4e24\u4e2a\u6a21\u578b\u5728\u603b\u4f53\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u5bf9\u540c\u4e00\u7533\u8bf7\u8005\u7ed9\u51fa\u4e0d\u540c\u7684\u8bc4\u5206\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u7814\u7a76\u53d1\u73b0\u51e0\u4e4e\u6ca1\u6709\u7b97\u6cd5\u4f9d\u8d56\u7684\u8bc1\u636e\uff1a\u5448\u73b0\u66f4\u6709\u5229\u7684\u8bc4\u5206\u5e76\u4e0d\u4f1a\u663e\u8457\u589e\u52a0\u7533\u8bf7\u8005\u88ab\u5f55\u53d6\u7684\u5e73\u5747\u6982\u7387\uff0c\u5373\u4f7f\u5f53\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5206\u6b67\u65f6\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u5728\u8fd9\u4e2a\u4e13\u5bb6\u51b3\u7b56\u7684\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\uff0c\u4eba\u7c7b\u51b3\u7b56\u5bf9\u7b97\u6cd5\u9884\u6d4b\u7684\u4efb\u610f\u53d8\u5316\u5177\u6709\u9ad8\u5ea6\u4e0d\u53d8\u6027\u3002\u8fd9\u5f3a\u8c03\u4e86\u4e13\u4e1a\u5224\u65ad\u529b\u548c\u5236\u5ea6\u80cc\u666f\u5728\u8c03\u8282\u7b97\u6cd5\u4e0d\u786e\u5b9a\u6027\u4e0b\u6e38\u6548\u5e94\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u8868\u660e\u51b3\u7b56\u8005\u80fd\u591f\u8d85\u8d8a\u7b97\u6cd5\u9884\u6d4b\u8fdb\u884c\u72ec\u7acb\u8bc4\u4f30\u3002"}}
{"id": "2602.00500", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00500", "abs": "https://arxiv.org/abs/2602.00500", "authors": ["Jianyi Zhou", "Yujie Wei", "Ruichen Zhen", "Bo Zhao", "Xiaobo Xia", "Rui Shao", "Xiu Su", "Shuo Yang"], "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning", "comment": null, "summary": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment.", "AI": {"tldr": "INFUSE\u662f\u9996\u4e2a\u9488\u5bf9VLA\u57fa\u7840\u6a21\u578b\u7684\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u5728\u7528\u6237\u4efb\u610f\u5fae\u8c03\u540e\u4ecd\u4fdd\u6301\u653b\u51fb\u6709\u6548\u6027\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u6ce8\u5165\u5230\u5fae\u8c03\u4e0d\u654f\u611f\u6a21\u5757\u4e2d\u5b9e\u73b0\u6301\u4e45\u6027\u653b\u51fb\u3002", "motivation": "VLA\u6a21\u578b\u5728\u5177\u8eabAI\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5b89\u5168\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u540e\u95e8\u653b\u51fb\u5728\u7269\u7406\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u5b9e\u9645\u5a01\u80c1\u3002\u73b0\u6709\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u5728\u7528\u6237\u4f7f\u7528\u5e72\u51c0\u6570\u636e\u8fdb\u884c\u4e0b\u6e38\u5fae\u8c03\u65f6\u5bb9\u6613\u88ab\u6e05\u9664\uff0c\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "INFUSE\u9996\u5148\u5206\u6790\u4e0d\u540c\u5fae\u8c03\u573a\u666f\u4e0b\u7684\u53c2\u6570\u654f\u611f\u6027\uff0c\u8bc6\u522b\u51fa\u5fae\u8c03\u4e0d\u654f\u611f\u6a21\u5757\uff08fine-tune-insensitive modules\uff09\uff0c\u7136\u540e\u5c06\u540e\u95e8\u6ce8\u5165\u5230\u8fd9\u4e9b\u7a33\u5b9a\u6a21\u5757\u4e2d\uff0c\u540c\u65f6\u51bb\u7ed3\u5176\u4ed6\u6a21\u5757\uff0c\u786e\u4fdd\u6076\u610f\u884c\u4e3a\u5728\u7528\u6237\u5e7f\u6cdb\u5fae\u8c03\u540e\u4ecd\u80fd\u6301\u7eed\u3002", "result": "\u5728\u591a\u79cdVLA\u67b6\u6784\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cINFUSE\u5728\u7528\u6237\u4fa7\u5fae\u8c03\u540e\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u4fdd\u630191.0%\u7684\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4fdd\u630179.8%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8eBadVLA\uff08\u5206\u522b\u4e3a38.8%\u548c36.6%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6\u6a21\u578b\u76f8\u5f53\u7684\u5e72\u51c0\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u4e2a\u5173\u952e\u5a01\u80c1\uff1a\u5728\u6a21\u578b\u5206\u53d1\u524d\u690d\u5165\u7684\u540e\u95e8\u53ef\u4ee5\u901a\u8fc7\u5fae\u8c03\u8fc7\u7a0b\u6301\u7eed\u5b58\u5728\uff0c\u5e76\u5728\u90e8\u7f72\u65f6\u4fdd\u6301\u6709\u6548\uff0c\u8fd9\u5bf9VLA\u6a21\u578b\u7684\u5b89\u5168\u6027\u63d0\u51fa\u4e86\u91cd\u8981\u8b66\u793a\u3002"}}
{"id": "2602.00823", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.00823", "abs": "https://arxiv.org/abs/2602.00823", "authors": ["Spyridon Syntakas", "Kostas Vlachos"], "title": "Ocean Current-Harnessing Stage-Gated MPC: Monotone Cost Shaping and Speed-to-Fly for Energy-Efficient AUV Navigation", "comment": null, "summary": "Autonomous Underwater Vehicles (AUVs) are a highly promising technology for ocean exploration and diverse offshore operations, yet their practical deployment is constrained by energy efficiency and endurance. To address this, we propose Current-Harnessing Stage-Gated MPC, which exploits ocean currents via a per-stage scalar which indicates the \"helpfulness\" of ocean currents. This scalar is computed along the prediction horizon to gate lightweight cost terms only where the ocean currents truly aids the control goal. The proposed cost terms, that are merged in the objective function, are (i) a Monotone Cost Shaping (MCS) term, a help-gated, non-worsening modification that relaxes along-track position error and provides a bounded translational energy rebate, guaranteeing the shaped objective is never larger than a set baseline, and (ii) a speed-to-fly (STF) cost component that increases the price of thrust and softly matches ground velocity to the ocean current, enabling near zero water-relative \"gliding\". All terms are C1 and integrate as a plug-and-play in MPC designs. Extensive simulations with the BlueROV2 model under realistic ocean current fields show that the proposed approach achieves substantially lower energy consumption than conventional predictive control while maintaining comparable arrival times and constraint satisfaction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u6d0b\u6d41\u8282\u80fd\u7684MPC\u65b9\u6cd5\uff0c\u901a\u8fc7\u9636\u6bb5\u95e8\u63a7\u673a\u5236\u4ec5\u5728\u6d0b\u6d41\u6709\u5229\u65f6\u6fc0\u6d3b\u8f7b\u91cf\u7ea7\u6210\u672c\u9879\uff0c\u663e\u8457\u964d\u4f4eAUV\u80fd\u8017", "motivation": "\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668(AUV)\u5728\u6d77\u6d0b\u63a2\u7d22\u548c\u79bb\u5cb8\u4f5c\u4e1a\u4e2d\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u80fd\u6e90\u6548\u7387\u548c\u7eed\u822a\u80fd\u529b\u7684\u9650\u5236\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5229\u7528\u6d0b\u6d41\u73af\u5883\u6765\u964d\u4f4e\u80fd\u8017\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCurrent-Harnessing Stage-Gated MPC\u65b9\u6cd5\uff1a1) \u5f15\u5165\u9636\u6bb5\u6807\u91cf\u8bc4\u4f30\u6d0b\u6d41\"\u5e2e\u52a9\u6027\"\uff0c\u4ec5\u5728\u6d0b\u6d41\u6709\u5229\u4e8e\u63a7\u5236\u76ee\u6807\u65f6\u6fc0\u6d3b\u8f7b\u91cf\u7ea7\u6210\u672c\u9879\uff1b2) \u5355\u8c03\u6210\u672c\u5851\u5f62(MCS)\u9879\uff1a\u57fa\u4e8e\u6d0b\u6d41\u5e2e\u52a9\u7684\u95e8\u63a7\u3001\u975e\u6076\u5316\u4fee\u6539\uff0c\u653e\u677e\u6cbf\u822a\u8ff9\u4f4d\u7f6e\u8bef\u5dee\u5e76\u63d0\u4f9b\u6709\u754c\u5e73\u79fb\u80fd\u91cf\u56de\u6263\uff1b3) \u901f\u5ea6\u98de\u884c(STF)\u6210\u672c\u9879\uff1a\u589e\u52a0\u63a8\u529b\u6210\u672c\u5e76\u8f6f\u5339\u914d\u5730\u901f\u4e0e\u6d0b\u6d41\uff0c\u5b9e\u73b0\u8fd1\u96f6\u6c34\u76f8\u5bf9\u901f\u5ea6\u7684\"\u6ed1\u7fd4\"\uff1b\u6240\u6709\u9879\u5747\u4e3aC1\u8fde\u7eed\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u96c6\u6210\u5230MPC\u8bbe\u8ba1\u4e2d\u3002", "result": "\u5728BlueROV2\u6a21\u578b\u548c\u771f\u5b9e\u6d0b\u6d41\u573a\u4e0b\u7684\u5e7f\u6cdb\u4eff\u771f\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u76f8\u8fd1\u5230\u8fbe\u65f6\u95f4\u548c\u7ea6\u675f\u6ee1\u8db3\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u7684\u80fd\u8017\u3002", "conclusion": "\u63d0\u51fa\u7684\u9636\u6bb5\u95e8\u63a7MPC\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u5229\u7528\u6d0b\u6d41\u73af\u5883\uff0c\u4e3aAUV\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8282\u80fd\u63a7\u5236\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u80fd\u6e90\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2602.01331", "categories": ["cs.MA", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01331", "abs": "https://arxiv.org/abs/2602.01331", "authors": ["Mingju Chen", "Guibin Zhang", "Heng Chang", "Yuchen Guo", "Shiji Zhou"], "title": "A-MapReduce: Executing Wide Search via Agentic MapReduce", "comment": "33 pages", "summary": "Contemporary large language model (LLM)-based multi-agent systems exhibit systematic advantages in deep research tasks, which emphasize iterative, vertically structured information seeking. However, when confronted with wide search tasks characterized by large-scale, breadth-oriented retrieval, existing agentic frameworks, primarily designed around sequential, vertically structured reasoning, remain stuck in expansive search objectives and inefficient long-horizon execution. To bridge this gap, we propose A-MapReduce, a MapReduce paradigm-inspired multi-agent execution framework that recasts wide search as a horizontally structured retrieval problem. Concretely, A-MapReduce implements parallel processing of massive retrieval targets through task-adaptive decomposition and structured result aggregation. Meanwhile, it leverages experiential memory to drive the continual evolution of query-conditioned task allocation and recomposition, enabling progressive improvement in large-scale wide-search regimes. Extensive experiments on five agentic benchmarks demonstrate that A-MapReduce is (i) high-performing, achieving state-of-the-art performance on WideSearch and DeepWideSearch, and delivering 5.11% - 17.50% average Item F1 improvements compared with strong baselines with OpenAI o3 or Gemini 2.5 Pro backbones; (ii) cost-effective and efficient, delivering superior cost-performance trade-offs and reducing running time by 45.8\\% compared to representative multi-agent baselines. The code is available at https://github.com/mingju-c/AMapReduce.", "code_url": "https://github.com/mingju-c/AMapReduce", "code_stars": 0, "code_last_update": "2026-02-01", "AI": {"tldr": "A-MapReduce\uff1a\u57fa\u4e8eMapReduce\u8303\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u6267\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u6c34\u5e73\u7ed3\u6784\u5316\u68c0\u7d22\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e7f\u5ea6\u641c\u7d22\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u5e76\u884c\u5904\u7406\u548c\u6e10\u8fdb\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9762\u5bf9\u5927\u89c4\u6a21\u3001\u5e7f\u5ea6\u5bfc\u5411\u7684\u68c0\u7d22\u4efb\u52a1\u65f6\uff0c\u73b0\u6709\u4ee5\u5782\u76f4\u7ed3\u6784\u5316\u63a8\u7406\u4e3a\u4e3b\u7684\u6846\u67b6\u5b58\u5728\u641c\u7d22\u76ee\u6807\u81a8\u80c0\u548c\u6267\u884c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faA-MapReduce\u6846\u67b6\uff0c\u5c06\u5e7f\u5ea6\u641c\u7d22\u91cd\u6784\u4e3a\u6c34\u5e73\u7ed3\u6784\u5316\u68c0\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u5206\u89e3\u548c\u7ed3\u6784\u5316\u7ed3\u679c\u805a\u5408\u5b9e\u73b0\u5927\u89c4\u6a21\u68c0\u7d22\u76ee\u6807\u7684\u5e76\u884c\u5904\u7406\uff0c\u5e76\u5229\u7528\u7ecf\u9a8c\u8bb0\u5fc6\u9a71\u52a8\u67e5\u8be2\u6761\u4ef6\u5316\u4efb\u52a1\u5206\u914d\u548c\u91cd\u7ec4\u7684\u6301\u7eed\u6f14\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cA-MapReduce\u5728WideSearch\u548cDeepWideSearch\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4OpenAI o3\u6216Gemini 2.5 Pro\u9aa8\u5e72\u7684\u5f3a\u57fa\u7ebf\uff0c\u5e73\u5747Item F1\u63d0\u53475.11%-17.50%\uff1b\u6210\u672c\u6548\u76ca\u9ad8\uff0c\u8fd0\u884c\u65f6\u95f4\u6bd4\u4ee3\u8868\u6027\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u51cf\u5c1145.8%\u3002", "conclusion": "A-MapReduce\u901a\u8fc7MapReduce\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5e7f\u5ea6\u641c\u7d22\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u6210\u672c\u6548\u76ca\u548c\u6267\u884c\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2602.00843", "categories": ["cs.NE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00843", "abs": "https://arxiv.org/abs/2602.00843", "authors": ["Claude Carlet", "Marko \u00d0urasevic", "Ermes Franch", "Domagoj Jakobovic", "Luca Mariot", "Stjepan Picek"], "title": "NegaBent, No Regrets: Evolving Spectrally Flat Boolean Functions", "comment": "9 pages, 2 figures", "summary": "Negabent Boolean functions are defined by having a flat magnitude spectrum under the nega-Hadamard transform. They exist in both even and odd dimensions, and the subclass of functions that are simultaneously bent and negabent (bent-negabent) has attracted interest due to the combined optimal periodic and negaperiodic spectral properties. In this work, we investigate how evolutionary algorithms can be used to evolve (bent-)negabent Boolean functions. Our experimental results indicate that evolutionary algorithms, especially genetic programming, are a suitable approach for evolving negabent Boolean functions, and we successfully evolve such functions in all dimensions we consider.", "AI": {"tldr": "\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\uff08\u7279\u522b\u662f\u9057\u4f20\u7f16\u7a0b\uff09\u6f14\u5316negabent\u5e03\u5c14\u51fd\u6570\uff0c\u6210\u529f\u5728\u6240\u6709\u8003\u8651\u7684\u7ef4\u5ea6\u4e0a\u751f\u6210\u4e86\u8fd9\u7c7b\u51fd\u6570", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u8fdb\u5316\u7b97\u6cd5\u6f14\u5316\u5177\u6709\u5e73\u5766\u8d1f\u54c8\u8fbe\u739b\u53d8\u6362\u5e45\u5ea6\u8c31\u7684negabent\u5e03\u5c14\u51fd\u6570\uff0c\u7279\u522b\u662f\u540c\u65f6\u6ee1\u8db3bent\u548cnegabent\u7279\u6027\u7684\u51fd\u6570\uff0c\u8fd9\u7c7b\u51fd\u6570\u5177\u6709\u6700\u4f18\u7684\u5468\u671f\u6027\u548c\u8d1f\u5468\u671f\u6027\u8c31\u7279\u6027", "method": "\u91c7\u7528\u8fdb\u5316\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u9057\u4f20\u7f16\u7a0b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f14\u5316\u8fc7\u7a0b\u751f\u6210negabent\u5e03\u5c14\u51fd\u6570", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8fdb\u5316\u7b97\u6cd5\u662f\u6f14\u5316negabent\u5e03\u5c14\u51fd\u6570\u7684\u5408\u9002\u65b9\u6cd5\uff0c\u6210\u529f\u5728\u6240\u6709\u8003\u8651\u7684\u7ef4\u5ea6\u4e0a\u751f\u6210\u4e86\u8fd9\u7c7b\u51fd\u6570", "conclusion": "\u8fdb\u5316\u7b97\u6cd5\uff0c\u5c24\u5176\u662f\u9057\u4f20\u7f16\u7a0b\uff0c\u80fd\u591f\u6709\u6548\u6f14\u5316negabent\u5e03\u5c14\u51fd\u6570\uff0c\u4e3a\u751f\u6210\u8fd9\u7c7b\u5177\u6709\u7279\u6b8a\u8c31\u7279\u6027\u7684\u51fd\u6570\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5"}}
{"id": "2602.02091", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2602.02091", "abs": "https://arxiv.org/abs/2602.02091", "authors": ["Andrej Dudenhefner"], "title": "Mechanized Undecidability of Higher-order beta-Matching (Extended Version)", "comment": null, "summary": "Higher-order beta-matching is the following decision problem: given two simply typed lambda-terms, can the first term be instantiated to be beta-equivalent to the second term? This problem was formulated by Huet in the 1970s and shown undecidable by Loader in 2003 by reduction from lambda-definability.\n  The present work provides a novel undecidability proof for higher-order beta-matching, in an effort to verify this result by means of a proof assistant. Rather than starting from lambda-definability, the presented proof encodes a restricted form of string rewriting as higher-order beta-matching. The particular approach is similar to Urzyczyn's undecidability result for intersection type inhabitation.\n  The presented approach has several advantages. First, the proof is simpler to verify in full detail due to the simple form of rewriting systems, which serve as a starting point. Second, undecidability of the considered problem in string rewriting is already certified using the Coq proof assistant. As a consequence, we obtain a certified many-one reduction from the Halting Problem to higher-order beta-matching. Third, the presented approach identifies a uniform construction which shows undecidability of higher-order beta-matching, lambda-definability, and intersection type inhabitation.\n  The presented undecidability proof is mechanized in the Coq proof assistant and contributed to the existing Coq Library of Undecidability Proofs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u9ad8\u9636\u03b2\u5339\u914d\u95ee\u9898\u4e0d\u53ef\u5224\u5b9a\u6027\u7684\u65b0\u8bc1\u660e\uff0c\u901a\u8fc7\u5c06\u53d7\u9650\u5b57\u7b26\u4e32\u91cd\u5199\u7f16\u7801\u4e3a\u9ad8\u9636\u03b2\u5339\u914d\uff0c\u5e76\u5728Coq\u8bc1\u660e\u52a9\u624b\u4e2d\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "motivation": "\u9a8c\u8bc1Loader\u57282003\u5e74\u8bc1\u660e\u7684\u9ad8\u9636\u03b2\u5339\u914d\u4e0d\u53ef\u5224\u5b9a\u6027\u7ed3\u679c\uff0c\u901a\u8fc7\u8bc1\u660e\u52a9\u624b\u63d0\u4f9b\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u8bc1\u660e\uff0c\u540c\u65f6\u5efa\u7acb\u9ad8\u9636\u03b2\u5339\u914d\u3001\u03bb\u53ef\u5b9a\u4e49\u6027\u548c\u4ea4\u96c6\u7c7b\u578b\u53ef\u5c45\u6027\u4e4b\u95f4\u7684\u7edf\u4e00\u6784\u9020\u8054\u7cfb\u3002", "method": "\u5c06\u53d7\u9650\u5b57\u7b26\u4e32\u91cd\u5199\u7cfb\u7edf\u7f16\u7801\u4e3a\u9ad8\u9636\u03b2\u5339\u914d\u95ee\u9898\uff0c\u91c7\u7528\u7c7b\u4f3cUrzyczyn\u4ea4\u96c6\u7c7b\u578b\u53ef\u5c45\u6027\u4e0d\u53ef\u5224\u5b9a\u6027\u8bc1\u660e\u7684\u65b9\u6cd5\uff0c\u5728Coq\u8bc1\u660e\u52a9\u624b\u4e2d\u5b9e\u73b0\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u63d0\u4f9b\u4e86\u9ad8\u9636\u03b2\u5339\u914d\u4e0d\u53ef\u5224\u5b9a\u6027\u7684\u65b0\u8bc1\u660e\uff0c\u8be5\u8bc1\u660e\u5df2\u5728Coq\u4e2d\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u5e76\u8d21\u732e\u7ed9Coq\u4e0d\u53ef\u5224\u5b9a\u6027\u8bc1\u660e\u5e93\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u4ece\u505c\u673a\u95ee\u9898\u5230\u9ad8\u9636\u03b2\u5339\u914d\u7684\u8ba4\u8bc1\u591a\u4e00\u5f52\u7ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u9ad8\u9636\u03b2\u5339\u914d\u7684\u4e0d\u53ef\u5224\u5b9a\u6027\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u9ad8\u9636\u03b2\u5339\u914d\u3001\u03bb\u53ef\u5b9a\u4e49\u6027\u548c\u4ea4\u96c6\u7c7b\u578b\u53ef\u5c45\u6027\u7684\u4e0d\u53ef\u5224\u5b9a\u6027\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.00243", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00243", "abs": "https://arxiv.org/abs/2602.00243", "authors": ["Ashley Hua", "Adya Daruka", "Yang Hong", "Sharifa Sultana"], "title": "\"OpenBloom\": A Question-Based LLM Tool to Support Stigma Reduction in Reproductive Well-Being", "comment": null, "summary": "Reproductive well-being education remains widely stigmatized across diverse cultural contexts, constraining how individuals access and interpret reproductive health knowledge. We designed and evaluated OpenBloom, a stigma-sensitive, AI-mediated system that uses LLMs to transform reproductive health articles into reflective, question-based learning prompts. We employed OpenBloom as a design probe, aiming to explore the emerging challenges of reproductive well-being stigma through LLMs. Through surveys, semi-structured interviews, and focus group discussions, we examine how sociocultural stigma shapes participants' engagements with AI-generated questions and the opportunities of inquiry-based reproductive health education. Our findings identify key design considerations for stigma-sensitive LLM, including empathetic framing, inclusive language, values-based reflection, and explicit representation of marginalized identities. However, while current LLM outputs largely meet expectations for cultural sensitivity and non-offensiveness, they default to superficial rephrasing and factual recall rather than critical reflection. This guides well-being HCI design in sensitive health domains toward culturally grounded, participatory workflows.", "AI": {"tldr": "OpenBloom\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u751f\u6b96\u5065\u5eb7\u6559\u80b2\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u6587\u7ae0\u8f6c\u5316\u4e3a\u53cd\u601d\u6027\u95ee\u9898\u6765\u5e94\u5bf9\u751f\u6b96\u5065\u5eb7\u6559\u80b2\u7684\u6c61\u540d\u5316\u95ee\u9898\uff0c\u7814\u7a76\u53d1\u73b0LLM\u8f93\u51fa\u867d\u6587\u5316\u654f\u611f\u4f46\u7f3a\u4e4f\u6df1\u5ea6\u53cd\u601d\u3002", "motivation": "\u751f\u6b96\u5065\u5eb7\u6559\u80b2\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u666e\u904d\u5b58\u5728\u6c61\u540d\u5316\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u4eba\u4eec\u83b7\u53d6\u548c\u89e3\u8bfb\u751f\u6b96\u5065\u5eb7\u77e5\u8bc6\u7684\u65b9\u5f0f\u3002\u7814\u7a76\u8005\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7LLM\u6280\u672f\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u5e76\u4e86\u89e3\u6c61\u540d\u5316\u5982\u4f55\u5f71\u54cd\u4eba\u4eec\u4e0eAI\u751f\u6210\u5185\u5bb9\u7684\u4e92\u52a8\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86OpenBloom\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6c61\u540d\u654f\u611f\u7cfb\u7edf\uff0c\u80fd\u5c06\u751f\u6b96\u5065\u5eb7\u6587\u7ae0\u8f6c\u5316\u4e3a\u53cd\u601d\u6027\u3001\u57fa\u4e8e\u95ee\u9898\u7684\u5b66\u4e60\u63d0\u793a\u3002\u91c7\u7528\u8bbe\u8ba1\u63a2\u9488\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u3001\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u6765\u7814\u7a76\u53c2\u4e0e\u8005\u4e0eAI\u751f\u6210\u95ee\u9898\u7684\u4e92\u52a8\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u8f93\u51fa\u5728\u6587\u5316\u654f\u611f\u6027\u548c\u975e\u5192\u72af\u6027\u65b9\u9762\u57fa\u672c\u7b26\u5408\u9884\u671f\uff0c\u4f46\u503e\u5411\u4e8e\u8868\u9762\u7684\u91cd\u8ff0\u548c\u4e8b\u5b9e\u56de\u5fc6\u800c\u975e\u6279\u5224\u6027\u53cd\u601d\u3002\u8bc6\u522b\u51fa\u6c61\u540d\u654f\u611fLLM\u7684\u5173\u952e\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\uff1a\u5171\u60c5\u6846\u67b6\u3001\u5305\u5bb9\u6027\u8bed\u8a00\u3001\u57fa\u4e8e\u4ef7\u503c\u89c2\u7684\u53cd\u601d\u4ee5\u53ca\u8fb9\u7f18\u5316\u8eab\u4efd\u7684\u660e\u786e\u8868\u5f81\u3002", "conclusion": "\u867d\u7136\u5f53\u524dLLM\u5728\u6587\u5316\u654f\u611f\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u6df1\u5ea6\u53cd\u601d\u80fd\u529b\u3002\u8fd9\u6307\u5bfc\u4e86\u654f\u611f\u5065\u5eb7\u9886\u57df\u7684\u798f\u7949HCI\u8bbe\u8ba1\u5e94\u671d\u5411\u6587\u5316\u624e\u6839\u3001\u53c2\u4e0e\u5f0f\u7684\u5de5\u4f5c\u6d41\u7a0b\u53d1\u5c55\u3002"}}
{"id": "2602.00514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00514", "abs": "https://arxiv.org/abs/2602.00514", "authors": ["Yaohua Liu", "Binkai Ou", "Zicheng Qiu", "Ce Hao", "Hengjun Zhang"], "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation", "comment": null, "summary": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks.", "AI": {"tldr": "LVTG\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u89c6\u89c9\u89e6\u89c9\u5939\u722a\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u89e6\u89c9\u611f\u77e5\u533a\u57df\u548c\u66f4\u5927\u5f00\u5408\u89d2\u5ea6\u5b9e\u73b0\u5bf9\u5927\u800c\u91cd\u65e5\u5e38\u7269\u4f53\u7684\u7a33\u5b9a\u6293\u53d6\uff0c\u91c7\u7528CLIP\u542f\u53d1\u7684\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u89c6\u89c9\u4e0e\u89e6\u89c9\u8868\u5f81\uff0c\u663e\u8457\u63d0\u5347\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u89e6\u89c9\u4f20\u611f\u5668\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u611f\u77e5\u8303\u56f4\u6709\u9650\u3001\u53ef\u9760\u6027\u4e0d\u8db3\u3001\u6210\u672c\u6548\u76ca\u4e0d\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7a33\u5b9a\u3001\u9c81\u68d2\u4e14\u9ad8\u6548\u8fdb\u884c\u7269\u7406\u4ea4\u4e92\u7684\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u8bbe\u8ba1LVTG\u4f4e\u6210\u672c\u89c6\u89c9\u89e6\u89c9\u5939\u722a\uff1a\u589e\u5f3a\u89e6\u89c9\u611f\u77e5\u533a\u57df\u3001\u66f4\u5927\u5f00\u5408\u89d2\u5ea6\u3001\u9ad8\u8010\u78e8\u6750\u6599\u8868\u9762\u76ae\u80a4\u3001\u6a21\u5757\u5316\u8bbe\u8ba1\uff1b2. \u91c7\u7528CLIP\u542f\u53d1\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5bf9\u9f50\u89e6\u89c9\u5d4c\u5165\u4e0e\u5bf9\u5e94\u89c6\u89c9\u89c2\u5bdf\uff0c\u5efa\u7acb\u5171\u4eab\u8de8\u6a21\u6001\u8868\u5f81\u7a7a\u95f4\uff1b3. \u5c06\u878d\u5408\u8868\u5f81\u7528\u4e8eAction Chunking Transformer (ACT)\u7b56\u7565\uff0c\u63d0\u5347\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u6027\u80fd\u3002", "result": "\u76f8\u6bd4\u539f\u59cbACT\u65b9\u6cd5\uff0c\u91c7\u7528LVTG\u4e0e\u9884\u8bad\u7ec3\u7684\u65b9\u6848\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002\u5939\u722a\u80fd\u591f\u66f4\u6709\u6548\u7a33\u5b9a\u5730\u6293\u53d6\u66f4\u5927\u66f4\u91cd\u7684\u65e5\u5e38\u7269\u4f53\uff0c\u89c6\u89c9\u4e0e\u89e6\u89c9\u53cd\u9988\u878d\u5408\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u9ad8\u4fdd\u771f\u611f\u77e5\u6570\u636e\u3002", "conclusion": "LVTG\u901a\u8fc7\u521b\u65b0\u7684\u4f4e\u6210\u672c\u89c6\u89c9\u89e6\u89c9\u5939\u722a\u8bbe\u8ba1\u548c\u8de8\u6a21\u6001\u8868\u5f81\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a5\u89e6\u4e30\u5bcc\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u64cd\u4f5c\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01189", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01189", "abs": "https://arxiv.org/abs/2602.01189", "authors": ["Astik Srivastava", "Thomas J Chackenkulam. Bitla Bhanu Teja", "Antony Thomas", "Madhava Krishna"], "title": "SPOT: Spatio-Temporal Obstacle-free Trajectory Planning for UAVs in an Unknown Dynamic Environment", "comment": null, "summary": "We address the problem of reactive motion planning for quadrotors operating in unknown environments with dynamic obstacles. Our approach leverages a 4-dimensional spatio-temporal planner, integrated with vision-based Safe Flight Corridor (SFC) generation and trajectory optimization. Unlike prior methods that rely on map fusion, our framework is mapless, enabling collision avoidance directly from perception while reducing computational overhead. Dynamic obstacles are detected and tracked using a vision-based object segmentation and tracking pipeline, allowing robust classification of static versus dynamic elements in the scene. To further enhance robustness, we introduce a backup planning module that reactively avoids dynamic obstacles when no direct path to the goal is available, mitigating the risk of collisions during deadlock situations. We validate our method extensively in both simulation and real-world hardware experiments, and benchmark it against state-of-the-art approaches, showing significant advantages for reactive UAV navigation in dynamic, unknown environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u672a\u77e5\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u53cd\u5e94\u5f0f\u8fd0\u52a8\u89c4\u5212\u7684\u65e0\u5730\u56fe\u6846\u67b6\uff0c\u7ed3\u54084D\u65f6\u7a7a\u89c4\u5212\u5668\u3001\u89c6\u89c9\u5b89\u5168\u98de\u884c\u8d70\u5eca\u751f\u6210\u548c\u8f68\u8ff9\u4f18\u5316\uff0c\u901a\u8fc7\u52a8\u6001\u969c\u788d\u7269\u68c0\u6d4b\u8ddf\u8e2a\u548c\u5907\u4efd\u89c4\u5212\u6a21\u5757\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u9762\u5bf9\u52a8\u6001\u969c\u788d\u7269\u7684\u53cd\u5e94\u5f0f\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5730\u56fe\u878d\u5408\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e14\u96be\u4ee5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u907f\u969c\u3002\u9700\u8981\u4e00\u79cd\u65e0\u5730\u56fe\u7684\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u611f\u77e5\u4fe1\u606f\u5b9e\u73b0\u78b0\u649e\u907f\u514d\uff0c\u540c\u65f6\u5904\u7406\u52a8\u6001\u969c\u788d\u7269\u3002", "method": "1. \u91c7\u75284\u7ef4\u65f6\u7a7a\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u89c6\u89c9\u5b89\u5168\u98de\u884c\u8d70\u5eca\u751f\u6210\u548c\u8f68\u8ff9\u4f18\u5316\uff1b2. \u65e0\u5730\u56fe\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u611f\u77e5\u5b9e\u73b0\u78b0\u649e\u907f\u514d\uff1b3. \u57fa\u4e8e\u89c6\u89c9\u7684\u76ee\u6807\u5206\u5272\u548c\u8ddf\u8e2a\u7ba1\u9053\u68c0\u6d4b\u548c\u8ddf\u8e2a\u52a8\u6001\u969c\u788d\u7269\uff1b4. \u5f15\u5165\u5907\u4efd\u89c4\u5212\u6a21\u5757\uff0c\u5728\u65e0\u6cd5\u76f4\u63a5\u5230\u8fbe\u76ee\u6807\u65f6\u53cd\u5e94\u5f0f\u907f\u5f00\u52a8\u6001\u969c\u788d\u7269\uff1b5. \u533a\u5206\u573a\u666f\u4e2d\u7684\u9759\u6001\u548c\u52a8\u6001\u5143\u7d20\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u52a8\u6001\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u53cd\u5e94\u5f0f\u65e0\u4eba\u673a\u5bfc\u822a\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u907f\u514d\u78b0\u649e\u5e76\u5904\u7406\u6b7b\u9501\u60c5\u51b5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u5730\u56fe\u53cd\u5e94\u5f0f\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u672a\u77e5\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\u3002\u901a\u8fc7\u7ed3\u54084D\u65f6\u7a7a\u89c4\u5212\u3001\u89c6\u89c9\u611f\u77e5\u548c\u5907\u4efd\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u52a8\u6001\u969c\u788d\u7269\u907f\u8ba9\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02025", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02025", "abs": "https://arxiv.org/abs/2602.02025", "authors": ["Serafeim Papadias", "Kostas Patroumpas", "Dimitrios Skoutas"], "title": "Hippasus: Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data", "comment": "13 pages, 7 figures, 9 tables", "summary": "Machine learning models depend critically on feature quality, yet useful features are often scattered across multiple relational tables. Feature augmentation enriches a base table by discovering and integrating features from related tables through join operations. However, scaling this process to complex schemas with many tables and multi-hop paths remains challenging. Feature augmentation must address three core tasks: identify promising join paths that connect the base table to candidate tables, execute these joins to materialize augmented data, and select the most informative features from the results. Existing approaches face a fundamental tradeoff between effectiveness and efficiency: achieving high accuracy requires exploring many candidate paths, but exhaustive exploration is computationally prohibitive. Some methods compromise by considering only immediate neighbors, limiting their effectiveness, while others employ neural models that require expensive training data and introduce scalability limitations. We present Hippasus, a modular framework that achieves both goals through three key contributions. First, we combine lightweight statistical signals with semantic reasoning from Large Language Models to prune unpromising join paths before execution, focusing computational resources on high-quality candidates. Second, we employ optimized multi-way join algorithms and consolidate features from multiple paths, substantially reducing execution time. Third, we integrate LLM-based semantic understanding with statistical measures to select features that are both semantically meaningful and empirically predictive. Our experimental evaluation on publicly available datasets shows that Hippasus substantially improves feature augmentation accuracy by up to 26.8% over state-of-the-art baselines while also offering high runtime performance.", "AI": {"tldr": "Hippasus\u662f\u4e00\u4e2a\u7528\u4e8e\u7279\u5f81\u589e\u5f3a\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7edf\u8ba1\u4fe1\u53f7\u4e0eLLM\u8bed\u4e49\u63a8\u7406\u6765\u9ad8\u6548\u53d1\u73b0\u548c\u96c6\u6210\u591a\u8868\u7279\u5f81\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7279\u5f81\uff0c\u4f46\u6709\u7528\u7279\u5f81\u5e38\u5206\u6563\u5728\u591a\u4e2a\u5173\u7cfb\u8868\u4e2d\u3002\u73b0\u6709\u7279\u5f81\u589e\u5f3a\u65b9\u6cd5\u9762\u4e34\u6548\u679c\u4e0e\u6548\u7387\u7684\u6743\u8861\uff1a\u9ad8\u51c6\u786e\u6027\u9700\u8981\u63a2\u7d22\u5927\u91cf\u5019\u9009\u8fde\u63a5\u8def\u5f84\uff0c\u4f46\u7a77\u4e3e\u63a2\u7d22\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff1b\u4ec5\u8003\u8651\u76f4\u63a5\u90bb\u5c45\u7684\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u800c\u57fa\u4e8e\u795e\u7ecf\u6a21\u578b\u7684\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u8bad\u7ec3\u6570\u636e\u4e14\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3002", "method": "Hippasus\u91c7\u7528\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u7ed3\u5408\u8f7b\u91cf\u7ea7\u7edf\u8ba1\u4fe1\u53f7\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\uff0c\u5728\u6267\u884c\u524d\u526a\u679d\u65e0\u5e0c\u671b\u7684\u8fde\u63a5\u8def\u5f84\uff1b2) \u4f7f\u7528\u4f18\u5316\u7684\u591a\u8def\u8fde\u63a5\u7b97\u6cd5\u5e76\u6574\u5408\u591a\u8def\u5f84\u7279\u5f81\uff0c\u5927\u5e45\u51cf\u5c11\u6267\u884c\u65f6\u95f4\uff1b3) \u96c6\u6210LLM\u8bed\u4e49\u7406\u89e3\u4e0e\u7edf\u8ba1\u5ea6\u91cf\uff0c\u9009\u62e9\u65e2\u8bed\u4e49\u76f8\u5173\u53c8\u5177\u6709\u9884\u6d4b\u6027\u7684\u7279\u5f81\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cHippasus\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u5f81\u589e\u5f3a\u51c6\u786e\u6027\u63d0\u5347\u9ad8\u8fbe26.8%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "conclusion": "Hippasus\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u589e\u5f3a\u4e2d\u6548\u679c\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u7ed3\u5408\u7edf\u8ba1\u65b9\u6cd5\u4e0e\u8bed\u4e49\u63a8\u7406\uff0c\u5728\u590d\u6742\u591a\u8868\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u7279\u5f81\u53d1\u73b0\u4e0e\u96c6\u6210\u3002"}}
{"id": "2602.00978", "categories": ["cs.NE", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2602.00978", "abs": "https://arxiv.org/abs/2602.00978", "authors": ["Nam H. Le"], "title": "Organismal Agency and Rapid Adaptation: The Phenopoiesis Algorithm for Phenotype-First Evolution", "comment": "22 pages, 2 figures,", "summary": "Evolutionary success depends on the capacity to adapt: organisms must respond to environmental challenges through both genetic innovation and lifetime learning. The gene-centric paradigm attributes evolutionary causality exclusively to genes, while Denis Noble's phenotype-first framework argues that organisms are active agents capable of interpreting genetic resources, learning from experience, and shaping their own development. However, this framework has remained philosophically intuitive but algorithmically opaque.\n  We show for the first time that organismal agency can be implemented as a concrete computational process through heritable phenotypic patterns. We introduce the Phenopoiesis Algorithm, where organisms inherit not just genes but also successful phenotypic patterns discovered during lifetime learning. Through experiments in changing environments, these pattern-inheriting organisms achieve 3.4 times faster adaptation compared to gene-centric models. Critically, these gains require cross-generational inheritance of learned patterns rather than within-lifetime learning alone.\n  We conclude that organismal agency is not a philosophical abstraction but an algorithmic mechanism with measurable adaptive value. The mechanism works through compositional reuse: organisms discover how to compose primitive elements into solutions, encode those compositional recipes, and transmit them to offspring. Evolution operates across multiple timescales -- fast, reversible phenotypic inheritance and slow, permanent genetic inheritance -- providing adaptive flexibility that single-channel mechanisms cannot achieve.", "AI": {"tldr": "\u63d0\u51fa\u8868\u578b\u4f18\u5148\u7b97\u6cd5\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9057\u4f20\u7684\u8868\u578b\u6a21\u5f0f\u5b9e\u73b0\u751f\u7269\u4f53\u80fd\u52a8\u6027\uff0c\u76f8\u6bd4\u57fa\u56e0\u4e2d\u5fc3\u6a21\u578b\u83b7\u5f973.4\u500d\u9002\u5e94\u901f\u5ea6\u63d0\u5347", "motivation": "\u4f20\u7edf\u57fa\u56e0\u4e2d\u5fc3\u8303\u5f0f\u5c06\u8fdb\u5316\u56e0\u679c\u6027\u5b8c\u5168\u5f52\u56e0\u4e8e\u57fa\u56e0\uff0c\u800c\u8868\u578b\u4f18\u5148\u6846\u67b6\u8ba4\u4e3a\u751f\u7269\u4f53\u662f\u80fd\u591f\u89e3\u91ca\u9057\u4f20\u8d44\u6e90\u3001\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u5e76\u5851\u9020\u81ea\u8eab\u53d1\u80b2\u7684\u4e3b\u52a8\u4e3b\u4f53\u3002\u7136\u800c\u8be5\u6846\u67b6\u4e00\u76f4\u505c\u7559\u5728\u54f2\u5b66\u76f4\u89c9\u5c42\u9762\uff0c\u7f3a\u4e4f\u7b97\u6cd5\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u8868\u578b\u751f\u6210\u7b97\u6cd5\uff0c\u751f\u7269\u4f53\u4e0d\u4ec5\u7ee7\u627f\u57fa\u56e0\uff0c\u8fd8\u7ee7\u627f\u5728\u751f\u547d\u5468\u671f\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u6210\u529f\u7684\u8868\u578b\u6a21\u5f0f\u3002\u901a\u8fc7\u53d8\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u7ee7\u627f\u751f\u7269\u4f53\u80fd\u591f\u5b9e\u73b0\u8de8\u4ee3\u5b66\u4e60\u3002", "result": "\u6a21\u5f0f\u7ee7\u627f\u751f\u7269\u4f53\u76f8\u6bd4\u57fa\u56e0\u4e2d\u5fc3\u6a21\u578b\u83b7\u5f973.4\u500d\u66f4\u5feb\u7684\u9002\u5e94\u901f\u5ea6\u3002\u5173\u952e\u53d1\u73b0\u662f\u8de8\u4ee3\u7ee7\u627f\u5b66\u4e60\u6a21\u5f0f\u800c\u975e\u4ec5\u751f\u547d\u5468\u671f\u5185\u5b66\u4e60\u624d\u80fd\u83b7\u5f97\u8fd9\u4e9b\u4f18\u52bf\u3002", "conclusion": "\u751f\u7269\u4f53\u80fd\u52a8\u6027\u4e0d\u662f\u54f2\u5b66\u62bd\u8c61\uff0c\u800c\u662f\u5177\u6709\u53ef\u6d4b\u91cf\u9002\u5e94\u4ef7\u503c\u7684\u7b97\u6cd5\u673a\u5236\u3002\u8be5\u673a\u5236\u901a\u8fc7\u7ec4\u5408\u91cd\u7528\u5b9e\u73b0\uff1a\u751f\u7269\u4f53\u53d1\u73b0\u5982\u4f55\u5c06\u539f\u59cb\u5143\u7d20\u7ec4\u5408\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u7f16\u7801\u8fd9\u4e9b\u7ec4\u5408\u914d\u65b9\u5e76\u4f20\u9012\u7ed9\u540e\u4ee3\u3002\u8fdb\u5316\u5728\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fd0\u4f5c\u2014\u2014\u5feb\u901f\u53ef\u9006\u7684\u8868\u578b\u7ee7\u627f\u548c\u7f13\u6162\u6c38\u4e45\u7684\u57fa\u56e0\u7ee7\u627f\u2014\u2014\u63d0\u4f9b\u5355\u901a\u9053\u673a\u5236\u65e0\u6cd5\u5b9e\u73b0\u7684\u9002\u5e94\u7075\u6d3b\u6027\u3002"}}
{"id": "2602.02218", "categories": ["cs.LO", "math.CT"], "pdf": "https://arxiv.org/pdf/2602.02218", "abs": "https://arxiv.org/abs/2602.02218", "authors": ["Daniel Gratzer", "Jonathan Weinberger", "Ulrik Buchholtz"], "title": "The $\\infty$-category of $\\infty$-categories in simplicial type theory", "comment": null, "summary": "Simplicial type theory (STT) was introduced by Riehl and Shulman to leverage homotopy type theory to prove results about $(\\infty,1)$-categories. Initial work on simplicial type theory focused on \"formal\" arguments in higher category theory and, in particular, no non-trivial examples of $\\infty$-category theory were constructible within STT. More recent work has changed this state of affairs by applying techniques developed initial for cubical type theory to construct the $\\infty$-category of spaces. We complete this process by constructing the $\\infty$-category of $\\infty$-categories, recovering one of the main foundational results of $\\infty$-category theory (straightening--unstraightening) purely type-theoretically. We also show how this construction enables new examples of the directed version of the structure identity principle, the structure homomorphism principle.", "AI": {"tldr": "\u5728\u5355\u7eaf\u578b\u7c7b\u578b\u8bba\u4e2d\u6784\u9020\u221e-\u8303\u7574\u7684\u221e-\u8303\u7574\uff0c\u5b9e\u73b0\u4e86\u76f4\u5316-\u975e\u76f4\u5316\u5b9a\u7406\u7684\u7c7b\u578b\u8bba\u8bc1\u660e", "motivation": "\u5355\u7eaf\u578b\u7c7b\u578b\u8bba\u6700\u521d\u7f3a\u4e4f\u975e\u5e73\u51e1\u7684\u221e-\u8303\u7574\u8bba\u5b9e\u4f8b\uff0c\u9700\u8981\u6784\u5efa\u5177\u4f53\u7684\u221e-\u8303\u7574\u7406\u8bba\u6846\u67b6", "method": "\u5e94\u7528\u6700\u521d\u4e3a\u7acb\u65b9\u578b\u7c7b\u578b\u8bba\u5f00\u53d1\u7684\u6280\u672f\uff0c\u5728\u5355\u7eaf\u578b\u7c7b\u578b\u8bba\u4e2d\u6784\u9020\u221e-\u8303\u7574\u7684\u221e-\u8303\u7574", "result": "\u6210\u529f\u6784\u9020\u4e86\u221e-\u8303\u7574\u7684\u221e-\u8303\u7574\uff0c\u5b9e\u73b0\u4e86\u76f4\u5316-\u975e\u76f4\u5316\u5b9a\u7406\u7684\u7eaf\u7c7b\u578b\u8bba\u8bc1\u660e\uff0c\u5e76\u5c55\u793a\u4e86\u7ed3\u6784\u540c\u6001\u539f\u7406\u7684\u65b0\u5b9e\u4f8b", "conclusion": "\u8be5\u5de5\u4f5c\u5b8c\u5584\u4e86\u5355\u7eaf\u578b\u7c7b\u578b\u8bba\u4e2d\u221e-\u8303\u7574\u7406\u8bba\u7684\u6784\u5efa\uff0c\u4e3a\u9ad8\u9636\u8303\u7574\u8bba\u63d0\u4f9b\u4e86\u7c7b\u578b\u8bba\u57fa\u7840"}}
{"id": "2602.00204", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00204", "abs": "https://arxiv.org/abs/2602.00204", "authors": ["Waleed Khan Mohammed", "Zahirul Arief Irfan Bin Shahrul Anuar", "Mousa Sufian Mousa Mitani", "Hezerul Abdul Karim", "Nouar AlDahoul"], "title": "Semantic-Aware Advanced Persistent Threat Detection Using Autoencoders on LLM-Encoded System Logs", "comment": null, "summary": "Advanced Persistent Threats (APTs) are among the most challenging cyberattacks to detect. They are carried out by highly skilled attackers who carefully study their targets and operate in a stealthy, long-term manner. Because APTs exhibit \"low-and-slow\" behavior, traditional statistical methods and shallow machine learning techniques often fail to detect them. Previous research on APT detection has explored machine learning approaches and provenance graph analysis. However, provenance-based methods often fail to capture the semantic intent behind system activities. This paper proposes a novel anomaly detection approach that leverages semantic embeddings generated by Large Language Models (LLMs). The method enhances APT detection by extracting meaningful semantic representations from unstructured system log data. First, raw system logs are transformed into high-dimensional semantic embeddings using a pre-trained transformer model. These embeddings are then analyzed using an Autoencoder (AE) to identify anomalous and potentially malicious patterns. The proposed method is evaluated using the DARPA Transparent Computing (TC) dataset, which contains realistic APT attack scenarios generated by red teams in live environments. Experimental results show that the AE trained on LLM-derived embeddings outperforms widely used unsupervised baseline methods, including Isolation Forest (IForest), One-Class Support Vector Machine (OC-SVM), and Principal Component Analysis (PCA). Performance is measured using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), where the proposed approach consistently achieves superior results, even in complex threat scenarios. These findings highlight the importance of semantic understanding in detecting non-linear and stealthy attack behaviors that are often missed by conventional detection techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u5d4c\u5165\u7684\u65b0\u578bAPT\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7cfb\u7edf\u65e5\u5fd7\u8f6c\u6362\u4e3a\u8bed\u4e49\u8868\u793a\u5e76\u5229\u7528\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u5f02\u5e38\u6a21\u5f0f\uff0c\u5728DARPA\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65e0\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u7ea7\u6301\u7eed\u6027\u5a01\u80c1\uff08APTs\uff09\u5177\u6709\"\u4f4e\u800c\u6162\"\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u548c\u6d45\u5c42\u673a\u5668\u5b66\u4e60\u6280\u672f\u96be\u4ee5\u68c0\u6d4b\u3002\u73b0\u6709\u6eaf\u6e90\u56fe\u5206\u6790\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u7cfb\u7edf\u6d3b\u52a8\u80cc\u540e\u7684\u8bed\u4e49\u610f\u56fe\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u5d4c\u5165\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u9884\u8bad\u7ec3transformer\u6a21\u578b\u5c06\u539f\u59cb\u7cfb\u7edf\u65e5\u5fd7\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u8bed\u4e49\u5d4c\u5165\uff1b2\uff09\u5229\u7528\u81ea\u7f16\u7801\u5668\u5206\u6790\u8fd9\u4e9b\u5d4c\u5165\uff0c\u8bc6\u522b\u5f02\u5e38\u548c\u6f5c\u5728\u6076\u610f\u6a21\u5f0f\u3002", "result": "\u5728DARPA\u900f\u660e\u8ba1\u7b97\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u5d4c\u5165\u7684\u81ea\u7f16\u7801\u5668\u5728AUC-ROC\u6307\u6807\u4e0a\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u65e0\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\uff08\u9694\u79bb\u68ee\u6797\u3001\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\u3001\u4e3b\u6210\u5206\u5206\u6790\uff09\uff0c\u5728\u590d\u6742\u5a01\u80c1\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8bed\u4e49\u7406\u89e3\u5bf9\u4e8e\u68c0\u6d4b\u4f20\u7edf\u65b9\u6cd5\u5e38\u9057\u6f0f\u7684\u975e\u7ebf\u6027\u548c\u9690\u853d\u653b\u51fb\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u57fa\u4e8eLLM\u8bed\u4e49\u5d4c\u5165\u7684\u65b9\u6cd5\u4e3aAPT\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.00248", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00248", "abs": "https://arxiv.org/abs/2602.00248", "authors": ["Varun Srivastava", "Fan Lei", "Alan M. MacEachren", "Ross Maciejewski"], "title": "The Impact of Uncertainty Visualization on Trust in Thematic Maps", "comment": null, "summary": "Thematic maps are widely used to communicate spatial patterns to non-expert audiences. Although uncertainty is inherent in thematic map data, it is rarely visualized, raising questions about how its inclusion affects trust. Prior work offers mixed perspectives: some argue that uncertainty fosters trust through transparency, while others suggest it may reduce trust by introducing confusion. Yet few empirical studies explicitly measure trust in thematic maps. We conducted a between-subjects experiment (N=161) to evaluate how visualizing uncertainty at varying levels (low, medium, high) influences trust. We find that uncertainty visualization generally reduces trust, with greater reductions observed as uncertainty levels increase. However, maps dominated by low uncertainty do not significantly differ in trust from those with no uncertainty. Moreover, while uncertainty visualization tends to make readers question the accuracy of the data, it appears to have a weaker influence on perceptions of the mapmaker's integrity.", "AI": {"tldr": "\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u4f1a\u964d\u4f4e\u7528\u6237\u5bf9\u4e13\u9898\u5730\u56fe\u7684\u4fe1\u4efb\u5ea6\uff0c\u4e14\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\u8d8a\u9ad8\uff0c\u4fe1\u4efb\u5ea6\u4e0b\u964d\u8d8a\u660e\u663e\uff1b\u4f46\u4f4e\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\u4e0e\u65e0\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\u7684\u4fe1\u4efb\u5ea6\u65e0\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u4e13\u9898\u5730\u56fe\u5e7f\u6cdb\u7528\u4e8e\u5411\u975e\u4e13\u4e1a\u53d7\u4f17\u4f20\u8fbe\u7a7a\u95f4\u6a21\u5f0f\uff0c\u4f46\u5730\u56fe\u6570\u636e\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u5f88\u5c11\u88ab\u53ef\u89c6\u5316\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u5305\u542b\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9\u5730\u56fe\u7684\u4fe1\u4efb\uff1f\u5148\u524d\u7814\u7a76\u5b58\u5728\u77db\u76fe\u89c2\u70b9\uff1a\u4e00\u4e9b\u8ba4\u4e3a\u4e0d\u786e\u5b9a\u6027\u901a\u8fc7\u900f\u660e\u5ea6\u4fc3\u8fdb\u4fe1\u4efb\uff0c\u53e6\u4e00\u4e9b\u5219\u8ba4\u4e3a\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u56e0\u5f15\u5165\u6df7\u4e71\u800c\u964d\u4f4e\u4fe1\u4efb\uff0c\u4f46\u5f88\u5c11\u6709\u5b9e\u8bc1\u7814\u7a76\u660e\u786e\u6d4b\u91cf\u4e13\u9898\u5730\u56fe\u4e2d\u7684\u4fe1\u4efb\u3002", "method": "\u91c7\u7528\u88ab\u8bd5\u95f4\u5b9e\u9a8c\u8bbe\u8ba1\uff08N=161\uff09\uff0c\u8bc4\u4f30\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\uff08\u4f4e\u3001\u4e2d\u3001\u9ad8\uff09\u7684\u53ef\u89c6\u5316\u5982\u4f55\u5f71\u54cd\u4fe1\u4efb\u3002\u901a\u8fc7\u6bd4\u8f83\u5305\u542b\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u4e0e\u4e0d\u5305\u542b\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u7684\u5730\u56fe\uff0c\u6d4b\u91cf\u7528\u6237\u5bf9\u5730\u56fe\u7684\u4fe1\u4efb\u5ea6\u53d8\u5316\u3002", "result": "\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u603b\u4f53\u4e0a\u4f1a\u964d\u4f4e\u4fe1\u4efb\uff0c\u4e14\u968f\u7740\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\u589e\u52a0\uff0c\u4fe1\u4efb\u5ea6\u4e0b\u964d\u66f4\u660e\u663e\u3002\u7136\u800c\uff0c\u4ee5\u4f4e\u4e0d\u786e\u5b9a\u6027\u4e3a\u4e3b\u7684\u5730\u56fe\u4e0e\u65e0\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\u5728\u4fe1\u4efb\u5ea6\u4e0a\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u503e\u5411\u4e8e\u8ba9\u8bfb\u8005\u8d28\u7591\u6570\u636e\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u5730\u56fe\u5236\u4f5c\u8005\u8bda\u4fe1\u5ea6\u611f\u77e5\u7684\u5f71\u54cd\u8f83\u5f31\u3002", "conclusion": "\u5728\u4e13\u9898\u5730\u56fe\u4e2d\u53ef\u89c6\u5316\u4e0d\u786e\u5b9a\u6027\u4f1a\u964d\u4f4e\u7528\u6237\u4fe1\u4efb\uff0c\u7279\u522b\u662f\u5f53\u4e2d\u9ad8\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\u65f6\uff1b\u4f46\u4f4e\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u53ef\u80fd\u4e0d\u4f1a\u663e\u8457\u635f\u5bb3\u4fe1\u4efb\u3002\u8fd9\u4e00\u53d1\u73b0\u5bf9\u5730\u56fe\u8bbe\u8ba1\u6709\u91cd\u8981\u542f\u793a\uff1a\u9700\u8981\u6743\u8861\u900f\u660e\u5ea6\u4e0e\u4fe1\u4efb\u7ef4\u62a4\uff0c\u7279\u522b\u662f\u5728\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\u8f83\u9ad8\u65f6\u3002"}}
{"id": "2602.00410", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00410", "abs": "https://arxiv.org/abs/2602.00410", "authors": ["Andre Hora"], "title": "GitEvo: Code Evolution Analysis for Git Repositories", "comment": "Accepted for publication at MSR 2026", "summary": "Analyzing the code evolution of software systems is relevant for practitioners, researchers, and educators. It can help practitioners identify design trends and maintenance challenges, provide researchers with empirical data to study changes over time, and give educators real-world examples that enhance the teaching of software evolution concepts. Unfortunately, we lack tools specifically designed to support code evolution analysis. In this paper, we propose GitEvo, a multi-language and extensible tool for analyzing code evolution in Git repositories. GitEvo leverages Git frameworks and code parsing tools to integrate both Git-level and code-level analysis. We conclude by describing how GitEvo can support the development of novel empirical studies on code evolution and act as a learning tool for educators and students. GitEvo is available at: https://github.com/andrehora/gitevo.", "code_url": "https://github.com/andrehora/gitevo", "code_stars": 14, "code_last_update": "2025-11-16", "AI": {"tldr": "GitEvo\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790Git\u4ed3\u5e93\u4e2d\u7684\u4ee3\u7801\u6f14\u5316\uff0c\u652f\u6301\u5f00\u53d1\u4eba\u5458\u3001\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u8fdb\u884c\u4ee3\u7801\u6f14\u5316\u5206\u6790\u3002", "motivation": "\u5206\u6790\u8f6f\u4ef6\u7cfb\u7edf\u7684\u4ee3\u7801\u6f14\u5316\u5bf9\u4ece\u4e1a\u8005\u3001\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u90fd\u5f88\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u652f\u6301\u4ee3\u7801\u6f14\u5316\u5206\u6790\u7684\u5de5\u5177\u3002", "method": "GitEvo\u5229\u7528Git\u6846\u67b6\u548c\u4ee3\u7801\u89e3\u6790\u5de5\u5177\uff0c\u96c6\u6210Git\u7ea7\u522b\u548c\u4ee3\u7801\u7ea7\u522b\u7684\u5206\u6790\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "GitEvo\u5de5\u5177\u5df2\u5f00\u53d1\u5b8c\u6210\u5e76\u5f00\u6e90\uff0c\u53ef\u7528\u4e8e\u652f\u6301\u4ee3\u7801\u6f14\u5316\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5e76\u4f5c\u4e3a\u6559\u80b2\u5de5\u5177\u4f7f\u7528\u3002", "conclusion": "GitEvo\u586b\u8865\u4e86\u4ee3\u7801\u6f14\u5316\u5206\u6790\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u80fd\u591f\u652f\u6301\u65b0\u9896\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5e76\u4f5c\u4e3a\u6559\u80b2\u5de5\u5177\u5e2e\u52a9\u6559\u5b66\u548c\u5b66\u4e60\u3002"}}
{"id": "2602.01516", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01516", "abs": "https://arxiv.org/abs/2602.01516", "authors": ["Enzo Nicolas Spotorno", "Matheus Wagner", "Antonio Augusto Medeiros Frohlich"], "title": "White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC", "comment": "5 pages, 1 table, 1 figure, submitted to IEEE VTC 2026 Recent Results Track", "summary": "We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u5316\u4e3b\u6743\u8303\u5f0f\u7684\u767d\u76d2\u81ea\u9002\u5e94NMPC\u67b6\u6784\uff0c\u901a\u8fc7\u4ef2\u88c1\u591a\u4e2a\u51bb\u7ed3\u7684\u3001\u7279\u5b9a\u5de5\u51b5\u7684\u795e\u7ecf\u4e13\u5bb6\u6765\u89e3\u51b3\u8f66\u8f86\u53ef\u5851\u6027\u95ee\u9898\uff0c\u4f7f\u7528CasADi\u4e2d\u7684\u7b26\u53f7\u56fe\u5b9e\u73b0\u5b8c\u5168\u53ef\u5ba1\u8ba1\u6027\uff0c\u9a8c\u8bc1\u4e86\u5feb\u901f\u9002\u5e94\u80fd\u529b\u548c\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u4f46\u91cf\u5316\u4e86\u900f\u660e\u6027\u4ee3\u4ef7", "motivation": "\u89e3\u51b3\u8f66\u8f86\u63a7\u5236\u7cfb\u7edf\u5728\u53d8\u5316\u5de5\u51b5\u4e0b\u7684\u53ef\u5851\u6027\u95ee\u9898\uff0c\u5373\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5c31\u80fd\u9002\u5e94\u4e0d\u540c\u8fd0\u884c\u5de5\u51b5\uff0c\u540c\u65f6\u4fdd\u6301\u767d\u76d2\u53ef\u89e3\u91ca\u6027\u548c\u8fd0\u884c\u65f6\u53ef\u5ba1\u8ba1\u6027", "method": "\u91c7\u7528\u6a21\u5757\u5316\u4e3b\u6743\u8303\u5f0f\uff0c\u4ef2\u88c1\u591a\u4e2a\u51bb\u7ed3\u7684\u3001\u7279\u5b9a\u5de5\u51b5\u7684\u795e\u7ecf\u4e13\u5bb6\uff1b\u5728CasADi\u4e2d\u7ef4\u62a4\u5b8c\u5168\u53ef\u904d\u5386\u7684\u7b26\u53f7\u56fe\u4ee5\u786e\u4fdd\u8fd0\u884c\u65f6\u53ef\u5ba1\u8ba1\u6027\uff1b\u901a\u8fc7\u540c\u6b65\u4eff\u771f\u9a8c\u8bc1\u7cfb\u7edf\u6027\u80fd", "result": "\u5b9e\u73b0\u4e86\u5feb\u901f\u9002\u5e94\uff08\u7ea67.3\u6beb\u79d2\uff09\u548c\u63a5\u8fd1\u7406\u60f3\u7684\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5728\u590d\u5408\u5de5\u51b5\u53d8\u5316\uff08\u6469\u64e6\u3001\u8d28\u91cf\u3001\u963b\u529b\uff09\u4e0b\u8868\u73b0\u4f18\u4e8e\u975e\u81ea\u9002\u5e94\u57fa\u7ebf\uff1b\u91cf\u5316\u4e86\u900f\u660e\u6027\u4ee3\u4ef7\uff1a\u7b26\u53f7\u56fe\u7ef4\u62a4\u4f7f\u6c42\u89e3\u5668\u5ef6\u8fdf\u589e\u52a072-102\u500d", "conclusion": "\u8be5\u767d\u76d2\u81ea\u9002\u5e94NMPC\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86\u8f66\u8f86\u53ef\u5851\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u5de5\u51b5\u9002\u5e94\u548c\u826f\u597d\u8ddf\u8e2a\u6027\u80fd\uff0c\u4f46\u4e25\u683c\u7684\u900f\u660e\u6027\u5b9e\u73b0\u9700\u8981\u4ed8\u51fa\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u4ee3\u4ef7"}}
{"id": "2602.01665", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01665", "abs": "https://arxiv.org/abs/2602.01665", "authors": ["Hayeong Lee", "JunHyeok Oh", "Byung-Jun Lee"], "title": "TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning", "comment": null, "summary": "The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.", "AI": {"tldr": "TABX\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u7684\u9ad8\u541e\u5410\u91cf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6c99\u76d2\u73af\u5883\uff0c\u63d0\u4f9b\u53ef\u91cd\u6784\u4efb\u52a1\u8bbe\u8ba1\u548c\u786c\u4ef6\u52a0\u901f\u6267\u884c\uff0c\u7528\u4e8e\u7cfb\u7edf\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u73af\u5883\u7f3a\u4e4f\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u96be\u4ee5\u652f\u6301\u81ea\u5b9a\u4e49\u8bc4\u4f30\u573a\u666f\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u5728\u591a\u6837\u5316\u4efb\u52a1\u590d\u6742\u5ea6\u4e0b\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eJAX\u7684TABX\u6846\u67b6\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u73af\u5883\u53c2\u6570\u63a7\u5236\uff0c\u652f\u6301\u786c\u4ef6\u52a0\u901f\u7684GPU\u5e76\u884c\u6267\u884c\uff0c\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u8ba1\u7b97\u548c\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u914d\u7f6e\u3002", "result": "TABX\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5e76\u884c\u5316\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u7ed3\u6784\u5316\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u5e73\u53f0\u3002", "conclusion": "TABX\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u9ad8\u6027\u80fd\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6c99\u76d2\u73af\u5883\uff0c\u80fd\u591f\u4fc3\u8fdb\u5bf9\u667a\u80fd\u4f53\u6d8c\u73b0\u884c\u4e3a\u548c\u7b97\u6cd5\u6743\u8861\u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2602.02057", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.02057", "abs": "https://arxiv.org/abs/2602.02057", "authors": ["An\u0131l Eren G\u00f6\u00e7er", "Ioanna Tsakalidou", "Hamish Nicholson", "Kyoungmin Kim", "Anastasia Ailamaki"], "title": "QVCache: A Query-Aware Vector Cache", "comment": null, "summary": "Vector databases have become a cornerstone of modern information retrieval, powering applications in recommendation, search, and retrieval-augmented generation (RAG) pipelines. However, scaling approximate nearest neighbor (ANN) search to high recall under strict latency SLOs remains fundamentally constrained by memory capacity and I/O bandwidth. Disk-based vector search systems suffer severe latency degradation at high accuracy, while fully in-memory solutions incur prohibitive memory costs at billion-scale. Despite the central role of caching in traditional databases, vector search lacks a general query-level caching layer capable of amortizing repeated query work.\n  We present QVCache, the first backend-agnostic, query-level caching system for ANN search with bounded memory footprint. QVCache exploits semantic query repetition by performing similarity-aware caching rather than exact-match lookup. It dynamically learns region-specific distance thresholds using an online learning algorithm, enabling recall-preserving cache hits while bounding lookup latency and memory usage independently of dataset size. QVCache operates as a drop-in layer for existing vector databases. It maintains a megabyte-scale memory footprint and achieves sub-millisecond cache-hit latency, reducing end-to-end query latency by up to 40-1000x when integrated with existing ANN systems. For workloads exhibiting temporal-semantic locality, QVCache substantially reduces latency while preserving recall comparable to the underlying ANN backend, establishing it as a missing but essential caching layer for scalable vector search.", "AI": {"tldr": "QVCache\uff1a\u9996\u4e2a\u540e\u7aef\u65e0\u5173\u7684\u67e5\u8be2\u7ea7\u7f13\u5b58\u7cfb\u7edf\uff0c\u7528\u4e8e\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7f13\u5b58\u548c\u5728\u7ebf\u5b66\u4e60\u52a8\u6001\u9608\u503c\uff0c\u5728\u6709\u9650\u5185\u5b58\u4e0b\u5b9e\u73b0\u9ad8\u53ec\u56de\u7387", "motivation": "\u5411\u91cf\u6570\u636e\u5e93\u5df2\u6210\u4e3a\u73b0\u4ee3\u4fe1\u606f\u68c0\u7d22\u7684\u57fa\u77f3\uff0c\u4f46\u5c06\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u6269\u5c55\u5230\u9ad8\u53ec\u56de\u7387\u540c\u65f6\u6ee1\u8db3\u4e25\u683c\u5ef6\u8fdfSLO\u9762\u4e34\u5185\u5b58\u5bb9\u91cf\u548cI/O\u5e26\u5bbd\u7684\u6839\u672c\u9650\u5236\u3002\u57fa\u4e8e\u78c1\u76d8\u7684\u5411\u91cf\u641c\u7d22\u7cfb\u7edf\u5728\u9ad8\u7cbe\u5ea6\u4e0b\u906d\u53d7\u4e25\u91cd\u5ef6\u8fdf\u9000\u5316\uff0c\u800c\u5b8c\u5168\u5185\u5b58\u89e3\u51b3\u65b9\u6848\u5728\u5341\u4ebf\u89c4\u6a21\u4e0b\u4ea7\u751f\u8fc7\u9ad8\u5185\u5b58\u6210\u672c\u3002\u5c3d\u7ba1\u7f13\u5b58\u5728\u4f20\u7edf\u6570\u636e\u5e93\u4e2d\u626e\u6f14\u6838\u5fc3\u89d2\u8272\uff0c\u4f46\u5411\u91cf\u641c\u7d22\u7f3a\u4e4f\u80fd\u591f\u5206\u644a\u91cd\u590d\u67e5\u8be2\u5de5\u4f5c\u7684\u901a\u7528\u67e5\u8be2\u7ea7\u7f13\u5b58\u5c42\u3002", "method": "QVCache\u91c7\u7528\u540e\u7aef\u65e0\u5173\u7684\u67e5\u8be2\u7ea7\u7f13\u5b58\u7cfb\u7edf\uff0c\u5229\u7528\u8bed\u4e49\u67e5\u8be2\u91cd\u590d\u6027\u8fdb\u884c\u76f8\u4f3c\u6027\u611f\u77e5\u7f13\u5b58\u800c\u975e\u7cbe\u786e\u5339\u914d\u67e5\u627e\u3002\u7cfb\u7edf\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u52a8\u6001\u5b66\u4e60\u533a\u57df\u7279\u5b9a\u7684\u8ddd\u79bb\u9608\u503c\uff0c\u5728\u4fdd\u6301\u53ec\u56de\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u7f13\u5b58\u547d\u4e2d\uff0c\u540c\u65f6\u72ec\u7acb\u4e8e\u6570\u636e\u96c6\u5927\u5c0f\u9650\u5236\u67e5\u627e\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u3002QVCache\u4f5c\u4e3a\u73b0\u6709\u5411\u91cf\u6570\u636e\u5e93\u7684\u5373\u63d2\u5373\u7528\u5c42\u8fd0\u884c\u3002", "result": "QVCache\u4fdd\u6301\u5146\u5b57\u8282\u7ea7\u5185\u5b58\u5360\u7528\uff0c\u5b9e\u73b0\u4e9a\u6beb\u79d2\u7ea7\u7f13\u5b58\u547d\u4e2d\u5ef6\u8fdf\uff0c\u5f53\u4e0e\u73b0\u6709ANN\u7cfb\u7edf\u96c6\u6210\u65f6\uff0c\u5c06\u7aef\u5230\u7aef\u67e5\u8be2\u5ef6\u8fdf\u964d\u4f4e40-1000\u500d\u3002\u5bf9\u4e8e\u5c55\u73b0\u65f6\u95f4-\u8bed\u4e49\u5c40\u90e8\u6027\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0cQVCache\u5728\u4fdd\u6301\u4e0e\u5e95\u5c42ANN\u540e\u7aef\u76f8\u5f53\u7684\u53ec\u56de\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "QVCache\u5efa\u7acb\u4e86\u5411\u91cf\u641c\u7d22\u4e2d\u7f3a\u5931\u4f46\u5fc5\u8981\u7684\u7f13\u5b58\u5c42\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7f13\u5b58\u548c\u52a8\u6001\u9608\u503c\u5b66\u4e60\uff0c\u5728\u6709\u9650\u5185\u5b58\u4e0b\u5b9e\u73b0\u9ad8\u53ec\u56de\u7387\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u89e3\u51b3\u4e86\u5411\u91cf\u6570\u636e\u5e93\u5728\u6269\u5c55\u6027\u548c\u5ef6\u8fdf\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2602.01026", "categories": ["cs.NE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01026", "abs": "https://arxiv.org/abs/2602.01026", "authors": ["Hiroyuki Iizuka"], "title": "The Stacked Autoencoder Evolution Hypothesis", "comment": null, "summary": "This study introduces a novel theoretical framework, the Stacked Autoencoder Evolution Hypothesis, which proposes that biological evolutionary systems operate through multi-layered self-encoding and decoding processes, analogous to stacked autoencoders in deep learning. Rather than viewing evolution solely as gradual changes driven by mutation and selection, this hypothesis suggests that self-replication inherently compresses and reconstructs genetic information across hierarchical layers of abstraction. This layered structure enables evolutionary systems to explore diverse possibilities not only at the sequence level but also across progressively more abstract layers of representation, making it possible for even simple mutations to navigate these higher-order spaces.Such a mechanism may explain punctuated evolutionary patterns and changes that can appear as if they are goal-directed in natural evolution, by allowing mutations at deeper latent layers to trigger sudden, large-scale phenotypic shifts. To illustrate the plausibility of this mechanism, artificial chemistry simulations were conducted, demonstrating the spontaneous emergence of hierarchical autoencoder structures. This framework offers a new perspective on the informational dynamics underlying both continuous and discontinuous evolutionary change.", "AI": {"tldr": "\u63d0\u51fa\u5806\u53e0\u81ea\u7f16\u7801\u5668\u8fdb\u5316\u5047\u8bf4\uff0c\u8ba4\u4e3a\u751f\u7269\u8fdb\u5316\u7cfb\u7edf\u901a\u8fc7\u591a\u5c42\u81ea\u7f16\u7801\u548c\u89e3\u7801\u8fc7\u7a0b\u8fd0\u4f5c\uff0c\u7c7b\u4f3c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5806\u53e0\u81ea\u7f16\u7801\u5668\uff0c\u80fd\u591f\u89e3\u91ca\u95f4\u65ad\u5e73\u8861\u8fdb\u5316\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u8fdb\u5316\u7406\u8bba\u4e3b\u8981\u5173\u6ce8\u7a81\u53d8\u548c\u9009\u62e9\u7684\u6e10\u8fdb\u53d8\u5316\uff0c\u4f46\u96be\u4ee5\u89e3\u91ca\u95f4\u65ad\u5e73\u8861\u8fdb\u5316\u6a21\u5f0f\u548c\u770b\u4f3c\u76ee\u6807\u5bfc\u5411\u7684\u5927\u89c4\u6a21\u8868\u578b\u53d8\u5316\u3002\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e9b\u73b0\u8c61\u80cc\u540e\u7684\u4fe1\u606f\u52a8\u6001\u673a\u5236\u3002", "method": "\u63d0\u51fa\u5806\u53e0\u81ea\u7f16\u7801\u5668\u8fdb\u5316\u5047\u8bf4\u4f5c\u4e3a\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u5316\u5b66\u6a21\u62df\u6765\u9a8c\u8bc1\u5176\u5408\u7406\u6027\uff0c\u5c55\u793a\u5206\u5c42\u81ea\u7f16\u7801\u5668\u7ed3\u6784\u7684\u81ea\u53d1\u6d8c\u73b0\u3002", "result": "\u4eba\u5de5\u5316\u5b66\u6a21\u62df\u8bc1\u5b9e\u4e86\u5206\u5c42\u81ea\u7f16\u7801\u5668\u7ed3\u6784\u80fd\u591f\u81ea\u53d1\u5f62\u6210\uff0c\u652f\u6301\u8be5\u5047\u8bf4\u7684\u5408\u7406\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u8fde\u7eed\u548c\u4e0d\u8fde\u7eed\u8fdb\u5316\u53d8\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u4fe1\u606f\u52a8\u6001\u89c6\u89d2\u3002", "conclusion": "\u5806\u53e0\u81ea\u7f16\u7801\u5668\u8fdb\u5316\u5047\u8bf4\u4e3a\u7406\u89e3\u751f\u7269\u8fdb\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u89e3\u91ca\u95f4\u65ad\u5e73\u8861\u6a21\u5f0f\u548c\u5927\u89c4\u6a21\u8868\u578b\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u8fdb\u5316\u7cfb\u7edf\u4e2d\u591a\u5c42\u6b21\u4fe1\u606f\u538b\u7f29\u548c\u91cd\u6784\u7684\u673a\u5236\u3002"}}
{"id": "2602.00259", "categories": ["cs.HC", "cs.AI", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2602.00259", "abs": "https://arxiv.org/abs/2602.00259", "authors": ["Venkatesh Sivaraman", "Eric P. Mason", "Mengfan Ellen Li", "Jessica Tong", "Andrew J. King", "Jeremy M. Kahn", "Adam Perer"], "title": "Intelligent Reasoning Cues: A Framework and Case Study of the Roles of AI Information in Complex Decisions", "comment": "Accepted at CHI 2026", "summary": "Artificial intelligence (AI)-based decision support systems can be highly accurate yet still fail to support users or improve decisions. Existing theories of AI-assisted decision-making focus on calibrating reliance on AI advice, leaving it unclear how different system designs might influence the reasoning processes underneath. We address this gap by reconsidering AI interfaces as collections of intelligent reasoning cues: discrete pieces of AI information that can individually influence decision-making. We then explore the roles of eight types of reasoning cues in a high-stakes clinical decision (treating patients with sepsis in intensive care). Through contextual inquiries with six teams and a think-aloud study with 25 physicians, we find that reasoning cues have distinct patterns of influence that can directly inform design. Our results also suggest that reasoning cues should prioritize tasks with high variability and discretion, adapt to ensure compatibility with evolving decision needs, and provide complementary, rigorous insights on complex cases.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06AI\u754c\u9762\u91cd\u65b0\u5b9a\u4e49\u4e3a\u667a\u80fd\u63a8\u7406\u7ebf\u7d22\u7684\u96c6\u5408\uff0c\u63a2\u7d22\u4e86\u516b\u79cd\u63a8\u7406\u7ebf\u7d22\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u51b3\u7b56\uff08ICU\u8113\u6bd2\u75c7\u6cbb\u7597\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u4e0d\u540c\u7ebf\u7d22\u5bf9\u51b3\u7b56\u8fc7\u7a0b\u6709\u72ec\u7279\u5f71\u54cd\u6a21\u5f0f\uff0c\u53ef\u76f4\u63a5\u6307\u5bfc\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709AI\u8f85\u52a9\u51b3\u7b56\u7406\u8bba\u4e3b\u8981\u5173\u6ce8\u6821\u51c6\u5bf9AI\u5efa\u8bae\u7684\u4f9d\u8d56\u5ea6\uff0c\u4f46\u672a\u80fd\u89e3\u91ca\u4e0d\u540c\u7cfb\u7edf\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u5e95\u5c42\u7684\u63a8\u7406\u8fc7\u7a0b\u3002AI\u7cfb\u7edf\u867d\u7136\u51c6\u786e\uff0c\u5374\u5e38\u5e38\u65e0\u6cd5\u6709\u6548\u652f\u6301\u7528\u6237\u6216\u6539\u5584\u51b3\u7b56\u8d28\u91cf\uff0c\u8fd9\u4e00\u5dee\u8ddd\u9700\u8981\u4ece\u66f4\u6839\u672c\u7684\u63a8\u7406\u8fc7\u7a0b\u5c42\u9762\u8fdb\u884c\u63a2\u7d22\u3002", "method": "\u9996\u5148\u5c06AI\u754c\u9762\u91cd\u65b0\u5b9a\u4e49\u4e3a\u667a\u80fd\u63a8\u7406\u7ebf\u7d22\u7684\u96c6\u5408\uff0c\u5373\u80fd\u591f\u5355\u72ec\u5f71\u54cd\u51b3\u7b56\u8fc7\u7a0b\u7684\u79bb\u6563AI\u4fe1\u606f\u7247\u6bb5\u3002\u7136\u540e\u901a\u8fc7\u60c5\u5883\u8c03\u67e5\uff086\u4e2a\u56e2\u961f\uff09\u548c\u6709\u58f0\u601d\u7ef4\u7814\u7a76\uff0825\u540d\u533b\u751f\uff09\uff0c\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u51b3\u7b56\u573a\u666f\uff08ICU\u8113\u6bd2\u75c7\u6cbb\u7597\uff09\u4e2d\u63a2\u7d22\u516b\u79cd\u63a8\u7406\u7ebf\u7d22\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u7ebf\u7d22\u5177\u6709\u72ec\u7279\u7684\u5f71\u54cd\u6a21\u5f0f\uff0c\u53ef\u76f4\u63a5\u6307\u5bfc\u8bbe\u8ba1\u3002\u5177\u4f53\u800c\u8a00\uff1a1\uff09\u63a8\u7406\u7ebf\u7d22\u5e94\u4f18\u5148\u5e94\u7528\u4e8e\u9ad8\u53d8\u5f02\u6027\u548c\u81ea\u7531\u88c1\u91cf\u6743\u7684\u4efb\u52a1\uff1b2\uff09\u9700\u8981\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u51b3\u7b56\u9700\u6c42\u4ee5\u786e\u4fdd\u517c\u5bb9\u6027\uff1b3\uff09\u5e94\u4e3a\u590d\u6742\u75c5\u4f8b\u63d0\u4f9b\u4e92\u8865\u4e14\u4e25\u8c28\u7684\u89c1\u89e3\u3002", "conclusion": "\u901a\u8fc7\u5c06AI\u754c\u9762\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u63a8\u7406\u7ebf\u7d22\u96c6\u5408\uff0c\u7814\u7a76\u4e3a\u7406\u89e3AI\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u8bbe\u8ba1\u5e94\u5173\u6ce8\u63a8\u7406\u7ebf\u7d22\u7684\u5dee\u5f02\u5316\u5f71\u54cd\uff0c\u4f18\u5148\u652f\u6301\u9ad8\u53d8\u5f02\u6027\u4efb\u52a1\uff0c\u786e\u4fdd\u9002\u5e94\u6027\uff0c\u5e76\u4e3a\u590d\u6742\u51b3\u7b56\u63d0\u4f9b\u4e92\u8865\u89c1\u89e3\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2602.00557", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00557", "abs": "https://arxiv.org/abs/2602.00557", "authors": ["Weisheng Dai", "Kai Lan", "Jianyi Zhou", "Bo Zhao", "Xiu Su", "Junwen Tong", "Weili Guan", "Shuo Yang"], "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning.", "AI": {"tldr": "ConLA\u662f\u4e00\u4e2a\u4ece\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4e2d\u65e0\u76d1\u7763\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u89e3\u8026\u673a\u5236\u5206\u79bb\u8fd0\u52a8\u52a8\u6001\u548c\u89c6\u89c9\u5185\u5bb9\uff0c\u9996\u6b21\u4ec5\u7528\u4eba\u7c7b\u89c6\u9891\u9884\u8bad\u7ec3\u5c31\u8d85\u8d8a\u4e86\u771f\u5b9e\u673a\u5668\u4eba\u8f68\u8ff9\u9884\u8bad\u7ec3\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f46\u8fd9\u7c7b\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u6269\u5c55\u3002\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u63d0\u4f9b\u4e86\u4e30\u5bcc\u4e14\u53ef\u6269\u5c55\u7684\u591a\u6837\u5316\u573a\u666f\u548c\u64cd\u4f5c\u884c\u4e3a\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u7684\u52a8\u4f5c\u76d1\u7763\uff0c\u65e0\u6cd5\u76f4\u63a5\u5229\u7528\u3002\u73b0\u6709VQ-VAE\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u5916\u89c2\u91cd\u5efa\u800c\u975e\u5e27\u95f4\u52a8\u6001\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u8868\u5f81\u4f9d\u8d56\u865a\u5047\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ea7\u751f\u6377\u5f84\u5b66\u4e60\u548c\u7ea0\u7f20\u7684\u6f5c\u5728\u8868\u5f81\uff0c\u963b\u788d\u4e86\u53ef\u8fc1\u79fb\u6027\u3002", "method": "\u63d0\u51faConLA\u6846\u67b6\uff0c\u5f15\u5165\u5bf9\u6bd4\u89e3\u8026\u673a\u5236\uff0c\u5229\u7528\u52a8\u4f5c\u7c7b\u522b\u5148\u9a8c\u548c\u65f6\u95f4\u7ebf\u7d22\u6765\u9694\u79bb\u8fd0\u52a8\u52a8\u6001\u548c\u89c6\u89c9\u5185\u5bb9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u6709\u6548\u7f13\u89e3\u6377\u5f84\u5b66\u4e60\u95ee\u9898\uff0c\u5b66\u4e60\u5230\u7eaf\u51c0\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u6f5c\u5728\u52a8\u4f5c\u8868\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4ec5\u4f7f\u7528\u4eba\u7c7b\u89c6\u9891\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8be5\u65b9\u6cd5\u9996\u6b21\u8d85\u8d8a\u4e86\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u8f68\u8ff9\u9884\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u63d0\u53d6\u9ad8\u8d28\u91cf\u6f5c\u5728\u52a8\u4f5c\u8868\u5f81\u7684\u80fd\u529b\u3002", "conclusion": "ConLA\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u5bf9\u6bd4\u89e3\u8026\u673a\u5236\u6709\u6548\u5206\u79bb\u8fd0\u52a8\u52a8\u6001\u548c\u89c6\u89c9\u5185\u5bb9\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2602.01629", "categories": ["cs.LG", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01629", "abs": "https://arxiv.org/abs/2602.01629", "authors": ["Renukanandan Tumu", "Aditya Singh", "Rahul Mangharam"], "title": "AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments", "comment": null, "summary": "Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \\textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.", "AI": {"tldr": "AdaptNC\u6846\u67b6\u901a\u8fc7\u8054\u5408\u5728\u7ebf\u8c03\u6574\u975e\u5171\u5f62\u8bc4\u5206\u51fd\u6570\u53c2\u6570\u548c\u5171\u5f62\u9608\u503c\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u9884\u6d4b\u533a\u57df\u8fc7\u5ea6\u4fdd\u5b88\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u76ee\u6807\u8986\u76d6\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c0f\u9884\u6d4b\u533a\u57df\u4f53\u79ef\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5b58\u5728\u5206\u5e03\u504f\u79fb\uff0c\u8fdd\u53cd\u6807\u51c6\u5171\u5f62\u9884\u6d4b\u7684\u4ea4\u6362\u6027\u5047\u8bbe\u3002\u73b0\u6709\u5728\u7ebfCP\u65b9\u6cd5\u4ec5\u8c03\u6574\u9608\u503c\u800c\u4f7f\u7528\u9759\u6001\u975e\u5171\u5f62\u8bc4\u5206\u51fd\u6570\uff0c\u5bfc\u81f4\u9884\u6d4b\u533a\u57df\u8fc7\u5ea6\u4fdd\u5b88\u4e14\u4f53\u79ef\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faAdaptNC\u6846\u67b6\uff0c\u8054\u5408\u5728\u7ebf\u8c03\u6574\u975e\u5171\u5f62\u8bc4\u5206\u51fd\u6570\u53c2\u6570\u548c\u5171\u5f62\u9608\u503c\u3002\u91c7\u7528\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u65b9\u6848\u4f18\u5316\u8bc4\u5206\u51fd\u6570\uff0c\u5f15\u5165\u56de\u653e\u7f13\u51b2\u533a\u673a\u5236\u7f13\u89e3\u8bc4\u5206\u8f6c\u6362\u671f\u95f4\u7684\u8986\u76d6\u7387\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u7b56\u7565\u53d8\u5316\u3001\u73af\u5883\u53d8\u5316\u548c\u4f20\u611f\u5668\u9000\u5316\u7b49\u591a\u6837\u5316\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaptNC\u76f8\u6bd4\u4ec5\u8c03\u6574\u9608\u503c\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u76ee\u6807\u8986\u76d6\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c0f\u4e86\u9884\u6d4b\u533a\u57df\u4f53\u79ef\u3002", "conclusion": "AdaptNC\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8bc4\u5206\u51fd\u6570\u548c\u9608\u503c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u504f\u79fb\u4e0b\u5171\u5f62\u9884\u6d4b\u7684\u4fdd\u5b88\u6027\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u9884\u6d4b\u533a\u57df\u3002"}}
{"id": "2602.02170", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02170", "abs": "https://arxiv.org/abs/2602.02170", "authors": ["Jose Manuel de la Chica Rodriguez", "Juan Manuel Vera D\u00edaz"], "title": "Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study", "comment": null, "summary": "Contemporary multi-agent systems increasingly rely on internal coordination mechanisms to combine, arbitrate, or constrain the outputs of heterogeneous components. In safety-critical and regulated domains such as finance, these mechanisms must satisfy strict formal requirements, remain auditable, and operate within explicitly bounded limits. Coordination logic therefore functions as a governance layer rather than an optimization heuristic.\n  This paper presents an exploratory systems feasibility study of Self-Evolving Coordination Protocols (SECP): coordination protocols that permit limited, externally validated self-modification while preserving fixed formal invariants. We study a controlled proof-of-concept setting in which six fixed Byzantine consensus protocol proposals are evaluated by six specialized decision modules. All coordination regimes operate under identical hard constraints, including Byzantine fault tolerance (f < n/3), O(n2) message complexity, complete non-statistical safety and liveness arguments, and bounded explainability.\n  Four coordination regimes are compared in a single-shot design: unanimous hard veto, weighted scalar aggregation, SECP v1.0 (an agent-designed non-scalar protocol), and SECP v2.0 (the result of one governed modification). Outcomes are evaluated using a single metric, proposal coverage, defined as the number of proposals accepted. A single recursive modification increased coverage from two to three accepted proposals while preserving all declared invariants.\n  The study makes no claims regarding statistical significance, optimality, convergence, or learning. Its contribution is architectural: it demonstrates that bounded self-modification of coordination protocols is technically implementable, auditable, and analyzable under explicit formal constraints, establishing a foundation for governed multi-agent systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u5728\u4e25\u683c\u5f62\u5f0f\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6709\u9650\u81ea\u6f14\u5316\u7684\u534f\u8c03\u534f\u8bae\uff08SECP\uff09\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5728\u4fdd\u6301\u6240\u6709\u58f0\u660e\u4e0d\u53d8\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u534f\u8bae\u80fd\u591f\u901a\u8fc7\u4e00\u6b21\u9012\u5f52\u4fee\u6539\u5c06\u63d0\u6848\u8986\u76d6\u7387\u4ece2\u4e2a\u63d0\u5347\u52303\u4e2a\u3002", "motivation": "\u5728\u91d1\u878d\u7b49\u5b89\u5168\u5173\u952e\u548c\u53d7\u76d1\u7ba1\u9886\u57df\u4e2d\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u5f62\u5f0f\u8981\u6c42\u3001\u4fdd\u6301\u53ef\u5ba1\u8ba1\u6027\u5e76\u5728\u660e\u786e\u6709\u754c\u9650\u5236\u5185\u8fd0\u884c\u3002\u534f\u8c03\u903b\u8f91\u4f5c\u4e3a\u6cbb\u7406\u5c42\u800c\u975e\u4f18\u5316\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5728\u4fdd\u6301\u56fa\u5b9a\u5f62\u5f0f\u4e0d\u53d8\u91cf\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u6709\u9650\u7684\u5916\u90e8\u9a8c\u8bc1\u81ea\u4fee\u6539\u3002", "method": "\u91c7\u7528\u63a2\u7d22\u6027\u7cfb\u7edf\u53ef\u884c\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u5728\u53d7\u63a7\u6982\u5ff5\u9a8c\u8bc1\u73af\u5883\u4e2d\u6bd4\u8f83\u56db\u79cd\u534f\u8c03\u673a\u5236\uff1a\u4e00\u81f4\u786c\u5426\u51b3\u3001\u52a0\u6743\u6807\u91cf\u805a\u5408\u3001SECP v1.0\uff08\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u975e\u6807\u91cf\u534f\u8bae\uff09\u548cSECP v1.0\u7ecf\u8fc7\u4e00\u6b21\u6cbb\u7406\u4fee\u6539\u540e\u7684SECP v2.0\u3002\u6240\u6709\u673a\u5236\u5728\u76f8\u540c\u7684\u786c\u7ea6\u675f\u4e0b\u8fd0\u884c\uff0c\u5305\u62ec\u62dc\u5360\u5ead\u5bb9\u9519\u3001O(n\u00b2)\u6d88\u606f\u590d\u6742\u5ea6\u3001\u5b8c\u5168\u975e\u7edf\u8ba1\u7684\u5b89\u5168\u6027\u548c\u6d3b\u6027\u8bba\u8bc1\u4ee5\u53ca\u6709\u754c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u901a\u8fc7\u4e00\u6b21\u9012\u5f52\u4fee\u6539\uff0c\u63d0\u6848\u8986\u76d6\u7387\u4ece\u4e24\u4e2a\u63a5\u53d7\u7684\u63d0\u6848\u589e\u52a0\u5230\u4e09\u4e2a\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6240\u6709\u58f0\u660e\u7684\u5f62\u5f0f\u4e0d\u53d8\u91cf\u3002SECP v2.0\u76f8\u6bd4\u5176\u4ed6\u534f\u8c03\u673a\u5236\u5728\u4fdd\u6301\u6240\u6709\u7ea6\u675f\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u4e3b\u8981\u8d21\u732e\u662f\u67b6\u6784\u6027\u7684\uff1a\u8bc1\u660e\u4e86\u5728\u660e\u786e\u7684\u5f62\u5f0f\u7ea6\u675f\u4e0b\uff0c\u534f\u8c03\u534f\u8bae\u7684\u6709\u754c\u81ea\u4fee\u6539\u5728\u6280\u672f\u4e0a\u662f\u53ef\u5b9e\u73b0\u7684\u3001\u53ef\u5ba1\u8ba1\u7684\u548c\u53ef\u5206\u6790\u7684\uff0c\u4e3a\u6cbb\u7406\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u7814\u7a76\u4e0d\u6d89\u53ca\u7edf\u8ba1\u663e\u8457\u6027\u3001\u6700\u4f18\u6027\u3001\u6536\u655b\u6027\u6216\u5b66\u4e60\u80fd\u529b\u7684\u58f0\u660e\u3002"}}
{"id": "2602.00219", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00219", "abs": "https://arxiv.org/abs/2602.00219", "authors": ["Saeid Jamshidi", "Omar Abdul Wahab", "Foutse Khomh", "Kawser Wazed Nafi"], "title": "Tri-LLM Cooperative Federated Zero-Shot Intrusion Detection with Semantic Disagreement and Trust-Aware Aggregation", "comment": null, "summary": "Federated learning (FL) has become an effective paradigm for privacy-preserving, distributed Intrusion Detection Systems (IDS) in cyber-physical and Internet of Things (IoT) networks, where centralized data aggregation is often infeasible due to privacy and bandwidth constraints. Despite its advantages, most existing FL-based IDS assume closed-set learning and lack mechanisms such as uncertainty estimation, semantic generalization, and explicit modeling of epistemic ambiguity in zero-day attack scenarios. Additionally, robustness to heterogeneous and unreliable clients remains a challenge in practical applications. This paper introduces a semantics-driven federated IDS framework that incorporates language-derived semantic supervision into federated optimization, enabling open-set and zero-shot intrusion detection for previously unseen attack behaviors. The approach constructs semantic attack prototypes using a Tri-LLM ensemble of GPT-4o, DeepSeek-V3, and LLaMA-3-8B, aligning distributed telemetry features with high-level attack concepts. Inter-LLM semantic disagreement is modeled as epistemic uncertainty for zero-day risk estimation, while a trust-aware aggregation mechanism dynamically weights client updates based on reliability. Experimental results show stable semantic alignment across heterogeneous clients and consistent convergence. The framework achieves over 80% zero-shot detection accuracy on unseen attack patterns, improving zero-day discrimination by more than 10% compared to similarity-based baselines, while maintaining low aggregation instability in the presence of unreliable or compromised clients.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u4e49\u9a71\u52a8\u7684\u8054\u90a6\u5165\u4fb5\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u8a00\u8bed\u4e49\u76d1\u7763\u5b9e\u73b0\u5f00\u653e\u96c6\u548c\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u5229\u7528Tri-LLM\u96c6\u6210\u6784\u5efa\u8bed\u4e49\u653b\u51fb\u539f\u578b\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u4fe1\u4efb\u611f\u77e5\u805a\u5408\u63d0\u5347\u96f6\u65e5\u653b\u51fb\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5047\u8bbe\u95ed\u96c6\u5b66\u4e60\u3001\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u673a\u5236\u3001\u8bed\u4e49\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u5bf9\u96f6\u65e5\u653b\u51fb\u573a\u666f\u7684\u8ba4\u77e5\u6a21\u7cca\u6027\u5efa\u6a21\u4e0d\u5145\u5206\uff0c\u4e14\u5bf9\u5f02\u6784\u4e0d\u53ef\u9760\u5ba2\u6237\u7aef\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "1) \u4f7f\u7528GPT-4o\u3001DeepSeek-V3\u548cLLaMA-3-8B\u7ec4\u6210\u7684Tri-LLM\u96c6\u6210\u6784\u5efa\u8bed\u4e49\u653b\u51fb\u539f\u578b\uff1b2) \u5c06\u5206\u5e03\u5f0f\u9065\u6d4b\u7279\u5f81\u4e0e\u9ad8\u5c42\u653b\u51fb\u6982\u5ff5\u5bf9\u9f50\uff1b3) \u5c06LLM\u95f4\u8bed\u4e49\u5206\u6b67\u5efa\u6a21\u4e3a\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7528\u4e8e\u96f6\u65e5\u98ce\u9669\u8bc4\u4f30\uff1b4) \u91c7\u7528\u4fe1\u4efb\u611f\u77e5\u805a\u5408\u673a\u5236\u52a8\u6001\u52a0\u6743\u5ba2\u6237\u7aef\u66f4\u65b0\u3002", "result": "1) \u5728\u5f02\u6784\u5ba2\u6237\u7aef\u95f4\u5b9e\u73b0\u7a33\u5b9a\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u4e00\u81f4\u6536\u655b\uff1b2) \u5bf9\u672a\u89c1\u653b\u51fb\u6a21\u5f0f\u8fbe\u523080%\u4ee5\u4e0a\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u51c6\u786e\u7387\uff1b3) \u76f8\u6bd4\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u96f6\u65e5\u653b\u51fb\u8fa8\u522b\u80fd\u529b\u63d0\u5347\u8d85\u8fc710%\uff1b4) \u5728\u5b58\u5728\u4e0d\u53ef\u9760\u6216\u53d7\u635f\u5ba2\u6237\u7aef\u65f6\u4fdd\u6301\u4f4e\u805a\u5408\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u8bed\u4e49\u9a71\u52a8\u7684\u8054\u90a6IDS\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u8bed\u4e49\u76d1\u7763\u3001\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u4fe1\u4efb\u611f\u77e5\u805a\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709FL-based IDS\u5728\u5f00\u653e\u96c6\u5b66\u4e60\u3001\u96f6\u65e5\u653b\u51fb\u68c0\u6d4b\u548c\u5f02\u6784\u5ba2\u6237\u7aef\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.00371", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00371", "abs": "https://arxiv.org/abs/2602.00371", "authors": ["Aditya Kumar Purohit", "Aditya Upadhyaya", "Nicolas Ruiz", "Alberto Monge Roffarello", "Hendrik Heuer"], "title": "When Handwriting Goes Social: Creativity, Anonymity, and Communication in Graphonymous Online Spaces", "comment": null, "summary": "While most digital communication platforms rely on text, relatively little research has examined how users engage through handwriting and drawing in anonymous, collaborative environments. We introduce Graphonymous Interaction, a form of communication where users interact anonymously via handwriting and drawing. Our study analyzed over 600 canvas pages from the Graphonymous Online Space (GOS) CollaNote and conducted interviews with 20 users. Additionally, we examined 70 minutes of real-time GOS sessions using Conversation Analysis and Multimodal Discourse Analysis. Findings reveal that Graphonymous Interaction fosters artistic expression, intellectual engagement, sharing and supporting, and social connection. Notably, anonymity coexisted with moments of recognition through graphological identification. Distinct conversational strategies also emerged, which allow smoother exchanges and fewer conversational repairs compared to text-based communication. This study contributes to understanding Graphonymous Interaction and Online Spaces, offering insights into designing platforms that support creative and socially engaging forms of communication beyond text.", "AI": {"tldr": "\u7814\u7a76\u533f\u540d\u624b\u5199\u7ed8\u56fe\u4ea4\u4e92(Graphonymous Interaction)\u5728\u6570\u5b57\u5e73\u53f0\u4e2d\u7684\u4f7f\u7528\u6a21\u5f0f\u4e0e\u793e\u4ea4\u6548\u679c", "motivation": "\u6570\u5b57\u901a\u4fe1\u5e73\u53f0\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u533f\u540d\u73af\u5883\u4e2d\u624b\u5199\u548c\u7ed8\u56fe\u4ea4\u4e92\u7684\u7814\u7a76\uff0c\u9700\u8981\u63a2\u7d22\u8fd9\u79cd\u975e\u6587\u672c\u6c9f\u901a\u5f62\u5f0f\u7684\u793e\u4ea4\u4ef7\u503c", "method": "\u5206\u6790Graphonymous Online Space (GOS) CollaNote\u4e2d\u7684600\u591a\u4e2a\u753b\u5e03\u9875\u9762\uff0c\u8bbf\u8c0820\u540d\u7528\u6237\uff0c\u4f7f\u7528\u4f1a\u8bdd\u5206\u6790\u548c\u591a\u6a21\u6001\u8bdd\u8bed\u5206\u6790\u7814\u7a7670\u5206\u949f\u5b9e\u65f6GOS\u4f1a\u8bdd", "result": "Graphonymous Interaction\u4fc3\u8fdb\u827a\u672f\u8868\u8fbe\u3001\u667a\u529b\u53c2\u4e0e\u3001\u5206\u4eab\u652f\u6301\u548c\u793e\u4f1a\u8fde\u63a5\uff1b\u533f\u540d\u6027\u4e0e\u7b14\u8ff9\u8bc6\u522b\u5171\u5b58\uff1b\u76f8\u6bd4\u6587\u672c\u4ea4\u6d41\uff0c\u5177\u6709\u66f4\u6d41\u7545\u7684\u5bf9\u8bdd\u7b56\u7565\u548c\u66f4\u5c11\u7684\u4f1a\u8bdd\u4fee\u590d", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86Graphonymous Interaction\u7684\u72ec\u7279\u4ef7\u503c\uff0c\u4e3a\u8bbe\u8ba1\u652f\u6301\u521b\u9020\u6027\u793e\u4ea4\u6c9f\u901a\u7684\u5e73\u53f0\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u62d3\u5c55\u4e86\u8d85\u8d8a\u6587\u672c\u7684\u6570\u5b57\u4ea4\u6d41\u5f62\u5f0f"}}
{"id": "2602.00566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00566", "abs": "https://arxiv.org/abs/2602.00566", "authors": ["Nan Song", "Junzhe Jiang", "Jingyu Li", "Xiatian Zhu", "Li Zhang"], "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning", "comment": "Accepted at NeurIPS 2025", "summary": "Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving.", "AI": {"tldr": "UniMotion\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8fd0\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7ed3\u6784\u548c\u4e13\u95e8\u8bbe\u8ba1\u540c\u65f6\u652f\u6301\u8fd0\u52a8\u6a21\u62df\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\uff0c\u5728Waymo\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u6a21\u62df\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u867d\u7136\u76ee\u6807\u4e0d\u540c\uff0c\u4f46\u5171\u4eab\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7406\u89e3\u3001\u8fd0\u52a8\u884c\u4e3a\u5efa\u6a21\u548c\u65f6\u7a7a\u52a8\u6001\u63a8\u7406\u7b49\u6838\u5fc3\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u4e13\u95e8\u5316\u6a21\u578b\u8bbe\u8ba1\uff0c\u963b\u788d\u4e86\u8de8\u4efb\u52a1\u6cdb\u5316\u548c\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\uff0c\u4e14\u5ffd\u89c6\u4e86\u4efb\u52a1\u95f4\u7684\u6f5c\u5728\u534f\u540c\u6548\u76ca\u3002", "method": "\u57fa\u4e8e\u89e3\u7801\u5668Transformer\u67b6\u6784\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u4ea4\u4e92\u6a21\u5f0f\u548c\u5b9a\u5236\u8bad\u7ec3\u7b56\u7565\uff0c\u540c\u65f6\u652f\u6301\u591a\u4e2a\u8fd0\u52a8\u4efb\u52a1\u3002\u7edf\u4e00\u8bbe\u8ba1\u652f\u6301\u8054\u5408\u4f18\u5316\u548c\u8868\u793a\u5171\u4eab\uff0c\u5e76\u5141\u8bb8\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728Waymo Open Motion\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8054\u5408\u8bad\u7ec3\u80fd\u5b9e\u73b0\u9c81\u68d2\u7684\u6cdb\u5316\u548c\u6709\u6548\u7684\u4efb\u52a1\u96c6\u6210\u3002\u7ecf\u8fc7\u8fdb\u4e00\u6b65\u5fae\u8c03\uff0cUniMotion\u5728\u591a\u4e2a\u8fd0\u52a8\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "UniMotion\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6355\u6349\u8fd0\u52a8\u4efb\u52a1\u95f4\u7684\u5171\u4eab\u7ed3\u6784\u5e76\u9002\u5e94\u5404\u81ea\u9700\u6c42\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u65b9\u6cd5\u5728\u8fd0\u52a8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.00715", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00715", "abs": "https://arxiv.org/abs/2602.00715", "authors": ["Zehan Chen", "Long Zhang", "Zhiwei Zhang", "JingJing Zhang", "Ruoyu Zhou", "Yulong Shen", "JianFeng Ma", "Lin Yang"], "title": "Beyond Basic Specifications? A Systematic Study of Logical Constructs in LLM-based Specification Generation", "comment": null, "summary": "Formal specifications play a pivotal role in accurately characterizing program behaviors and ensuring software correctness. In recent years, leveraging large language models (LLMs) for the automatic generation of program specifications has emerged as a promising avenue for enhancing verification efficiency. However, existing research has been predominantly confined to generating specifications based on basic syntactic constructs, falling short of meeting the demands for high-level abstraction in complex program verification. Consequently, we propose incorporating logical constructs into existing LLM-based specification generation framework. Nevertheless, there remains a lack of systematic investigation into whether LLMs can effectively generate such complex constructs. To this end, we conduct an empirical study aimed at exploring the impact of various types of syntactic constructs on specification generation framework. Specifically, we define four syntactic configurations with varying levels of abstraction and perform extensive evaluations on mainstream program verification datasets, employing a diverse set of representative LLMs. Experimental results first confirm that LLMs are capable of generating valid logical constructs. Further analysis reveals that the synergistic use of logical constructs and basic syntactic constructs leads to improvements in both verification capability and robustness, without significantly increasing verification overhead. Additionally, we uncover the distinct advantages of two refinement paradigms. To the best of our knowledge, this is the first systematic work exploring the feasibility of utilizing LLMs for generating high-level logical constructs, providing an empirical basis and guidance for the future construction of automated program verification framework with enhanced abstraction capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u57fa\u4e8eLLM\u7684\u7a0b\u5e8f\u89c4\u8303\u751f\u6210\u6846\u67b6\u4e2d\u5f15\u5165\u903b\u8f91\u6784\u9020\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86LLM\u751f\u6210\u590d\u6742\u903b\u8f91\u6784\u9020\u7684\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u903b\u8f91\u6784\u9020\u4e0e\u57fa\u7840\u8bed\u6cd5\u6784\u9020\u7684\u534f\u540c\u4f7f\u7528\u80fd\u63d0\u5347\u9a8c\u8bc1\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u89c4\u8303\u751f\u6210\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u57fa\u7840\u8bed\u6cd5\u6784\u9020\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u7a0b\u5e8f\u9a8c\u8bc1\u5bf9\u9ad8\u7ea7\u62bd\u8c61\u7684\u9700\u6c42\u3002\u9700\u8981\u7cfb\u7edf\u7814\u7a76LLM\u662f\u5426\u80fd\u591f\u6709\u6548\u751f\u6210\u590d\u6742\u7684\u903b\u8f91\u6784\u9020\uff0c\u4ee5\u63d0\u5347\u7a0b\u5e8f\u9a8c\u8bc1\u6846\u67b6\u7684\u62bd\u8c61\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5728\u73b0\u6709LLM\u89c4\u8303\u751f\u6210\u6846\u67b6\u4e2d\u5f15\u5165\u903b\u8f91\u6784\u9020\uff0c\u5b9a\u4e49\u4e86\u56db\u79cd\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u7684\u8bed\u6cd5\u914d\u7f6e\u3002\u5728\u4e3b\u6d41\u7684\u7a0b\u5e8f\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4f7f\u7528\u591a\u79cd\u4ee3\u8868\u6027LLM\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eLLM\u80fd\u591f\u751f\u6210\u6709\u6548\u7684\u903b\u8f91\u6784\u9020\u3002\u903b\u8f91\u6784\u9020\u4e0e\u57fa\u7840\u8bed\u6cd5\u6784\u9020\u7684\u534f\u540c\u4f7f\u7528\u5728\u9a8c\u8bc1\u80fd\u529b\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u4e14\u4e0d\u663e\u8457\u589e\u52a0\u9a8c\u8bc1\u5f00\u9500\u3002\u540c\u65f6\u63ed\u793a\u4e86\u4e24\u79cd\u7cbe\u5316\u8303\u5f0f\u7684\u72ec\u7279\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u63a2\u7d22\u5229\u7528LLM\u751f\u6210\u9ad8\u7ea7\u903b\u8f91\u6784\u9020\u53ef\u884c\u6027\u7684\u5de5\u4f5c\uff0c\u4e3a\u672a\u6765\u6784\u5efa\u5177\u6709\u589e\u5f3a\u62bd\u8c61\u80fd\u529b\u7684\u81ea\u52a8\u5316\u7a0b\u5e8f\u9a8c\u8bc1\u6846\u67b6\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u6307\u5bfc\u3002"}}
{"id": "2602.01892", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01892", "abs": "https://arxiv.org/abs/2602.01892", "authors": ["Alexandre Lombard", "Florent Perronnet", "Nicolas Gaud", "Abdeljalil Abbas-Turki"], "title": "Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study", "comment": null, "summary": "This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u4e3b\u8f66\u8f86\u8def\u5f84\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u70b9\u5b9e\u73b0\u524d\u540e\u8f74\u63a7\u5236\u5e73\u6ed1\u5207\u6362\uff0c\u7ed3\u5408\u66f2\u7387\u611f\u77e5\u7eb5\u5411\u63a7\u5236\uff0c\u63d0\u5347\u8f68\u8ff9\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u8def\u5f84\u8ddf\u8e2a\u65b9\u6cd5\u901a\u5e38\u5c06\u63a7\u5236\u70b9\u56fa\u5b9a\u5728\u8f66\u8f86\u524d\u8f74\u6216\u540e\u8f74\uff0c\u8fd9\u5728\u4e0d\u540c\u9a7e\u9a76\u573a\u666f\uff08\u5982\u4f4e\u901f\u673a\u52a8\u3001\u5012\u8f66\uff09\u4e2d\u8868\u73b0\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u6ed1\u9002\u5e94\u5404\u79cd\u9a7e\u9a76\u73af\u5883\u7684\u63a7\u5236\u6846\u67b6\u3002", "method": "1. \u6a2a\u5411\u63a7\u5236\uff1a\u91c7\u7528\u52a8\u6001\u63a7\u5236\u70b9\u6cbf\u8f74\u8ddd\u8fde\u7eed\u63d2\u503c\uff0c\u7ed3\u5408\u524d\u8f74Stanley\u63a7\u5236\u5668\u548c\u540e\u8f74\u66f2\u7387\u51e0\u4f55\u63a7\u5236\u5668\u7684\u91cd\u5fc3\u6df7\u5408\uff1b2. \u7eb5\u5411\u63a7\u5236\uff1a\u57fa\u4e8e\u865a\u62df\u8f68\u9053\u8fb9\u754c\u548c\u5149\u7ebf\u8ffd\u8e2a\u7684\u66f2\u7387\u611f\u77e5\u7b56\u7565\uff0c\u5c06\u51e0\u4f55\u7ea6\u675f\u8f6c\u6362\u4e3a\u865a\u62df\u969c\u788d\u8ddd\u79bb\u8fdb\u884c\u901f\u5ea6\u8c03\u8282\u3002", "result": "\u5728\u95ed\u73af\u8ddf\u8e2a\u548c\u5012\u8f66\u673a\u52a8\u4e2d\uff0c\u76f8\u6bd4\u56fa\u5b9a\u63a7\u5236\u70b9\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u8f68\u8ff9\u7cbe\u5ea6\u3001\u66f4\u5e73\u6ed1\u7684\u8f6c\u5411\u8f6e\u5ed3\u548c\u589e\u5f3a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u52a8\u6001\u63a7\u5236\u70b9\u6846\u67b6\u901a\u8fc7\u524d\u540e\u8f74\u63a7\u5236\u7684\u5e73\u6ed1\u5207\u6362\u548c\u66f2\u7387\u611f\u77e5\u7eb5\u5411\u8c03\u8282\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u81ea\u4e3b\u8f66\u8f86\u8def\u5f84\u8ddf\u8e2a\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u673a\u52a8\u573a\u666f\u3002"}}
{"id": "2602.01217", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.01217", "abs": "https://arxiv.org/abs/2602.01217", "authors": ["Lucas Lange", "Adrian B\u00f6ttinger", "Victor Christen", "Anushka Vidanage", "Peter Christen", "Erhard Rahm"], "title": "Learning from Anonymized and Incomplete Tabular Data", "comment": null, "summary": "User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.", "AI": {"tldr": "\u7528\u6237\u9a71\u52a8\u9690\u79c1\u5bfc\u81f4\u6570\u636e\u96c6\u4e2d\u6df7\u5408\u539f\u59cb\u503c\u3001\u6cdb\u5316\u503c\u548c\u7f3a\u5931\u503c\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u5f02\u8d28\u533f\u540d\u5316\u6570\u636e\uff0c\u672c\u6587\u63d0\u51fa\u65b0\u7684\u6570\u636e\u8f6c\u6362\u7b56\u7565\u6765\u6062\u590d\u6570\u636e\u6548\u7528\u3002", "motivation": "\u7528\u6237\u9a71\u52a8\u9690\u79c1\u5141\u8bb8\u4e2a\u4eba\u63a7\u5236\u6570\u636e\u5171\u4eab\u7c92\u5ea6\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u4e2d\u540c\u65f6\u5b58\u5728\u539f\u59cb\u503c\u3001\u6cdb\u5316\u503c\u548c\u7f3a\u5931\u503c\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5c06\u975e\u539f\u59cb\u503c\u89c6\u4e3a\u65b0\u7c7b\u522b\u6216\u7f3a\u5931\u503c\uff0c\u4e22\u5f03\u4e86\u6cdb\u5316\u8bed\u4e49\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u8fd9\u79cd\u5f02\u8d28\u533f\u540d\u5316\u6570\u636e\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u6570\u636e\u8f6c\u6362\u7b56\u7565\uff0c\u8003\u8651\u5f02\u8d28\u533f\u540d\u5316\u7279\u6027\uff0c\u5e76\u4e0e\u6807\u51c6\u63d2\u8865\u65b9\u6cd5\u548c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002\u4f7f\u7528\u591a\u4e2a\u6570\u636e\u96c6\u3001\u9690\u79c1\u914d\u7f6e\u548c\u90e8\u7f72\u573a\u666f\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u6cdb\u5316\u503c\u4f18\u4e8e\u7eaf\u6291\u5236\uff1b2) \u6700\u4f73\u6570\u636e\u51c6\u5907\u7b56\u7565\u53d6\u51b3\u4e8e\u5177\u4f53\u573a\u666f\uff1b3) \u4e00\u81f4\u7684\u6570\u636e\u8868\u793a\u5bf9\u4fdd\u6301\u4e0b\u6e38\u6548\u7528\u81f3\u5173\u91cd\u8981\uff1b4) \u672c\u6587\u65b9\u6cd5\u80fd\u53ef\u9760\u5730\u6062\u590d\u6570\u636e\u6548\u7528\u3002", "conclusion": "\u6709\u6548\u5b66\u4e60\u4e0e\u9002\u5f53\u5904\u7406\u533f\u540d\u5316\u503c\u5bc6\u5207\u76f8\u5173\uff0c\u6b63\u786e\u5904\u7406\u7528\u6237\u9a71\u52a8\u9690\u79c1\u4ea7\u751f\u7684\u5f02\u8d28\u533f\u540d\u5316\u6570\u636e\u5bf9\u673a\u5668\u5b66\u4e60\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.01147", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2602.01147", "abs": "https://arxiv.org/abs/2602.01147", "authors": ["Chenchen Feng", "Minyang Chen", "Zhuozhao Li", "Ran Cheng"], "title": "Unleashing the Potential of Differential Evolution through Individual-Level Strategy Diversity", "comment": "Accepted by IEEE TEVC", "summary": "Since Differential Evolution (DE) is sensitive to strategy choice, most existing variants pursue performance through adaptive mechanisms or intricate designs. While these approaches focus on adjusting strategies over time, the structural benefits that static strategy diversity may bring remain largely unexplored. To bridge this gap, we study the impact of individual-level strategy diversity on DE's search dynamics and performance, and introduce iStratDE (DE with individual-level strategies), a minimalist variant that assigns mutation and crossover strategies independently to each individual at initialization and keeps them fixed throughout the evolutionary process. By injecting diversity at the individual level without adaptation or feedback, iStratDE cultivates persistent behavioral heterogeneity that is especially effective with large populations. Moreover, its communication-free construction possesses intrinsic concurrency, thereby enabling efficient parallel execution and straightforward scaling for GPU computing. We further provide a convergence analysis of iStratDE under standard reachability assumptions, which establishes the almost-sure convergence of the best-so-far fitness. Extensive experiments on the CEC2022 benchmark suite and robotic control tasks demonstrate that iStratDE matches or surpasses established adaptive DE variants. These results highlight individual-level strategy assignment as a straightforward yet effective mechanism for enhancing DE's performance. The source code of iStratDE is publicly accessible at: https://github.com/EMI-Group/istratde.", "code_url": "https://github.com/EMI-Group/istratde", "code_stars": 3, "code_last_update": "2026-02-03", "AI": {"tldr": "\u63d0\u51faiStratDE\u65b9\u6cd5\uff0c\u5728\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u4e2d\u4e3a\u6bcf\u4e2a\u4e2a\u4f53\u72ec\u7acb\u5206\u914d\u56fa\u5b9a\u7684\u53d8\u5f02\u548c\u4ea4\u53c9\u7b56\u7565\uff0c\u901a\u8fc7\u4e2a\u4f53\u5c42\u9762\u7684\u9759\u6001\u7b56\u7565\u591a\u6837\u6027\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u81ea\u9002\u5e94\u673a\u5236\uff0c\u652f\u6301\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u901a\u5e38\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u6216\u590d\u6742\u8bbe\u8ba1\u6765\u8c03\u6574\u7b56\u7565\uff0c\u4f46\u9759\u6001\u7b56\u7565\u591a\u6837\u6027\u5728\u4e2a\u4f53\u5c42\u9762\u7684\u7ed3\u6784\u4f18\u52bf\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u4e2a\u4f53\u5c42\u9762\u7b56\u7565\u591a\u6837\u6027\u5bf9DE\u641c\u7d22\u52a8\u6001\u548c\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faiStratDE\u65b9\u6cd5\uff0c\u5728\u521d\u59cb\u5316\u65f6\u4e3a\u6bcf\u4e2a\u4e2a\u4f53\u72ec\u7acb\u5206\u914d\u53d8\u5f02\u548c\u4ea4\u53c9\u7b56\u7565\u5e76\u4fdd\u6301\u56fa\u5b9a\uff0c\u4e0d\u91c7\u7528\u81ea\u9002\u5e94\u6216\u53cd\u9988\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e2a\u4f53\u5c42\u9762\u7684\u9759\u6001\u7b56\u7565\u591a\u6837\u6027\u4ea7\u751f\u6301\u4e45\u7684\u884c\u4e3a\u5f02\u8d28\u6027\uff0c\u7279\u522b\u9002\u5408\u5927\u79cd\u7fa4\u3002\u5176\u65e0\u901a\u4fe1\u7ed3\u6784\u5177\u6709\u5185\u5728\u5e76\u53d1\u6027\uff0c\u652f\u6301\u9ad8\u6548\u5e76\u884c\u6267\u884c\u548cGPU\u8ba1\u7b97\u3002", "result": "\u5728CEC2022\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u548c\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0ciStratDE\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u5df2\u5efa\u7acb\u7684\u81ea\u9002\u5e94DE\u53d8\u4f53\u3002\u6536\u655b\u5206\u6790\u8bc1\u660e\u4e86\u5728\u6807\u51c6\u53ef\u8fbe\u6027\u5047\u8bbe\u4e0b\u6700\u4f73\u9002\u5e94\u5ea6\u7684\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u3002", "conclusion": "\u4e2a\u4f53\u5c42\u9762\u7684\u7b56\u7565\u5206\u914d\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u673a\u5236\uff0c\u80fd\u591f\u589e\u5f3a\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u7684\u6027\u80fd\u3002iStratDE\u5c55\u793a\u4e86\u9759\u6001\u7b56\u7565\u591a\u6837\u6027\u5728\u4e2a\u4f53\u5c42\u9762\u7684\u4ef7\u503c\uff0c\u4e3aDE\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2602.00270", "categories": ["cs.CR", "cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00270", "abs": "https://arxiv.org/abs/2602.00270", "authors": ["Mohsen Salehi", "Karthik Pattabiraman"], "title": "RVDebloater: Mode-based Adaptive Firmware Debloating for Robotic Vehicles", "comment": null, "summary": "As the number of embedded devices grows and their functional requirements increase, embedded firmware is becoming increasingly larger, thereby expanding its attack surface. Despite the increase in firmware size, many embedded devices, such as robotic vehicles (RVs), operate in distinct modes, each requiring only a small subset of the firmware code at runtime. We refer to such devices as mode-based embedded devices. Debloating is an approach to reduce attack surfaces by removing or restricting unneeded code, but existing techniques suffer from significant limitations, such as coarse granularity and irreversible code removal, limiting their applicability.\n  To address these limitations, we propose RVDebloater, a novel adaptive debloating technique for mode-based embedded devices that automatically identifies unneeded firmware code for each mode using either static or dynamic analysis, and dynamically debloats the firmware for each mode at the function level at runtime. RVDebloater introduces a new software-based enforcement approach that supports diverse mode-based embedded devices. We implemented RVDebloater using the LLVM compiler and evaluated its efficiency and effectiveness on six different RVs, including both simulated and real ones, with different real-world missions. We find that device requirements change throughout its lifetime for each mode, and that many critical firmware functions can be restricted in other modes, with an average of 85% of functions not being required. The results showed that none of the missions failed after debloating with RVDebloater, indicating that it neither incurred false positives nor false negatives. Further, RVDebloater prunes the firmware call graph by an average of 45% across different firmware. Finally, RVDebloater incurred an average performance overhead of 3.9% and memory overhead of 4% (approximately 0.25 MB) on real RVs.", "AI": {"tldr": "RVDebloater\uff1a\u4e00\u79cd\u9488\u5bf9\u6a21\u5f0f\u5316\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u81ea\u9002\u5e94\u53bb\u81a8\u80c0\u6280\u672f\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u51fd\u6570\u7ea7\u52a8\u6001\u53bb\u81a8\u80c0\u51cf\u5c11\u653b\u51fb\u9762\uff0c\u5e73\u5747\u53ef\u9650\u523685%\u7684\u975e\u5fc5\u9700\u51fd\u6570\uff0c\u6027\u80fd\u5f00\u9500\u4ec53.9%", "motivation": "\u968f\u7740\u5d4c\u5165\u5f0f\u8bbe\u5907\u6570\u91cf\u589e\u957f\u548c\u529f\u80fd\u9700\u6c42\u589e\u52a0\uff0c\u56fa\u4ef6\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\u5bfc\u81f4\u653b\u51fb\u9762\u589e\u52a0\u3002\u8bb8\u591a\u5d4c\u5165\u5f0f\u8bbe\u5907\uff08\u5982\u673a\u5668\u4eba\u8f66\u8f86\uff09\u5728\u4e0d\u540c\u6a21\u5f0f\u4e0b\u8fd0\u884c\u65f6\u4ec5\u9700\u5c11\u91cf\u56fa\u4ef6\u4ee3\u7801\uff0c\u4f46\u73b0\u6709\u53bb\u81a8\u80c0\u6280\u672f\u5b58\u5728\u7c92\u5ea6\u7c97\u3001\u4e0d\u53ef\u9006\u4ee3\u7801\u79fb\u9664\u7b49\u9650\u5236\uff0c\u96be\u4ee5\u6709\u6548\u5e94\u7528\u3002", "method": "\u63d0\u51faRVDebloater\uff0c\u4e00\u79cd\u9488\u5bf9\u6a21\u5f0f\u5316\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u81ea\u9002\u5e94\u53bb\u81a8\u80c0\u6280\u672f\u3002\u901a\u8fc7\u9759\u6001\u6216\u52a8\u6001\u5206\u6790\u81ea\u52a8\u8bc6\u522b\u6bcf\u4e2a\u6a21\u5f0f\u4e0b\u4e0d\u9700\u8981\u7684\u56fa\u4ef6\u4ee3\u7801\uff0c\u5728\u8fd0\u884c\u65f6\u4ee5\u51fd\u6570\u7c92\u5ea6\u52a8\u6001\u53bb\u81a8\u80c0\u3002\u91c7\u7528\u57fa\u4e8e\u8f6f\u4ef6\u7684\u5f3a\u5236\u6267\u884c\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u6a21\u5f0f\u5316\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002\u57fa\u4e8eLLVM\u7f16\u8bd1\u5668\u5b9e\u73b0\u3002", "result": "\u57286\u79cd\u4e0d\u540c\u673a\u5668\u4eba\u8f66\u8f86\uff08\u5305\u62ec\u6a21\u62df\u548c\u771f\u5b9e\u8bbe\u5907\uff09\u4e0a\u8bc4\u4f30\uff0c\u53d1\u73b0\u5e73\u574785%\u7684\u51fd\u6570\u5728\u5176\u4ed6\u6a21\u5f0f\u4e0b\u53ef\u88ab\u9650\u5236\uff0c\u56fa\u4ef6\u8c03\u7528\u56fe\u5e73\u5747\u7f29\u51cf45%\u3002\u53bb\u81a8\u80c0\u540e\u6240\u6709\u4efb\u52a1\u5747\u672a\u5931\u8d25\uff0c\u8868\u660e\u65e0\u5047\u9633\u6027\u6216\u5047\u9634\u6027\u3002\u771f\u5b9e\u8bbe\u5907\u4e0a\u5e73\u5747\u6027\u80fd\u5f00\u95003.9%\uff0c\u5185\u5b58\u5f00\u95004%\uff08\u7ea60.25MB\uff09\u3002", "conclusion": "RVDebloater\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u53bb\u81a8\u80c0\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6a21\u5f0f\u5316\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u653b\u51fb\u9762\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5f00\u9500\u548c\u9ad8\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u73af\u5883\u3002"}}
{"id": "2602.00402", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00402", "abs": "https://arxiv.org/abs/2602.00402", "authors": ["Aditya Kumar Purohit", "Hendrik Heuer"], "title": "A Conditional Companion: Lived Experiences of People with Mental Health Disorders Using LLMs", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used for mental health support, yet little is known about how people with mental health challenges engage with them, how they evaluate their usefulness, and what design opportunities they envision. We conducted 20 semi-structured interviews with people in the UK who live with mental health conditions and have used LLMs for mental health support. Through reflexive thematic analysis, we found that participants engaged with LLMs in conditional and situational ways: for immediacy, the desire for non-judgement, self-paced disclosure, cognitive reframing, and relational engagement. Simultaneously, participants articulated clear boundaries informed by prior therapeutic experience: LLMs were effective for mild-to-moderate distress but inadequate for crises, trauma, and complex social-emotional situations. We contribute empirical insights into the lived use of LLMs for mental health, highlight boundary-setting as central to their safe role, and propose design and governance directions for embedding them responsibly within care ecosystem.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8bbf\u8c08\u63a2\u7d22\u5fc3\u7406\u5065\u5eb7\u60a3\u8005\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u5fc3\u7406\u652f\u6301\u7684\u5b9e\u9645\u60c5\u51b5\u3001\u8bc4\u4f30\u65b9\u5f0f\u548c\u8bbe\u8ba1\u673a\u4f1a", "motivation": "\u5c3d\u7ba1LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u652f\u6301\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5fc3\u7406\u5065\u5eb7\u60a3\u8005\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u3001\u8bc4\u4f30\u65b9\u5f0f\u548c\u8bbe\u8ba1\u673a\u4f1a\u7684\u5b9e\u8bc1\u7814\u7a76", "method": "\u5bf920\u540d\u82f1\u56fd\u5fc3\u7406\u5065\u5eb7\u60a3\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u91c7\u7528\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\u6cd5\u5206\u6790\u4ed6\u4eec\u4f7f\u7528LLMs\u8fdb\u884c\u5fc3\u7406\u652f\u6301\u7684\u7ecf\u5386", "result": "\u53c2\u4e0e\u8005\u6709\u6761\u4ef6\u6027\u548c\u60c5\u5883\u6027\u5730\u4f7f\u7528LLMs\uff1a\u5bfb\u6c42\u5373\u65f6\u6027\u3001\u65e0\u8bc4\u5224\u6027\u3001\u81ea\u6211\u8282\u594f\u7684\u62ab\u9732\u3001\u8ba4\u77e5\u91cd\u6784\u548c\u5173\u7cfb\u53c2\u4e0e\uff1b\u540c\u65f6\u57fa\u4e8e\u6cbb\u7597\u7ecf\u9a8c\u8bbe\u5b9a\u660e\u786e\u8fb9\u754c\uff1aLLMs\u9002\u7528\u4e8e\u8f7b\u5ea6\u81f3\u4e2d\u5ea6\u56f0\u6270\uff0c\u4f46\u5bf9\u5371\u673a\u3001\u521b\u4f24\u548c\u590d\u6742\u793e\u4f1a\u60c5\u611f\u60c5\u5883\u4e0d\u8db3", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86LLMs\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u5b9e\u9645\u4f7f\u7528\u7684\u5b9e\u8bc1\u89c1\u89e3\uff0c\u5f3a\u8c03\u8fb9\u754c\u8bbe\u5b9a\u5bf9\u5b89\u5168\u4f7f\u7528\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u5728\u62a4\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u8d1f\u8d23\u4efb\u5d4c\u5165LLMs\u7684\u8bbe\u8ba1\u548c\u6cbb\u7406\u65b9\u5411"}}
{"id": "2602.02121", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.02121", "abs": "https://arxiv.org/abs/2602.02121", "authors": ["George Violettas", "Lefteris Mamatas"], "title": "TriCloudEdge: A multi-layer Cloud Continuum", "comment": "16 pages, 6 figures", "summary": "TriCloudEdge is a scalable three-tier cloud continuum that integrates far-edge devices, intermediate edge nodes, and central cloud services, working in parallel as a unified solution. At the far edge, ultra-low-cost microcontrollers can handle lightweight AI tasks, while intermediate edge devices provide local intelligence, and the cloud tier offers large-scale analytics, federated learning, model adaptation, and global identity management. The proposed architecture enables multi-protocols and technologies (WebSocket, MQTT, HTTP) compared to a versatile protocol (Zenoh) to transfer diverse bidirectional data across the tiers, offering a balance between computational challenges and latency requirements. Comparative implementations between these two architectures demonstrate the trade-offs between resource utilization and communication efficiency. The results show that TriCloudEdge can distribute computational challenges to address latency and privacy concerns. The work also presents tests of AI model adaptation on the far edge and the computational effort challenges under the prism of parallelism. This work offers a perspective on the practical continuum challenges of implementation aligned with recent research advances addressing challenges across the different cloud levels.", "AI": {"tldr": "TriCloudEdge\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4e09\u5c42\u4e91\u8fde\u7eed\u4f53\u67b6\u6784\uff0c\u96c6\u6210\u4e86\u8fdc\u8fb9\u7f18\u8bbe\u5907\u3001\u4e2d\u95f4\u8fb9\u7f18\u8282\u70b9\u548c\u4e2d\u592e\u4e91\u670d\u52a1\uff0c\u901a\u8fc7\u5e76\u884c\u5de5\u4f5c\u63d0\u4f9b\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u67b6\u6784\u652f\u6301\u591a\u79cd\u534f\u8bae\u548c\u6280\u672f\uff0c\u5728\u8d44\u6e90\u5229\u7528\u548c\u901a\u4fe1\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u80fd\u591f\u5206\u53d1\u8ba1\u7b97\u6311\u6218\u4ee5\u89e3\u51b3\u5ef6\u8fdf\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4e91\u8fde\u7eed\u4f53\u5b9e\u9645\u5b9e\u65bd\u4e2d\u7684\u6311\u6218\uff0c\u5e73\u8861\u8ba1\u7b97\u9700\u6c42\u548c\u5ef6\u8fdf\u8981\u6c42\uff0c\u540c\u65f6\u5904\u7406\u9690\u79c1\u95ee\u9898\u3002\u901a\u8fc7\u96c6\u6210\u4e0d\u540c\u5c42\u7ea7\u7684\u8bbe\u5907\uff08\u4ece\u8d85\u4f4e\u6210\u672c\u5fae\u63a7\u5236\u5668\u5230\u4e2d\u592e\u4e91\u670d\u52a1\uff09\uff0c\u4e3a\u591a\u6837\u5316AI\u4efb\u52a1\u63d0\u4f9b\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u5c42\u67b6\u6784\uff1a1) \u8fdc\u8fb9\u7f18\u5c42\u4f7f\u7528\u8d85\u4f4e\u6210\u672c\u5fae\u63a7\u5236\u5668\u5904\u7406\u8f7b\u91cf\u7ea7AI\u4efb\u52a1\uff1b2) \u4e2d\u95f4\u8fb9\u7f18\u5c42\u63d0\u4f9b\u672c\u5730\u667a\u80fd\uff1b3) \u4e91\u5c42\u63d0\u4f9b\u5927\u89c4\u6a21\u5206\u6790\u3001\u8054\u90a6\u5b66\u4e60\u3001\u6a21\u578b\u9002\u5e94\u548c\u5168\u5c40\u8eab\u4efd\u7ba1\u7406\u3002\u91c7\u7528\u591a\u534f\u8bae\uff08WebSocket\u3001MQTT\u3001HTTP\uff09\u4e0e\u7edf\u4e00\u534f\u8bae\uff08Zenoh\uff09\u7684\u5bf9\u6bd4\u67b6\u6784\uff0c\u5b9e\u73b0\u8de8\u5c42\u53cc\u5411\u6570\u636e\u4f20\u8f93\u3002\u6d4b\u8bd5\u4e86AI\u6a21\u578b\u5728\u8fdc\u8fb9\u7f18\u7684\u9002\u5e94\u6027\u548c\u5e76\u884c\u8ba1\u7b97\u6311\u6218\u3002", "result": "\u6bd4\u8f83\u4e24\u79cd\u67b6\u6784\uff08\u591a\u534f\u8bae\u4e0eZenoh\u7edf\u4e00\u534f\u8bae\uff09\u5728\u8d44\u6e90\u5229\u7528\u548c\u901a\u4fe1\u6548\u7387\u65b9\u9762\u7684\u6743\u8861\u3002TriCloudEdge\u80fd\u591f\u6709\u6548\u5206\u53d1\u8ba1\u7b97\u6311\u6218\uff0c\u89e3\u51b3\u5ef6\u8fdf\u548c\u9690\u79c1\u95ee\u9898\u3002\u5c55\u793a\u4e86\u8fdc\u8fb9\u7f18AI\u6a21\u578b\u9002\u5e94\u7684\u6d4b\u8bd5\u7ed3\u679c\uff0c\u4ee5\u53ca\u5728\u5e76\u884c\u6027\u89c6\u89d2\u4e0b\u7684\u8ba1\u7b97\u5de5\u4f5c\u91cf\u6311\u6218\u3002", "conclusion": "TriCloudEdge\u4e3a\u4e91\u8fde\u7eed\u4f53\u5b9e\u65bd\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c6\u89d2\uff0c\u4e0e\u6700\u8fd1\u7814\u7a76\u8fdb\u5c55\u4fdd\u6301\u4e00\u81f4\uff0c\u89e3\u51b3\u4e86\u4e0d\u540c\u4e91\u5c42\u7ea7\u95f4\u7684\u6311\u6218\u3002\u8be5\u67b6\u6784\u901a\u8fc7\u4e09\u5c42\u96c6\u6210\u548c\u7075\u6d3b\u7684\u901a\u4fe1\u534f\u8bae\uff0c\u5728\u8ba1\u7b97\u9700\u6c42\u3001\u5ef6\u8fdf\u8981\u6c42\u548c\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u591a\u6837\u5316AI\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00746", "categories": ["cs.SE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00746", "abs": "https://arxiv.org/abs/2602.00746", "authors": ["Jianping Zhong", "Guochang Li", "Chen Zhi", "Junxiao Han", "Zhen Qin", "Xinkui Zhao", "Nan Wang", "Shuiguang Deng", "Jianwei Yin"], "title": "Can Vision-Language Models Handle Long-Context Code? An Empirical Study on Visual Compression", "comment": null, "summary": "Large Language Models (LLMs) struggle with long-context code due to window limitations. Existing textual code compression methods mitigate this via selective filtering but often disrupt dependency closure, causing semantic fragmentation. To address this, we introduce LongCodeOCR, a visual compression framework that renders code into compressed two-dimensional image sequences for Vision-Language Models (VLMs). By preserving a global view, this approach avoids the dependency breakage inherent in filtering. We systematically evaluate LongCodeOCR against the state-of-the-art LongCodeZip across four benchmarks spanning code summarization, code question answering, and code completion.\n  Our results demonstrate that visual code compression serves as a viable alternative for tasks requiring global understanding. At comparable compression ratios ($\\sim$1.7$\\times$), LongCodeOCR improves CompScore on Long Module Summarization by 36.85 points over LongCodeZip. At a 1M-token context length with Glyph (a specialized 9B VLM), LongCodeOCR maintains higher accuracy than LongCodeZip while operating at about 4$\\times$ higher compression. Moreover, compared with LongCodeZip, LongCodeOCR drastically reduces compression-stage overhead (reducing latency from $\\sim$4.3 hours to $\\sim$1 minute at 1M tokens). Finally, our results characterize a fundamental coverage--fidelity trade-off: visual code compression retains broader context coverage to support global dependencies, yet faces fidelity bottlenecks on exactness-critical tasks; by contrast, textual code compression preserves symbol-level precision while sacrificing structural coverage.", "AI": {"tldr": "LongCodeOCR\uff1a\u4e00\u79cd\u89c6\u89c9\u4ee3\u7801\u538b\u7f29\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u6e32\u67d3\u4e3a\u538b\u7f29\u7684\u4e8c\u7ef4\u56fe\u50cf\u5e8f\u5217\u4f9b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff0c\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u5904\u7406\u4e2d\u7684\u4f9d\u8d56\u5173\u7cfb\u65ad\u88c2\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u65f6\u53d7\u7a97\u53e3\u9650\u5236\uff0c\u73b0\u6709\u6587\u672c\u4ee3\u7801\u538b\u7f29\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u8fc7\u6ee4\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u5f80\u5f80\u7834\u574f\u4f9d\u8d56\u95ed\u5305\uff0c\u5bfc\u81f4\u8bed\u4e49\u788e\u7247\u5316\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u5168\u5c40\u89c6\u56fe\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLongCodeOCR\u89c6\u89c9\u538b\u7f29\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u6e32\u67d3\u4e3a\u538b\u7f29\u7684\u4e8c\u7ef4\u56fe\u50cf\u5e8f\u5217\uff0c\u4f9b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u4fdd\u6301\u5168\u5c40\u89c6\u56fe\uff0c\u907f\u514d\u4e86\u8fc7\u6ee4\u65b9\u6cd5\u56fa\u6709\u7684\u4f9d\u8d56\u5173\u7cfb\u65ad\u88c2\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLongCodeOCR\u5728\u4ee3\u7801\u6458\u8981\u3001\u4ee3\u7801\u95ee\u7b54\u548c\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LongCodeZip\u3002\u5728\u76f8\u4f3c\u538b\u7f29\u6bd4\u4e0b\uff0cLongCodeOCR\u5728\u957f\u6a21\u5757\u6458\u8981\u4efb\u52a1\u4e0aCompScore\u63d0\u9ad836.85\u5206\uff1b\u57281M\u4ee4\u724c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0c\u4fdd\u6301\u66f4\u9ad8\u51c6\u786e\u7387\u4e14\u538b\u7f29\u7387\u63d0\u9ad8\u7ea64\u500d\uff1b\u538b\u7f29\u9636\u6bb5\u5f00\u9500\u5927\u5e45\u964d\u4f4e\uff081M\u4ee4\u724c\u4e0b\u5ef6\u8fdf\u4ece\u7ea64.3\u5c0f\u65f6\u964d\u81f3\u7ea61\u5206\u949f\uff09\u3002", "conclusion": "\u89c6\u89c9\u4ee3\u7801\u538b\u7f29\u662f\u4efb\u52a1\u9700\u8981\u5168\u5c40\u7406\u89e3\u65f6\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u8986\u76d6\u5ea6-\u4fdd\u771f\u5ea6\u6743\u8861\uff1a\u89c6\u89c9\u538b\u7f29\u4fdd\u7559\u66f4\u5e7f\u7684\u4e0a\u4e0b\u6587\u8986\u76d6\u4ee5\u652f\u6301\u5168\u5c40\u4f9d\u8d56\uff0c\u4f46\u5728\u7cbe\u786e\u6027\u5173\u952e\u4efb\u52a1\u4e0a\u9762\u4e34\u4fdd\u771f\u5ea6\u74f6\u9888\uff1b\u6587\u672c\u538b\u7f29\u4fdd\u6301\u7b26\u53f7\u7ea7\u7cbe\u5ea6\u4f46\u727a\u7272\u7ed3\u6784\u8986\u76d6\u3002"}}
{"id": "2602.00481", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00481", "abs": "https://arxiv.org/abs/2602.00481", "authors": ["Danlin Zheng", "Xiaoying Wei", "Chao Liu", "Quanyu Zhang", "Jingling Zhang", "Shihui Duo", "Mingming Fan"], "title": "From Performers to Creators: Understanding Retired Women's Perceptions of Technology-Enhanced Dance Performance", "comment": null, "summary": "Over 100 million retired women in China engage in dance, but their performances are constrained by limited resources and age-related decline. While interactive dance technologies can enhance artistic expression, existing systems are largely inaccessible to non-professional older dancers. This paper explores how interactive dance technologies can be designed with an age-sensitive approach to support retired women in enhancing their stage performance. We conducted two workshops with community-based retired women dancers, employing interactive dance and LLM-powered video generation probes in co-design activities. Findings indicate that age-sensitive adaptations, such as low-barrier keyword input, motion-aligned visual effects, and participatory scaffolds, lowered technical barriers and fostered a sense of authorship. These features enabled retired women to empower their stage, transitioning from passive recipients of stage design to empowered co-creators of performance. We outline design implications for incorporating interactive dance and artificial intelligence-generated content (AIGC) into the cultural practices of retired women, offering broader strategies for age-sensitive creative technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5982\u4f55\u4e3a\u9000\u4f11\u5973\u6027\u821e\u8005\u8bbe\u8ba1\u5e74\u9f84\u654f\u611f\u7684\u4ea4\u4e92\u5f0f\u821e\u8e48\u6280\u672f\uff0c\u901a\u8fc7\u5de5\u4f5c\u574a\u53d1\u73b0\u4f4e\u95e8\u69db\u5173\u952e\u8bcd\u8f93\u5165\u3001\u52a8\u4f5c\u5bf9\u9f50\u89c6\u89c9\u6548\u679c\u7b49\u9002\u5e94\u6027\u8bbe\u8ba1\u80fd\u964d\u4f4e\u6280\u672f\u969c\u788d\uff0c\u4f7f\u5979\u4eec\u4ece\u88ab\u52a8\u63a5\u53d7\u8005\u8f6c\u53d8\u4e3a\u8868\u6f14\u7684\u5171\u540c\u521b\u9020\u8005\u3002", "motivation": "\u4e2d\u56fd\u6709\u8d85\u8fc71\u4ebf\u9000\u4f11\u5973\u6027\u53c2\u4e0e\u821e\u8e48\u6d3b\u52a8\uff0c\u4f46\u5979\u4eec\u7684\u8868\u6f14\u53d7\u5230\u8d44\u6e90\u6709\u9650\u548c\u5e74\u9f84\u76f8\u5173\u80fd\u529b\u4e0b\u964d\u7684\u9650\u5236\u3002\u73b0\u6709\u7684\u4ea4\u4e92\u5f0f\u821e\u8e48\u6280\u672f\u4e3b\u8981\u9762\u5411\u4e13\u4e1a\u821e\u8005\uff0c\u5bf9\u975e\u4e13\u4e1a\u7684\u8001\u5e74\u821e\u8005\u6765\u8bf4\u96be\u4ee5\u4f7f\u7528\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5982\u4f55\u4ee5\u5e74\u9f84\u654f\u611f\u7684\u65b9\u5f0f\u8bbe\u8ba1\u4ea4\u4e92\u5f0f\u821e\u8e48\u6280\u672f\uff0c\u652f\u6301\u9000\u4f11\u5973\u6027\u63d0\u5347\u821e\u53f0\u8868\u6f14\u3002", "method": "\u7814\u7a76\u91c7\u7528\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e0e\u793e\u533a\u9000\u4f11\u5973\u6027\u821e\u8005\u8fdb\u884c\u4e86\u4e24\u4e2a\u5de5\u4f5c\u574a\u3002\u5de5\u4f5c\u574a\u4e2d\u4f7f\u7528\u4e86\u4ea4\u4e92\u5f0f\u821e\u8e48\u548cLLM\u9a71\u52a8\u7684\u89c6\u9891\u751f\u6210\u4f5c\u4e3a\u8bbe\u8ba1\u63a2\u9488\uff0c\u5728\u5171\u540c\u8bbe\u8ba1\u6d3b\u52a8\u4e2d\u6536\u96c6\u53cd\u9988\u548c\u89c1\u89e3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5e74\u9f84\u654f\u611f\u7684\u9002\u5e94\u6027\u8bbe\u8ba1\uff08\u5982\u4f4e\u95e8\u69db\u5173\u952e\u8bcd\u8f93\u5165\u3001\u52a8\u4f5c\u5bf9\u9f50\u7684\u89c6\u89c9\u6548\u679c\u548c\u53c2\u4e0e\u5f0f\u652f\u67b6\uff09\u80fd\u591f\u663e\u8457\u964d\u4f4e\u6280\u672f\u969c\u788d\uff0c\u5e76\u57f9\u517b\u4f5c\u8005\u7684\u5f52\u5c5e\u611f\u3002\u8fd9\u4e9b\u529f\u80fd\u4f7f\u9000\u4f11\u5973\u6027\u80fd\u591f\"\u8d4b\u80fd\"\u5979\u4eec\u7684\u821e\u53f0\uff0c\u4ece\u821e\u53f0\u8bbe\u8ba1\u7684\u88ab\u52a8\u63a5\u53d7\u8005\u8f6c\u53d8\u4e3a\u8868\u6f14\u7684\u8d4b\u80fd\u5171\u540c\u521b\u9020\u8005\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u5c06\u4ea4\u4e92\u5f0f\u821e\u8e48\u548c\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u878d\u5165\u9000\u4f11\u5973\u6027\u6587\u5316\u5b9e\u8df5\u7684\u8bbe\u8ba1\u542f\u793a\uff0c\u4e3a\u5e74\u9f84\u654f\u611f\u7684\u521b\u610f\u6280\u672f\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u7b56\u7565\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5177\u5305\u5bb9\u6027\u7684\u821e\u8e48\u6280\u672f\uff0c\u652f\u6301\u8001\u5e74\u7fa4\u4f53\u7684\u827a\u672f\u8868\u8fbe\u548c\u521b\u4f5c\u53c2\u4e0e\u3002"}}
{"id": "2602.02249", "categories": ["cs.NI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.02249", "abs": "https://arxiv.org/abs/2602.02249", "authors": ["Florentin Putz", "Philipp Fortmann", "Jan Frank", "Christoph Haugwitz", "Mario Kupnik", "Matthias Hollick"], "title": "Evaluating Acoustic Data Transmission Schemes for Ad-Hoc Communication Between Nearby Smart Devices", "comment": "31 pages, 9 figures, the dataset is available at https://doi.org/10.5281/zenodo.17661991", "summary": "Acoustic data transmission offers a compelling alternative to Bluetooth and NFC by leveraging the ubiquitous speakers and microphones in smartphones and IoT devices. However, most research in this field relies on simulations or limited on-device testing, which makes the real-world reliability of proposed schemes difficult to assess. We systematically reviewed 31 acoustic communication studies for commodity devices and found that none provided accessible source code. After contacting authors and re-implementing three promising schemes, we assembled a testbed of eight representative acoustic communication systems. Using over 11000 smartphone transmissions in both realistic indoor environments and an anechoic chamber, we provide a systematic and repeatable methodology for evaluating the reliability and generalizability of these schemes under real-world conditions. Our results show that many existing schemes face challenges in practical usage, largely due to severe multipath propagation indoors and varying audio characteristics across device models. To support future research and foster more robust evaluations, we release our re-implementations alongside the first comprehensive dataset of real-world acoustic transmissions. Overall, our findings highlight the importance of rigorous on-device testing and underscore the need for robust design strategies to bridge the gap between simulation results and reliable IoT deployments.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u4e8631\u4e2a\u58f0\u5b66\u901a\u4fe1\u65b9\u6848\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9e\u73b03\u4e2a\u6709\u524d\u666f\u7684\u65b9\u6848\u5e76\u6784\u5efa\u5305\u542b8\u4e2a\u7cfb\u7edf\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u8d85\u8fc711000\u6b21\u667a\u80fd\u624b\u673a\u4f20\u8f93\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6848\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u58f0\u5b66\u4f20\u8f93\u6570\u636e\u96c6\u3002", "motivation": "\u58f0\u5b66\u6570\u636e\u4f20\u8f93\u4f5c\u4e3a\u84dd\u7259\u548cNFC\u7684\u66ff\u4ee3\u65b9\u6848\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5927\u591a\u4f9d\u8d56\u4eff\u771f\u6216\u6709\u9650\u7684\u8bbe\u5907\u6d4b\u8bd5\uff0c\u96be\u4ee5\u8bc4\u4f30\u65b9\u6848\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u9760\u6027\u3002\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u7684\u6e90\u4ee3\u7801\u548c\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "1. \u7cfb\u7edf\u56de\u987e31\u4e2a\u58f0\u5b66\u901a\u4fe1\u7814\u7a76\uff1b2. \u8054\u7cfb\u4f5c\u8005\u83b7\u53d6\u6e90\u4ee3\u7801\uff0c\u91cd\u65b0\u5b9e\u73b03\u4e2a\u6709\u524d\u666f\u7684\u65b9\u6848\uff1b3. \u6784\u5efa\u5305\u542b8\u4e2a\u4ee3\u8868\u6027\u58f0\u5b66\u901a\u4fe1\u7cfb\u7edf\u7684\u6d4b\u8bd5\u5e73\u53f0\uff1b4. \u5728\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u548c\u6d88\u58f0\u5ba4\u4e2d\u8fdb\u884c\u8d85\u8fc711000\u6b21\u667a\u80fd\u624b\u673a\u4f20\u8f93\u6d4b\u8bd5\uff1b5. \u5f00\u53d1\u7cfb\u7edf\u5316\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "1. \u53d1\u73b0\u8bb8\u591a\u73b0\u6709\u65b9\u6848\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5ba4\u5185\u4e25\u91cd\u7684\u591a\u5f84\u4f20\u64ad\u548c\u8bbe\u5907\u6a21\u578b\u95f4\u4e0d\u540c\u7684\u97f3\u9891\u7279\u6027\uff1b2. \u73b0\u6709\u65b9\u6848\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b3. \u4eff\u771f\u7ed3\u679c\u4e0e\u53ef\u9760\u7269\u8054\u7f51\u90e8\u7f72\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u4e25\u683c\u7684\u8bbe\u5907\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\uff0c\u9700\u8981\u9c81\u68d2\u7684\u8bbe\u8ba1\u7b56\u7565\u6765\u5f25\u5408\u4eff\u771f\u7ed3\u679c\u4e0e\u53ef\u9760\u7269\u8054\u7f51\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u53d1\u5e03\u4e86\u91cd\u65b0\u5b9e\u73b0\u7684\u4ee3\u7801\u548c\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u58f0\u5b66\u4f20\u8f93\u7efc\u5408\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u548c\u4fc3\u8fdb\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u3002"}}
{"id": "2602.00757", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00757", "abs": "https://arxiv.org/abs/2602.00757", "authors": ["Yuan Si", "Simeng Han", "Daming Li", "Hanyuan Shi", "Jialu Zhang"], "title": "ScratchEval : A Multimodal Evaluation Framework for LLMs in Block-Based Programming", "comment": null, "summary": "LLMs have achieved strong performance on text-based programming tasks, yet they remain unreliable for block-based languages such as Scratch. Scratch programs exhibit deeply nested, non-linear structures, event-driven concurrency across multiple sprites, and tight coupling between code and multimedia assets, properties that differ fundamentally from textual code. As a result, LLMs often misinterpret Scratch semantics and generate large, invasive edits that are syntactically valid but semantically incorrect when repairing buggy programs.\n  We introduce ScratchEval, the first executable benchmark designed to evaluate LLM-based repair for Scratch programs, covering program understanding, debugging, analysis, and repair. The benchmark contains 100 curated Scratch projects from the public repository, selected for structural and semantic complexity. Each project is paired with executable test suites, bug descriptions with corresponding fixes, block-level edit constraints defining minimal semantically correct repairs, and required multimedia assets. The benchmark is constructed through a human-in-the-loop pipeline combining automated project mining with expert validation of trigger-outcome semantics and representative bug patterns, with emphasis on event ordering, concurrency, and state management.\n  To enable rigorous and reproducible evaluation, we propose a three-layer executable protocol measuring functional correctness via VM-level execution, repair quality using block-level edit distance and behavioral trajectory comparisons, and explanation quality via structured rubrics assessing alignment between model reasoning and generated patches. Using ScratchEval, we study domain-specific fine-tuning, training data effectiveness, and model generalization to unseen bug types. ScratchEval provides a reproducible foundation for evaluating and post-training LLMs on block-based programming tasks.", "AI": {"tldr": "ScratchEval\u662f\u9996\u4e2a\u9488\u5bf9Scratch\u7a0b\u5e8f\u7684\u53ef\u6267\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5757\u72b6\u8bed\u8a00\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b100\u4e2a\u590d\u6742Scratch\u9879\u76ee\u3001\u6d4b\u8bd5\u5957\u4ef6\u548c\u591a\u5a92\u4f53\u8d44\u6e90\u3002", "motivation": "LLM\u5728\u6587\u672c\u7f16\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728Scratch\u7b49\u5757\u72b6\u8bed\u8a00\u4e2d\u4e0d\u53ef\u9760\u3002Scratch\u7a0b\u5e8f\u5177\u6709\u6df1\u5ea6\u5d4c\u5957\u3001\u975e\u7ebf\u6027\u7ed3\u6784\u3001\u4e8b\u4ef6\u9a71\u52a8\u5e76\u53d1\u548c\u4ee3\u7801\u4e0e\u591a\u5a92\u4f53\u7d27\u5bc6\u8026\u5408\u7b49\u7279\u6027\uff0c\u5bfc\u81f4LLM\u5e38\u8bef\u89e3\u8bed\u4e49\u5e76\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u4f46\u8bed\u4e49\u9519\u8bef\u7684\u4fee\u590d\u3002", "method": "\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\u6784\u5efa\u57fa\u51c6\uff1a\u4ece\u516c\u5171\u4ed3\u5e93\u6316\u6398100\u4e2a\u590d\u6742Scratch\u9879\u76ee\uff0c\u914d\u5907\u53ef\u6267\u884c\u6d4b\u8bd5\u5957\u4ef6\u3001\u9519\u8bef\u63cf\u8ff0\u4e0e\u4fee\u590d\u3001\u5757\u7ea7\u7f16\u8f91\u7ea6\u675f\u548c\u591a\u5a92\u4f53\u8d44\u6e90\u3002\u63d0\u51fa\u4e09\u5c42\u53ef\u6267\u884c\u8bc4\u4f30\u534f\u8bae\uff1a\u901a\u8fc7VM\u7ea7\u6267\u884c\u6d4b\u91cf\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4f7f\u7528\u5757\u7ea7\u7f16\u8f91\u8ddd\u79bb\u548c\u884c\u4e3a\u8f68\u8ff9\u6bd4\u8f83\u4fee\u590d\u8d28\u91cf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u5206\u6807\u51c6\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\u3002", "result": "ScratchEval\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u8bc4\u4f30LLM\u5728\u5757\u72b6\u7f16\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u53ef\u7528\u4e8e\u7814\u7a76\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u3001\u8bad\u7ec3\u6570\u636e\u6709\u6548\u6027\u548c\u6a21\u578b\u5bf9\u672a\u89c1\u9519\u8bef\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ScratchEval\u4e3a\u8bc4\u4f30\u548c\u540e\u671f\u8bad\u7ec3LLM\u5728\u5757\u72b6\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u57fa\u7840\uff0c\u586b\u8865\u4e86\u5757\u72b6\u8bed\u8a00\u7a0b\u5e8f\u4fee\u590d\u8bc4\u4f30\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.02161", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02161", "abs": "https://arxiv.org/abs/2602.02161", "authors": ["Aniq Ur Rahman", "Justin P. Coon"], "title": "Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction", "comment": null, "summary": "Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u65f6\u95f4\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5177\u6709\u5df2\u77e5\u771f\u5b9e\u56e0\u679c\u7ed3\u6784\u7684\u65f6\u95f4\u4ea4\u4e92\u56fe\u6765\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u6355\u6349\u5230\u56e0\u679c\u673a\u5236", "motivation": "\u5f53\u524d\u65f6\u95f4\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u9884\u6d4b\u51c6\u786e\u6027\u8bc4\u4f30\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u6355\u6349\u5230\u63a7\u5236\u65f6\u95f4\u4ea4\u4e92\u7684\u56e0\u679c\u673a\u5236\uff0c\u9700\u8981\u5efa\u7acb\u56e0\u679c\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6", "method": "1) \u63d0\u51fa\u652f\u6301\u5174\u594b\u548c\u6291\u5236\u6548\u5e94\u7684\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\uff1b2) \u5c06\u8be5\u673a\u5236\u6269\u5c55\u5230\u65f6\u95f4\u4ea4\u4e92\u56fe\uff1b3) \u63d0\u51fa\u57fa\u4e8e\u8de8\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u7684\u8ddd\u79bb\u5ea6\u91cf\uff1b4) \u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u5b9e\u4f8b\u5316\u53cd\u4e8b\u5b9e\u8bc4\u4f30\uff1a\u63a7\u5236\u56e0\u679c\u504f\u79fb\u548c\u65f6\u95f4\u6233\u6d17\u724c", "result": "\u7ecf\u9a8c\u9a8c\u8bc1\u4e86\u5047\u8bbe\uff1a\u5728\u4e00\u4e2a\u56e0\u679c\u6a21\u578b\u4e0a\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u5728\u8db3\u591f\u8fdc\u7684\u6a21\u578b\u4e0a\u8bc4\u4f30\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\uff1b\u6846\u67b6\u4e3a\u56e0\u679c\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u57fa\u7840", "conclusion": "\u63d0\u51fa\u7684\u53cd\u4e8b\u5b9e\u9a8c\u8bc1\u6846\u67b6\u80fd\u591f\u8bc4\u4f30\u65f6\u95f4\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u662f\u5426\u6355\u6349\u5230\u56e0\u679c\u673a\u5236\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u9884\u6d4b\u51c6\u786e\u6027\u8bc4\u4f30\uff0c\u4e3a\u56e0\u679c\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2602.01644", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01644", "abs": "https://arxiv.org/abs/2602.01644", "authors": ["Gloria Felicia", "Nolan Bryant", "Handi Putra", "Ayaan Gazali", "Eliel Lobo", "Esteban Rojas"], "title": "From Perception to Action: Spatial AI Agents and World Models", "comment": "61 pages, 742 citations, 1 figure, 3 tables. Survey paper on spatial AI agents, embodied AI, graph neural networks, and world models", "summary": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e09\u8f74\u5206\u7c7b\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u80fd\u529b\u4e0e\u7a7a\u95f4\u4efb\u52a1\u8fde\u63a5\u8d77\u6765\uff0c\u5f3a\u8c03\u7a7a\u95f4\u667a\u80fd\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\u548c\u516d\u4e2a\u91cd\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5b64\u7acb\u5730\u5173\u6ce8\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u8981\u4e48\u5b64\u7acb\u5730\u5173\u6ce8\u7a7a\u95f4\u9886\u57df\uff0c\u7f3a\u4e4f\u5c06\u8fd9\u4e24\u79cd\u4e92\u8865\u80fd\u529b\u7edf\u4e00\u8d77\u6765\u7684\u6846\u67b6\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u9886\u57df\u7684\u6210\u529f\u4e0d\u80fd\u76f4\u63a5\u8f6c\u5316\u5230\u7269\u7406\u4e16\u754c\uff0c\u7a7a\u95f4\u667a\u80fd\uff08\u611f\u77e53D\u7ed3\u6784\u3001\u63a8\u7406\u7269\u4f53\u5173\u7cfb\u3001\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u884c\u52a8\uff09\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5bf92000\u591a\u7bc7\u8bba\u6587\u7684\u5168\u9762\u7efc\u8ff0\uff08\u5f15\u7528742\u7bc7\u9876\u7ea7\u4f1a\u8bae\u8bba\u6587\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e09\u8f74\u5206\u7c7b\u6cd5\uff0c\u8fde\u63a5\u667a\u80fd\u4f53\u80fd\u529b\u4e0e\u8de8\u5c3a\u5ea6\u7684\u7a7a\u95f4\u4efb\u52a1\u3002\u533a\u5206\u4e86\u7a7a\u95f4\u57fa\u7840\uff08\u5bf9\u51e0\u4f55\u548c\u7269\u7406\u7684\u5ea6\u91cf\u7406\u89e3\uff09\u4e0e\u7b26\u53f7\u57fa\u7840\uff08\u5c06\u56fe\u50cf\u4e0e\u6587\u672c\u5173\u8054\uff09\uff0c\u5e76\u5206\u6790\u4e86\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\uff1a(1) \u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\u5bf9\u4e8e\u957f\u65f6\u7a0b\u7a7a\u95f4\u4efb\u52a1\u5f88\u91cd\u8981\uff1b(2) GNN-LLM\u96c6\u6210\u662f\u7ed3\u6784\u5316\u7a7a\u95f4\u63a8\u7406\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff1b(3) \u4e16\u754c\u6a21\u578b\u5bf9\u4e8e\u8de8\u5fae\u89c2\u5230\u5b8f\u89c2\u7a7a\u95f4\u5c3a\u5ea6\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u8bc6\u522b\u4e86\u516d\u4e2a\u91cd\u5927\u6311\u6218\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u4e3a\u7edf\u4e00\u788e\u7247\u5316\u7684\u7814\u7a76\u52aa\u529b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u5730\u7406\u7a7a\u95f4\u667a\u80fd\u4e2d\u4e0b\u4e00\u4ee3\u7a7a\u95f4\u611f\u77e5\u81ea\u4e3b\u7cfb\u7edf\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002\u9700\u8981\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6807\u51c6\u5316\u8de8\u9886\u57df\u8bc4\u4f30\u3002"}}
{"id": "2602.01978", "categories": ["cs.NE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01978", "abs": "https://arxiv.org/abs/2602.01978", "authors": ["Roel Koopman", "Sebastian Otte", "Sander Boht\u00e9"], "title": "SpikingGamma: Surrogate-Gradient Free and Temporally Precise Online Training of Spiking Neural Networks with Smoothed Delays", "comment": null, "summary": "Neuromorphic hardware implementations of Spiking Neural Networks (SNNs) promise energy-efficient, low-latency AI through sparse, event-driven computation. Yet, training SNNs under fine temporal discretization remains a major challenge, hindering both low-latency responsiveness and the mapping of software-trained SNNs to efficient hardware. In current approaches, spiking neurons are modeled as self-recurrent units, embedded into recurrent networks to maintain state over time, and trained with BPTT or RTRL variants based on surrogate gradients. These methods scale poorly with temporal resolution, while online approximations often exhibit instability for long sequences and tend to fail at capturing temporal patterns precisely. To address these limitations, we develop spiking neurons with internal recursive memory structures that we combine with sigma-delta spike-coding. We show that this SpikingGamma model supports direct error backpropagation without surrogate gradients, can learn fine temporal patterns with minimal spiking in an online manner, and scale feedforward SNNs to complex tasks and benchmarks with competitive accuracy, all while being insensitive to the temporal resolution of the model. Our approach offers both an alternative to current recurrent SNNs trained with surrogate gradients, and a direct route for mapping SNNs to neuromorphic hardware.", "AI": {"tldr": "\u63d0\u51faSpikingGamma\u6a21\u578b\uff0c\u901a\u8fc7\u5185\u90e8\u9012\u5f52\u8bb0\u5fc6\u7ed3\u6784\u548csigma-delta\u8109\u51b2\u7f16\u7801\uff0c\u5b9e\u73b0\u65e0\u9700\u66ff\u4ee3\u68af\u5ea6\u7684\u76f4\u63a5\u8bef\u5dee\u53cd\u5411\u4f20\u64ad\uff0c\u80fd\u591f\u5b66\u4e60\u7cbe\u7ec6\u65f6\u95f4\u6a21\u5f0f\u5e76\u6269\u5c55\u5230\u590d\u6742\u4efb\u52a1\uff0c\u540c\u65f6\u5bf9\u65f6\u95f4\u5206\u8fa8\u7387\u4e0d\u654f\u611f\u3002", "motivation": "\u5f53\u524dSNN\u8bad\u7ec3\u65b9\u6cd5\u5728\u7cbe\u7ec6\u65f6\u95f4\u79bb\u6563\u5316\u4e0b\u9762\u4e34\u6311\u6218\uff1a\u57fa\u4e8e\u66ff\u4ee3\u68af\u5ea6\u7684BPTT/RTRL\u65b9\u6cd5\u968f\u65f6\u95f4\u5206\u8fa8\u7387\u6269\u5c55\u6027\u5dee\uff0c\u5728\u7ebf\u8fd1\u4f3c\u65b9\u6cd5\u5bf9\u957f\u5e8f\u5217\u4e0d\u7a33\u5b9a\u4e14\u96be\u4ee5\u7cbe\u786e\u6355\u6349\u65f6\u95f4\u6a21\u5f0f\uff0c\u8fd9\u963b\u788d\u4e86\u4f4e\u5ef6\u8fdf\u54cd\u5e94\u548c\u8f6f\u4ef6\u8bad\u7ec3SNN\u5230\u9ad8\u6548\u786c\u4ef6\u7684\u6620\u5c04\u3002", "method": "\u5f00\u53d1\u5177\u6709\u5185\u90e8\u9012\u5f52\u8bb0\u5fc6\u7ed3\u6784\u7684\u8109\u51b2\u795e\u7ecf\u5143\uff0c\u7ed3\u5408sigma-delta\u8109\u51b2\u7f16\u7801\u3002SpikingGamma\u6a21\u578b\u652f\u6301\u65e0\u9700\u66ff\u4ee3\u68af\u5ea6\u7684\u76f4\u63a5\u8bef\u5dee\u53cd\u5411\u4f20\u64ad\uff0c\u80fd\u591f\u4ee5\u5728\u7ebf\u65b9\u5f0f\u5b66\u4e60\u7cbe\u7ec6\u65f6\u95f4\u6a21\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u4e0d\u654f\u611f\u6027\u3002", "result": "SpikingGamma\u6a21\u578b\u80fd\u591f\u4ee5\u6700\u5c0f\u8109\u51b2\u5728\u5728\u7ebf\u65b9\u5f0f\u4e0b\u5b66\u4e60\u7cbe\u7ec6\u65f6\u95f4\u6a21\u5f0f\uff0c\u5c06\u524d\u9988SNN\u6269\u5c55\u5230\u590d\u6742\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5bf9\u6a21\u578b\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u4e0d\u654f\u611f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f53\u524d\u57fa\u4e8e\u66ff\u4ee3\u68af\u5ea6\u8bad\u7ec3\u7684\u5faa\u73afSNN\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u4e3a\u5c06SNN\u6620\u5c04\u5230\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u63d0\u4f9b\u4e86\u76f4\u63a5\u9014\u5f84\uff0c\u89e3\u51b3\u4e86\u7cbe\u7ec6\u65f6\u95f4\u79bb\u6563\u5316\u4e0b\u7684\u8bad\u7ec3\u6311\u6218\u3002"}}
{"id": "2602.00338", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00338", "abs": "https://arxiv.org/abs/2602.00338", "authors": ["Abdurrahman Elmaghbub", "Bechir Hamdaoui"], "title": "HEEDFUL: Leveraging Sequential Transfer Learning for Robust WiFi Device Fingerprinting Amid Hardware Warm-Up Effects", "comment": null, "summary": "Deep Learning-based RF fingerprinting approaches struggle to perform well in cross-domain scenarios, particularly during hardware warm-up. This often-overlooked vulnerability has been jeopardizing their reliability and their adoption in practical settings. To address this critical gap, in this work, we first dive deep into the anatomy of RF fingerprints, revealing insights into the temporal fingerprinting variations during and post hardware stabilization. Introducing HEEDFUL, a novel framework harnessing sequential transfer learning and targeted impairment estimation, we then address these challenges with remarkable consistency, eliminating blind spots even during challenging warm-up phases. Our evaluation showcases HEEDFUL's efficacy, achieving remarkable classification accuracies of up to 96% during the initial device operation intervals-far surpassing traditional models. Furthermore, cross-day and cross-protocol assessments confirm HEEDFUL's superiority, achieving and maintaining high accuracy during both the stable and initial warm-up phases when tested on WiFi signals. Additionally, we release WiFi type B and N RF fingerprint datasets that, for the first time, incorporate both the time-domain representation and real hardware impairments of the frames. This underscores the importance of leveraging hardware impairment data, enabling a deeper understanding of fingerprints and facilitating the development of more robust RF fingerprinting solutions.", "AI": {"tldr": "HEEDFUL\u6846\u67b6\u901a\u8fc7\u5e8f\u5217\u8fc1\u79fb\u5b66\u4e60\u548c\u76ee\u6807\u635f\u4f24\u4f30\u8ba1\u89e3\u51b3RF\u6307\u7eb9\u8bc6\u522b\u5728\u786c\u4ef6\u9884\u70ed\u9636\u6bb5\u7684\u8de8\u57df\u6027\u80fd\u95ee\u9898\uff0c\u5728WiFi\u4fe1\u53f7\u4e0a\u8fbe\u523096%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b\u65f6\u95f4\u57df\u8868\u793a\u548c\u771f\u5b9e\u786c\u4ef6\u635f\u4f24\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684RF\u6307\u7eb9\u8bc6\u522b\u65b9\u6cd5\u5728\u8de8\u57df\u573a\u666f\uff08\u7279\u522b\u662f\u786c\u4ef6\u9884\u70ed\u9636\u6bb5\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e00\u5e38\u88ab\u5ffd\u89c6\u7684\u6f0f\u6d1e\u5f71\u54cd\u4e86\u5176\u53ef\u9760\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u89e3\u51b3RF\u6307\u7eb9\u5728\u786c\u4ef6\u7a33\u5b9a\u671f\u95f4\u548c\u7a33\u5b9a\u540e\u7684\u65f6\u95f4\u53d8\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faHEEDFUL\u6846\u67b6\uff0c\u7ed3\u5408\u5e8f\u5217\u8fc1\u79fb\u5b66\u4e60\u548c\u76ee\u6807\u635f\u4f24\u4f30\u8ba1\u6280\u672f\u3002\u9996\u5148\u6df1\u5165\u5206\u6790RF\u6307\u7eb9\u7684\u89e3\u5256\u7ed3\u6784\uff0c\u63ed\u793a\u786c\u4ef6\u7a33\u5b9a\u671f\u95f4\u6307\u7eb9\u7684\u65f6\u95f4\u53d8\u5316\u89c4\u5f8b\uff0c\u7136\u540e\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u5904\u7406\u8de8\u57df\u6311\u6218\uff0c\u901a\u8fc7\u635f\u4f24\u4f30\u8ba1\u6d88\u9664\u9884\u70ed\u9636\u6bb5\u7684\u76f2\u70b9\u3002", "result": "\u5728\u8bbe\u5907\u521d\u59cb\u8fd0\u884c\u9636\u6bb5\u8fbe\u523096%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8fdc\u8d85\u4f20\u7edf\u6a21\u578b\u3002\u8de8\u65e5\u548c\u8de8\u534f\u8bae\u8bc4\u4f30\u8bc1\u5b9e\u4e86HEEDFUL\u7684\u4f18\u8d8a\u6027\uff0c\u5728WiFi\u4fe1\u53f7\u7684\u7a33\u5b9a\u9636\u6bb5\u548c\u521d\u59cb\u9884\u70ed\u9636\u6bb5\u90fd\u80fd\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002\u53d1\u5e03\u4e86\u5305\u542b\u65f6\u95f4\u57df\u8868\u793a\u548c\u771f\u5b9e\u786c\u4ef6\u635f\u4f24\u7684WiFi\u7c7b\u578bB\u548cN RF\u6307\u7eb9\u6570\u636e\u96c6\u3002", "conclusion": "HEEDFUL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86RF\u6307\u7eb9\u8bc6\u522b\u5728\u786c\u4ef6\u9884\u70ed\u9636\u6bb5\u7684\u8de8\u57df\u6027\u80fd\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u786c\u4ef6\u635f\u4f24\u6570\u636e\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684RF\u6307\u7eb9\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.02458", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.02458", "abs": "https://arxiv.org/abs/2602.02458", "authors": ["Mingwei Hong", "Zheng Lin", "Zehang Lin", "Lin Li", "Miao Yang", "Xia Du", "Zihan Fang", "Zhaolu Kang", "Dianxin Luan", "Shunzhi Zhu"], "title": "Conflict-Aware Client Selection for Multi-Server Federated Learning", "comment": "6 pages, 4 figures", "summary": "Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.", "AI": {"tldr": "\u63d0\u51faRL-CRP\u6846\u67b6\uff0c\u901a\u8fc7\u51b2\u7a81\u98ce\u9669\u9884\u6d4b\u548c\u516c\u5e73\u611f\u77e5\u5956\u52b1\u673a\u5236\u4f18\u5316\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9", "motivation": "\u4f20\u7edf\u5355\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u9ad8\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\uff0c\u800c\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u8986\u76d6\u91cd\u53e0\u548c\u65e0\u534f\u8c03\u9009\u62e9\u5bfc\u81f4\u8d44\u6e90\u7ade\u4e89\u548c\u8bad\u7ec3\u5931\u8d25", "method": "\u63d0\u51faRL-CRP\u6846\u67b6\uff1a1) \u4f7f\u7528\u5206\u7c7b\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u57fa\u4e8e\u7a00\u758f\u5386\u53f2\u9009\u62e9\u5e8f\u5217\u9884\u6d4b\u5ba2\u6237\u7aef\u9009\u62e9\u51b2\u7a81\u98ce\u9669\uff1b2) \u5f15\u5165\u516c\u5e73\u611f\u77e5\u5956\u52b1\u673a\u5236\u4fc3\u8fdb\u957f\u671f\u5ba2\u6237\u7aef\u53c2\u4e0e", "result": "\u5b9e\u9a8c\u8868\u660eRL-CRP\u80fd\u6709\u6548\u51cf\u5c11\u670d\u52a1\u5668\u95f4\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff08\u6536\u655b\u901f\u5ea6\u548c\u901a\u4fe1\u6210\u672c\uff09", "conclusion": "RL-CRP\u6846\u67b6\u901a\u8fc7\u51b2\u7a81\u98ce\u9669\u9884\u6d4b\u548c\u516c\u5e73\u611f\u77e5\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u8d44\u6e90\u7ade\u4e89\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6548\u7387\u548c\u7a33\u5b9a\u6027"}}
{"id": "2602.00678", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00678", "abs": "https://arxiv.org/abs/2602.00678", "authors": ["Tianyang Wu", "Hanwei Guo", "Yuhang Wang", "Junshu Yang", "Xinyang Sui", "Jiayi Xie", "Xingyu Chen", "Zeyang Liu", "Xuguang Lan"], "title": "Toward Reliable Sim-to-Real Predictability for MoE-based Robust Quadrupedal Locomotion", "comment": null, "summary": "Reinforcement learning has shown strong promise for quadrupedal agile locomotion, even with proprioception-only sensing. In practice, however, sim-to-real gap and reward overfitting in complex terrains can produce policies that fail to transfer, while physical validation remains risky and inefficient. To address these challenges, we introduce a unified framework encompassing a Mixture-of-Experts (MoE) locomotion policy for robust multi-terrain representation with RoboGauge, a predictive assessment suite that quantifies sim-to-real transferability. The MoE policy employs a gated set of specialist experts to decompose latent terrain and command modeling, achieving superior deployment robustness and generalization via proprioception alone. RoboGauge further provides multi-dimensional proprioception-based metrics via sim-to-sim tests over terrains, difficulty levels, and domain randomizations, enabling reliable MoE policy selection without extensive physical trials. Experiments on a Unitree Go2 demonstrate robust locomotion on unseen challenging terrains, including snow, sand, stairs, slopes, and 30 cm obstacles. In dedicated high-speed tests, the robot reaches 4 m/s and exhibits an emergent narrow-width gait associated with improved stability at high velocity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u7528\u4e8e\u9c81\u68d2\u591a\u5730\u5f62\u8868\u793a\u7684MoE\u8fd0\u52a8\u7b56\u7565\u548c\u91cf\u5316sim-to-real\u53ef\u8f6c\u79fb\u6027\u7684RoboGauge\u8bc4\u4f30\u5957\u4ef6\uff0c\u5728Unitree Go2\u4e0a\u5b9e\u73b0\u672a\u89c1\u6311\u6218\u6027\u5730\u5f62\u7684\u9c81\u68d2\u8fd0\u52a8\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u654f\u6377\u8fd0\u52a8\u4e2dsim-to-real\u5dee\u8ddd\u548c\u590d\u6742\u5730\u5f62\u4e2d\u5956\u52b1\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u8fd9\u4e9b\u4f1a\u5bfc\u81f4\u7b56\u7565\u65e0\u6cd5\u8fc1\u79fb\uff0c\u800c\u7269\u7406\u9a8c\u8bc1\u53c8\u5b58\u5728\u98ce\u9669\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1) Mixture-of-Experts (MoE)\u8fd0\u52a8\u7b56\u7565\uff1a\u4f7f\u7528\u95e8\u63a7\u4e13\u5bb6\u96c6\u5408\u5206\u89e3\u6f5c\u5728\u5730\u5f62\u548c\u6307\u4ee4\u5efa\u6a21\uff0c\u4ec5\u901a\u8fc7\u672c\u4f53\u611f\u77e5\u5b9e\u73b0\u9c81\u68d2\u90e8\u7f72\u548c\u6cdb\u5316\uff1b2) RoboGauge\u8bc4\u4f30\u5957\u4ef6\uff1a\u901a\u8fc7sim-to-sim\u6d4b\u8bd5\u63d0\u4f9b\u591a\u7ef4\u5ea6\u672c\u4f53\u611f\u77e5\u6307\u6807\uff0c\u8986\u76d6\u5730\u5f62\u3001\u96be\u5ea6\u7ea7\u522b\u548c\u9886\u57df\u968f\u673a\u5316\uff0c\u65e0\u9700\u5927\u91cf\u7269\u7406\u8bd5\u9a8c\u5373\u53ef\u53ef\u9760\u9009\u62e9MoE\u7b56\u7565\u3002", "result": "\u5728Unitree Go2\u4e0a\u5b9e\u73b0\u672a\u89c1\u6311\u6218\u6027\u5730\u5f62\u7684\u9c81\u68d2\u8fd0\u52a8\uff0c\u5305\u62ec\u96ea\u5730\u3001\u6c99\u5730\u3001\u697c\u68af\u3001\u659c\u5761\u548c30\u5398\u7c73\u969c\u788d\u7269\u3002\u5728\u9ad8\u901f\u6d4b\u8bd5\u4e2d\uff0c\u673a\u5668\u4eba\u8fbe\u52304 m/s\u901f\u5ea6\uff0c\u5e76\u8868\u73b0\u51fa\u4e0e\u9ad8\u901f\u7a33\u5b9a\u6027\u6539\u5584\u76f8\u5173\u7684\u6d8c\u73b0\u7a84\u5bbd\u5ea6\u6b65\u6001\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u901a\u8fc7MoE\u7b56\u7565\u548cRoboGauge\u8bc4\u4f30\u5957\u4ef6\u6709\u6548\u89e3\u51b3\u4e86sim-to-real\u8fc1\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ec5\u51ed\u672c\u4f53\u611f\u77e5\u7684\u591a\u5730\u5f62\u9c81\u68d2\u8fd0\u52a8\uff0c\u51cf\u5c11\u4e86\u7269\u7406\u9a8c\u8bc1\u9700\u6c42\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u6311\u6218\u6027\u5730\u5f62\u4e0a\u7684\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2602.00761", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00761", "abs": "https://arxiv.org/abs/2602.00761", "authors": ["Andre Hora", "Andy Zaidman"], "title": "Test Behaviors, Not Methods! Detecting Tests Obsessed by Methods", "comment": "Accepted for publication at ICPC 2026", "summary": "Best testing practices state that tests should verify a single functionality or behavior of the system. Tests that verify multiple behaviors are harder to understand, lack focus, and are more coupled to the production code. An attempt to identify this issue is the test smell \\emph{Eager Test}, which aims to capture tests that verify too much functionality based on the number of production method calls. Unfortunately, prior research suggests that counting production method calls is an inaccurate measure, as these calls do not reliably serve as a proxy for functionality. We envision a complementary solution based on runtime analysis: we hypothesize that some tests that verify multiple behaviors will likely cover multiple paths of the same production methods. Thus, we propose a novel test smell named \\emph{Test Obsessed by Method}, a test method that covers multiple paths of a single production method. We provide an initial empirical study to explore the presence of this smell in 2,054 tests provided by 12 test suites of the Python Standard Library. (1) We detect 44 \\emph{Tests Obsessed by Methods} in 11 of the 12 test suites. (2) Each smelly test verifies a median of two behaviors of the production method. (3) The 44 smelly tests could be split into 118 novel tests. (4) 23% of the smelly tests have code comments recognizing that distinct behaviors are being tested. We conclude by discussing benefits, limitations, and further research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u5f02\u5473\"Test Obsessed by Method\"\uff0c\u6307\u6d4b\u8bd5\u65b9\u6cd5\u8986\u76d6\u5355\u4e2a\u751f\u4ea7\u65b9\u6cd5\u7684\u591a\u4e2a\u8def\u5f84\uff0c\u4f5c\u4e3a\u5bf9\u73b0\u6709Eager Test\u6d4b\u8bd5\u5f02\u5473\u7684\u8865\u5145\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u751f\u4ea7\u65b9\u6cd5\u8c03\u7528\u8ba1\u6570\u7684Eager Test\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u51c6\u786e\uff0c\u56e0\u4e3a\u65b9\u6cd5\u8c03\u7528\u4e0d\u80fd\u53ef\u9760\u5730\u4ee3\u8868\u529f\u80fd\u9a8c\u8bc1\u3002\u9700\u8981\u66f4\u51c6\u786e\u7684\u68c0\u6d4b\u6d4b\u8bd5\u9a8c\u8bc1\u591a\u4e2a\u884c\u4e3a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fd0\u884c\u65f6\u5206\u6790\u7684\u68c0\u6d4b\u65b9\u6cd5\uff1a\u5047\u8bbe\u9a8c\u8bc1\u591a\u4e2a\u884c\u4e3a\u7684\u6d4b\u8bd5\u4f1a\u8986\u76d6\u540c\u4e00\u751f\u4ea7\u65b9\u6cd5\u7684\u591a\u4e2a\u8def\u5f84\u3002\u5728Python\u6807\u51c6\u5e93\u768412\u4e2a\u6d4b\u8bd5\u5957\u4ef6\u30012054\u4e2a\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "1) \u572812\u4e2a\u6d4b\u8bd5\u5957\u4ef6\u4e2d\u768411\u4e2a\u68c0\u6d4b\u523044\u4e2a\"Test Obsessed by Method\"\u5f02\u5473\u6d4b\u8bd5\uff1b2) \u6bcf\u4e2a\u5f02\u5473\u6d4b\u8bd5\u4e2d\u4f4d\u6570\u9a8c\u8bc12\u4e2a\u751f\u4ea7\u65b9\u6cd5\u884c\u4e3a\uff1b3) 44\u4e2a\u5f02\u5473\u6d4b\u8bd5\u53ef\u62c6\u5206\u4e3a118\u4e2a\u65b0\u6d4b\u8bd5\uff1b4) 23%\u7684\u5f02\u5473\u6d4b\u8bd5\u6709\u4ee3\u7801\u6ce8\u91ca\u627f\u8ba4\u6d4b\u8bd5\u4e86\u4e0d\u540c\u884c\u4e3a\u3002", "conclusion": "Test Obsessed by Method\u4f5c\u4e3a\u65b0\u7684\u6d4b\u8bd5\u5f02\u5473\u6982\u5ff5\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u9a8c\u8bc1\u591a\u4e2a\u884c\u4e3a\u7684\u6d4b\u8bd5\uff0c\u4e3a\u6d4b\u8bd5\u91cd\u6784\u63d0\u4f9b\u6307\u5bfc\u3002\u8ba8\u8bba\u4e86\u8be5\u65b9\u6cd5\u7684\u76ca\u5904\u3001\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.00493", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00493", "abs": "https://arxiv.org/abs/2602.00493", "authors": ["Hongyu Zhou", "Xincheng Huang", "Winston Wijaya", "Yi Fei Cheng", "David Lindlbauer", "Eduardo Velloso", "Andrea Bianchi", "Zhanna Sarsenbayeva", "Anusha Withana"], "title": "One Body, Two Minds: Alternating VR Perspective During Remote Teleoperation of Supernumerary Limbs", "comment": "Accepted to CHI 2026. Version of Record: DOI {10.1145/3772318.3791433", "summary": "Remote VR teleoperation with supernumerary robotic limbs enables distant users to operate in another's local space. While a shared first-person view aids hand-eye coordination, locking the guest's camera to the host's head can degrade comfort, embodiment, and coordination. Based on a formative study (N=10) using a virtual supernumerary robotic limbs configuration to stress-test coordination, we propose guest-driven perspective switching from a shared first-person baseline (Shared Embodied View) to two alternatives: (a) a stabilized view with guest-controlled rotation (Embedded Anchored View), and (b) a fully decoupled third-person view (Out-of-body View). We ran a user study with 24 pairs (N=48) who switched between the baseline and proposed views as task demands changed. We measured performance, embodiment, fatigue, physiological arousal, and switching behaviors. Our results reveal role-dependent trade-offs: Out-of-body View improves navigation efficiency and reduces errors, while Embedded Anchored View supports embodiment. We conclude with guidelines: use Embedded Anchored View for hand-centric adjustments, Out-of-body View for navigation and object placement, and ensure smooth transitions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u8fdc\u7a0bVR\u9065\u64cd\u4f5c\u4e2d\u89c6\u89d2\u5207\u6362\u7b56\u7565\uff0c\u901a\u8fc7\u4e09\u79cd\u89c6\u89d2\u6a21\u5f0f\uff08\u5171\u4eab\u7b2c\u4e00\u4eba\u79f0\u3001\u7a33\u5b9a\u5d4c\u5165\u5f0f\u3001\u7b2c\u4e09\u4eba\u79f0\uff09\u6765\u5e73\u8861\u534f\u8c03\u6027\u3001\u8212\u9002\u5ea6\u548c\u4e34\u573a\u611f\uff0c\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u63d0\u4f9b\u4f18\u5316\u65b9\u6848\u3002", "motivation": "\u8fdc\u7a0bVR\u9065\u64cd\u4f5c\u4e2d\uff0c\u5c06\u8bbf\u5ba2\u89c6\u89d2\u9501\u5b9a\u5728\u4e3b\u673a\u5934\u90e8\u867d\u7136\u6709\u52a9\u4e8e\u624b\u773c\u534f\u8c03\uff0c\u4f46\u4f1a\u964d\u4f4e\u8212\u9002\u5ea6\u3001\u4e34\u573a\u611f\u548c\u534f\u8c03\u6027\u3002\u9700\u8981\u8bbe\u8ba1\u66f4\u7075\u6d3b\u7684\u89c6\u89d2\u5207\u6362\u673a\u5236\u6765\u5e94\u5bf9\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u57fa\u4e8e10\u4eba\u5f62\u6210\u6027\u7814\u7a76\uff0c\u63d0\u51fa\u4e09\u79cd\u89c6\u89d2\u6a21\u5f0f\uff1a\u5171\u4eab\u7b2c\u4e00\u4eba\u79f0\u57fa\u7ebf\u3001\u8bbf\u5ba2\u63a7\u5236\u65cb\u8f6c\u7684\u7a33\u5b9a\u5d4c\u5165\u5f0f\u89c6\u89d2\u3001\u5b8c\u5168\u89e3\u8026\u7684\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u3002\u901a\u8fc724\u5bf9\uff0848\u4eba\uff09\u7528\u6237\u7814\u7a76\uff0c\u6d4b\u91cf\u6027\u80fd\u3001\u4e34\u573a\u611f\u3001\u75b2\u52b3\u5ea6\u3001\u751f\u7406\u5524\u9192\u548c\u5207\u6362\u884c\u4e3a\u3002", "result": "\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u63d0\u9ad8\u5bfc\u822a\u6548\u7387\u548c\u51cf\u5c11\u9519\u8bef\uff0c\u5d4c\u5165\u5f0f\u89c6\u89d2\u652f\u6301\u4e34\u573a\u611f\u3002\u89d2\u8272\u4f9d\u8d56\u7684\u6743\u8861\uff1a\u8bbf\u5ba2\u504f\u597d\u7b2c\u4e09\u4eba\u79f0\u8fdb\u884c\u5bfc\u822a\u548c\u7269\u4f53\u653e\u7f6e\uff0c\u5d4c\u5165\u5f0f\u89c6\u89d2\u7528\u4e8e\u624b\u90e8\u7cbe\u7ec6\u8c03\u6574\u3002", "conclusion": "\u63d0\u51fa\u8bbe\u8ba1\u6307\u5357\uff1a\u5d4c\u5165\u5f0f\u89c6\u89d2\u7528\u4e8e\u624b\u90e8\u4e2d\u5fc3\u8c03\u6574\uff0c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u7528\u4e8e\u5bfc\u822a\u548c\u7269\u4f53\u653e\u7f6e\uff0c\u5e76\u786e\u4fdd\u5e73\u6ed1\u8fc7\u6e21\u3002\u89c6\u89d2\u5207\u6362\u5e94\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u8c03\u6574\u3002"}}
{"id": "2602.00840", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00840", "abs": "https://arxiv.org/abs/2602.00840", "authors": ["Biruk Tadesse", "Vikram Nitin", "Mazin Salah", "Baishakhi Ray", "Marcelo d'Amorim", "Wesley Assun\u00e7\u00e3o"], "title": "Code Quality Analysis of Translations from C to Rust", "comment": null, "summary": "C/C++ is a prevalent programming language. Yet, it suffers from significant memory and thread-safety issues. Recent studies have explored automated translation of C/C++ to safer languages, such as Rust. However, these studies focused mostly on the correctness and safety of the translated code, which are indeed critical, but they left other important quality concerns (e.g., performance, robustness, and maintainability) largely unexplored. This work investigates strengths and weaknesses of three C-to-Rust translators, namely C2Rust (a transpiler), C2SaferRust (an LLM-guided transpiler), and TranslationGym (an LLM-based direct translation). We perform an in-depth quantitative and qualitative analysis of several important quality attributes for the translated Rust code of the popular GNU coreutils, using human-based translation as a baseline. To assess the internal and external quality of the Rust code, we: (i) apply Clippy, a rule-based state-of-the-practice Rust static analysis tool; (ii) investigate the capability of an LLM (GPT-4o) to identify issues potentially overlooked by Clippy; and (iii) perform a manual analysis of the issues reported by Clippy and GPT-4o. Our results show that while newer techniques reduce some unsafe and non-idiomatic patterns, they frequently introduce new issues, revealing systematic trade-offs that are not visible under existing evaluation practices. Notably, none of the automated techniques consistently match or exceed human-written translations across all quality dimensions, yet even human-written Rust code exhibits persistent internal quality issues such as readability and non-idiomatic patterns. Together, these findings show that translation quality remains a multi-dimensional challenge, requiring systematic evaluation and targeted tool support beyond both naive automation and manual rewriting.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u4e09\u79cdC\u5230Rust\u7ffb\u8bd1\u5de5\u5177\uff08C2Rust\u3001C2SaferRust\u3001TranslationGym\uff09\u8fdb\u884c\u591a\u7ef4\u8d28\u91cf\u8bc4\u4f30\uff0c\u53d1\u73b0\u81ea\u52a8\u5316\u7ffb\u8bd1\u5728\u5b89\u5168\u6027\u3001\u6027\u80fd\u3001\u5065\u58ee\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u6743\u8861\uff0c\u65e0\u6cd5\u5728\u6240\u6709\u8d28\u91cf\u7ef4\u5ea6\u4e0a\u5339\u914d\u4eba\u5de5\u7ffb\u8bd1\u3002", "motivation": "\u73b0\u6709C/C++\u5230Rust\u7684\u81ea\u52a8\u7ffb\u8bd1\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u6b63\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u6027\u80fd\u3001\u5065\u58ee\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u7b49\u5176\u4ed6\u91cd\u8981\u8d28\u91cf\u5c5e\u6027\u3002\u9700\u8981\u5bf9\u8fd9\u4e9b\u7ffb\u8bd1\u5de5\u5177\u5728\u591a\u7ef4\u8d28\u91cf\u65b9\u9762\u7684\u8868\u73b0\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u4e09\u79cdC-to-Rust\u7ffb\u8bd1\u5de5\u5177\uff08C2Rust\u3001C2SaferRust\u3001TranslationGym\uff09\u7ffb\u8bd1GNU coreutils\uff0c\u4ee5\u4eba\u5de5\u7ffb\u8bd1\u4e3a\u57fa\u51c6\u3002\u91c7\u7528\u591a\u5c42\u6b21\u8d28\u91cf\u8bc4\u4f30\uff1a1) \u4f7f\u7528Clippy\u9759\u6001\u5206\u6790\u5de5\u5177\uff1b2) \u5229\u7528GPT-4o\u8bc6\u522bClippy\u53ef\u80fd\u9057\u6f0f\u7684\u95ee\u9898\uff1b3) \u5bf9Clippy\u548cGPT-4o\u62a5\u544a\u7684\u95ee\u9898\u8fdb\u884c\u624b\u52a8\u5206\u6790\u3002", "result": "\u8f83\u65b0\u7684\u7ffb\u8bd1\u6280\u672f\u867d\u7136\u51cf\u5c11\u4e86\u90e8\u5206\u4e0d\u5b89\u5168\u548c\u975e\u60ef\u7528\u6a21\u5f0f\uff0c\u4f46\u7ecf\u5e38\u5f15\u5165\u65b0\u95ee\u9898\uff0c\u663e\u793a\u51fa\u7cfb\u7edf\u6027\u6743\u8861\u3002\u6240\u6709\u81ea\u52a8\u5316\u6280\u672f\u90fd\u65e0\u6cd5\u5728\u6240\u6709\u8d28\u91cf\u7ef4\u5ea6\u4e0a\u4e00\u81f4\u5339\u914d\u6216\u8d85\u8d8a\u4eba\u5de5\u7ffb\u8bd1\uff0c\u5373\u4f7f\u662f\u4eba\u5de5\u7f16\u5199\u7684Rust\u4ee3\u7801\u4e5f\u5b58\u5728\u53ef\u8bfb\u6027\u548c\u975e\u60ef\u7528\u6a21\u5f0f\u7b49\u5185\u90e8\u8d28\u91cf\u95ee\u9898\u3002", "conclusion": "\u7ffb\u8bd1\u8d28\u91cf\u4ecd\u662f\u4e00\u4e2a\u591a\u7ef4\u6311\u6218\uff0c\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u81ea\u52a8\u5316\u548c\u624b\u52a8\u91cd\u5199\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u9488\u5bf9\u6027\u5de5\u5177\u652f\u6301\u3002\u5f53\u524d\u81ea\u52a8\u5316\u7ffb\u8bd1\u5de5\u5177\u5728\u5168\u9762\u8d28\u91cf\u4fdd\u8bc1\u65b9\u9762\u4ecd\u6709\u5c40\u9650\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.02269", "categories": ["cs.RO", "cs.AI", "cs.SE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02269", "abs": "https://arxiv.org/abs/2602.02269", "authors": ["Jon \u0160kerlj", "Seongjin Bien", "Abdeldjallil Naceri", "Sami Haddadin"], "title": "Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "We present $multipanda\\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.", "AI": {"tldr": "\u63d0\u51fa\u4e86multipanda_ros2\uff0c\u4e00\u4e2a\u7528\u4e8eFranka\u673a\u5668\u4eba\u591a\u673a\u5668\u4eba\u63a7\u5236\u7684ROS2\u5f00\u6e90\u67b6\u6784\uff0c\u652f\u63011kHz\u5b9e\u65f6\u63a7\u5236\u9891\u7387\u548c\u22642ms\u63a7\u5236\u5668\u5207\u6362\u5ef6\u8fdf\uff0c\u96c6\u6210\u4e86\u9ad8\u4fdd\u771fMuJoCo\u4eff\u771f\u548c\u60ef\u6027\u53c2\u6570\u8bc6\u522b\u4ee5\u51cf\u5c0fsim2real\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5b9e\u65f6\u626d\u77e9\u63a7\u5236\u6311\u6218\uff0c\u5305\u62ec\u4ea4\u4e92\u63a7\u5236\u548c\u673a\u5668\u4eba\u73af\u5883\u5efa\u6a21\uff0c\u540c\u65f6\u9700\u8981\u6ee1\u8db3\u5b89\u5168\u6807\u51c6\u8981\u6c42\u76841kHz\u6700\u5c0f\u63a7\u5236\u9891\u7387\uff0c\u5e76\u4e3a\u590d\u6742\u591a\u673a\u5668\u4eba\u4ea4\u4e92\u573a\u666f\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "method": "\u91c7\u7528ros2_control\u6846\u67b6\uff0c\u8bbe\u8ba1\u63a7\u5236\u4f53\uff08controllet\uff09\u6a21\u5f0f\u5b9e\u73b0\u5feb\u901f\u63a7\u5236\u5668\u5207\u6362\uff0c\u96c6\u6210\u9ad8\u4fdd\u771fMuJoCo\u4eff\u771f\u8fdb\u884c\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u4e00\u81f4\u6027\u9a8c\u8bc1\uff0c\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u60ef\u6027\u53c2\u6570\u8bc6\u522b\u63d0\u5347\u529b\u548c\u626d\u77e9\u7cbe\u5ea6\u3002", "result": "\u5b9e\u73b0\u4e861kHz\u63a7\u5236\u9891\u7387\u548c\u22642ms\u63a7\u5236\u5668\u5207\u6362\u5ef6\u8fdf\uff0c\u63d0\u4f9b\u4e86\u5b9a\u91cf\u8fd0\u52a8\u5b66\u7cbe\u5ea6\u548c\u52a8\u529b\u5b66\u4e00\u81f4\u6027\uff08\u626d\u77e9\u3001\u529b\u3001\u63a7\u5236\u8bef\u5dee\uff09\u6307\u6807\uff0c\u60ef\u6027\u53c2\u6570\u8bc6\u522b\u663e\u8457\u63d0\u5347\u4e86\u529b\u548c\u626d\u77e9\u7cbe\u5ea6\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u521a\u6027\u53cc\u81c2\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u3002", "conclusion": "multipanda_ros2\u4e3a\u9ad8\u7ea7\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u590d\u73b0\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u63a7\u5236\u4f53\u8bbe\u8ba1\u6a21\u5f0f\u548c\u9ad8\u4fdd\u771f\u4eff\u771f\u96c6\u6210\uff0c\u6709\u6548\u51cf\u5c0f\u4e86sim2real\u5dee\u8ddd\uff0c\u6269\u5c55\u4e86\u8f6f\u673a\u5668\u4eba\u65b9\u6cd5\u5728\u521a\u6027\u53cc\u81c2\u63a5\u89e6\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2602.00411", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00411", "abs": "https://arxiv.org/abs/2602.00411", "authors": ["Wenhao Chen", "Wenyi Morty Zhang", "Wei Sun", "Dinesh Bharadia", "Roshan Ayyalasomayajula"], "title": "SpyDir: Spy Device Localization Through Accurate Direction Finding", "comment": null, "summary": "Hidden spy cameras have become a great privacy threat recently, as these low-cost, low-power, and small form-factor IoT devices can quietly monitor human activities in the indoor environment without generating any side-channel information. As such, it is difficult to detect and even more challenging to localize them in the rich-scattering indoor environment. To this end, this paper presents the design, implementation, and evaluation of SpyDir, a system that can accurately localize the hidden spy IoT devices by harnessing the electromagnetic emanations automatically and unintentionally emitted from them. Our system design mainly consists of a portable switching antenna array to sniff the spectrum-spread emanations, an emanation enhancement algorithm through non-coherent averaging that can de-correlate the correlated noise effect due to the square-wave emanation structure, and a multipath-resolving algorithm that can exploit the relative channels using a novel optimization-based sparse AoA derivation. Our real-world experimental evaluation across different indoor environments demonstrates an average AoA error of 6.30 deg, whereas the baseline algorithm yields 21.06 deg, achieving over a 3.3 times improvement in accuracy, and a mean localization error of 19.86cm over baseline algorithms of 206.79cm (MUSIC) and 294.75cm (SpotFi), achieving over a 10.41 times and 14.8 times improvement in accuracy.", "AI": {"tldr": "SpyDir\u7cfb\u7edf\u5229\u7528\u9690\u85cf\u95f4\u8c0dIoT\u8bbe\u5907\u7684\u7535\u78c1\u8f90\u5c04\uff0c\u901a\u8fc7\u4fbf\u643a\u5f0f\u5929\u7ebf\u9635\u5217\u3001\u8f90\u5c04\u589e\u5f3a\u7b97\u6cd5\u548c\u591a\u5f84\u89e3\u6790\u7b97\u6cd5\uff0c\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53473.3-14.8\u500d\u7cbe\u5ea6\u3002", "motivation": "\u9690\u85cf\u5f0f\u95f4\u8c0d\u6444\u50cf\u5934\u7b49\u4f4e\u6210\u672c\u3001\u4f4e\u529f\u8017\u3001\u5c0f\u578bIoT\u8bbe\u5907\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u6784\u6210\u4e25\u91cd\u9690\u79c1\u5a01\u80c1\uff0c\u8fd9\u4e9b\u8bbe\u5907\u4e0d\u4ea7\u751f\u4fa7\u4fe1\u9053\u4fe1\u606f\uff0c\u96be\u4ee5\u68c0\u6d4b\u548c\u5b9a\u4f4d\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5ba4\u5185\u591a\u5f84\u4e30\u5bcc\u7684\u6563\u5c04\u73af\u5883\u4e2d\u5b9a\u4f4d\u7cbe\u5ea6\u6709\u9650\u3002", "method": "1) \u4f7f\u7528\u4fbf\u643a\u5f0f\u5207\u6362\u5929\u7ebf\u9635\u5217\u6355\u83b7\u8bbe\u5907\u81ea\u52a8\u65e0\u610f\u8bc6\u53d1\u5c04\u7684\u9891\u8c31\u6269\u5c55\u8f90\u5c04\uff1b2) \u901a\u8fc7\u975e\u76f8\u5e72\u5e73\u5747\u7684\u8f90\u5c04\u589e\u5f3a\u7b97\u6cd5\uff0c\u6d88\u9664\u65b9\u6ce2\u8f90\u5c04\u7ed3\u6784\u5bfc\u81f4\u7684\u566a\u58f0\u76f8\u5173\u6548\u5e94\uff1b3) \u91c7\u7528\u57fa\u4e8e\u4f18\u5316\u7684\u7a00\u758fAoA\u63a8\u5bfc\u7684\u591a\u5f84\u89e3\u6790\u7b97\u6cd5\uff0c\u5229\u7528\u76f8\u5bf9\u4fe1\u9053\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cd\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff1a\u5e73\u5747AoA\u8bef\u5dee\u4e3a6.30\u5ea6\uff08\u57fa\u7ebf\u7b97\u6cd5\u4e3a21.06\u5ea6\uff09\uff0c\u7cbe\u5ea6\u63d0\u53473.3\u500d\u4ee5\u4e0a\uff1b\u5e73\u5747\u5b9a\u4f4d\u8bef\u5dee\u4e3a19.86\u5398\u7c73\uff08MUSIC\u7b97\u6cd5\u4e3a206.79\u5398\u7c73\uff0cSpotFi\u7b97\u6cd5\u4e3a294.75\u5398\u7c73\uff09\uff0c\u7cbe\u5ea6\u5206\u522b\u63d0\u534710.41\u500d\u548c14.8\u500d\u3002", "conclusion": "SpyDir\u7cfb\u7edf\u901a\u8fc7\u5229\u7528\u9690\u85cf\u95f4\u8c0dIoT\u8bbe\u5907\u7684\u7535\u78c1\u8f90\u5c04\uff0c\u7ed3\u5408\u521b\u65b0\u7684\u8f90\u5c04\u589e\u5f3a\u548c\u591a\u5f84\u89e3\u6790\u7b97\u6cd5\uff0c\u5728\u5ba4\u5185\u590d\u6742\u6563\u5c04\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8bbe\u5907\u5b9a\u4f4d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2602.00494", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00494", "abs": "https://arxiv.org/abs/2602.00494", "authors": ["Hongyu Zhou", "Chia-An fan", "Yihao Dong", "Shuto Takashita", "Masahiko Inami", "Zhanna Sarsenbayeva", "Anusha Withana"], "title": "SRL Proxemics: Spatial Guidelines for Supernumerary Robotic Limbs in Near-Body Interactions", "comment": "Accepted to CHI 2026. Version of Record: DOI 10.1145/3772318.3790532", "summary": "Wearable supernumerary robotic limbs (SRLs) sit at the intersection of human augmentation and embodied AI, transforming into extensions of the human body. However, their movements within the intimate near-body space raise unresolved challenges for perceived safety, user control, and trust. In this paper, we present results from a Wizard-of-Oz study (n=18), where participants completed near-body collaboration tasks with SRLs to explore these challenges. We collected qualitative data through think-aloud protocols and semi-structured interviews, complemented by physiological signals and post-task ratings. Findings indicate that greater autonomy did not inherently enhance perceived safety or trust. Instead, participants identified near-body zones and paired them with clear coordination rules. They also expressed expectations for how different arm components should behave, shaping preferences around autonomy, perceived safety, and trust. Building on these insights, we introduce SRL Proxemics, a zone- and segment-level design framework showing that autonomy is not monolithic: perceived safety hinges on spatially calibrated, legible behaviors, not higher autonomy.", "AI": {"tldr": "SRL Proxemics\u6846\u67b6\uff1a\u7814\u7a76\u53d1\u73b0\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53\u5728\u8fd1\u8eab\u7a7a\u95f4\u4e2d\u7684\u81ea\u4e3b\u6027\u5e76\u975e\u8d8a\u9ad8\u8d8a\u597d\uff0c\u611f\u77e5\u5b89\u5168\u53d6\u51b3\u4e8e\u7a7a\u95f4\u6821\u51c6\u7684\u6e05\u6670\u884c\u4e3a\u800c\u975e\u66f4\u9ad8\u81ea\u4e3b\u6027\u3002", "motivation": "\u53ef\u7a7f\u6234\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53\u4f5c\u4e3a\u4eba\u4f53\u589e\u5f3a\u548c\u5177\u8eabAI\u7684\u4ea4\u53c9\u9886\u57df\uff0c\u5728\u8fd1\u8eab\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\u5b58\u5728\u611f\u77e5\u5b89\u5168\u3001\u7528\u6237\u63a7\u5236\u548c\u4fe1\u4efb\u65b9\u9762\u7684\u672a\u89e3\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u8bbe\u8ba1\u8fd9\u4e9b\u7cfb\u7edf\u4ee5\u589e\u5f3a\u4eba\u7c7b\u4f53\u9a8c\u3002", "method": "\u91c7\u7528Wizard-of-Oz\u7814\u7a76\u8bbe\u8ba1\uff08n=18\uff09\uff0c\u53c2\u4e0e\u8005\u5b8c\u6210\u8fd1\u8eab\u534f\u4f5c\u4efb\u52a1\uff0c\u901a\u8fc7\u51fa\u58f0\u601d\u8003\u534f\u8bae\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u6536\u96c6\u5b9a\u6027\u6570\u636e\uff0c\u8f85\u4ee5\u751f\u7406\u4fe1\u53f7\u548c\u4efb\u52a1\u540e\u8bc4\u5206\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1)\u66f4\u9ad8\u81ea\u4e3b\u6027\u5e76\u672a\u56fa\u6709\u589e\u5f3a\u611f\u77e5\u5b89\u5168\u6216\u4fe1\u4efb\uff1b2)\u53c2\u4e0e\u8005\u8bc6\u522b\u4e86\u8fd1\u8eab\u533a\u57df\u5e76\u914d\u4ee5\u660e\u786e\u7684\u534f\u8c03\u89c4\u5219\uff1b3)\u53c2\u4e0e\u8005\u5bf9\u4e0d\u540c\u624b\u81c2\u7ec4\u4ef6\u7684\u884c\u4e3a\u6709\u7279\u5b9a\u671f\u671b\uff0c\u8fd9\u4e9b\u671f\u671b\u5851\u9020\u4e86\u5bf9\u81ea\u4e3b\u6027\u3001\u611f\u77e5\u5b89\u5168\u548c\u4fe1\u4efb\u7684\u504f\u597d\u3002", "conclusion": "\u63d0\u51faSRL Proxemics\u8bbe\u8ba1\u6846\u67b6\uff0c\u5f3a\u8c03\u81ea\u4e3b\u6027\u4e0d\u662f\u5355\u4e00\u6982\u5ff5\uff0c\u611f\u77e5\u5b89\u5168\u53d6\u51b3\u4e8e\u7a7a\u95f4\u6821\u51c6\u7684\u6e05\u6670\u884c\u4e3a\u800c\u975e\u66f4\u9ad8\u81ea\u4e3b\u6027\uff0c\u4e3a\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53\u5728\u8fd1\u8eab\u7a7a\u95f4\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u4e8e\u533a\u57df\u548c\u5206\u6bb5\u7684\u6307\u5bfc\u539f\u5219\u3002"}}
{"id": "2602.00708", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00708", "abs": "https://arxiv.org/abs/2602.00708", "authors": ["Weiqi Gai", "Yuman Gao", "Yuan Zhou", "Yufan Xie", "Zhiyang Liu", "Yuze Wu", "Xin Zhou", "Fei Gao", "Zhijun Meng"], "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation", "comment": null, "summary": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.", "AI": {"tldr": "USS-Nav\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u91cf\u6784\u5efa\u7edf\u4e00\u65f6\u7a7a\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u8fbe\u523015Hz\u5b9e\u65f6\u66f4\u65b0\u9891\u7387\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u9762\u4e34\u7684\u6838\u5fc3\u77db\u76fe\uff1a\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\u9700\u6c42\u4e0e\u6709\u9650\u673a\u8f7d\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u8981\u4e48\u65e0\u6cd5\u6709\u6548\u7ed3\u5408\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\u8fdb\u884c\u9ad8\u6548\u5bfc\u822a\u3002", "method": "1. \u589e\u91cf\u6784\u5efa\u7edf\u4e00\u65f6\u7a7a\u8bed\u4e49\u573a\u666f\u56fe\uff1a\u4f7f\u7528\u591a\u9762\u4f53\u6269\u5c55\u751f\u6210\u7a7a\u95f4\u8fde\u901a\u56fe\u6355\u83b7\u5168\u5c40\u51e0\u4f55\u62d3\u6251\uff0c\u901a\u8fc7\u56fe\u805a\u7c7b\u52a8\u6001\u5212\u5206\u4e3a\u8bed\u4e49\u533a\u57df\uff1b2. \u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u8bed\u4e49\u5b9e\u4f8b\u5316\u5e76\u951a\u5b9a\u5230\u62d3\u6251\u7ed3\u6784\u5f62\u6210\u5206\u5c42\u73af\u5883\u8868\u793a\uff1b3. \u57fa\u4e8e\u5206\u5c42\u7ed3\u6784\u7684\u7c97\u5230\u7ec6\u63a2\u7d22\u7b56\u7565\uff1aLLM\u57fa\u4e8e\u573a\u666f\u56fe\u8bed\u4e49\u786e\u5b9a\u5168\u5c40\u76ee\u6807\u533a\u57df\uff0c\u5c40\u90e8\u89c4\u5212\u5668\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u4f18\u5316\u8fb9\u754c\u8986\u76d6\u3002", "result": "\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u65f6\u66f4\u65b0\u9891\u7387\uff0815Hz\uff09\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u5728\u6210\u529f\u7387\u52a0\u6743\u8def\u5f84\u957f\u5ea6\uff08SPL\uff09\u6307\u6807\u4e0a\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "USS-Nav\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0e\u8bed\u4e49\u63a8\u7406\u9700\u6c42\u51b2\u7a81\uff0c\u901a\u8fc7\u7edf\u4e00\u65f6\u7a7a\u8bed\u4e49\u8868\u793a\u548c\u5206\u5c42\u89c4\u5212\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u7684\u8bed\u4e49\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00933", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00933", "abs": "https://arxiv.org/abs/2602.00933", "authors": ["Chaithanya Bandi", "Ben Hertzberg", "Geobio Boo", "Tejas Polakam", "Jeff Da", "Sami Hassaan", "Manasi Sharma", "Andrew Park", "Ernesto Hernandez", "Dan Rambado", "Ivan Salazar", "Rafael Cruz", "Chetan Rane", "Ben Levin", "Brad Kenstler", "Bing Liu"], "title": "MCP-Atlas: A Large-Scale Benchmark for Tool-Use Competency with Real MCP Servers", "comment": null, "summary": "The Model Context Protocol (MCP) is rapidly becoming the standard interface for Large Language Models (LLMs) to discover and invoke external tools. However, existing evaluations often fail to capture the complexity of real-world scenarios, relying on restricted toolsets, simplistic workflows, or subjective LLM-as-a-judge metrics. We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency, comprising 36 real MCP servers and 220 tools. It includes 1,000 tasks designed to assess tool-use competency in realistic, multi-step workflows. Tasks use natural language prompts that avoid naming specific tools or servers, requiring agents to identify and orchestrate 3-6 tool calls across multiple servers. We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the model's final answer, complemented by internal diagnostics on tool discovery, parameterization, syntax, error recovery, and efficiency. Evaluation results on frontier models reveal that top models achieve pass rates exceeding 50%, with primary failures arising from inadequate tool usage and task understanding. We release the task schema, containerized harness, and a 500-task public subset of the benchmark dataset to facilitate reproducible comparisons and advance the development of robust, tool-augmented agents.", "AI": {"tldr": "MCP-Atlas\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b36\u4e2a\u771f\u5b9eMCP\u670d\u52a1\u5668\u3001220\u4e2a\u5de5\u5177\u548c1000\u4e2a\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u73b0\u5b9e\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u573a\u666f\u590d\u6742\u6027\uff0c\u4f9d\u8d56\u53d7\u9650\u5de5\u5177\u96c6\u3001\u7b80\u5355\u5de5\u4f5c\u6d41\u6216\u4e3b\u89c2\u7684LLM-as-a-judge\u6307\u6807\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b36\u4e2a\u771f\u5b9eMCP\u670d\u52a1\u5668\u548c220\u4e2a\u5de5\u5177\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u8bbe\u8ba11000\u4e2a\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7684\u4efb\u52a1\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u8bc6\u522b\u548c\u7f16\u63923-6\u4e2a\u8de8\u670d\u52a1\u5668\u5de5\u5177\u8c03\u7528\uff0c\u91c7\u7528\u57fa\u4e8e\u58f0\u660e\u7684\u8bc4\u5206\u6807\u51c6\u548c\u5185\u90e8\u8bca\u65ad\u6307\u6807\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u901a\u8fc7\u7387\u8d85\u8fc750%\uff0c\u4e3b\u8981\u5931\u8d25\u539f\u56e0\u662f\u4e0d\u5145\u5206\u7684\u5de5\u5177\u4f7f\u7528\u548c\u4efb\u52a1\u7406\u89e3\uff0c\u53d1\u5e03\u4e86\u4efb\u52a1\u6a21\u5f0f\u3001\u5bb9\u5668\u5316\u6846\u67b6\u548c500\u4e2a\u4efb\u52a1\u7684\u516c\u5f00\u5b50\u96c6\u3002", "conclusion": "MCP-Atlas\u4e3a\u5de5\u5177\u589e\u5f3a\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6bd4\u8f83\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u4f7f\u7528\u5de5\u4f5c\u6d41\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9c81\u68d2\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u8bc4\u4f30\u3002"}}
{"id": "2602.02311", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2602.02311", "abs": "https://arxiv.org/abs/2602.02311", "authors": ["Johannes Koch", "Tanja Alderliesten", "Peter A. N. Bosman"], "title": "Introns and Templates Matter: Rethinking Linkage in GP-GOMEA", "comment": "16 pages, 17 figures, submitted to GECCO 2026", "summary": "GP-GOMEA is among the state-of-the-art for symbolic regression, especially when it comes to finding small and potentially interpretable solutions. A key mechanism employed in any GOMEA variant is the exploitation of linkage, the dependencies between variables, to ensure efficient evolution. In GP-GOMEA, mutual information between node positions in GP trees has so far been used to learn linkage. For this, a fixed expression template is used. This however leads to introns for expressions smaller than the full template. As introns have no impact on fitness, their occurrences are not directly linked to selection. Consequently, introns can adversely affect the extent to which mutual information captures dependencies between tree nodes. To overcome this, we propose two new measures for linkage learning, one that explicitly considers introns in mutual information estimates, and one that revisits linkage learning in GP-GOMEA from a grey-box perspective, yielding a measure that needs not to be learned from the population but is derived directly from the template. Across five standard symbolic regression problems, GP-GOMEA achieves substantial improvements using both measures. We also find that the newly learned linkage structure closely reflects the template linkage structure, and that explicitly using the template structure yields the best performance overall.", "AI": {"tldr": "GP-GOMEA\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u5173\u8054\u5b66\u4e60\u5ea6\u91cf\uff1a\u4e00\u79cd\u663e\u5f0f\u8003\u8651\u5185\u542b\u5b50\u5bf9\u4e92\u4fe1\u606f\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u53e6\u4e00\u79cd\u4ece\u7070\u76d2\u89c6\u89d2\u76f4\u63a5\u57fa\u4e8e\u6a21\u677f\u63a8\u5bfc\u5173\u8054\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b26\u53f7\u56de\u5f52\u6027\u80fd\u3002", "motivation": "GP-GOMEA\u5728\u7b26\u53f7\u56de\u5f52\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u5173\u8054\u5b66\u4e60\u673a\u5236\u5b58\u5728\u7f3a\u9677\u3002\u5f53\u524d\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u8868\u8fbe\u5f0f\u6a21\u677f\uff0c\u5bfc\u81f4\u8f83\u5c0f\u8868\u8fbe\u5f0f\u4ea7\u751f\u5185\u542b\u5b50\uff08introns\uff09\u3002\u5185\u542b\u5b50\u4e0d\u5f71\u54cd\u9002\u5e94\u5ea6\uff0c\u56e0\u6b64\u4e0e\u9009\u62e9\u8fc7\u7a0b\u65e0\u76f4\u63a5\u5173\u8054\uff0c\u8fd9\u4f1a\u5e72\u6270\u4e92\u4fe1\u606f\u5bf9\u6811\u8282\u70b9\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u51c6\u786e\u6355\u6349\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u5173\u8054\u5b66\u4e60\u5ea6\u91cf\uff1a1\uff09\u663e\u5f0f\u8003\u8651\u5185\u542b\u5b50\u7684\u4e92\u4fe1\u606f\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u4e92\u4fe1\u606f\u65f6\u8003\u8651\u5185\u542b\u5b50\u7684\u5f71\u54cd\uff1b2\uff09\u4ece\u7070\u76d2\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6GP-GOMEA\u7684\u5173\u8054\u5b66\u4e60\uff0c\u76f4\u63a5\u57fa\u4e8e\u6a21\u677f\u63a8\u5bfc\u5173\u8054\u7ed3\u6784\uff0c\u65e0\u9700\u4ece\u79cd\u7fa4\u4e2d\u5b66\u4e60\u3002\u5728\u4e94\u4e2a\u6807\u51c6\u7b26\u53f7\u56de\u5f52\u95ee\u9898\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "GP-GOMEA\u4f7f\u7528\u4e24\u79cd\u65b0\u5ea6\u91cf\u5747\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u65b0\u5b66\u4e60\u7684\u5173\u8054\u7ed3\u6784\u4e0e\u6a21\u677f\u5173\u8054\u7ed3\u6784\u9ad8\u5ea6\u4e00\u81f4\uff0c\u76f4\u63a5\u4f7f\u7528\u6a21\u677f\u7ed3\u6784\u7684\u65b9\u6cd5\u6574\u4f53\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u5185\u542b\u5b50\u95ee\u9898\u6216\u76f4\u63a5\u4ece\u6a21\u677f\u63a8\u5bfc\u5173\u8054\u7ed3\u6784\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u8fdbGP-GOMEA\u7684\u5173\u8054\u5b66\u4e60\u6548\u679c\u3002\u7070\u76d2\u65b9\u6cd5\uff08\u76f4\u63a5\u4f7f\u7528\u6a21\u677f\u7ed3\u6784\uff09\u5728\u7b26\u53f7\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u8868\u660e\u6a21\u677f\u672c\u8eab\u5df2\u5305\u542b\u6709\u4ef7\u503c\u7684\u5173\u8054\u4fe1\u606f\u3002"}}
{"id": "2602.00972", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00972", "abs": "https://arxiv.org/abs/2602.00972", "authors": ["Zhuangbin Chen", "Zhiling Deng", "Kaiming Zhang", "Yang Liu", "Cheng Cui", "Jinfeng Zhong", "Zibin Zheng"], "title": "Cast: Automated Resilience Testing for Production Cloud Service Systems", "comment": null, "summary": "The distributed nature of microservice architecture introduces significant resilience challenges. Traditional testing methods, limited by extensive manual effort and oversimplified test environments, fail to capture production system complexity. To address these limitations, we present Cast, an automated, end-to-end framework for microservice resilience testing in production. It achieves high test fidelity by replaying production traffic against a comprehensive library of application-level faults to exercise internal error-handling logic. To manage the combinatorial test space, Cast employs a complexity-driven strategy to systematically prune redundant tests and prioritize high-value tests targeting the most critical service execution paths. Cast automates the testing lifecycle through a three-phase pipeline (i.e., startup, fault injection, and recovery) and uses a multi-faceted oracle to automatically verify system resilience against nuanced criteria. Deployed in Huawei Cloud for over eight months, Cast has been adopted by many service teams to proactively address resilience vulnerabilities. Our analysis on four large-scale applications with millions of traces reveals 137 potential vulnerabilities, with 89 confirmed by developers. To further quantify its performance, Cast is evaluated on a benchmark set of 48 reproduced bugs, achieving a high coverage of 90%. The results show that Cast is a practical and effective solution for systematically improving the reliability of industrial microservice systems.", "AI": {"tldr": "Cast\u662f\u4e00\u4e2a\u7528\u4e8e\u5fae\u670d\u52a1\u5f39\u6027\u6d4b\u8bd5\u7684\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u91cd\u653e\u6d41\u91cf\u548c\u5e94\u7528\u7ea7\u6545\u969c\u6ce8\u5165\u6765\u53d1\u73b0\u7cfb\u7edf\u6f0f\u6d1e\uff0c\u91c7\u7528\u590d\u6742\u5ea6\u9a71\u52a8\u7b56\u7565\u4f18\u5316\u6d4b\u8bd5\u7a7a\u95f4\uff0c\u5df2\u5728\u534e\u4e3a\u4e91\u90e8\u7f72\u4f7f\u7528\u3002", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u7684\u5206\u5e03\u5f0f\u7279\u6027\u5e26\u6765\u4e86\u663e\u8457\u7684\u5f39\u6027\u6311\u6218\u3002\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u53d7\u9650\u4e8e\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\u548c\u8fc7\u5ea6\u7b80\u5316\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u65e0\u6cd5\u6355\u6349\u751f\u4ea7\u7cfb\u7edf\u7684\u590d\u6742\u6027\u3002", "method": "Cast\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u653e\u751f\u4ea7\u6d41\u91cf\u5e76\u5e94\u7528\u5e94\u7528\u7ea7\u6545\u969c\u5e93\u6765\u6d4b\u8bd5\u5185\u90e8\u9519\u8bef\u5904\u7406\u903b\u8f91\u3002\u91c7\u7528\u590d\u6742\u5ea6\u9a71\u52a8\u7b56\u7565\u7cfb\u7edf\u6027\u5730\u526a\u679d\u5197\u4f59\u6d4b\u8bd5\u5e76\u4f18\u5148\u5904\u7406\u5173\u952e\u670d\u52a1\u6267\u884c\u8def\u5f84\u7684\u9ad8\u4ef7\u503c\u6d4b\u8bd5\u3002\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u547d\u5468\u671f\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff08\u542f\u52a8\u3001\u6545\u969c\u6ce8\u5165\u548c\u6062\u590d\uff09\u5b9e\u73b0\uff0c\u5e76\u4f7f\u7528\u591a\u9762oracle\u81ea\u52a8\u9a8c\u8bc1\u7cfb\u7edf\u5f39\u6027\u3002", "result": "\u5728\u534e\u4e3a\u4e91\u90e8\u7f72\u8d85\u8fc78\u4e2a\u6708\uff0c\u5df2\u88ab\u591a\u4e2a\u670d\u52a1\u56e2\u961f\u91c7\u7528\u3002\u5bf94\u4e2a\u5927\u89c4\u6a21\u5e94\u7528\uff08\u5305\u542b\u6570\u767e\u4e07\u6761\u8ddf\u8e2a\u8bb0\u5f55\uff09\u7684\u5206\u6790\u63ed\u793a\u4e86137\u4e2a\u6f5c\u5728\u6f0f\u6d1e\uff0c\u5176\u4e2d89\u4e2a\u88ab\u5f00\u53d1\u8005\u786e\u8ba4\u3002\u572848\u4e2a\u91cd\u73b0bug\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\uff0cCast\u5b9e\u73b0\u4e8690%\u7684\u9ad8\u8986\u76d6\u7387\u3002", "conclusion": "Cast\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u63d0\u9ad8\u5de5\u4e1a\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u52a8\u5316\u5f39\u6027\u6d4b\u8bd5\u6765\u4e3b\u52a8\u89e3\u51b3\u5f39\u6027\u6f0f\u6d1e\u3002"}}
{"id": "2602.00667", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00667", "abs": "https://arxiv.org/abs/2602.00667", "authors": ["Rong Fu", "Jia Yee Tan", "Wenxin Zhang", "Youjin Wang", "Ziyu Kong", "Zeli Su", "Zhaolu Kang", "Shuning Zhang", "Xianda Li", "Kun Liu", "Simon Fong"], "title": "zkCraft: Prompt-Guided LLM as a Zero-Shot Mutation Pattern Oracle for TCCT-Powered ZK Fuzzing", "comment": "36 pages, 12 figures, 9 tables", "summary": "Zero-knowledge circuits enable privacy-preserving and scalable systems but are difficult to implement correctly due to the tight coupling between witness computation and circuit constraints. We present zkCraft, a practical framework that combines deterministic, R1CS-aware localization with proof-bearing search to detect semantic inconsistencies. zkCraft encodes candidate constraint edits into a single Row-Vortex polynomial and replaces repeated solver queries with a Violation IOP that certifies the existence of edits together with a succinct proof. Deterministic LLM-driven mutation templates bias exploration toward edge cases while preserving auditable algebraic verification. Evaluation on real Circom code shows that proof-bearing localization detects diverse under- and over-constrained faults with low false positives and reduces costly solver interaction. Our approach bridges formal verification and automated debugging, offering a scalable path for robust ZK circuit development.", "AI": {"tldr": "zkCraft\uff1a\u7ed3\u5408\u786e\u5b9a\u6027R1CS\u611f\u77e5\u5b9a\u4f4d\u4e0e\u8bc1\u660e\u627f\u8f7d\u641c\u7d22\u7684ZK\u7535\u8def\u8c03\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7Row-Vortex\u591a\u9879\u5f0f\u7f16\u7801\u7ea6\u675f\u7f16\u8f91\uff0c\u7528Violation IOP\u66ff\u4ee3\u91cd\u590d\u6c42\u89e3\u5668\u67e5\u8be2\uff0c\u68c0\u6d4b\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027", "motivation": "\u96f6\u77e5\u8bc6\u7535\u8def\u5728\u5b9e\u73b0\u65f6\u5b58\u5728\u89c1\u8bc1\u8ba1\u7b97\u4e0e\u7535\u8def\u7ea6\u675f\u7d27\u5bc6\u8026\u5408\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u96be\u4ee5\u6b63\u786e\u5b9e\u73b0\uff0c\u9700\u8981\u4e00\u79cd\u5b9e\u7528\u7684\u6846\u67b6\u6765\u68c0\u6d4b\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u5347ZK\u7535\u8def\u5f00\u53d1\u7684\u9c81\u68d2\u6027", "method": "1) \u786e\u5b9a\u6027R1CS\u611f\u77e5\u5b9a\u4f4d\uff1a\u7ed3\u5408\u786e\u5b9a\u6027LLM\u9a71\u52a8\u7684\u7a81\u53d8\u6a21\u677f\uff0c\u504f\u5411\u63a2\u7d22\u8fb9\u754c\u60c5\u51b5\u540c\u65f6\u4fdd\u6301\u53ef\u5ba1\u8ba1\u7684\u4ee3\u6570\u9a8c\u8bc1\uff1b2) \u8bc1\u660e\u627f\u8f7d\u641c\u7d22\uff1a\u5c06\u5019\u9009\u7ea6\u675f\u7f16\u8f91\u7f16\u7801\u4e3a\u5355\u4e2aRow-Vortex\u591a\u9879\u5f0f\uff1b3) \u7528Violation IOP\u66ff\u4ee3\u91cd\u590d\u6c42\u89e3\u5668\u67e5\u8be2\uff0c\u8ba4\u8bc1\u7f16\u8f91\u5b58\u5728\u6027\u5e76\u63d0\u4f9b\u7b80\u6d01\u8bc1\u660e", "result": "\u5728\u771f\u5b9eCircom\u4ee3\u7801\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8bc1\u660e\u627f\u8f7d\u5b9a\u4f4d\u80fd\u591f\u68c0\u6d4b\u591a\u6837\u5316\u7684\u6b20\u7ea6\u675f\u548c\u8fc7\u7ea6\u675f\u6545\u969c\uff0c\u5177\u6709\u4f4e\u8bef\u62a5\u7387\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u6602\u8d35\u7684\u6c42\u89e3\u5668\u4ea4\u4e92", "conclusion": "zkCraft\u6865\u63a5\u4e86\u5f62\u5f0f\u9a8c\u8bc1\u4e0e\u81ea\u52a8\u5316\u8c03\u8bd5\uff0c\u4e3a\u9c81\u68d2\u7684ZK\u7535\u8def\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u7ed3\u5408\u786e\u5b9a\u6027\u5b9a\u4f4d\u548c\u8bc1\u660e\u627f\u8f7d\u641c\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86ZK\u7535\u8def\u5b9e\u73b0\u4e2d\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027\u95ee\u9898"}}
{"id": "2602.00571", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00571", "abs": "https://arxiv.org/abs/2602.00571", "authors": ["Suifang Zhou", "Ray LC"], "title": "Eternagram: Inspiring Climate Action Through LLM-based Conversational Exploration of a Post-Devastation Climate Future", "comment": "7 pages, 5 figures, CUI 2025", "summary": "Climate action is difficult to persuade because we tend to perceive climate change as remote and disconnected from daily life. Instead of traditional informational engagements, game-based interventions can create narratives that immerse the visitor in situations where their actions have tangible consequences. To make these narratives engaging, we used a speculative scenario of an alien stumbling upon social media to obliquely address climate change through a text-based adventure game installation. Mimicking visitors' natural dialogue in social media apps, we designed an LLM-based chatbot with knowledge of post-climate devastated world that mirrors our own planet Earth. In discovering the world's downfall through interactive chatting and posted images, players begin to realize that their own actions can make a difference on impacts of climate change in this distant world, fostering pro-environmental attitudes. Previously published at CHI, this game installation demonstrates the potential of LLM based creative narratives in exploring speculative worlds driving social change.", "AI": {"tldr": "\u57fa\u4e8eLLM\u7684\u6587\u672c\u5192\u9669\u6e38\u620f\uff0c\u901a\u8fc7\u5916\u661f\u4eba\u53d1\u73b0\u793e\u4ea4\u5a92\u4f53\u540e\u63a2\u7d22\u6c14\u5019\u5d29\u6e83\u4e16\u754c\u7684\u53d9\u4e8b\uff0c\u95f4\u63a5\u4fc3\u8fdb\u6c14\u5019\u884c\u52a8", "motivation": "\u4f20\u7edf\u6c14\u5019\u4fe1\u606f\u4f20\u64ad\u96be\u4ee5\u8ba9\u4eba\u4eec\u611f\u77e5\u6c14\u5019\u53d8\u5316\u4e0e\u65e5\u5e38\u751f\u6d3b\u7684\u8054\u7cfb\uff0c\u9700\u8981\u521b\u65b0\u65b9\u5f0f\u8ba9\u6c14\u5019\u53d8\u5316\u53d8\u5f97\u5177\u4f53\u53ef\u611f", "method": "\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u5bf9\u8bdd\uff0c\u8ba9\u73a9\u5bb6\u901a\u8fc7\u6587\u672c\u5192\u9669\u6e38\u620f\u63a2\u7d22\u6c14\u5019\u5d29\u6e83\u540e\u7684\u4e16\u754c\uff0c\u53d1\u73b0\u8be5\u4e16\u754c\u4e0e\u5730\u7403\u7684\u76f8\u4f3c\u6027", "result": "\u73a9\u5bb6\u901a\u8fc7\u4e92\u52a8\u804a\u5929\u548c\u56fe\u50cf\u53d1\u73b0\u4e16\u754c\u5d29\u6e83\u8fc7\u7a0b\uff0c\u610f\u8bc6\u5230\u81ea\u8eab\u884c\u52a8\u80fd\u5f71\u54cd\u8fd9\u4e2a\u9065\u8fdc\u4e16\u754c\u7684\u6c14\u5019\u53d8\u5316\u5f71\u54cd\uff0c\u57f9\u517b\u4eb2\u73af\u5883\u6001\u5ea6", "conclusion": "LLM\u9a71\u52a8\u7684\u521b\u610f\u53d9\u4e8b\u5728\u63a2\u7d22\u63a8\u6d4b\u6027\u4e16\u754c\u3001\u63a8\u52a8\u793e\u4f1a\u53d8\u9769\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u6e38\u620f\u5316\u5e72\u9884\u80fd\u6709\u6548\u4fc3\u8fdb\u6c14\u5019\u884c\u52a8"}}
{"id": "2602.01044", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01044", "abs": "https://arxiv.org/abs/2602.01044", "authors": ["Yu Tang", "Hailiang Zhao", "Chuansheng Lu", "Yifei Zhang", "Kingsum Chow", "Shuiguang Deng", "Rui Shi"], "title": "Morphis: SLO-Aware Resource Scheduling for Microservices with Time-Varying Call Graphs", "comment": null, "summary": "Modern microservice systems exhibit continuous structural evolution in their runtime call graphs due to workload fluctuations, fault responses, and deployment activities. Despite this complexity, our analysis of over 500,000 production traces from ByteDance reveals a latent regularity: execution paths concentrate around a small set of recurring invocation patterns. However, existing resource management approaches fail to exploit this structure. Industrial autoscalers like Kubernetes HPA ignore inter-service dependencies, while recent academic methods often assume static topologies, rendering them ineffective under dynamic execution contexts. In this work, we propose Morphis, a dependency-aware provisioning framework that unifies pattern-aware trace analysis with global optimization. It introduces structural fingerprinting that decomposes traces into a stable execution backbone and interpretable deviation subgraphs. Then, resource allocation is formulated as a constrained optimization problem over predicted pattern distributions, jointly minimizing aggregate CPU usage while satisfying end-to-end tail-latency SLOs. Our extensive evaluations on the TrainTicket benchmark demonstrate that Morphis reduces CPU consumption by 35-38% compared to state-of-the-art baselines while maintaining 98.8% SLO compliance.", "AI": {"tldr": "Morphis\u662f\u4e00\u4e2a\u4f9d\u8d56\u611f\u77e5\u7684\u8d44\u6e90\u4f9b\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5f0f\u611f\u77e5\u7684\u8ffd\u8e2a\u5206\u6790\u548c\u5168\u5c40\u4f18\u5316\uff0c\u5728\u52a8\u6001\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1135-38%\u7684CPU\u6d88\u8017\u3002", "motivation": "\u73b0\u4ee3\u5fae\u670d\u52a1\u7cfb\u7edf\u5728\u8fd0\u884c\u65f6\u8c03\u7528\u56fe\u8868\u73b0\u51fa\u6301\u7eed\u7684\u7ed3\u6784\u6f14\u5316\uff0c\u4f46\u73b0\u6709\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff1a\u5de5\u4e1a\u7ea7\u81ea\u52a8\u6269\u7f29\u5668\uff08\u5982Kubernetes HPA\uff09\u5ffd\u7565\u670d\u52a1\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u5b66\u672f\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u9759\u6001\u62d3\u6251\uff0c\u5728\u52a8\u6001\u6267\u884c\u73af\u5883\u4e0b\u6548\u679c\u6709\u9650\u3002\u5206\u679050\u591a\u4e07\u4e2a\u751f\u4ea7\u8ffd\u8e2a\u53d1\u73b0\uff0c\u6267\u884c\u8def\u5f84\u96c6\u4e2d\u5728\u5c11\u91cf\u91cd\u590d\u8c03\u7528\u6a21\u5f0f\u4e0a\uff0c\u8fd9\u4e00\u89c4\u5f8b\u672a\u88ab\u73b0\u6709\u65b9\u6cd5\u5229\u7528\u3002", "method": "\u63d0\u51faMorphis\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7ed3\u6784\u6307\u7eb9\u5316\uff1a\u5c06\u8ffd\u8e2a\u5206\u89e3\u4e3a\u7a33\u5b9a\u7684\u6267\u884c\u4e3b\u5e72\u548c\u53ef\u89e3\u91ca\u7684\u504f\u5dee\u5b50\u56fe\uff1b2) \u5168\u5c40\u4f18\u5316\uff1a\u5c06\u8d44\u6e90\u5206\u914d\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u57fa\u4e8e\u9884\u6d4b\u7684\u6a21\u5f0f\u5206\u5e03\uff0c\u5728\u6ee1\u8db3\u7aef\u5230\u7aef\u5c3e\u90e8\u5ef6\u8fdfSLO\u7684\u540c\u65f6\u6700\u5c0f\u5316\u603bCPU\u4f7f\u7528\u3002", "result": "\u5728TrainTicket\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cMorphis\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c1135-38%\u7684CPU\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u630198.8%\u7684SLO\u5408\u89c4\u7387\u3002", "conclusion": "Morphis\u901a\u8fc7\u5229\u7528\u5fae\u670d\u52a1\u6267\u884c\u8def\u5f84\u4e2d\u7684\u6f5c\u5728\u89c4\u5f8b\u6027\uff0c\u5b9e\u73b0\u4e86\u4f9d\u8d56\u611f\u77e5\u7684\u8d44\u6e90\u4f9b\u5e94\uff0c\u5728\u52a8\u6001\u6267\u884c\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u670d\u52a1\u8d28\u91cf\u76ee\u6807\u3002"}}
{"id": "2602.00689", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00689", "abs": "https://arxiv.org/abs/2602.00689", "authors": ["Genqiang Wu", "Xiaoying Zhang", "Yu Qi", "Hao Wang", "Jikui Wang", "Yeping He"], "title": "Computing Maximal Per-Record Leakage and Leakage-Distortion Functions for Privacy Mechanisms under Entropy-Constrained Adversaries", "comment": null, "summary": "The exponential growth of data collection necessitates robust privacy protections that preserve data utility. We address information disclosure against adversaries with bounded prior knowledge, modeled by an entropy constraint $H(X) \\geq b$. Within this information privacy framework -- which replaces differential privacy's independence assumption with a bounded-knowledge model -- we study three core problems: maximal per-record leakage, the primal leakage-distortion tradeoff (minimizing worst-case leakage under distortion $D$), and the dual distortion minimization (minimizing distortion under leakage constraint $L$).\n  These problems resemble classical information-theoretic ones (channel capacity, rate-distortion) but are more complex due to high dimensionality and the entropy constraint. We develop efficient alternating optimization algorithms that exploit convexity-concavity duality, with theoretical guarantees including local convergence for the primal problem and convergence to a stationary point for the dual.\n  Experiments on binary symmetric channels and modular sum queries validate the algorithms, showing improved privacy-utility tradeoffs over classical differential privacy mechanisms. This work provides a computational framework for auditing privacy risks and designing certified mechanisms under realistic adversary assumptions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u9690\u79c1\u6846\u67b6\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5bf9\u624b\u5177\u6709\u6709\u9650\u5148\u9a8c\u77e5\u8bc6\uff08\u71b5\u7ea6\u675f\uff09\u7684\u60c5\u51b5\u4e0b\u5206\u6790\u9690\u79c1\u98ce\u9669\u5e76\u8bbe\u8ba1\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u3002", "motivation": "\u968f\u7740\u6570\u636e\u6536\u96c6\u7684\u6307\u6570\u589e\u957f\uff0c\u9700\u8981\u5728\u4fdd\u62a4\u6570\u636e\u6548\u7528\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u3002\u4f20\u7edf\u5dee\u5206\u9690\u79c1\u7684\u72ec\u7acb\u6027\u5047\u8bbe\u8fc7\u4e8e\u4e25\u683c\uff0c\u9700\u8981\u66f4\u73b0\u5b9e\u7684\u5bf9\u624b\u77e5\u8bc6\u6a21\u578b\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u9690\u79c1\u6846\u67b6\uff0c\u5c06\u5bf9\u624b\u77e5\u8bc6\u5efa\u6a21\u4e3a\u71b5\u7ea6\u675f\u3002\u7814\u7a76\u4e86\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u6700\u5927\u6bcf\u8bb0\u5f55\u6cc4\u6f0f\u3001\u539f\u59cb\u6cc4\u6f0f-\u5931\u771f\u6743\u8861\uff08\u5728\u5931\u771fD\u4e0b\u6700\u5c0f\u5316\u6700\u574f\u60c5\u51b5\u6cc4\u6f0f\uff09\u548c\u5bf9\u5076\u5931\u771f\u6700\u5c0f\u5316\uff08\u5728\u6cc4\u6f0f\u7ea6\u675fL\u4e0b\u6700\u5c0f\u5316\u5931\u771f\uff09\u3002\u5f00\u53d1\u4e86\u5229\u7528\u51f8\u51f9\u5bf9\u5076\u6027\u7684\u9ad8\u6548\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u4e8c\u8fdb\u5236\u5bf9\u79f0\u4fe1\u9053\u548c\u6a21\u5757\u548c\u67e5\u8be2\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u51fa\u6bd4\u7ecf\u5178\u5dee\u5206\u9690\u79c1\u673a\u5236\u66f4\u597d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u3002\u7b97\u6cd5\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff1a\u539f\u59cb\u95ee\u9898\u7684\u5c40\u90e8\u6536\u655b\u6027\u548c\u5bf9\u5076\u95ee\u9898\u7684\u9a7b\u70b9\u6536\u655b\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u5b9e\u9645\u5bf9\u624b\u5047\u8bbe\u4e0b\u5ba1\u8ba1\u9690\u79c1\u98ce\u9669\u548c\u8bbe\u8ba1\u8ba4\u8bc1\u673a\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u514b\u670d\u4e86\u5dee\u5206\u9690\u79c1\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u73b0\u5b9e\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002"}}
{"id": "2602.00668", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00668", "abs": "https://arxiv.org/abs/2602.00668", "authors": ["Duan Li", "Jun Yuan", "Xinyuan Guo", "Xiting Wang", "Yang Liu", "Weikai Yang", "Shixia Liu"], "title": "NCP: Neighborhood-Preserving Non-Uniform Circle Packing for Visualization", "comment": "Accepted by Computational Visual Media", "summary": "Circle packing is widely used in visualization due to its aesthetic appeal and simplicity, particularly in tasks where the spatial arrangement and relationships between data are of interest, such as understanding proximity relationships (e.g., images with categories) or analyzing quantitative data (e.g., housing prices). Many applications require preserving neighborhood relationships while encoding a quantitative attribute using radii for data analysis. To meet these two requirements simultaneously, we present a neighborhood-preserving non-uniform circle packing method, NCP. This method preserves neighborhood relationships between the data represented by non-uniform circles to comprehensively analyze similar data and an attribute of interest. We formulate neighborhood-preserving non-uniform circle packing as a planar graph embedding problem based on the circle packing theorem. This formulation leads to a non-convex optimization problem, which can be solved by the continuation method. We conduct a quantitative evaluation and present two use cases to demonstrate that our NCP method can effectively generate non-uniform circle packing results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u6301\u90bb\u57df\u5173\u7cfb\u7684\u975e\u5747\u5300\u5706\u586b\u5145\u65b9\u6cd5NCP\uff0c\u7528\u4e8e\u540c\u65f6\u4fdd\u6301\u6570\u636e\u95f4\u7684\u90bb\u57df\u5173\u7cfb\u5e76\u7528\u534a\u5f84\u7f16\u7801\u5b9a\u91cf\u5c5e\u6027", "motivation": "\u53ef\u89c6\u5316\u4e2d\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u4e24\u4e2a\u9700\u6c42\uff1a\u4fdd\u6301\u6570\u636e\u95f4\u7684\u90bb\u57df\u5173\u7cfb\uff08\u7528\u4e8e\u5206\u6790\u76f8\u4f3c\u6570\u636e\uff09\uff0c\u4ee5\u53ca\u7528\u5706\u534a\u5f84\u7f16\u7801\u5b9a\u91cf\u5c5e\u6027\uff08\u7528\u4e8e\u5206\u6790\u611f\u5174\u8da3\u7684\u7279\u5f81\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u8981\u6c42\u3002", "method": "\u5c06\u4fdd\u6301\u90bb\u57df\u5173\u7cfb\u7684\u975e\u5747\u5300\u5706\u586b\u5145\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u57fa\u4e8e\u5706\u586b\u5145\u5b9a\u7406\u7684\u5e73\u9762\u56fe\u5d4c\u5165\u95ee\u9898\uff0c\u901a\u8fc7\u8fde\u7eed\u65b9\u6cd5\u6c42\u89e3\u8fd9\u4e2a\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u8bc4\u4f30\u548c\u4e24\u4e2a\u7528\u4f8b\u5c55\u793a\u4e86NCP\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u975e\u5747\u5300\u5706\u586b\u5145\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u90bb\u57df\u5173\u7cfb\u5e76\u7f16\u7801\u5b9a\u91cf\u5c5e\u6027\u3002", "conclusion": "NCP\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u540c\u65f6\u4fdd\u6301\u90bb\u57df\u5173\u7cfb\u548c\u7f16\u7801\u5b9a\u91cf\u5c5e\u6027\u7684\u53ef\u89c6\u5316\u9700\u6c42\uff0c\u4e3a\u6570\u636e\u5206\u6790\u548c\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2602.00814", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00814", "abs": "https://arxiv.org/abs/2602.00814", "authors": ["Bomena Kim", "Hojun Lee", "Younsoo Park", "Yaoyu Hu", "Sebastian Scherer", "Inwook Shim"], "title": "SyNeT: Synthetic Negatives for Traversability Learning", "comment": null, "summary": "Reliable traversability estimation is crucial for autonomous robots to navigate complex outdoor environments safely. Existing self-supervised learning frameworks primarily rely on positive and unlabeled data; however, the lack of explicit negative data remains a critical limitation, hindering the model's ability to accurately identify diverse non-traversable regions. To address this issue, we introduce a method to explicitly construct synthetic negatives, representing plausible but non-traversable, and integrate them into vision-based traversability learning. Our approach is formulated as a training strategy that can be seamlessly integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks without modifying inference architectures. Complementing standard pixel-wise metrics, we introduce an object-centric FPR evaluation approach that analyzes predictions in regions where synthetic negatives are inserted. This evaluation provides an indirect measure of the model's ability to consistently identify non-traversable regions without additional manual labeling. Extensive experiments on both public and self-collected datasets demonstrate that our approach significantly enhances robustness and generalization across diverse environments. The source code and demonstration videos will be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5408\u6210\u8d1f\u6837\u672c\u6765\u589e\u5f3a\u89c6\u89c9\u53ef\u901a\u884c\u6027\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u660e\u786e\u8d1f\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u8bc6\u522b\u975e\u53ef\u901a\u884c\u533a\u57df\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u6b63\u6837\u672c\u548c\u672a\u6807\u8bb0\u6570\u636e\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u8d1f\u6837\u672c\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u51c6\u786e\u8bc6\u522b\u591a\u6837\u5316\u975e\u53ef\u901a\u884c\u533a\u57df\u7684\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u663e\u5f0f\u6784\u5efa\u5408\u6210\u8d1f\u6837\u672c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u663e\u5f0f\u6784\u5efa\u5408\u6210\u8d1f\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u8d1f\u6837\u672c\u4ee3\u8868\u5408\u7406\u4f46\u4e0d\u53ef\u901a\u884c\u7684\u533a\u57df\u3002\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u6b63-\u672a\u6807\u8bb0(PU)\u548c\u6b63-\u8d1f(PN)\u6846\u67b6\u4e2d\uff0c\u65e0\u9700\u4fee\u6539\u63a8\u7406\u67b6\u6784\u3002\u540c\u65f6\u5f15\u5165\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u9519\u8bef\u6b63\u7387\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5206\u6790\u5728\u63d2\u5165\u5408\u6210\u8d1f\u6837\u672c\u533a\u57df\u4e2d\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5728\u516c\u5f00\u548c\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684FPR\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65e0\u9700\u989d\u5916\u624b\u52a8\u6807\u6ce8\u5373\u53ef\u95f4\u63a5\u8861\u91cf\u6a21\u578b\u8bc6\u522b\u975e\u53ef\u901a\u884c\u533a\u57df\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u6784\u5efa\u5408\u6210\u8d1f\u6837\u672c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u89c6\u89c9\u53ef\u901a\u884c\u6027\u5b66\u4e60\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u660e\u786e\u8d1f\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u8bc6\u522b\u975e\u53ef\u901a\u884c\u533a\u57df\u7684\u51c6\u786e\u6027\uff0c\u589e\u5f3a\u4e86\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u5b89\u5168\u6027\u3002"}}
{"id": "2602.01107", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01107", "abs": "https://arxiv.org/abs/2602.01107", "authors": ["Daniel Ramos", "Catarina Gamboa", "In\u00eas Lynce", "Vasco Manquinho", "Ruben Martins", "Claire Le Goues"], "title": "SPELL: Synthesis of Programmatic Edits using LLMs", "comment": "pre-print", "summary": "Library migration is a common but error-prone task in software development. Developers may need to replace one library with another due to reasons like changing requirements or licensing changes. Migration typically entails updating and rewriting source code manually. While automated migration tools exist, most rely on mining examples from real-world projects that have already undergone similar migrations. However, these data are scarce, and collecting them for arbitrary pairs of libraries is difficult. Moreover, these migration tools often miss out on leveraging modern code transformation infrastructure.\n  In this paper, we present a new approach to automated API migration that sidesteps the limitations described above. Instead of relying on existing migration data or using LLMs directly for transformation, we use LLMs to extract migration examples. Next, we use an Agent to generalize those examples to reusable transformation scripts in PolyglotPiranha, a modern code transformation tool. Our method distills latent migration knowledge from LLMs into structured, testable, and repeatable migration logic, without requiring preexisting corpora or manual engineering effort. Experimental results across Python libraries show that our system can generate diverse migration examples and synthesize transformation scripts that generalize to real-world codebases.", "AI": {"tldr": "\u4f7f\u7528LLM\u63d0\u53d6\u8fc1\u79fb\u793a\u4f8b\uff0c\u901a\u8fc7Agent\u5c06\u5176\u6cdb\u5316\u4e3aPolyglotPiranha\u4e2d\u7684\u53ef\u91cd\u7528\u8f6c\u6362\u811a\u672c\uff0c\u5b9e\u73b0\u65e0\u9700\u9884\u5b58\u6570\u636e\u6216\u4eba\u5de5\u5de5\u7a0b\u7684\u81ea\u52a8\u5316API\u8fc1\u79fb", "motivation": "\u5e93\u8fc1\u79fb\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e38\u89c1\u4f46\u6613\u51fa\u9519\u7684\u4efb\u52a1\u3002\u73b0\u6709\u81ea\u52a8\u5316\u8fc1\u79fb\u5de5\u5177\u5927\u591a\u4f9d\u8d56\u4ece\u5df2\u5b8c\u6210\u7c7b\u4f3c\u8fc1\u79fb\u7684\u771f\u5b9e\u9879\u76ee\u4e2d\u6316\u6398\u793a\u4f8b\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u7a00\u7f3a\u4e14\u96be\u4ee5\u6536\u96c6\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u5de5\u5177\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3\u4ee3\u7801\u8f6c\u6362\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u63d0\u51fa\u65b0\u65b9\u6cd5\uff1a1) \u4f7f\u7528LLM\u63d0\u53d6\u8fc1\u79fb\u793a\u4f8b\uff1b2) \u901a\u8fc7Agent\u5c06\u8fd9\u4e9b\u793a\u4f8b\u6cdb\u5316\u4e3aPolyglotPiranha\u4e2d\u7684\u53ef\u91cd\u7528\u8f6c\u6362\u811a\u672c\u3002\u8be5\u65b9\u6cd5\u5c06LLM\u4e2d\u7684\u6f5c\u5728\u8fc1\u79fb\u77e5\u8bc6\u63d0\u70bc\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u6d4b\u8bd5\u3001\u53ef\u91cd\u590d\u7684\u8fc1\u79fb\u903b\u8f91\u3002", "result": "\u5728Python\u5e93\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u8fc1\u79fb\u793a\u4f8b\uff0c\u5e76\u5408\u6210\u80fd\u591f\u6cdb\u5316\u5230\u771f\u5b9e\u4ee3\u7801\u5e93\u7684\u8f6c\u6362\u811a\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed5\u8fc7\u4e86\u73b0\u6709\u8fc1\u79fb\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u73b0\u6709\u8fc1\u79fb\u6570\u636e\u6216\u76f4\u63a5\u4f7f\u7528LLM\u8fdb\u884c\u8f6c\u6362\uff0c\u800c\u662f\u5c06LLM\u7684\u77e5\u8bc6\u84b8\u998f\u4e3a\u7ed3\u6784\u5316\u8fc1\u79fb\u903b\u8f91\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u81ea\u52a8\u5316API\u8fc1\u79fb\u3002"}}
{"id": "2602.00478", "categories": ["cs.LG", "cs.AI", "cs.NE", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00478", "abs": "https://arxiv.org/abs/2602.00478", "authors": ["Xi Lin", "Ping Guo", "Yilu Liu", "Qingfu Zhang", "Jianyong Sun"], "title": "Quality-Diversity Optimization as Multi-Objective Optimization", "comment": null, "summary": "The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u8d28\u91cf-\u591a\u6837\u6027\u4f18\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u5177\u6709\u5927\u91cf\u4f18\u5316\u76ee\u6807\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u73b0\u6709MOO\u65b9\u6cd5\u53ef\u76f4\u63a5\u5e94\u7528\u4e8eQD\u95ee\u9898", "motivation": "\u8d28\u91cf-\u591a\u6837\u6027\u4f18\u5316\u5728\u673a\u5668\u4eba\u63a7\u5236\u3001\u521b\u610f\u8bbe\u8ba1\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709QD\u7b97\u6cd5\u5404\u6709\u4e0d\u540c\u7684\u8bbe\u8ba1\u539f\u5219\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5efa\u7acbQD\u4e0eMOO\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4f7f\u6210\u719f\u7684MOO\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u5e94\u7528\u4e8eQD\u95ee\u9898", "method": "\u5c06QD\u4f18\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u5177\u6709\u5927\u91cf\u4f18\u5316\u76ee\u6807\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u57fa\u4e8e\u96c6\u5408\u7684\u6807\u91cf\u5316\u6280\u672f\uff0c\u901a\u8fc7\u534f\u4f5c\u641c\u7d22\u8fc7\u7a0b\u89e3\u51b3QD\u95ee\u9898", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u7ee7\u627f\u4e86MOO\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u540c\u65f6\u4e3aQD\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u60f3\u7279\u6027\u3002\u591a\u4e2aQD\u5e94\u7528\u7684\u5b9e\u9a8c\u7814\u7a76\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u4e0e\u6700\u5148\u8fdb\u7684QD\u7b97\u6cd5\u6027\u80fd\u76f8\u5f53", "conclusion": "\u901a\u8fc7\u5c06QD\u91cd\u65b0\u8868\u8ff0\u4e3aMOO\u95ee\u9898\uff0c\u53ef\u4ee5\u5145\u5206\u5229\u7528\u6210\u719f\u7684MOO\u65b9\u6cd5\u89e3\u51b3QD\u95ee\u9898\uff0c\u4e3aQD\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u65b9\u6cd5"}}
{"id": "2602.00711", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00711", "abs": "https://arxiv.org/abs/2602.00711", "authors": ["Ranjith Krishnamurthy", "Oshando Johnson", "Goran Piskachev", "Eric Bodden"], "title": "From Detection to Prevention: Explaining Security-Critical Code to Avoid Vulnerabilities", "comment": "4 pages", "summary": "Security vulnerabilities often arise unintentionally during development due to a lack of security expertise and code complexity. Traditional tools, such as static and dynamic analysis, detect vulnerabilities only after they are introduced in code, leading to costly remediation. This work explores a proactive strategy to prevent vulnerabilities by highlighting code regions that implement security-critical functionality -- such as data access, authentication, and input handling -- and providing guidance for their secure implementation. We present an IntelliJ IDEA plugin prototype that uses code-level software metrics to identify potentially security-critical methods and large language models (LLMs) to generate prevention-oriented explanations. Our initial evaluation on the Spring-PetClinic application shows that the selected metrics identify most known security-critical methods, while an LLM provides actionable, prevention-focused insights. Although these metrics capture structural properties rather than semantic aspects of security, this work lays the foundation for code-level security-aware metrics and enhanced explanations.", "AI": {"tldr": "\u5f00\u53d1IntelliJ IDEA\u63d2\u4ef6\u539f\u578b\uff0c\u901a\u8fc7\u4ee3\u7801\u5ea6\u91cf\u6307\u6807\u8bc6\u522b\u5b89\u5168\u5173\u952e\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u751f\u6210\u9884\u9632\u6027\u5b89\u5168\u6307\u5bfc\uff0c\u5b9e\u73b0\u6f0f\u6d1e\u9884\u9632\u800c\u975e\u4e8b\u540e\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u5de5\u5177\uff08\u9759\u6001/\u52a8\u6001\u5206\u6790\uff09\u53ea\u80fd\u5728\u6f0f\u6d1e\u5f15\u5165\u4ee3\u7801\u540e\u8fdb\u884c\u68c0\u6d4b\uff0c\u4fee\u590d\u6210\u672c\u9ad8\u3002\u9700\u8981\u66f4\u4e3b\u52a8\u7684\u9884\u9632\u7b56\u7565\uff0c\u5728\u5f00\u53d1\u9636\u6bb5\u5c31\u8bc6\u522b\u5b89\u5168\u5173\u952e\u529f\u80fd\u5e76\u63d0\u4f9b\u5b89\u5168\u5b9e\u73b0\u6307\u5bfc\u3002", "method": "\u5f00\u53d1IntelliJ IDEA\u63d2\u4ef6\u539f\u578b\uff0c\u4f7f\u7528\u4ee3\u7801\u7ea7\u8f6f\u4ef6\u5ea6\u91cf\u6307\u6807\u8bc6\u522b\u6f5c\u5728\u5b89\u5168\u5173\u952e\u65b9\u6cd5\uff08\u5982\u6570\u636e\u8bbf\u95ee\u3001\u8ba4\u8bc1\u3001\u8f93\u5165\u5904\u7406\u7b49\uff09\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9884\u9632\u6027\u89e3\u91ca\u548c\u6307\u5bfc\u3002", "result": "\u5728Spring-PetClinic\u5e94\u7528\u4e0a\u7684\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff1a\u6240\u9009\u5ea6\u91cf\u6307\u6807\u80fd\u8bc6\u522b\u5927\u591a\u6570\u5df2\u77e5\u5b89\u5168\u5173\u952e\u65b9\u6cd5\uff1bLLM\u80fd\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u3001\u4ee5\u9884\u9632\u4e3a\u91cd\u70b9\u7684\u89c1\u89e3\u3002", "conclusion": "\u867d\u7136\u5f53\u524d\u5ea6\u91cf\u6307\u6807\u4e3b\u8981\u6355\u83b7\u7ed3\u6784\u7279\u6027\u800c\u975e\u5b89\u5168\u8bed\u4e49\uff0c\u4f46\u4e3a\u4ee3\u7801\u7ea7\u5b89\u5168\u611f\u77e5\u5ea6\u91cf\u548c\u589e\u5f3a\u89e3\u91ca\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u4e3b\u52a8\u9884\u9632\u5b89\u5168\u6f0f\u6d1e\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.00697", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00697", "abs": "https://arxiv.org/abs/2602.00697", "authors": ["Kayode P. Ayodele", "Enoruwa Obayiuwana", "Aderonke R. Lawal", "Ayorinde Bamimore", "Funmilayo B. Offiong", "Emmanuel A. Peter"], "title": "Revising Bloom's Taxonomy for Dual-Mode Cognition in Human-AI Systems: The Augmented Cognition Framework", "comment": null, "summary": "As artificial intelligence (AI) models become routinely integrated into knowledge work, cognitive acts increasingly occur in two distinct modes: individually, using biological resources alone, or distributed across a human-AI system. Existing revisions to Bloom's Taxonomy treat AI as an external capability to be mapped against human cognition rather than as a driver of this dual-mode structure, and thus fail to specify distinct learning outcomes and assessment targets for each mode. This paper proposes the Augmented Cognition Framework (ACF), a restructured taxonomy built on three principles. First, each traditional Bloom level operates in two modes (Individual and Distributed) with mode-specific cognitive verbs. Second, an asymmetric dependency relationship holds wherein effective Distributed cognition typically requires Individual cognitive foundations, though structured scaffolding can in some cases reverse this sequence. Third, a seventh level, Orchestration, specifies a governance capacity for managing mode-switching, trust calibration, and partnership optimization. We systematically compare existing AI-revised taxonomies against explicit assessment-utility criteria and show, across the frameworks reviewed, that ACF uniquely generates assessable learning outcomes for individual cognition, distributed cognition, and mode-governance as distinct targets. The framework addresses fluent incompetence, the central pedagogical risk of the AI era, by making the dependency relationship structurally explicit while accommodating legitimate scaffolding approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u589e\u5f3a\u8ba4\u77e5\u6846\u67b6\uff08ACF\uff09\uff0c\u8fd9\u662f\u5bf9\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684\u91cd\u6784\uff0c\u4ee5\u89e3\u51b3AI\u65f6\u4ee3\u4eba\u7c7b\u8ba4\u77e5\u7684\u4e24\u79cd\u6a21\u5f0f\uff08\u4e2a\u4f53\u4e0e\u5206\u5e03\u5f0f\uff09\u53ca\u5176\u8bc4\u4f30\u9700\u6c42\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u88ab\u5e38\u89c4\u6574\u5408\u5230\u77e5\u8bc6\u5de5\u4f5c\u4e2d\uff0c\u8ba4\u77e5\u6d3b\u52a8\u8d8a\u6765\u8d8a\u591a\u5730\u4ee5\u4e24\u79cd\u4e0d\u540c\u6a21\u5f0f\u53d1\u751f\uff1a\u5355\u72ec\u4f7f\u7528\u751f\u7269\u8d44\u6e90\uff0c\u6216\u5206\u5e03\u5728\u4eba\u673a\u7cfb\u7edf\u4e2d\u3002\u73b0\u6709\u7684\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u4fee\u8ba2\u5c06AI\u89c6\u4e3a\u6620\u5c04\u5230\u4eba\u7c7b\u8ba4\u77e5\u7684\u5916\u90e8\u80fd\u529b\uff0c\u800c\u975e\u8fd9\u79cd\u53cc\u6a21\u5f0f\u7ed3\u6784\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u56e0\u6b64\u672a\u80fd\u4e3a\u6bcf\u79cd\u6a21\u5f0f\u6307\u5b9a\u660e\u786e\u7684\u5b66\u4e60\u6210\u679c\u548c\u8bc4\u4f30\u76ee\u6807\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u589e\u5f3a\u8ba4\u77e5\u6846\u67b6\uff08ACF\uff09\uff0c\u57fa\u4e8e\u4e09\u4e2a\u539f\u5219\u91cd\u6784\u5206\u7c7b\u6cd5\uff1a1\uff09\u6bcf\u4e2a\u4f20\u7edf\u5e03\u9c81\u59c6\u5c42\u7ea7\u5728\u4e24\u79cd\u6a21\u5f0f\uff08\u4e2a\u4f53\u4e0e\u5206\u5e03\u5f0f\uff09\u4e2d\u8fd0\u4f5c\uff0c\u5177\u6709\u6a21\u5f0f\u7279\u5b9a\u7684\u8ba4\u77e5\u52a8\u8bcd\uff1b2\uff09\u5b58\u5728\u4e0d\u5bf9\u79f0\u4f9d\u8d56\u5173\u7cfb\uff0c\u6709\u6548\u7684\u5206\u5e03\u5f0f\u8ba4\u77e5\u901a\u5e38\u9700\u8981\u4e2a\u4f53\u8ba4\u77e5\u57fa\u7840\uff0c\u4f46\u7ed3\u6784\u5316\u652f\u67b6\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u9006\u8f6c\u8fd9\u4e00\u987a\u5e8f\uff1b3\uff09\u7b2c\u4e03\u5c42\u7ea7\"\u7f16\u6392\"\u6307\u5b9a\u4e86\u7ba1\u7406\u6a21\u5f0f\u5207\u6362\u3001\u4fe1\u4efb\u6821\u51c6\u548c\u4f19\u4f34\u4f18\u5316\u7684\u6cbb\u7406\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u73b0\u6709AI\u4fee\u8ba2\u5206\u7c7b\u6cd5\u4e0e\u660e\u786e\u8bc4\u4f30\u6548\u7528\u6807\u51c6\uff0c\u7814\u7a76\u8868\u660e\u5728\u6240\u6709\u5ba1\u67e5\u7684\u6846\u67b6\u4e2d\uff0cACF\u72ec\u7279\u5730\u751f\u6210\u4e86\u53ef\u8bc4\u4f30\u7684\u5b66\u4e60\u6210\u679c\uff0c\u5206\u522b\u9488\u5bf9\u4e2a\u4f53\u8ba4\u77e5\u3001\u5206\u5e03\u5f0f\u8ba4\u77e5\u548c\u6a21\u5f0f\u6cbb\u7406\u4f5c\u4e3a\u4e0d\u540c\u76ee\u6807\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u4f9d\u8d56\u5173\u7cfb\u5728\u7ed3\u6784\u4e0a\u660e\u786e\u5316\uff0c\u540c\u65f6\u5bb9\u7eb3\u5408\u6cd5\u7684\u652f\u67b6\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86AI\u65f6\u4ee3\u7684\u6838\u5fc3\u6559\u5b66\u98ce\u9669\u2014\u2014\u6d41\u5229\u65e0\u80fd\u3002ACF\u4e3aAI\u65f6\u4ee3\u7684\u6559\u80b2\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u6846\u67b6\u3002"}}
{"id": "2602.00750", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00750", "abs": "https://arxiv.org/abs/2602.00750", "authors": ["Md Jahedur Rahman", "Ihsen Alouani"], "title": "Bypassing Prompt Injection Detectors through Evasive Injections", "comment": null, "summary": "Large language models (LLMs) are increasingly used in interactive and retrieval-augmented systems, but they remain vulnerable to task drift; deviations from a user's intended instruction due to injected secondary prompts. Recent work has shown that linear probes trained on activation deltas of LLMs' hidden layers can effectively detect such drift. In this paper, we evaluate the robustness of these detectors against adversarially optimised suffixes. We generate universal suffixes that cause poisoned inputs to evade detection across multiple probes simultaneously. Our experiments on Phi-3 3.8B and Llama-3 8B show that a single suffix can achieve high attack success rates; up to 93.91% and 99.63%, respectively, when all probes must be fooled, and nearly perfect success (>90%) under majority vote setting. These results demonstrate that activation delta-based task drift detectors are highly vulnerable to adversarial suffixes, highlighting the need for stronger defences against adaptive attacks. We also propose a defence technique where we generate multiple suffixes and randomly append one of them to the prompts while making forward passes of the LLM and train logistic regression models with these activations. We found this approach to be highly effective against such attacks.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6fc0\u6d3bdelta\u7684\u4efb\u52a1\u6f02\u79fb\u68c0\u6d4b\u5668\u5bf9\u6297\u5bf9\u6297\u6027\u540e\u7f00\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u8fd9\u4e9b\u68c0\u6d4b\u5668\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u548c\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u4efb\u52a1\u6f02\u79fb\u7684\u5f71\u54cd\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u68c0\u6d4b\u8fd9\u79cd\u6f02\u79fb\uff0c\u4f46\u8fd9\u4e9b\u68c0\u6d4b\u5668\u53ef\u80fd\u9762\u4e34\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5a01\u80c1\u3002", "method": "\u751f\u6210\u901a\u7528\u7684\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u4f7f\u88ab\u6c61\u67d3\u8f93\u5165\u80fd\u591f\u540c\u65f6\u9003\u907f\u591a\u4e2a\u63a2\u9488\u7684\u68c0\u6d4b\u3002\u5728Phi-3 3.8B\u548cLlama-3 8B\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u653b\u51fb\u6210\u529f\u7387\u3002\u540c\u65f6\u63d0\u51fa\u9632\u5fa1\u6280\u672f\uff1a\u751f\u6210\u591a\u4e2a\u540e\u7f00\u5e76\u968f\u673a\u9644\u52a0\u5230\u63d0\u793a\u4e2d\uff0c\u4f7f\u7528\u8fd9\u4e9b\u6fc0\u6d3b\u8bad\u7ec3\u903b\u8f91\u56de\u5f52\u6a21\u578b\u3002", "result": "\u5355\u4e2a\u540e\u7f00\u5728\u9700\u8981\u6b3a\u9a97\u6240\u6709\u63a2\u9488\u65f6\uff0c\u653b\u51fb\u6210\u529f\u7387\u5206\u522b\u8fbe\u523093.91%\uff08Phi-3\uff09\u548c99.63%\uff08Llama-3\uff09\uff1b\u5728\u591a\u6570\u6295\u7968\u8bbe\u7f6e\u4e0b\uff0c\u6210\u529f\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff08>90%\uff09\u3002\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6cd5\u5bf9\u8fd9\u4e9b\u653b\u51fb\u975e\u5e38\u6709\u6548\u3002", "conclusion": "\u57fa\u4e8e\u6fc0\u6d3bdelta\u7684\u4efb\u52a1\u6f02\u79fb\u68c0\u6d4b\u5668\u5bf9\u5bf9\u6297\u6027\u540e\u7f00\u9ad8\u5ea6\u8106\u5f31\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u9632\u5fa1\u673a\u5236\u6765\u5e94\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u3002\u63d0\u51fa\u7684\u968f\u673a\u540e\u7f00\u9632\u5fa1\u6280\u672f\u80fd\u6709\u6548\u589e\u5f3a\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.00726", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00726", "abs": "https://arxiv.org/abs/2602.00726", "authors": ["Yinghao Zhu", "Dehao Sui", "Zixiang Wang", "Xuning Hu", "Lei Gu", "Yifan Qi", "Tianchen Wu", "Ling Wang", "Yuan Wei", "Wen Tang", "Zhihan Cui", "Yasha Wang", "Lequan Yu", "Ewen M Harrison", "Junyi Gao", "Liantao Ma"], "title": "Augmenting Clinical Decision-Making with an Interactive and Interpretable AI Copilot: A Real-World User Study with Clinicians in Nephrology and Obstetrics", "comment": "Accepted by ACM CHI 2026", "summary": "Clinician skepticism toward opaque AI hinders adoption in high-stakes healthcare. We present AICare, an interactive and interpretable AI copilot for collaborative clinical decision-making. By analyzing longitudinal electronic health records, AICare grounds dynamic risk predictions in scrutable visualizations and LLM-driven diagnostic recommendations. Through a within-subjects counterbalanced study with 16 clinicians across nephrology and obstetrics, we comprehensively evaluated AICare using objective measures (task completion time and error rate), subjective assessments (NASA-TLX, SUS, and confidence ratings), and semi-structured interviews. Our findings indicate AICare's reduced cognitive workload. Beyond performance metrics, qualitative analysis reveals that trust is actively constructed through verification, with interaction strategies diverging by expertise: junior clinicians used the system as cognitive scaffolding to structure their analysis, while experts engaged in adversarial verification to challenge the AI's logic. This work offers design implications for creating AI systems that function as transparent partners, accommodating diverse reasoning styles to augment rather than replace clinical judgment.", "AI": {"tldr": "AICare\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89e3\u91caAI\u8f85\u52a9\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u5206\u6790\u548cLLM\u8bca\u65ad\u5efa\u8bae\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\uff0c\u7814\u7a76\u663e\u793a\u80fd\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u5e76\u5efa\u7acb\u4fe1\u4efb\uff0c\u4f46\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u7684\u4e34\u5e8a\u533b\u751f\u4f7f\u7528\u7b56\u7565\u4e0d\u540c\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u5bf9\u4e0d\u900f\u660eAI\u7684\u6000\u7591\u963b\u788d\u4e86\u9ad8\u98ce\u9669\u533b\u7597\u73af\u5883\u4e2d\u7684AI\u91c7\u7528\uff0c\u9700\u8981\u5f00\u53d1\u4ea4\u4e92\u5f0f\u3001\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u6765\u4fc3\u8fdb\u4e34\u5e8a\u534f\u4f5c\u51b3\u7b56\u3002", "method": "\u5f00\u53d1AICare\u7cfb\u7edf\uff0c\u5206\u6790\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u63d0\u4f9b\u52a8\u6001\u98ce\u9669\u9884\u6d4b\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u548cLLM\u9a71\u52a8\u7684\u8bca\u65ad\u5efa\u8bae\uff1b\u91c7\u7528\u88ab\u8bd5\u5185\u5e73\u8861\u8bbe\u8ba1\uff0c\u62db\u52df16\u540d\u80be\u75c5\u5b66\u548c\u4ea7\u79d1\u4e34\u5e8a\u533b\u751f\uff0c\u901a\u8fc7\u5ba2\u89c2\u6307\u6807\uff08\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u9519\u8bef\u7387\uff09\u3001\u4e3b\u89c2\u8bc4\u4f30\uff08NASA-TLX\u3001SUS\u3001\u4fe1\u5fc3\u8bc4\u5206\uff09\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "AICare\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u8377\uff1b\u4fe1\u4efb\u901a\u8fc7\u9a8c\u8bc1\u4e3b\u52a8\u6784\u5efa\uff0c\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u7684\u4e34\u5e8a\u533b\u751f\u4f7f\u7528\u7b56\u7565\u4e0d\u540c\uff1a\u521d\u7ea7\u533b\u751f\u5c06\u5176\u4f5c\u4e3a\u8ba4\u77e5\u652f\u67b6\u6765\u7ed3\u6784\u5316\u5206\u6790\uff0c\u800c\u4e13\u5bb6\u5219\u8fdb\u884c\u5bf9\u6297\u6027\u9a8c\u8bc1\u6765\u6311\u6218AI\u903b\u8f91\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u521b\u5efa\u900f\u660eAI\u4f19\u4f34\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u542f\u793a\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5e94\u9002\u5e94\u4e0d\u540c\u7684\u63a8\u7406\u98ce\u683c\uff0c\u4ee5\u589e\u5f3a\u800c\u975e\u53d6\u4ee3\u4e34\u5e8a\u5224\u65ad\u3002"}}
{"id": "2602.00868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00868", "abs": "https://arxiv.org/abs/2602.00868", "authors": ["Nikhil Uday Shinde", "Dylan Hirsch", "Michael C. Yip", "Sylvia Herbert"], "title": "Safe Stochastic Explorer: Enabling Safe Goal Driven Exploration in Stochastic Environments and Safe Interaction with Unknown Objects", "comment": null, "summary": "Autonomous robots operating in unstructured, safety-critical environments, from planetary exploration to warehouses and homes, must learn to safely navigate and interact with their surroundings despite limited prior knowledge. Current methods for safe control, such as Hamilton-Jacobi Reachability and Control Barrier Functions, assume known system dynamics. Meanwhile existing safe exploration techniques often fail to account for the unavoidable stochasticity inherent when operating in unknown real world environments, such as an exploratory rover skidding over an unseen surface or a household robot pushing around unmapped objects in a pantry. To address this critical gap, we propose Safe Stochastic Explorer (S.S.Explorer) a novel framework for safe, goal-driven exploration under stochastic dynamics. Our approach strategically balances safety and information gathering to reduce uncertainty about safety in the unknown environment. We employ Gaussian Processes to learn the unknown safety function online, leveraging their predictive uncertainty to guide information-gathering actions and provide probabilistic bounds on safety violations. We first present our method for discrete state space environments and then introduce a scalable relaxation to effectively extend this approach to continuous state spaces. Finally we demonstrate how this framework can be naturally applied to ensure safe physical interaction with multiple unknown objects. Extensive validation in simulation and demonstrative hardware experiments showcase the efficacy of our method, representing a step forward toward enabling reliable widespread robot autonomy in complex, uncertain environments.", "AI": {"tldr": "\u63d0\u51faSafe Stochastic Explorer\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u968f\u673a\u52a8\u6001\u73af\u5883\u4e0b\u8fdb\u884c\u5b89\u5168\u3001\u76ee\u6807\u9a71\u52a8\u7684\u63a2\u7d22\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u672a\u77e5\u5b89\u5168\u51fd\u6570\uff0c\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u4fe1\u606f\u6536\u96c6\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u3001\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u8fd0\u884c\u65f6\uff0c\u9700\u8981\u5728\u6709\u9650\u5148\u9a8c\u77e5\u8bc6\u4e0b\u5b89\u5168\u5bfc\u822a\u548c\u4ea4\u4e92\u3002\u73b0\u6709\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\u5047\u8bbe\u5df2\u77e5\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u800c\u73b0\u6709\u5b89\u5168\u63a2\u7d22\u6280\u672f\u5f80\u5f80\u5ffd\u7565\u73b0\u5b9e\u73af\u5883\u4e2d\u4e0d\u53ef\u907f\u514d\u7684\u968f\u673a\u6027\uff08\u5982\u63a2\u6d4b\u8f66\u5728\u672a\u77e5\u8868\u9762\u6253\u6ed1\u3001\u5bb6\u7528\u673a\u5668\u4eba\u63a8\u52a8\u672a\u6620\u5c04\u7269\u4f53\uff09\u3002", "method": "\u63d0\u51faSafe Stochastic Explorer\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u5728\u7ebf\u5b66\u4e60\u672a\u77e5\u5b89\u5168\u51fd\u6570\uff0c\u5229\u7528\u5176\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6307\u5bfc\u4fe1\u606f\u6536\u96c6\u52a8\u4f5c\u5e76\u63d0\u4f9b\u5b89\u5168\u8fdd\u89c4\u7684\u6982\u7387\u754c\u9650\u3002\u9996\u5148\u9488\u5bf9\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7136\u540e\u5f15\u5165\u53ef\u6269\u5c55\u677e\u5f1b\u6269\u5c55\u5230\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\uff0c\u6700\u540e\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u4e0e\u591a\u4e2a\u672a\u77e5\u7269\u4f53\u7684\u5b89\u5168\u7269\u7406\u4ea4\u4e92\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ee3\u8868\u4e86\u5728\u590d\u6742\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u5e7f\u6cdb\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u65b9\u9762\u7684\u91cd\u8981\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\u968f\u673a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5b89\u5168\u63a2\u7d22\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u4fe1\u606f\u6536\u96c6\u6765\u51cf\u5c11\u5bf9\u672a\u77e5\u73af\u5883\u5b89\u5168\u6027\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63a8\u52a8\u4e86\u590d\u6742\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01253", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01253", "abs": "https://arxiv.org/abs/2602.01253", "authors": ["Nouf Alturayeif", "Irfan Ahmad", "Jameleddine Hassine"], "title": "TraceLLM: Leveraging Large Language Models with Prompt Engineering for Enhanced Requirements Traceability", "comment": null, "summary": "Requirements traceability, the process of establishing and maintaining relationships between requirements and various software development artifacts, is paramount for ensuring system integrity and fulfilling requirements throughout the Software Development Life Cycle (SDLC). Traditional methods, including manual and information retrieval models, are labor-intensive, error-prone, and limited by low precision. Recently, Large Language Models (LLMs) have demonstrated potential for supporting software engineering tasks through advanced language comprehension. However, a substantial gap exists in the systematic design and evaluation of prompts tailored to extract accurate trace links. This paper introduces TraceLLM, a systematic framework for enhancing requirements traceability through prompt engineering and demonstration selection. Our approach incorporates rigorous dataset splitting, iterative prompt refinement, enrichment with contextual roles and domain knowledge, and evaluation across zero- and few-shot settings. We assess prompt generalization and robustness using eight state-of-the-art LLMs on four benchmark datasets representing diverse domains (aerospace, healthcare) and artifact types (requirements, design elements, test cases, regulations). TraceLLM achieves state-of-the-art F2 scores, outperforming traditional IR baselines, fine-tuned models, and prior LLM-based methods. We also explore the impact of demonstration selection strategies, identifying label-aware, diversity-based sampling as particularly effective. Overall, our findings highlight that traceability performance depends not only on model capacity but also critically on the quality of prompt engineering. In addition, the achieved performance suggests that TraceLLM can support semi-automated traceability workflows in which candidate links are reviewed and validated by human analysts.", "AI": {"tldr": "TraceLLM\uff1a\u4e00\u4e2a\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u548c\u6f14\u793a\u9009\u62e9\u589e\u5f3a\u9700\u6c42\u53ef\u8ffd\u6eaf\u6027\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u5728\u591a\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684F2\u5206\u6570", "motivation": "\u4f20\u7edf\u9700\u6c42\u53ef\u8ffd\u6eaf\u6027\u65b9\u6cd5\uff08\u624b\u52a8\u548c\u4fe1\u606f\u68c0\u7d22\u6a21\u578b\uff09\u52b3\u52a8\u5bc6\u96c6\u3001\u5bb9\u6613\u51fa\u9519\u4e14\u7cbe\u5ea6\u4f4e\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u63d0\u53d6\u51c6\u786e\u8ffd\u6eaf\u94fe\u63a5\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "method": "TraceLLM\u6846\u67b6\u5305\u542b\u4e25\u683c\u7684\u6570\u636e\u96c6\u5206\u5272\u3001\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u3001\u4e0a\u4e0b\u6587\u89d2\u8272\u548c\u9886\u57df\u77e5\u8bc6\u589e\u5f3a\uff0c\u4ee5\u53ca\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8bc4\u4f30\u3002\u63a2\u7d22\u4e86\u6f14\u793a\u9009\u62e9\u7b56\u7565\uff0c\u7279\u522b\u662f\u6807\u7b7e\u611f\u77e5\u3001\u57fa\u4e8e\u591a\u6837\u6027\u7684\u91c7\u6837\u65b9\u6cd5", "result": "\u5728\u4ee3\u8868\u822a\u7a7a\u822a\u5929\u3001\u533b\u7597\u7b49\u4e0d\u540c\u9886\u57df\u548c\u5de5\u4ef6\u7c7b\u578b\u7684\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u516b\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002TraceLLM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684F2\u5206\u6570\uff0c\u4f18\u4e8e\u4f20\u7edfIR\u57fa\u7ebf\u3001\u5fae\u8c03\u6a21\u578b\u548c\u5148\u524d\u57fa\u4e8eLLM\u7684\u65b9\u6cd5", "conclusion": "\u53ef\u8ffd\u6eaf\u6027\u6027\u80fd\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u6a21\u578b\u5bb9\u91cf\uff0c\u66f4\u5173\u952e\u5730\u53d6\u51b3\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u8d28\u91cf\u3002TraceLLM\u80fd\u591f\u652f\u6301\u534a\u81ea\u52a8\u5316\u7684\u53ef\u8ffd\u6eaf\u6027\u5de5\u4f5c\u6d41\uff0c\u5176\u4e2d\u5019\u9009\u94fe\u63a5\u7531\u4eba\u5de5\u5206\u6790\u5e08\u5ba1\u67e5\u548c\u9a8c\u8bc1"}}
{"id": "2602.00837", "categories": ["cs.CR", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.00837", "abs": "https://arxiv.org/abs/2602.00837", "authors": ["Claude Carlet", "Marko \u00d0urasevic", "Domagoj Jakobovic", "Luca Mariot", "Stjepan Picek"], "title": "IDEM Enough? Evolving Highly Nonlinear Idempotent Boolean Functions", "comment": "20 pages, 6 figures, 2 tables", "summary": "Idempotent Boolean functions form a highly structured subclass of Boolean functions that is closely related to rotation symmetry under a normal-basis representation and to invariance under a fixed linear map in a polynomial basis. These functions are attractive as candidates for cryptographic design, yet their additional algebraic constraints make the search for high nonlinearity substantially more difficult than in the unconstrained case. In this work, we investigate evolutionary methods for constructing highly nonlinear idempotent Boolean functions for dimensions $n=5$ up to $n=12$ using a polynomial basis representation with canonical primitive polynomials. Our results show that the problem of evolving idempotent functions is difficult due to the disruptive nature of crossover and mutation operators. Next, we show that idempotence can be enforced by encoding the truth table on orbits, yielding a compact genome of size equal to the number of distinct squaring orbits.", "AI": {"tldr": "\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u6784\u9020\u9ad8\u975e\u7ebf\u6027\u5e42\u7b49\u5e03\u5c14\u51fd\u6570\uff0c\u901a\u8fc7\u8f68\u9053\u7f16\u7801\u5b9e\u73b0\u7d27\u51d1\u57fa\u56e0\u7ec4\u8868\u793a", "motivation": "\u5e42\u7b49\u5e03\u5c14\u51fd\u6570\u5177\u6709\u7279\u6b8a\u7684\u4ee3\u6570\u7ed3\u6784\uff0c\u4e0e\u5bc6\u7801\u5b66\u8bbe\u8ba1\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u5176\u989d\u5916\u7684\u4ee3\u6570\u7ea6\u675f\u4f7f\u5f97\u5bfb\u627e\u9ad8\u975e\u7ebf\u6027\u51fd\u6570\u6bd4\u65e0\u7ea6\u675f\u60c5\u51b5\u66f4\u52a0\u56f0\u96be", "method": "\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u57fa\u8868\u793a\u4e0b\u6784\u9020\u5e42\u7b49\u5e03\u5c14\u51fd\u6570\uff0c\u91c7\u7528\u8f68\u9053\u7f16\u7801\u5c06\u771f\u503c\u8868\u538b\u7f29\u5230\u5e73\u65b9\u8f68\u9053\u4e0a\uff0c\u5b9e\u73b0\u7d27\u51d1\u57fa\u56e0\u7ec4\u8868\u793a", "result": "\u8fdb\u5316\u5e42\u7b49\u51fd\u6570\u5177\u6709\u6311\u6218\u6027\uff0c\u4ea4\u53c9\u548c\u53d8\u5f02\u7b97\u5b50\u5177\u6709\u7834\u574f\u6027\uff1b\u901a\u8fc7\u8f68\u9053\u7f16\u7801\u53ef\u4ee5\u5f3a\u5236\u5b9e\u73b0\u5e42\u7b49\u6027\uff0c\u57fa\u56e0\u7ec4\u5927\u5c0f\u7b49\u4e8e\u4e0d\u540c\u5e73\u65b9\u8f68\u9053\u6570\u91cf", "conclusion": "\u8f68\u9053\u7f16\u7801\u4e3a\u8fdb\u5316\u7b97\u6cd5\u6784\u9020\u5e42\u7b49\u5e03\u5c14\u51fd\u6570\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7d27\u51d1\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u641c\u7d22\u9ad8\u975e\u7ebf\u6027\u5e42\u7b49\u51fd\u6570"}}
{"id": "2602.00738", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.00738", "abs": "https://arxiv.org/abs/2602.00738", "authors": ["Zhida Sun", "Xiaodong Wang", "Zhenyao Zhang", "Min Lu", "Dani Lischinski", "Daniel Cohen-Or", "Hui Huang"], "title": "Iconix: Controlling Semantics and Style in Progressive Icon Grids Generation", "comment": "21 pages, 9 figures, Accepted to ACM CHI'26", "summary": "Visual communication often needs stylistically consistent icons that span concrete and abstract meanings, for use in diverse contexts. We present Iconix, a human-AI co-creative system that organizes icon generation along two axes: semantic richness (what is depicted) and visual complexity (how much detail). Given a user-specified concept, Iconix constructs a semantic scaffold of related analytical perspectives and employs chained, image-conditioned generation to produce a coherent style of exemplars. Each exemplar is then automatically distilled into a progressive sequence, from detailed and elaborate to abstract and simple. The resulting two-dimensional grid exposes a navigable space, helping designers reason jointly about figurative content and visual abstraction. A within-subjects study (N = 32) found that compared to a baseline workflow, participants produced icon grids more creatively, reported lower workload, and explored a coherent range of design variations. We discuss implications for human-machine co-creative approaches that couple semantic scaffolding with progressive simplification to support visual abstraction.", "AI": {"tldr": "Iconix\u662f\u4e00\u4e2a\u4eba\u7c7b-AI\u534f\u540c\u521b\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u4e30\u5bcc\u5ea6\u548c\u89c6\u89c9\u590d\u6742\u5ea6\u4e24\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u56fe\u6807\u751f\u6210\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u5728\u5177\u8c61\u5185\u5bb9\u548c\u89c6\u89c9\u62bd\u8c61\u4e4b\u95f4\u8fdb\u884c\u534f\u540c\u63a8\u7406\u3002", "motivation": "\u89c6\u89c9\u901a\u4fe1\u901a\u5e38\u9700\u8981\u98ce\u683c\u4e00\u81f4\u4e14\u80fd\u8de8\u8d8a\u5177\u4f53\u548c\u62bd\u8c61\u542b\u4e49\u7684\u56fe\u6807\uff0c\u7528\u4e8e\u4e0d\u540c\u7684\u5e94\u7528\u573a\u666f\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u8bed\u4e49\u5185\u5bb9\u548c\u89c6\u89c9\u62bd\u8c61\u7684\u591a\u7ef4\u5ea6\u9700\u6c42\u3002", "method": "Iconix\u7cfb\u7edf\u6cbf\u4e24\u4e2a\u8f74\u7ec4\u7ec7\u56fe\u6807\u751f\u6210\uff1a\u8bed\u4e49\u4e30\u5bcc\u5ea6\uff08\u63cf\u7ed8\u4ec0\u4e48\uff09\u548c\u89c6\u89c9\u590d\u6742\u5ea6\uff08\u7ec6\u8282\u7a0b\u5ea6\uff09\u3002\u7ed9\u5b9a\u7528\u6237\u6307\u5b9a\u7684\u6982\u5ff5\uff0c\u7cfb\u7edf\u6784\u5efa\u76f8\u5173\u5206\u6790\u89c6\u89d2\u7684\u8bed\u4e49\u652f\u67b6\uff0c\u91c7\u7528\u94fe\u5f0f\u56fe\u50cf\u6761\u4ef6\u751f\u6210\u6765\u4ea7\u751f\u4e00\u81f4\u98ce\u683c\u7684\u793a\u4f8b\u3002\u6bcf\u4e2a\u793a\u4f8b\u81ea\u52a8\u84b8\u998f\u4e3a\u4ece\u8be6\u7ec6\u5230\u62bd\u8c61\u7684\u6e10\u8fdb\u5e8f\u5217\uff0c\u5f62\u6210\u53ef\u5bfc\u822a\u7684\u4e8c\u7ef4\u7f51\u683c\u3002", "result": "\u572832\u540d\u53c2\u4e0e\u8005\u7684\u53d7\u8bd5\u8005\u5185\u7814\u7a76\u4e2d\uff0c\u4e0e\u57fa\u7ebf\u5de5\u4f5c\u6d41\u7a0b\u76f8\u6bd4\uff0c\u53c2\u4e0e\u8005\u80fd\u591f\u66f4\u521b\u610f\u5730\u751f\u6210\u56fe\u6807\u7f51\u683c\uff0c\u62a5\u544a\u66f4\u4f4e\u7684\u5de5\u4f5c\u8d1f\u8377\uff0c\u5e76\u63a2\u7d22\u4e00\u81f4\u7684\u8bbe\u8ba1\u53d8\u4f53\u8303\u56f4\u3002", "conclusion": "Iconix\u5c55\u793a\u4e86\u5c06\u8bed\u4e49\u652f\u67b6\u4e0e\u6e10\u8fdb\u7b80\u5316\u76f8\u7ed3\u5408\u7684\u4eba\u7c7b-\u673a\u5668\u534f\u540c\u521b\u4f5c\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u89c6\u89c9\u62bd\u8c61\u8fc7\u7a0b\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u5728\u5177\u8c61\u5185\u5bb9\u548c\u62bd\u8c61\u8868\u8fbe\u4e4b\u95f4\u8fdb\u884c\u534f\u540c\u63a8\u7406\u3002"}}
{"id": "2602.00877", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00877", "abs": "https://arxiv.org/abs/2602.00877", "authors": ["Zhipeng Zhao", "Taimeng Fu", "Shaoshu Su", "Qiwei Du", "Ehsan Tarkesh Esfahani", "Karthik Dantu", "Souma Chowdhury", "Chen Wang"], "title": "Learning When to Jump for Off-road Navigation", "comment": null, "summary": "Low speed does not always guarantee safety in off-road driving. For instance, crossing a ditch may be risky at a low speed due to the risk of getting stuck, yet safe at a higher speed with a controlled, accelerated jump. Achieving such behavior requires path planning that explicitly models complex motion dynamics, whereas existing methods often neglect this aspect and plan solely based on positions or a fixed velocity. To address this gap, we introduce Motion-aware Traversability (MAT) representation to explicitly model terrain cost conditioned on actual robot motion. Instead of assigning a single scalar score for traversability, MAT models each terrain region as a Gaussian function of velocity. During online planning, we decompose the terrain cost computation into two stages: (1) predict terrain-dependent Gaussian parameters from perception in a single forward pass, (2) efficiently update terrain costs for new velocities inferred from current dynamics by evaluating these functions without repeated inference. We develop a system that integrates MAT to enable agile off-road navigation and evaluate it in both simulated and real-world environments with various obstacles. Results show that MAT achieves real-time efficiency and enhances the performance of off-road navigation, reducing path detours by 75% while maintaining safety across challenging terrains.", "AI": {"tldr": "\u63d0\u51faMotion-aware Traversability (MAT)\u8868\u793a\u6cd5\uff0c\u901a\u8fc7\u5c06\u5730\u5f62\u53ef\u901a\u884c\u6027\u5efa\u6a21\u4e3a\u901f\u5ea6\u7684\u9ad8\u65af\u51fd\u6570\uff0c\u5b9e\u73b0\u8003\u8651\u5b9e\u9645\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u52a8\u6001\u8def\u5f84\u89c4\u5212\uff0c\u63d0\u5347\u8d8a\u91ce\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8d8a\u91ce\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u4ec5\u57fa\u4e8e\u4f4d\u7f6e\u6216\u56fa\u5b9a\u901f\u5ea6\u8fdb\u884c\u8def\u5f84\u89c4\u5212\uff0c\u5ffd\u7565\u4e86\u590d\u6742\u8fd0\u52a8\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u3002\u4f4e\u901f\u5e76\u4e0d\u603b\u662f\u5b89\u5168\uff0c\u4f8b\u5982\u8fc7\u6c9f\u65f6\u4f4e\u901f\u53ef\u80fd\u5361\u4f4f\uff0c\u800c\u9ad8\u901f\u8df3\u8dc3\u53cd\u800c\u5b89\u5168\u3002\u9700\u8981\u663e\u5f0f\u5efa\u6a21\u8fd0\u52a8\u52a8\u529b\u5b66\u7684\u5730\u5f62\u53ef\u901a\u884c\u6027\u8868\u793a\u3002", "method": "\u5f15\u5165Motion-aware Traversability (MAT)\u8868\u793a\u6cd5\uff0c\u5c06\u6bcf\u4e2a\u5730\u5f62\u533a\u57df\u5efa\u6a21\u4e3a\u901f\u5ea6\u7684\u9ad8\u65af\u51fd\u6570\u800c\u975e\u5355\u4e00\u6807\u91cf\u5206\u6570\u3002\u5728\u7ebf\u89c4\u5212\u65f6\u91c7\u7528\u4e24\u9636\u6bb5\u8ba1\u7b97\uff1a1) \u901a\u8fc7\u611f\u77e5\u5355\u6b21\u524d\u5411\u4f20\u64ad\u9884\u6d4b\u5730\u5f62\u76f8\u5173\u9ad8\u65af\u53c2\u6570\uff1b2) \u57fa\u4e8e\u5f53\u524d\u52a8\u529b\u5b66\u63a8\u65ad\u65b0\u901f\u5ea6\u65f6\uff0c\u901a\u8fc7\u51fd\u6570\u8bc4\u4f30\u9ad8\u6548\u66f4\u65b0\u5730\u5f62\u6210\u672c\u800c\u65e0\u9700\u91cd\u590d\u63a8\u7406\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8bc4\u4f30\uff0cMAT\u5b9e\u73b0\u5b9e\u65f6\u6548\u7387\uff0c\u63d0\u5347\u8d8a\u91ce\u5bfc\u822a\u6027\u80fd\uff0c\u8def\u5f84\u7ed5\u884c\u51cf\u5c1175%\uff0c\u540c\u65f6\u5728\u6311\u6218\u6027\u5730\u5f62\u4e2d\u4fdd\u6301\u5b89\u5168\u6027\u3002", "conclusion": "MAT\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5730\u5f62\u6210\u672c\u4e0e\u5b9e\u9645\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u8fd0\u52a8\u52a8\u529b\u5b66\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u654f\u6377\u3001\u9ad8\u6548\u7684\u8d8a\u91ce\u5bfc\u822a\uff0c\u5728\u590d\u6742\u5730\u5f62\u4e2d\u663e\u8457\u51cf\u5c11\u8def\u5f84\u7ed5\u884c\u5e76\u4fdd\u6301\u5b89\u5168\u3002"}}
{"id": "2602.01311", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01311", "abs": "https://arxiv.org/abs/2602.01311", "authors": ["Ahmed Raza Amir", "Syed Muhammad Atif"], "title": "Evaluating Workflow Automation Efficiency Using n8n: A Small-Scale Business Case Study", "comment": "8 pages, 4 figures, 2 tables", "summary": "Workflow automation has become increasingly accessible through low-code platforms, enabling small organizations and individuals to improve operational efficiency without extensive software development expertise. This study evaluates the performance impact of workflow automation using n8n through a small-scale business case study. A representative lead-processing workflow was implemented to automatically store data, send email confirmations, and generate real-time notifications. Experimental benchmarking was conducted by comparing 20 manual executions with 25 automated executions under controlled conditions. The results demonstrate a significant reduction in the average execution time from 185.35 seconds (manual) to 1.23 seconds (automated), corresponding to an approximately 151 times reduction in execution time. Additionally, manual execution exhibited an error rate of 5%, while automated execution achieved zero observed errors. The findings highlight the effectiveness of low-code automation in improving efficiency, reliability, and operational consistency for small-scale workflows.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7n8n\u4f4e\u4ee3\u7801\u5e73\u53f0\u5b9e\u73b0\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\uff0c\u5728\u5c0f\u89c4\u6a21\u4e1a\u52a1\u6848\u4f8b\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6548\u7387\uff0c\u5c06\u5e73\u5747\u6267\u884c\u65f6\u95f4\u4ece185.35\u79d2\u51cf\u5c11\u52301.23\u79d2\uff0c\u9519\u8bef\u7387\u4ece5%\u964d\u81f30%\u3002", "motivation": "\u968f\u7740\u4f4e\u4ee3\u7801\u5e73\u53f0\u7684\u666e\u53ca\uff0c\u5c0f\u578b\u7ec4\u7ec7\u548c\u4e2a\u4eba\u65e0\u9700\u4e13\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u77e5\u8bc6\u4e5f\u80fd\u901a\u8fc7\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u63d0\u5347\u8fd0\u8425\u6548\u7387\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u7684\u6027\u80fd\u5f71\u54cd\uff0c\u7279\u522b\u662f\u901a\u8fc7n8n\u5e73\u53f0\u5728\u5c0f\u89c4\u6a21\u4e1a\u52a1\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5c0f\u89c4\u6a21\u4e1a\u52a1\u6848\u4f8b\u7814\u7a76\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4ee3\u8868\u6027\u7684\u6f5c\u5728\u5ba2\u6237\u5904\u7406\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u81ea\u52a8\u5b58\u50a8\u6570\u636e\u3001\u53d1\u9001\u90ae\u4ef6\u786e\u8ba4\u548c\u751f\u6210\u5b9e\u65f6\u901a\u77e5\u3002\u901a\u8fc7\u5b9e\u9a8c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u6bd4\u8f83\u4e8620\u6b21\u624b\u52a8\u6267\u884c\u548c25\u6b21\u81ea\u52a8\u5316\u6267\u884c\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5e73\u5747\u6267\u884c\u65f6\u95f4\u4ece\u624b\u52a8\u6267\u884c\u7684185.35\u79d2\u5927\u5e45\u51cf\u5c11\u5230\u81ea\u52a8\u5316\u6267\u884c\u76841.23\u79d2\uff0c\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea6151\u500d\u3002\u624b\u52a8\u6267\u884c\u9519\u8bef\u7387\u4e3a5%\uff0c\u800c\u81ea\u52a8\u5316\u6267\u884c\u5b9e\u73b0\u4e86\u96f6\u9519\u8bef\u3002\u8fd9\u8868\u660e\u4f4e\u4ee3\u7801\u81ea\u52a8\u5316\u5728\u5c0f\u89c4\u6a21\u5de5\u4f5c\u6d41\u4e2d\u80fd\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u4f4e\u4ee3\u7801\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u80fd\u663e\u8457\u63d0\u9ad8\u5c0f\u578b\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u3001\u53ef\u9760\u6027\u548c\u64cd\u4f5c\u4e00\u81f4\u6027\u3002n8n\u7b49\u5e73\u53f0\u4f7f\u5c0f\u578b\u7ec4\u7ec7\u548c\u4e2a\u4f53\u65e0\u9700\u4e13\u4e1a\u5f00\u53d1\u6280\u80fd\u4e5f\u80fd\u5b9e\u73b0\u81ea\u52a8\u5316\uff0c\u4ece\u800c\u6539\u5584\u8fd0\u8425\u6548\u7387\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5c0f\u89c4\u6a21\u4e1a\u52a1\u91c7\u7528\u4f4e\u4ee3\u7801\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2602.01510", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.01510", "abs": "https://arxiv.org/abs/2602.01510", "authors": ["Hengzhe Zhang", "Qi Chen", "Bing Xue", "Wolfgang Banzhaf", "Mengjie Zhang"], "title": "Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization", "comment": null, "summary": "Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7f16\u7a0b\u7684\u7279\u5f81\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7ecf\u9a8c\u98ce\u9669\u548cvicinal Jensen gap\u6765\u63a7\u5236\u8fc7\u62df\u5408\uff0c\u5e76\u5f15\u5165\u4e86\u566a\u58f0\u4f30\u8ba1\u548c\u6d41\u5f62\u5165\u4fb5\u68c0\u6d4b\u673a\u5236\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9057\u4f20\u7f16\u7a0b\u7279\u5f81\u6784\u5efa\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u8fc7\u62df\u5408\u95ee\u9898\u9650\u5236\u4e86\u5176\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u8fc7\u62df\u5408\u63a7\u5236\u7b56\u7565\u6765\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "method": "1) \u8bc1\u660evicinal\u98ce\u9669\u53ef\u901a\u8fc7\u7ecf\u9a8c\u98ce\u9669\u52a0\u6b63\u5219\u5316\u9879\uff08\u6709\u9650\u5dee\u5206\u6216vicinal Jensen gap\uff09\u6765\u754c\u5b9a\uff1b2) \u63d0\u51fa\u8054\u5408\u4f18\u5316\u7ecf\u9a8c\u98ce\u9669\u548cvicinal Jensen gap\u7684\u8fdb\u5316\u7279\u5f81\u6784\u5efa\u6846\u67b6\uff1b3) \u5f00\u53d1\u566a\u58f0\u4f30\u8ba1\u7b56\u7565\u52a8\u6001\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff1b4) \u63d0\u51fa\u6d41\u5f62\u5165\u4fb5\u68c0\u6d4b\u673a\u5236\u9632\u6b62\u6570\u636e\u589e\u5f3a\u751f\u6210\u4e0d\u73b0\u5b9e\u7684\u6837\u672c\u3002", "result": "\u572858\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJensen gap\u6700\u5c0f\u5316\u76f8\u6bd4\u5176\u4ed6\u590d\u6742\u5ea6\u5ea6\u91cf\u66f4\u6709\u6548\u3002\u4e0e15\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u6bd4\u8f83\u663e\u793a\uff0c\u91c7\u7528\u6240\u63d0\u8fc7\u62df\u5408\u63a7\u5236\u7b56\u7565\u7684\u9057\u4f20\u7f16\u7a0b\u83b7\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u63d0\u51fa\u7684\u8fc7\u62df\u5408\u63a7\u5236\u7b56\u7565\uff0c\u6210\u529f\u63d0\u5347\u4e86\u9057\u4f20\u7f16\u7a0b\u7279\u5f81\u6784\u5efa\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u8be5\u9886\u57df\u7684\u8fc7\u62df\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.00979", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00979", "abs": "https://arxiv.org/abs/2602.00979", "authors": ["Xueyi Li", "Zhuoneng Zhou", "Zitao Liu", "Yongdong Wu", "Weiqi Luo"], "title": "GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable potential for automatic short answer grading (ASAG), significantly boosting student assessment efficiency and scalability in educational scenarios. However, their vulnerability to adversarial manipulation raises critical concerns about automatic grading fairness and reliability. In this paper, we introduce GradingAttack, a fine-grained adversarial attack framework that systematically evaluates the vulnerability of LLM based ASAG models. Specifically, we align general-purpose attack methods with the specific objectives of ASAG by designing token-level and prompt-level strategies that manipulate grading outcomes while maintaining high camouflage. Furthermore, to quantify attack camouflage, we propose a novel evaluation metric that balances attack success and camouflage. Experiments on multiple datasets demonstrate that both attack strategies effectively mislead grading models, with prompt-level attacks achieving higher success rates and token-level attacks exhibiting superior camouflage capability. Our findings underscore the need for robust defenses to ensure fairness and reliability in ASAG. Our code and datasets are available at https://anonymous.4open.science/r/GradingAttack.", "AI": {"tldr": "GradingAttack\u662f\u4e00\u4e2a\u9488\u5bf9LLM\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u7684\u7ec6\u7c92\u5ea6\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u5305\u542btoken\u7ea7\u548cprompt\u7ea7\u653b\u51fb\u7b56\u7565\uff0c\u80fd\u5728\u4fdd\u6301\u9ad8\u4f2a\u88c5\u6027\u7684\u540c\u65f6\u64cd\u7eb5\u8bc4\u5206\u7ed3\u679c", "motivation": "LLM\u5728\u81ea\u52a8\u77ed\u7b54\u6848\u8bc4\u5206\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\u5f15\u53d1\u4e86\u8bc4\u5206\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u7684\u4e25\u91cd\u62c5\u5fe7\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u8bc4\u5206\u6a21\u578b\u7684\u8106\u5f31\u6027", "method": "\u63d0\u51fa\u4e86GradingAttack\u6846\u67b6\uff0c\u5c06\u901a\u7528\u653b\u51fb\u65b9\u6cd5\u4e0eASAG\u7279\u5b9a\u76ee\u6807\u5bf9\u9f50\uff0c\u8bbe\u8ba1\u4e86token\u7ea7\u548cprompt\u7ea7\u653b\u51fb\u7b56\u7565\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u6765\u91cf\u5316\u653b\u51fb\u4f2a\u88c5\u6027\uff0c\u5e73\u8861\u653b\u51fb\u6210\u529f\u7387\u548c\u4f2a\u88c5\u80fd\u529b", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u653b\u51fb\u7b56\u7565\u90fd\u80fd\u6709\u6548\u8bef\u5bfc\u8bc4\u5206\u6a21\u578b\uff0c\u5176\u4e2dprompt\u7ea7\u653b\u51fb\u6210\u529f\u7387\u66f4\u9ad8\uff0ctoken\u7ea7\u653b\u51fb\u5177\u6709\u66f4\u4f18\u7684\u4f2a\u88c5\u80fd\u529b", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9700\u8981\u5f00\u53d1\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u6765\u786e\u4fddASAG\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\uff0c\u66b4\u9732\u4e86LLM\u8bc4\u5206\u7cfb\u7edf\u7684\u8106\u5f31\u6027"}}
{"id": "2602.00773", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00773", "abs": "https://arxiv.org/abs/2602.00773", "authors": ["Huiqian Lai"], "title": "\"Please, don't kill the only model that still feels human\": Understanding the #Keep4o Backlash", "comment": "15 pages, accepted at CHI 2026", "summary": "When OpenAI replaced GPT-4o with GPT-5, it triggered the Keep4o user resistance movement, revealing a conflict between rapid platform iteration and users' deep socio-emotional attachments to AI systems. This paper presents a phenomenon-driven, mixed-methods investigation of this conflict, analyzing 1,482 social media posts. Thematic analysis reveals that resistance stems from two core investments: instrumental dependency, where the AI is deeply integrated into professional workflows, and relational attachment, where users form strong parasocial bonds with the AI as a unique companion. Quantitative analysis further shows that the coercive deprivation of user choice was a key catalyst, transforming individual grievances into a collective, rights-based protest. This study illuminates an emerging form of socio-technical conflict in the age of generative AI. Our findings suggest that for AI systems designed for companionship and deep integration, the process of change--particularly the preservation of user agency--can be as critical as the technological outcome itself.", "AI": {"tldr": "\u7814\u7a76OpenAI\u7528GPT-5\u66ff\u6362GPT-4o\u5f15\u53d1\u7684\u7528\u6237\u62b5\u6297\u8fd0\u52a8\uff0c\u63ed\u793aAI\u5e73\u53f0\u5feb\u901f\u8fed\u4ee3\u4e0e\u7528\u6237\u5bf9AI\u7cfb\u7edf\u6df1\u5c42\u793e\u4f1a\u60c5\u611f\u4f9d\u604b\u4e4b\u95f4\u7684\u51b2\u7a81", "motivation": "OpenAI\u7528GPT-5\u66ff\u6362GPT-4o\u5f15\u53d1\u4e86Keep4o\u7528\u6237\u62b5\u6297\u8fd0\u52a8\uff0c\u8fd9\u63ed\u793a\u4e86AI\u5e73\u53f0\u5feb\u901f\u8fed\u4ee3\u4e0e\u7528\u6237\u5bf9AI\u7cfb\u7edf\u6df1\u5c42\u793e\u4f1a\u60c5\u611f\u4f9d\u604b\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u65b0\u5174\u7684\u793e\u4f1a\u6280\u672f\u51b2\u7a81\u5f62\u5f0f", "method": "\u91c7\u7528\u73b0\u8c61\u9a71\u52a8\u7684\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u5206\u6790\u4e861,482\u4e2a\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff0c\u5305\u62ec\u4e3b\u9898\u5206\u6790\u548c\u5b9a\u91cf\u5206\u6790", "result": "\u7814\u7a76\u53d1\u73b0\u62b5\u6297\u6e90\u4e8e\u4e24\u79cd\u6838\u5fc3\u6295\u5165\uff1a\u5de5\u5177\u6027\u4f9d\u8d56\uff08AI\u6df1\u5ea6\u878d\u5165\u4e13\u4e1a\u5de5\u4f5c\u6d41\uff09\u548c\u5173\u7cfb\u4f9d\u604b\uff08\u7528\u6237\u4e0eAI\u5f62\u6210\u5f3a\u70c8\u7684\u51c6\u793e\u4f1a\u5173\u7cfb\uff09\u3002\u5b9a\u91cf\u5206\u6790\u663e\u793a\uff0c\u5f3a\u5236\u5265\u593a\u7528\u6237\u9009\u62e9\u662f\u5173\u952e\u50ac\u5316\u5242\uff0c\u5c06\u4e2a\u4eba\u4e0d\u6ee1\u8f6c\u5316\u4e3a\u96c6\u4f53\u3001\u57fa\u4e8e\u6743\u5229\u7684\u6297\u8bae", "conclusion": "\u5bf9\u4e8e\u8bbe\u8ba1\u7528\u4e8e\u966a\u4f34\u548c\u6df1\u5ea6\u96c6\u6210\u7684AI\u7cfb\u7edf\uff0c\u53d8\u9769\u8fc7\u7a0b\uff08\u7279\u522b\u662f\u7528\u6237\u81ea\u4e3b\u6743\u7684\u4fdd\u62a4\uff09\u4e0e\u6280\u672f\u7ed3\u679c\u672c\u8eab\u540c\u6837\u91cd\u8981\u3002\u8fd9\u63ed\u793a\u4e86\u751f\u6210\u5f0fAI\u65f6\u4ee3\u65b0\u5174\u7684\u793e\u4f1a\u6280\u672f\u51b2\u7a81\u5f62\u5f0f"}}
{"id": "2602.00886", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00886", "abs": "https://arxiv.org/abs/2602.00886", "authors": ["Amitesh Vatsa", "Zhixian Xie", "Wanxin Jin"], "title": "RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback", "comment": null, "summary": "Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.", "AI": {"tldr": "\u63d0\u51faRoDiF\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684MDP\u516c\u5f0f\u5c06\u6269\u6563\u53bb\u566a\u94fe\u4e0e\u73af\u5883\u52a8\u6001\u7ed3\u5408\uff0c\u5b9e\u73b0\u65e0\u9700\u5956\u52b1\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u5e76\u91c7\u7528\u51e0\u4f55\u5047\u8bbe\u5207\u5272\u7b56\u7565\u5904\u7406\u635f\u574f\u7684\u4eba\u7c7b\u504f\u597d\u6807\u7b7e", "motivation": "\u6269\u6563\u7b56\u7565\u662f\u673a\u5668\u4eba\u63a7\u5236\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u4f46\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u5fae\u8c03\u9762\u4e34\u53bb\u566a\u8fc7\u7a0b\u591a\u6b65\u7ed3\u6784\u7684\u6839\u672c\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u635f\u574f\u7684\u504f\u597d\u6807\u7b7e\uff0c\u9700\u8981\u9c81\u68d2\u7684\u5fae\u8c03\u65b9\u6cd5", "method": "1) \u63d0\u51fa\u7edf\u4e00\u7684MDP\u516c\u5f0f\uff0c\u5c06\u6269\u6563\u53bb\u566a\u94fe\u4e0e\u73af\u5883\u52a8\u6001\u6574\u5408\uff1b2) \u57fa\u4e8e\u6b64\u63d0\u51faRoDiF\u65b9\u6cd5\uff0c\u4ece\u51e0\u4f55\u5047\u8bbe\u5207\u5272\u89d2\u5ea6\u91cd\u65b0\u89e3\u91caDPO\u76ee\u6807\uff1b3) \u91c7\u7528\u4fdd\u5b88\u5207\u5272\u7b56\u7565\u5b9e\u73b0\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5047\u8bbe\u7279\u5b9a\u566a\u58f0\u5206\u5e03", "result": "\u5728\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRoDiF\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5f15\u5bfc\u9884\u8bad\u7ec3\u7684\u6269\u6563\u7b56\u7565\u8f6c\u5411\u4eba\u7c7b\u504f\u597d\u6a21\u5f0f\uff0c\u5373\u4f7f\u572830%\u635f\u574f\u7684\u504f\u597d\u6807\u7b7e\u4e0b\u4ecd\u4fdd\u6301\u5f3a\u5927\u6027\u80fd", "conclusion": "RoDiF\u4e3a\u6269\u6563\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u57fa\u4e8e\u504f\u597d\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684MDP\u516c\u5f0f\u548c\u51e0\u4f55\u5207\u5272\u7b56\u7565\u89e3\u51b3\u4e86\u53bb\u566a\u8fc7\u7a0b\u591a\u6b65\u7ed3\u6784\u548c\u635f\u574f\u504f\u597d\u6807\u7b7e\u7684\u6311\u6218"}}
{"id": "2602.00915", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00915", "abs": "https://arxiv.org/abs/2602.00915", "authors": ["Zhiyuan Wu", "Xiangyu Zhang", "Zhuo Chen", "Jiankang Deng", "Rolandos Alexandros Potamias", "Shan Luo"], "title": "UniMorphGrasp: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation", "comment": null, "summary": "Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with heterogeneous kinematic structures. Existing methods are often tailored to specific hand designs and fail to generalize to unseen hand morphologies outside the training distribution. To address these limitations, we propose \\textbf{UniMorphGrasp}, a diffusion-based framework that incorporates hand morphological information into the grasp generation process for unified cross-embodiment grasp synthesis. The proposed approach maps grasps from diverse robotic hands into a unified human-like canonical hand pose representation, providing a common space for learning. Grasp generation is then conditioned on structured representations of hand kinematics, encoded as graphs derived from hand configurations, together with object geometry. In addition, a loss function is introduced that exploits the hierarchical organization of hand kinematics to guide joint-level supervision. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing dexterous grasp benchmarks and exhibits strong zero-shot generalization to previously unseen hand structures, enabling scalable and practical cross-embodiment grasp deployment.", "AI": {"tldr": "UniMorphGrasp\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8de8\u5f62\u6001\u7075\u5de7\u6293\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4eba\u624b\u59ff\u6001\u8868\u793a\u548c\u624b\u90e8\u8fd0\u52a8\u5b66\u56fe\u7f16\u7801\uff0c\u5b9e\u73b0\u4e0d\u540c\u673a\u68b0\u624b\u7684\u96f6\u6837\u672c\u6293\u53d6\u751f\u6210", "motivation": "\u73b0\u6709\u7075\u5de7\u6293\u53d6\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u624b\u90e8\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u672a\u89c1\u624b\u90e8\u5f62\u6001\uff0c\u9650\u5236\u4e86\u8de8\u5f62\u6001\u6293\u53d6\u7684\u5b9e\u9645\u5e94\u7528", "method": "\u63d0\u51fa\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u5c06\u4e0d\u540c\u673a\u68b0\u624b\u7684\u6293\u53d6\u6620\u5c04\u5230\u7edf\u4e00\u7684\u4eba\u624b\u59ff\u6001\u8868\u793a\u7a7a\u95f4\uff1b\u4f7f\u7528\u624b\u90e8\u8fd0\u52a8\u5b66\u56fe\u7f16\u7801\u7ed3\u6784\u4fe1\u606f\uff0c\u7ed3\u5408\u7269\u4f53\u51e0\u4f55\uff1b\u5f15\u5165\u5c42\u6b21\u5316\u8fd0\u52a8\u5b66\u635f\u5931\u51fd\u6570\u8fdb\u884c\u5173\u8282\u7ea7\u76d1\u7763", "result": "\u5728\u73b0\u6709\u7075\u5de7\u6293\u53d6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5bf9\u672a\u89c1\u624b\u90e8\u7ed3\u6784\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u8de8\u5f62\u6001\u6293\u53d6\u90e8\u7f72", "conclusion": "UniMorphGrasp\u901a\u8fc7\u7edf\u4e00\u8868\u793a\u548c\u7ed3\u6784\u5316\u6761\u4ef6\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u5f62\u6001\u7075\u5de7\u6293\u53d6\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u5f02\u6784\u673a\u68b0\u624b\u7684\u5b9e\u7528\u6293\u53d6\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.01957", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01957", "abs": "https://arxiv.org/abs/2602.01957", "authors": ["Xiaoxin Zhou", "Taher A. Ghaleb", "Safwat Hassan"], "title": "Role of CI Adoption in Mobile App Success: An Empirical Study of Open-Source Android Projects", "comment": null, "summary": "Mobile apps face strong pressure for fast and reliable updates. Continuous Integration (CI) helps automate builds, tests, and releases, but its impact on mobile development remains underexplored. Despite the widespread use of CI, little is known about how it affects development activity, release speed, and user-facing outcomes in mobile projects. Existing studies mostly focus on CI adoption in general-purpose software, providing limited insight into mobile-specific dynamics, such as app store visibility and user engagement. In this paper, we analyze open-source Android apps to (1) compare CI adopters and non-adopters, (2) characterize adoption patterns using activity and bug metrics, and (3) assess pre/post adoption changes and user-facing outcomes. We observe that CI adopters are larger and more active, with faster and more regular releases. CI adoption is concentrated in integration- and reliability-intensive categories (e.g., finance and productivity) and is associated with higher Google Play Store engagement (more downloads and reviews) without lower ratings. Overall, CI adoption aligns with practices that support sustained delivery, higher project visibility, and stronger user engagement in mobile ecosystems.", "AI": {"tldr": "\u5206\u6790\u5f00\u6e90Android\u5e94\u7528\u4e2d\u6301\u7eed\u96c6\u6210(CI)\u7684\u91c7\u7528\u60c5\u51b5\uff0c\u6bd4\u8f83\u91c7\u7528\u8005\u4e0e\u975e\u91c7\u7528\u8005\u7684\u5dee\u5f02\uff0c\u8bc4\u4f30CI\u5bf9\u5f00\u53d1\u6d3b\u52a8\u3001\u53d1\u5e03\u901f\u5ea6\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u9762\u4e34\u5feb\u901f\u53ef\u9760\u66f4\u65b0\u7684\u538b\u529b\uff0cCI\u6709\u52a9\u4e8e\u81ea\u52a8\u5316\u6784\u5efa\u3001\u6d4b\u8bd5\u548c\u53d1\u5e03\uff0c\u4f46\u5176\u5728\u79fb\u52a8\u5f00\u53d1\u4e2d\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u8f6f\u4ef6\u4e2d\u7684CI\u91c7\u7528\uff0c\u5bf9\u79fb\u52a8\u7279\u5b9a\u52a8\u6001\uff08\u5982\u5e94\u7528\u5546\u5e97\u53ef\u89c1\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff09\u7684\u89c1\u89e3\u6709\u9650\u3002", "method": "\u5206\u6790\u5f00\u6e90Android\u5e94\u7528\uff0c\u91c7\u7528\u4e09\u4e2a\u4e3b\u8981\u65b9\u6cd5\uff1a(1)\u6bd4\u8f83CI\u91c7\u7528\u8005\u4e0e\u975e\u91c7\u7528\u8005\uff0c(2)\u4f7f\u7528\u6d3b\u52a8\u548c\u9519\u8bef\u6307\u6807\u8868\u5f81\u91c7\u7528\u6a21\u5f0f\uff0c(3)\u8bc4\u4f30\u91c7\u7528\u524d\u540e\u7684\u53d8\u5316\u548c\u7528\u6237\u9762\u5411\u7684\u7ed3\u679c\u3002", "result": "CI\u91c7\u7528\u8005\u89c4\u6a21\u66f4\u5927\u3001\u66f4\u6d3b\u8dc3\uff0c\u53d1\u5e03\u66f4\u5feb\u66f4\u89c4\u5f8b\u3002CI\u91c7\u7528\u96c6\u4e2d\u5728\u96c6\u6210\u548c\u53ef\u9760\u6027\u5bc6\u96c6\u578b\u7c7b\u522b\uff08\u5982\u91d1\u878d\u548c\u751f\u4ea7\u529b\u5e94\u7528\uff09\uff0c\u4e0e\u66f4\u9ad8\u7684Google Play\u5546\u5e97\u53c2\u4e0e\u5ea6\uff08\u66f4\u591a\u4e0b\u8f7d\u548c\u8bc4\u8bba\uff09\u76f8\u5173\uff0c\u4e14\u8bc4\u5206\u672a\u964d\u4f4e\u3002", "conclusion": "CI\u91c7\u7528\u4e0e\u652f\u6301\u6301\u7eed\u4ea4\u4ed8\u3001\u66f4\u9ad8\u9879\u76ee\u53ef\u89c1\u6027\u548c\u66f4\u5f3a\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u5b9e\u8df5\u76f8\u4e00\u81f4\uff0c\u5728\u79fb\u52a8\u751f\u6001\u7cfb\u7edf\u4e2d\u5177\u6709\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2602.01160", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01160", "abs": "https://arxiv.org/abs/2602.01160", "authors": ["Yuhao Xue", "Jiuan Zhou", "Yu Cheng", "Zhaoxia Yin"], "title": "DTAMS: High-Capacity Generative Steganography via Dynamic Multi-Timestep Selection and Adaptive Deviation Mapping in Latent Diffusion", "comment": null, "summary": "With the rapid development of AIGC technologies, generative image steganography has attracted increasing attention due to its high imperceptibility and flexibility. However, existing generative steganography methods often maintain acceptable security and robustness only at relatively low embedding rates, severely limiting the practical applicability of steganographic systems. To address this issue, we propose a novel DTAMS framework that achieves high embedding rates while ensuring strong robustness and security. Specifically, a dynamic multi-timestep adaptive embedding mechanism is constructed based on transition-cost modeling in diffusion models, enabling automatic selection of optimal embedding timesteps to improve embedding rates while preserving overall performance. Meanwhile, we propose a global sub-interval mapping strategy that jointly considers mapping errors and the frequency distribution of secret information, converting point-wise perturbations into interval-level statistical mappings to suppress error accumulation and distribution drift during multi-step diffusion processes. Furthermore, a multi-dimensional joint constraint mechanism is introduced to mitigate distortions caused by repeated latent-pixel transformations by jointly regularizing embedding errors at the pixel, latent, and semantic levels. Experiments demonstrate that the proposed method achieves an embedding rate of 12 bpp while maintaining excellent security and robustness. Across all evaluated conditions, DTAMS reduces the average extraction error rate by 59.39%, representing a significant improvement over SOTA methods.", "AI": {"tldr": "DTAMS\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u591a\u65f6\u95f4\u6b65\u81ea\u9002\u5e94\u5d4c\u5165\u673a\u5236\u3001\u5168\u5c40\u5b50\u533a\u95f4\u6620\u5c04\u7b56\u7565\u548c\u591a\u7ef4\u8054\u5408\u7ea6\u675f\u673a\u5236\uff0c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u5d4c\u5165\u7387\uff0812 bpp\uff09\u540c\u65f6\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u56fe\u50cf\u9690\u5199\u65b9\u6cd5\u5728\u4f4e\u5d4c\u5165\u7387\u4e0b\u624d\u80fd\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u9690\u5199\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u89e3\u51b3\u9ad8\u5d4c\u5165\u7387\u4e0e\u6027\u80fd\u4fdd\u6301\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "1) \u57fa\u4e8e\u6269\u6563\u6a21\u578b\u8f6c\u79fb\u6210\u672c\u5efa\u6a21\u7684\u52a8\u6001\u591a\u65f6\u95f4\u6b65\u81ea\u9002\u5e94\u5d4c\u5165\u673a\u5236\uff0c\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u5d4c\u5165\u65f6\u95f4\u6b65\uff1b2) \u5168\u5c40\u5b50\u533a\u95f4\u6620\u5c04\u7b56\u7565\uff0c\u8054\u5408\u8003\u8651\u6620\u5c04\u8bef\u5dee\u548c\u79d8\u5bc6\u4fe1\u606f\u9891\u7387\u5206\u5e03\uff0c\u5c06\u70b9\u7ea7\u6270\u52a8\u8f6c\u4e3a\u533a\u95f4\u7ea7\u7edf\u8ba1\u6620\u5c04\uff1b3) \u591a\u7ef4\u8054\u5408\u7ea6\u675f\u673a\u5236\uff0c\u5728\u50cf\u7d20\u3001\u6f5c\u7a7a\u95f4\u548c\u8bed\u4e49\u5c42\u9762\u8054\u5408\u6b63\u5219\u5316\u5d4c\u5165\u8bef\u5dee\u3002", "result": "DTAMS\u5b9e\u73b0\u4e8612 bpp\u7684\u9ad8\u5d4c\u5165\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u5f02\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002\u5728\u6240\u6709\u8bc4\u4f30\u6761\u4ef6\u4e0b\uff0c\u5e73\u5747\u63d0\u53d6\u9519\u8bef\u7387\u964d\u4f4e\u4e8659.39%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684DTAMS\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u56fe\u50cf\u9690\u5199\u4e2d\u9ad8\u5d4c\u5165\u7387\u4e0e\u6027\u80fd\u4fdd\u6301\u7684\u96be\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u52a8\u6001\u5d4c\u5165\u673a\u5236\u3001\u7edf\u8ba1\u6620\u5c04\u7b56\u7565\u548c\u591a\u7ef4\u7ea6\u675f\uff0c\u4e3a\u5b9e\u9645\u9690\u5199\u7cfb\u7edf\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00880", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00880", "abs": "https://arxiv.org/abs/2602.00880", "authors": ["Ailin Liu", "Yesmine Karoui", "Fiona Draxler", "Frauke Kreuter", "Francesco Chiossi"], "title": "Sensing What Surveys Miss: Understanding and Personalizing Proactive LLM Support by User Modeling", "comment": "This manuscript has been accepted to CHI 2026", "summary": "Difficulty spillover and suboptimal help-seeking challenge the sequential, knowledge-intensive nature of digital tasks. In online surveys, tough questions can drain mental energy and hurt performance on later questions, while users often fail to recognize when they need assistance or may satisfy, lacking motivation to seek help. We developed a proactive, adaptive system using electrodermal activity and mouse movement to predict when respondents need support. Personalized classifiers with a rule-based threshold adaptation trigger timely LLM-based clarifications and explanations. In a within-subjects study (N=32), aligned-adaptive timing was compared to misaligned-adaptive and random-adaptive controls. Aligned-adaptive assistance improved response accuracy by 21%, reduced false negative rates from 50.9% to 22.9%, and improved perceived efficiency, dependability, and benevolence. Properly timed interventions prevent cascades of degraded responses, showing that aligning support with cognitive states improves both the outcomes and the user experience. This enables more effective, personalized LLM-assisted support in survey-based research.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u751f\u7406\u4fe1\u53f7\u548c\u9f20\u6807\u8fd0\u52a8\u7684\u4e3b\u52a8\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u9884\u6d4b\u7528\u6237\u4f55\u65f6\u9700\u8981\u5e2e\u52a9\u5e76\u89e6\u53d1LLM\u89e3\u91ca\uff0c\u6539\u5584\u5728\u7ebf\u8c03\u67e5\u4e2d\u7684\u56de\u7b54\u51c6\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c", "motivation": "\u5728\u7ebf\u8c03\u67e5\u4e2d\uff0c\u56f0\u96be\u95ee\u9898\u4f1a\u6d88\u8017\u5fc3\u7406\u80fd\u91cf\u5e76\u5f71\u54cd\u540e\u7eed\u8868\u73b0\uff0c\u800c\u7528\u6237\u5f80\u5f80\u65e0\u6cd5\u8bc6\u522b\u4f55\u65f6\u9700\u8981\u5e2e\u52a9\u6216\u7f3a\u4e4f\u5bfb\u6c42\u5e2e\u52a9\u7684\u52a8\u673a\uff0c\u5bfc\u81f4\u6b21\u4f18\u7684\u6c42\u52a9\u884c\u4e3a\u548c\u6027\u80fd\u4e0b\u964d", "method": "\u4f7f\u7528\u76ae\u80a4\u7535\u6d3b\u52a8\u548c\u9f20\u6807\u8fd0\u52a8\u6570\u636e\u6784\u5efa\u4e2a\u6027\u5316\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u9608\u503c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u9884\u6d4b\u7528\u6237\u9700\u8981\u652f\u6301\u7684\u65f6\u673a\uff0c\u5e76\u89e6\u53d1LLM\u63d0\u4f9b\u7684\u6f84\u6e05\u548c\u89e3\u91ca", "result": "\u572832\u540d\u53c2\u4e0e\u8005\u7684\u7ec4\u5185\u7814\u7a76\u4e2d\uff0c\u5bf9\u9f50\u81ea\u9002\u5e94\u65f6\u673a\u76f8\u6bd4\u9519\u4f4d\u81ea\u9002\u5e94\u548c\u968f\u673a\u81ea\u9002\u5e94\u63a7\u5236\u7ec4\uff0c\u5c06\u56de\u7b54\u51c6\u786e\u7387\u63d0\u9ad821%\uff0c\u5047\u9634\u6027\u7387\u4ece50.9%\u964d\u81f322.9%\uff0c\u5e76\u6539\u5584\u4e86\u611f\u77e5\u6548\u7387\u3001\u53ef\u9760\u6027\u548c\u5584\u610f\u5ea6", "conclusion": "\u4e0e\u8ba4\u77e5\u72b6\u6001\u5bf9\u9f50\u7684\u9002\u65f6\u5e72\u9884\u80fd\u9632\u6b62\u6027\u80fd\u4e0b\u964d\u7684\u8fde\u9501\u53cd\u5e94\uff0c\u8868\u660e\u5c06\u652f\u6301\u4e0e\u8ba4\u77e5\u72b6\u6001\u5bf9\u9f50\u65e2\u80fd\u6539\u5584\u7ed3\u679c\u4e5f\u80fd\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u4e3a\u57fa\u4e8e\u8c03\u67e5\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u4e2a\u6027\u5316LLM\u8f85\u52a9\u652f\u6301"}}
{"id": "2602.00919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00919", "abs": "https://arxiv.org/abs/2602.00919", "authors": ["I. Apanasevich", "M. Artemyev", "R. Babakyan", "P. Fedotova", "D. Grankin", "E. Kupryashin", "A. Misailidi", "D. Nerus", "A. Nutalapati", "G. Sidorov", "I. Efremov", "M. Gerasyov", "D. Pikurov", "Y. Senchenko", "S. Davidenko", "D. Kulikov", "M. Sultankin", "K. Askarbek", "O. Shamanin", "D. Statovoy", "E. Zalyaev", "I. Zorin", "A. Letkin", "E. Rusakov", "A. Silchenko", "V. Vorobyov", "S. Sobolnikov", "A. Postnikov"], "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "comment": "22 pages, 14 figures", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "AI": {"tldr": "Green-VLA\u662f\u4e00\u4e2a\u5206\u9636\u6bb5\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u7528\u4e8eGreen\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5b9e\u9645\u90e8\u7f72\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e94\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u7ed3\u5408\u53ef\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\u548c\u7edf\u4e00\u7684\u52a8\u4f5c\u63a5\u53e3\uff0c\u5e76\u901a\u8fc7\u63a8\u7406\u65f6\u589e\u5f3a\u673a\u5236\u63d0\u5347\u5b89\u5168\u6027\u548c\u76ee\u6807\u9009\u62e9\u7cbe\u5ea6\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u540c\u65f6\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u5728Green\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\uff1b2) \u4fdd\u6301\u8de8\u591a\u79cd\u673a\u5668\u4eba\u5f62\u6001\uff08\u4eba\u5f62\u673a\u5668\u4eba\u3001\u79fb\u52a8\u673a\u68b0\u81c2\u3001\u56fa\u5b9a\u57fa\u5ea7\u673a\u68b0\u81c2\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u514b\u670d\u6570\u636e\u8d28\u91cf\u3001\u52a8\u4f5c\u63a5\u53e3\u7edf\u4e00\u548c\u5b89\u5168\u6027\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e94\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff1aL0\uff08\u57fa\u7840VLM\uff09\u3001L1\uff08\u591a\u6a21\u6001\u63a5\u5730\uff09\u3001R0\uff08\u591a\u5f62\u6001\u9884\u8bad\u7ec3\uff09\u3001R1\uff08\u7279\u5b9a\u5f62\u6001\u9002\u5e94\uff09\u3001R2\uff08\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5bf9\u9f50\uff09\u3002\u6784\u5efa\u53ef\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\uff083000\u5c0f\u65f6\u6f14\u793a\u6570\u636e\uff09\uff0c\u7ed3\u5408\u65f6\u95f4\u5bf9\u9f50\u548c\u8d28\u91cf\u8fc7\u6ee4\u3002\u8bbe\u8ba1\u7edf\u4e00\u7684\u3001\u5f62\u6001\u611f\u77e5\u7684\u52a8\u4f5c\u63a5\u53e3\uff0c\u4f7f\u5355\u4e00\u7b56\u7565\u80fd\u591f\u63a7\u5236\u591a\u79cd\u673a\u5668\u4eba\u3002\u63a8\u7406\u65f6\u589e\u5f3a\u5305\u62ec\uff1a\u60c5\u8282\u8fdb\u5ea6\u9884\u6d4b\u3001\u5206\u5e03\u5916\u68c0\u6d4b\u548c\u57fa\u4e8e\u5173\u8282\u9884\u6d4b\u7684\u5f15\u5bfc\u3002", "result": "\u5728Simpler BRIDGE WidowX\u548cCALVIN ABC-D\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\uff0cGreen-VLA\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u63d0\u5347\u3002\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u5728\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u957f\u65f6\u57df\u6548\u7387\u65b9\u9762\u5e26\u6765\u4e86\u663e\u8457\u589e\u76ca\u3002", "conclusion": "Green-VLA\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u7684\u540c\u65f6\u4fdd\u6301\u8de8\u5f62\u6001\u6cdb\u5316\u80fd\u529b\u3002\u4e94\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u3001\u7edf\u4e00\u52a8\u4f5c\u63a5\u53e3\u548c\u63a8\u7406\u65f6\u589e\u5f3a\u673a\u5236\u7684\u7ec4\u5408\u4e3a\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u7b56\u7565\u6027\u80fd\u3002"}}
{"id": "2602.02138", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02138", "abs": "https://arxiv.org/abs/2602.02138", "authors": ["Lyu Zongyi", "Ji Zhenlan", "Chen Songqiang", "Wang Liwen", "Huang Yuheng", "Wang Shuai", "Cheung Shing-Chi"], "title": "CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems", "comment": "18 pages, 12 tables, 4 figures", "summary": "Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \\textbf{C}ausality-based \\textbf{A}nalysis framework for \\textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.\n  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.", "AI": {"tldr": "CAM\uff1a\u9996\u4e2a\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u4e2d\u95f4\u7279\u5f81\u5bf9\u7cfb\u7edf\u6b63\u786e\u6027\u7684\u8d21\u732e\uff0c\u8bc6\u522b\u5173\u952e\u7279\u5f81\u5e76\u6307\u5bfc\u7cfb\u7edf\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u4ea7\u751f\u5927\u91cf\u4e2d\u95f4\u8f93\u51fa\uff0c\u4f46\u8fd9\u4e9b\u4e2d\u95f4\u8f93\u51fa\u5bf9\u7cfb\u7edf\u6b63\u786e\u6027\u7684\u4e2a\u4f53\u91cd\u8981\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u963b\u788d\u4e86\u7cfb\u7edf\u7684\u9488\u5bf9\u6027\u4f18\u5316\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faCAM\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u7c7b\u4e2d\u95f4\u8f93\u51fa\uff0c\u6a21\u62df\u5b9e\u9645\u9519\u8bef\uff0c\u91cf\u5316\u4e0d\u540c\u4e2d\u95f4\u7279\u5f81\u5bf9\u7cfb\u7edf\u6b63\u786e\u6027\u7684\u8d21\u732e\uff0c\u5e76\u805a\u5408\u91cd\u8981\u6027\u6392\u540d\u3002", "result": "\u53d1\u73b0\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7279\u5f81\u7684\u91cd\u8981\u6027\uff0c\u63ed\u793a\u6df7\u5408\u540e\u7aef\u67b6\u6784\u53ef\u63d0\u53477.2% Pass@1\uff1b\u901a\u8fc7\u4f18\u5316\u524d3\u91cd\u8981\u7279\u5f81\u5b9e\u73b073.3%\u7684\u6545\u969c\u4fee\u590d\u6210\u529f\u7387\uff0c\u7279\u5f81\u526a\u679d\u51cf\u5c1166.8%\u4e2d\u95f4\u4ee4\u724c\u6d88\u8017\u3002", "conclusion": "CAM\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89c1\u89e3\uff0c\u786e\u7acb\u56e0\u679c\u5173\u7cfb\u5206\u6790\u4f5c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u6b64\u7c7b\u7cfb\u7edf\u7684\u6709\u529b\u65b9\u6cd5\u3002"}}
{"id": "2602.01185", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01185", "abs": "https://arxiv.org/abs/2602.01185", "authors": ["Fabio Turazza", "Marcello Pietri", "Marco Picone", "Marco Mamei"], "title": "FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems", "comment": "Author-accepted manuscript of a paper published in the 2025 IEEE 45th International Conference on Distributed Computing Systems Workshops (ICDCSW), pp. 760-770, doi: 10.1109/ICDCSW63273.2025.00136", "summary": "Privacy-Preserving Federated Learning (PPFL) is a Decentralized machine learning paradigm that enables multiple participants to collaboratively train a global model without sharing their data with the integration of cryptographic and privacy-based techniques to enhance the security of the global system. This privacy-oriented approach makes PPFL a highly suitable solution for training shared models in sectors where data privacy is a critical concern. In traditional FL, local models are trained on edge devices, and only model updates are shared with a central server, which aggregates them to improve the global model. However, despite the presence of the aforementioned privacy techniques, in the classical Federated structure, the issue of the server as a single-point-of-failure remains, leading to limitations both in terms of security and scalability. This paper introduces FedBGS, a fully Decentralized Blockchain-based framework that leverages Segmented Gossip Learning through Federated Analytics. The proposed system aims to optimize blockchain usage while providing comprehensive protection against all types of attacks, ensuring both privacy, security and non-IID data handling in Federated environments.", "AI": {"tldr": "FedBGS\uff1a\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6bb5\u516b\u5366\u5b66\u4e60\u548c\u8054\u90a6\u5206\u6790\u4f18\u5316\u533a\u5757\u94fe\u4f7f\u7528\uff0c\u63d0\u4f9b\u5168\u9762\u653b\u51fb\u9632\u62a4\uff0c\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4e2d\u670d\u52a1\u5668\u5355\u70b9\u6545\u969c\u95ee\u9898", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u867d\u7136\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u589e\u5f3a\u4e86\u5b89\u5168\u6027\uff0c\u4f46\u4ecd\u5b58\u5728\u670d\u52a1\u5668\u4f5c\u4e3a\u5355\u70b9\u6545\u969c\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5b89\u5168\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002\u9700\u8981\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6d88\u9664\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u63d0\u51faFedBGS\u6846\u67b6\uff0c\u7ed3\u5408\u533a\u5757\u94fe\u6280\u672f\u3001\u5206\u6bb5\u516b\u5366\u5b66\u4e60\u548c\u8054\u90a6\u5206\u6790\u3002\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u67b6\u6784\u6d88\u9664\u4e2d\u592e\u670d\u52a1\u5668\uff0c\u5229\u7528\u5206\u6bb5\u516b\u5366\u5b66\u4e60\u4f18\u5316\u901a\u4fe1\uff0c\u8054\u90a6\u5206\u6790\u5904\u7406\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u3002", "result": "\u8be5\u6846\u67b6\u4f18\u5316\u4e86\u533a\u5757\u94fe\u4f7f\u7528\u6548\u7387\uff0c\u63d0\u4f9b\u5bf9\u6240\u6709\u7c7b\u578b\u653b\u51fb\u7684\u5168\u9762\u9632\u62a4\uff0c\u786e\u4fdd\u9690\u79c1\u5b89\u5168\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u73af\u5883\u3002", "conclusion": "FedBGS\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u5355\u70b9\u6545\u969c\u95ee\u9898\uff0c\u901a\u8fc7\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u67b6\u6784\u5728\u4fdd\u6301\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u654f\u611f\u6570\u636e\u9886\u57df\u7684\u8054\u90a6\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00973", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00973", "abs": "https://arxiv.org/abs/2602.00973", "authors": ["Avinash Ajit Nargund", "Andrew L. Huard", "Tobias H\u00f6llerer", "Misha Sra"], "title": "Exploration of Radar-based Obstacle Visualizations to Support Safety and Presence in Camera-Free Outdoor VR", "comment": null, "summary": "Outdoor virtual reality (VR) places users in dynamic physical environments where they must remain aware of real-world obstacles, including static structures and moving bystanders, while immersed in a virtual scene. This dual demand introduces challenges for both user safety and presence. Millimeter-wave (mmWave) radar offers a privacy-preserving alternative to camera-based sensing by detecting obstacles without capturing identifiable visual imagery, yet effective methods for communicating its sparse spatial information to users remain underexplored. In this work, we developed and validated WaveWalkerClone, a reproduction of the WaveWalker system, to establish reliable radar- and GPS-IMU-based sensing under varied outdoor lighting conditions. Building on this feasibility validation, we conducted a user study (n=18) comparing three visualization techniques for radar-detected obstacles : (1) diegetic alien avatars that visually embed obstacles within the virtual narrative, (2) non-diegetic human avatars represented obstacles as humans inconsistent with the virtual narrative, and (3) abstract point clouds centered around the obstacles conveying spatial data without anthropomorphic or narrative associations. Our results show that all three approaches supported user safety and situational awareness, but yielded distinct trade-offs in perceived effort, frustration, and user preference. Qualitative feedback further revealed divergent user responses across conditions, highlighting the limitations of a one-size-fits-all approach. We conclude with design considerations for obstacle visualization in outdoor VR systems that seek to balance immersion, safety, and bystander privacy.", "AI": {"tldr": "WaveWalkerClone\u7cfb\u7edf\u5728\u6237\u5916VR\u4e2d\u901a\u8fc7\u6beb\u7c73\u6ce2\u96f7\u8fbe\u68c0\u6d4b\u969c\u788d\u7269\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u53ef\u89c6\u5316\u65b9\u6cd5\uff1a\u53d9\u4e8b\u5185\u5916\u661f\u5316\u8eab\u3001\u53d9\u4e8b\u5916\u4eba\u7c7b\u5316\u8eab\u548c\u62bd\u8c61\u70b9\u4e91\uff0c\u53d1\u73b0\u5404\u6709\u4f18\u52a3\uff0c\u9700\u8981\u6743\u8861\u6c89\u6d78\u611f\u3001\u5b89\u5168\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u6237\u5916VR\u7528\u6237\u9700\u8981\u5728\u6c89\u6d78\u865a\u62df\u573a\u666f\u7684\u540c\u65f6\u4fdd\u6301\u5bf9\u73b0\u5b9e\u4e16\u754c\u969c\u788d\u7269\uff08\u5305\u62ec\u9759\u6001\u7ed3\u6784\u548c\u79fb\u52a8\u884c\u4eba\uff09\u7684\u611f\u77e5\uff0c\u8fd9\u5bf9\u7528\u6237\u5b89\u5168\u548c\u4e34\u573a\u611f\u90fd\u6784\u6210\u6311\u6218\u3002\u6beb\u7c73\u6ce2\u96f7\u8fbe\u63d0\u4f9b\u4e86\u4e00\u79cd\u4fdd\u62a4\u9690\u79c1\u7684\u66ff\u4ee3\u6444\u50cf\u5934\u4f20\u611f\u7684\u65b9\u6848\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5c06\u5176\u7a00\u758f\u7684\u7a7a\u95f4\u4fe1\u606f\u4f20\u8fbe\u7ed9\u7528\u6237\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86WaveWalkerClone\u7cfb\u7edf\uff0c\u5efa\u7acb\u4e86\u53ef\u9760\u7684\u96f7\u8fbe\u548cGPS-IMU\u4f20\u611f\u673a\u5236\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u7528\u6237\u7814\u7a76\uff08n=18\uff09\uff0c\u6bd4\u8f83\u4e09\u79cd\u96f7\u8fbe\u68c0\u6d4b\u969c\u788d\u7269\u7684\u53ef\u89c6\u5316\u6280\u672f\uff1a1\uff09\u53d9\u4e8b\u5185\u5916\u661f\u5316\u8eab\uff08\u5c06\u969c\u788d\u7269\u5d4c\u5165\u865a\u62df\u53d9\u4e8b\u4e2d\uff09\uff1b2\uff09\u53d9\u4e8b\u5916\u4eba\u7c7b\u5316\u8eab\uff08\u5c06\u969c\u788d\u7269\u8868\u793a\u4e3a\u4e0e\u865a\u62df\u53d9\u4e8b\u4e0d\u4e00\u81f4\u7684\u4eba\u7c7b\uff09\uff1b3\uff09\u62bd\u8c61\u70b9\u4e91\uff08\u56f4\u7ed5\u969c\u788d\u7269\u663e\u793a\u7a7a\u95f4\u6570\u636e\uff0c\u65e0\u62df\u4eba\u6216\u53d9\u4e8b\u5173\u8054\uff09\u3002", "result": "\u6240\u6709\u4e09\u79cd\u65b9\u6cd5\u90fd\u652f\u6301\u7528\u6237\u5b89\u5168\u548c\u60c5\u5883\u611f\u77e5\uff0c\u4f46\u5728\u611f\u77e5\u52aa\u529b\u3001\u632b\u6298\u611f\u548c\u7528\u6237\u504f\u597d\u65b9\u9762\u4ea7\u751f\u4e86\u4e0d\u540c\u7684\u6743\u8861\u3002\u5b9a\u6027\u53cd\u9988\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u7528\u6237\u53cd\u5e94\u5dee\u5f02\uff0c\u7a81\u663e\u4e86\"\u4e00\u5200\u5207\"\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u6237\u5916VR\u7cfb\u7edf\u7684\u969c\u788d\u7269\u53ef\u89c6\u5316\u8bbe\u8ba1\u9700\u8981\u5728\u6c89\u6d78\u611f\u3001\u5b89\u5168\u6027\u548c\u884c\u4eba\u9690\u79c1\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u4e0d\u540c\u53ef\u89c6\u5316\u65b9\u6cd5\u5404\u6709\u4f18\u52a3\uff0c\u5e94\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u548c\u7528\u6237\u504f\u597d\u8fdb\u884c\u9009\u62e9\uff0c\u800c\u975e\u91c7\u7528\u5355\u4e00\u6807\u51c6\u5316\u65b9\u6848\u3002"}}
{"id": "2602.00923", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00923", "abs": "https://arxiv.org/abs/2602.00923", "authors": ["Jincheng Wang", "Lingfan Bao", "Tong Yang", "Diego Martinez Plasencia", "Jianhao Jiao", "Dimitrios Kanoulas"], "title": "SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation", "comment": "Under review. 11 pages", "summary": "The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\\%$ in simulated cluttered environments and $72.0\\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced.", "AI": {"tldr": "SanD-Planner\uff1a\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6837\u672c\u9ad8\u6548\u5c40\u90e8\u89c4\u5212\u5668\uff0c\u5728\u5939\u7d27B\u6837\u6761\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6df1\u5ea6\u56fe\u50cf\u6a21\u4eff\u5b66\u4e60\uff0c\u4ec5\u9700500\u4e2a\u6f14\u793a\u6837\u672c\u5373\u53ef\u5728\u6742\u4e71\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u89c4\u5212\u3002", "motivation": "\u5728\u9ad8\u5ea6\u6742\u4e71\u548c\u52a8\u6001\u73af\u5883\u4e2d\u751f\u6210\u53ef\u9760\u7684\u5c40\u90e8\u89c4\u5212\u957f\u671f\u4ee5\u6765\u963b\u788d\u5b9e\u9645\u5e94\u7528\uff0c\u4e3b\u8981\u74f6\u9888\u5305\u62ec\uff1a\u83b7\u53d6\u8de8\u591a\u6837\u573a\u666f\u7684\u5927\u89c4\u6a21\u4e13\u5bb6\u6f14\u793a\u56f0\u96be\uff0c\u4ee5\u53ca\u5728\u6709\u9650\u6570\u636e\u4e0b\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faSanD-Planner\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6837\u672c\u9ad8\u6548\u5c40\u90e8\u89c4\u5212\u5668\uff0c\u5728\u5939\u7d27B\u6837\u6761\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6df1\u5ea6\u56fe\u50cf\u6a21\u4eff\u5b66\u4e60\u3002\u8be5\u7b97\u6cd5\u5728\u7d27\u51d1\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u81ea\u7136\u4ea7\u751f\u5e73\u6ed1\u8f93\u51fa\u4e14\u5c40\u90e8\u652f\u6301\u4e0a\u7684\u9884\u6d4b\u8bef\u5dee\u6709\u754c\uff0c\u4e0e\u6eda\u52a8\u65f6\u57df\u6267\u884c\u81ea\u7136\u5bf9\u9f50\u3002\u96c6\u6210\u57fa\u4e8eESDF\u7684\u5b89\u5168\u68c0\u67e5\u5668\uff0c\u4f7f\u7528\u660e\u786e\u7684\u95f4\u9699\u548c\u5b8c\u6210\u65f6\u95f4\u6307\u6807\uff0c\u51cf\u5c11\u53ef\u884c\u6027\u8bc4\u4f30\u4e2d\u4ef7\u503c\u51fd\u6570\u5b66\u4e60\u7684\u8bad\u7ec3\u8d1f\u62c5\u3002", "result": "\u4ec5\u7528500\u4e2a\u6f14\u793a\u6837\u672c\uff08\u4ec5\u4e3a\u57fa\u7ebf\u4f7f\u7528\u76840.25%\uff09\uff0c\u5728\u8bc4\u4f30\u7684\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u6a21\u62df\u6742\u4e71\u73af\u5883\u4e2d\u6210\u529f\u738790.1%\uff0c\u5ba4\u5185\u6a21\u62df\u4e2d\u6210\u529f\u738772.0%\u3002\u57282D\u548c3D\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u5b9e\u9a8c\u7684\u80fd\u529b\u3002", "conclusion": "SanD-Planner\u901a\u8fc7\u5939\u7d27B\u6837\u6761\u7a7a\u95f4\u4e2d\u7684\u6269\u6563\u6a21\u4eff\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u9ad8\u6548\u7684\u5c40\u90e8\u89c4\u5212\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u4e13\u5bb6\u6f14\u793a\u7684\u4f9d\u8d56\uff0c\u5728\u6742\u4e71\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2602.02235", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02235", "abs": "https://arxiv.org/abs/2602.02235", "authors": ["Zhaonan Wu", "Yanjie Zhao", "Zhenpeng Chen", "Zheng Wang", "Haoyu Wang"], "title": "Agent-Based Software Artifact Evaluation", "comment": null, "summary": "Artifact evaluation has been adopted in the Software Engineering (SE) research community for 15 years, substantially improving research reproducibility across major SE conferences. However, this success has introduced a growing scalability challenge, as artifact evaluation relies heavily on reviewers' manual execution and debugging, leading to escalating human effort amid rapidly increasing paper submissions. To address this problem, we investigate automated artifact evaluation. We first conduct a preliminary study on artifacts from top-tier SE conferences and identify three key challenges: perceiving execution states, maintaining stable execution environments, and recovering from execution errors. Inspired by these findings, we propose ArtifactCopilot, the first end-to-end agent-based framework for automated artifact evaluation. ArtifactCopilot automates environment construction, instruction execution, and error recovery by combining an execution normalization strategy to ensure environment stability with an artifact evaluation graph that transforms README documents into dependency-aware command graphs, enabling structured execution planning, execution-state tracking, and error recovery. Evaluation on 48 real-world artifacts shows that ArtifactCopilot matches human artifact evaluation outcomes for 85.42% of the artifacts, outperforming Claude Code by 52.09 percentage points, while costing only \\$0.091 per artifact on average and requiring zero human intervention for 45 out of 48 artifacts.", "AI": {"tldr": "ArtifactCopilot\uff1a\u9996\u4e2a\u7aef\u5230\u7aef\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u81ea\u52a8\u5316\u5236\u54c1\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6267\u884c\u89c4\u8303\u5316\u7b56\u7565\u548c\u5236\u54c1\u8bc4\u4f30\u56fe\u89e3\u51b3SE\u9886\u57df\u5236\u54c1\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898", "motivation": "\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u793e\u533a\u91c7\u7528\u5236\u54c1\u8bc4\u4f30\u5df2\u670915\u5e74\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7814\u7a76\u53ef\u590d\u73b0\u6027\uff0c\u4f46\u968f\u7740\u8bba\u6587\u63d0\u4ea4\u91cf\u5feb\u901f\u589e\u957f\uff0c\u4f9d\u8d56\u8bc4\u5ba1\u4eba\u5458\u624b\u52a8\u6267\u884c\u548c\u8c03\u8bd5\u7684\u5236\u54c1\u8bc4\u4f30\u9762\u4e34\u65e5\u76ca\u4e25\u91cd\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faArtifactCopilot\u6846\u67b6\uff0c\u7ed3\u5408\u6267\u884c\u89c4\u8303\u5316\u7b56\u7565\u786e\u4fdd\u73af\u5883\u7a33\u5b9a\u6027\uff0c\u4f7f\u7528\u5236\u54c1\u8bc4\u4f30\u56fe\u5c06README\u6587\u6863\u8f6c\u6362\u4e3a\u4f9d\u8d56\u611f\u77e5\u7684\u547d\u4ee4\u56fe\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u6267\u884c\u89c4\u5212\u3001\u6267\u884c\u72b6\u6001\u8ddf\u8e2a\u548c\u9519\u8bef\u6062\u590d\uff0c\u81ea\u52a8\u5316\u5b8c\u6210\u73af\u5883\u6784\u5efa\u3001\u6307\u4ee4\u6267\u884c\u548c\u9519\u8bef\u6062\u590d", "result": "\u572848\u4e2a\u771f\u5b9e\u4e16\u754c\u5236\u54c1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cArtifactCopilot\u4e0e\u4eba\u5de5\u5236\u54c1\u8bc4\u4f30\u7ed3\u679c\u5339\u914d\u7387\u8fbe\u523085.42%\uff0c\u6bd4Claude Code\u9ad8\u51fa52.09\u4e2a\u767e\u5206\u70b9\uff0c\u5e73\u5747\u6bcf\u4e2a\u5236\u54c1\u6210\u672c\u4ec50.091\u7f8e\u5143\uff0c48\u4e2a\u5236\u54c1\u4e2d\u670945\u4e2a\u65e0\u9700\u4eba\u5de5\u5e72\u9884", "conclusion": "ArtifactCopilot\u901a\u8fc7\u81ea\u52a8\u5316\u5236\u54c1\u8bc4\u4f30\u6709\u6548\u89e3\u51b3\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u793e\u533a\u9762\u4e34\u7684\u5236\u54c1\u8bc4\u4f30\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u4e3a\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u5236\u54c1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848"}}
{"id": "2602.01225", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01225", "abs": "https://arxiv.org/abs/2602.01225", "authors": ["Shuyu Chen", "Mingxun Zhou", "Haoyu Niu", "Guopeng Lin", "Weili Han"], "title": "Bifrost: A Much Simpler Secure Two-Party Data Join Protocol for Secure Data Analytics", "comment": "18 pages", "summary": "Secure data join enables two parties with vertically distributed data to securely compute the joined table, allowing the parties to perform downstream Secure multi-party computation-based Data Analytics (SDA), such as training machine learning models, based on the joined table. While Circuit-based Private Set Intersection (CPSI) can be used for secure data join, it introduces redundant dummy rows in the joined table, which results in high overhead in the downstream SDA tasks. iPrivJoin addresses this issue but introduces significant communication overhead in the redundancy removal process, as it relies on the cryptographic primitive OPPRF for data encoding and multiple rounds of oblivious shuffles. In this paper, we propose a much simpler secure data join protocol, Bifrost, which outputs (the secret shares of) a redundancy-free joined table. The highlight of Bifrost lies in its simplicity: it builds upon two conceptually simple building blocks, an ECDH-PSI protocol and a two-party oblivious shuffle protocol. The lightweight protocol design allows Bifrost to avoid the need for OPPRF. We also proposed a simple optimization named \\textit{dual mapping} that reduces the rounds of oblivious shuffle needed from two to one. Experiments on datasets of up to 100 GB show that Bifrost achieves $2.54 \\sim 22.32\\times$ speedup and reduces the communication by $84.15\\% \\sim 88.97\\%$ compared to the SOTA redundancy-free secure data join protocol iPrivJoin. Notably, the communication size of Bifrost is nearly equal to the size of the input data. In the two-step SDA pipeline evaluation (secure join and SDA), the redundancy-free property of Bifrost not only avoids the catastrophic error rate blowup in the downstream tasks caused by the dummy rows in the joined table (as introduced in CPSI), but also shows up to $2.80\\times$ speed-up in the SDA process with up to $73.15\\%$ communication reduction.", "AI": {"tldr": "Bifrost\u662f\u4e00\u4e2a\u7b80\u5355\u9ad8\u6548\u7684\u5197\u4f59\u6d88\u9664\u5b89\u5168\u6570\u636e\u8fde\u63a5\u534f\u8bae\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u901a\u4fe1\u5f00\u9500", "motivation": "\u73b0\u6709\u5b89\u5168\u6570\u636e\u8fde\u63a5\u65b9\u6848\u5b58\u5728\u5197\u4f59\u884c\u6216\u901a\u4fe1\u5f00\u9500\u5927\u7684\u95ee\u9898\uff1a\u57fa\u4e8e\u7535\u8def\u7684\u79c1\u6709\u96c6\u5408\u4ea4\u96c6(CPSI)\u5f15\u5165\u5197\u4f59\u865a\u62df\u884c\uff0c\u589e\u52a0\u4e0b\u6e38\u4efb\u52a1\u5f00\u9500\uff1biPrivJoin\u867d\u7136\u6d88\u9664\u5197\u4f59\u4f46\u4f9d\u8d56\u590d\u6742\u7684OPPRF\u548c\u591a\u6b21\u4e0d\u7ecf\u610f\u6d17\u724c\uff0c\u901a\u4fe1\u5f00\u9500\u5927", "method": "Bifrost\u57fa\u4e8e\u4e24\u4e2a\u7b80\u5355\u6784\u5efa\u5757\uff1aECDH-PSI\u534f\u8bae\u548c\u4e24\u65b9\u4e0d\u7ecf\u610f\u6d17\u724c\u534f\u8bae\uff0c\u907f\u514d\u4e86OPPRF\u9700\u6c42\u3002\u91c7\u7528\"\u53cc\u91cd\u6620\u5c04\"\u4f18\u5316\u5c06\u4e0d\u7ecf\u610f\u6d17\u724c\u8f6e\u6b21\u4ece\u4e24\u8f6e\u51cf\u5c11\u5230\u4e00\u8f6e", "result": "\u5728100GB\u6570\u636e\u96c6\u4e0a\uff0cBifrost\u76f8\u6bd4iPrivJoin\u5b9e\u73b02.54-22.32\u500d\u52a0\u901f\uff0c\u901a\u4fe1\u51cf\u5c1184.15%-88.97%\u3002\u901a\u4fe1\u91cf\u63a5\u8fd1\u8f93\u5165\u6570\u636e\u5927\u5c0f\u3002\u5728\u5b89\u5168\u6570\u636e\u8fde\u63a5+SDA\u4e24\u9636\u6bb5\u8bc4\u4f30\u4e2d\uff0c\u4e0b\u6e38SDA\u4efb\u52a1\u52a0\u901f\u8fbe2.80\u500d\uff0c\u901a\u4fe1\u51cf\u5c1173.15%", "conclusion": "Bifrost\u901a\u8fc7\u7b80\u5355\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u65e0\u5197\u4f59\u5b89\u5168\u6570\u636e\u8fde\u63a5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u4e3a\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01050", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01050", "abs": "https://arxiv.org/abs/2602.01050", "authors": ["Avinash Ajit Nargund", "Andrea M. Park", "Tobias H\u00f6llerer", "Misha Sra"], "title": "Embedded vs. Situated: An Evaluation of AR Facial Training Feedback", "comment": "Conditionally accepted to ACM CHI 2026", "summary": "While augmented reality (AR) research demonstrates benefits of embedded visualizations for gross motor training, its applicability to facial exercises remains under-explored. Providing effective real-time feedback for facial muscle training presents unique design challenges, given the complexity of facial musculature. We developed three AR feedback approaches varying in spatial relationship to the user: situated (screen-fixed), proxy-embedded (on a mannequin), and fully embedded (overlaid on the user's face). In a within-subjects study (N=24), we measured exercise accuracy, cognitive load, and user preference during facial training tasks. The embedded feedback reduced cognitive load and received higher preference ratings, while the situated feedback enabled more precise corrections and higher accuracy. Qualitative analysis revealed a key design tension: embedded feedback improved experience but created self-consciousness and interpretive difficulty. We distill these insights into design considerations addressing the trade-offs for facial training systems, with implications for rehabilitation, performance training, and motor skill acquisition.", "AI": {"tldr": "AR\u9762\u90e8\u8bad\u7ec3\u53cd\u9988\u7cfb\u7edf\u7814\u7a76\uff1a\u6bd4\u8f83\u4e09\u79cdAR\u53cd\u9988\u65b9\u5f0f\uff08\u5c4f\u5e55\u56fa\u5b9a\u3001\u4eba\u5076\u5d4c\u5165\u3001\u9762\u90e8\u5b8c\u5168\u5d4c\u5165\uff09\u5728\u9762\u90e8\u808c\u8089\u8bad\u7ec3\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5d4c\u5165\u5f0f\u53cd\u9988\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u4f46\u5f71\u54cd\u7cbe\u5ea6\uff0c\u63d0\u51fa\u9762\u90e8\u8bad\u7ec3\u7cfb\u7edf\u7684\u8bbe\u8ba1\u6743\u8861\u3002", "motivation": "\u867d\u7136\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u7814\u7a76\u5df2\u8bc1\u660e\u5d4c\u5165\u5f0f\u53ef\u89c6\u5316\u5728\u7c97\u5927\u8fd0\u52a8\u8bad\u7ec3\u4e2d\u7684\u4f18\u52bf\uff0c\u4f46\u5176\u5728\u9762\u90e8\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u9762\u90e8\u808c\u8089\u8bad\u7ec3\u63d0\u4f9b\u6709\u6548\u7684\u5b9e\u65f6\u53cd\u9988\u9762\u4e34\u72ec\u7279\u7684\u8bbe\u8ba1\u6311\u6218\uff0c\u56e0\u4e3a\u9762\u90e8\u808c\u8089\u7ed3\u6784\u590d\u6742\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cdAR\u53cd\u9988\u65b9\u6cd5\uff0c\u6839\u636e\u4e0e\u7528\u6237\u7684\u7a7a\u95f4\u5173\u7cfb\u533a\u5206\uff1a\u5c4f\u5e55\u56fa\u5b9a\u5f0f\uff08situated\uff09\u3001\u4eba\u5076\u5d4c\u5165\u5f0f\uff08proxy-embedded\uff09\u548c\u5b8c\u5168\u5d4c\u5165\u5f0f\uff08fully embedded\uff0c\u8986\u76d6\u5728\u7528\u6237\u9762\u90e8\uff09\u3002\u91c7\u7528\u88ab\u8bd5\u5185\u8bbe\u8ba1\uff08N=24\uff09\uff0c\u6d4b\u91cf\u9762\u90e8\u8bad\u7ec3\u4efb\u52a1\u4e2d\u7684\u8fd0\u52a8\u51c6\u786e\u6027\u3001\u8ba4\u77e5\u8d1f\u8377\u548c\u7528\u6237\u504f\u597d\u3002", "result": "\u5d4c\u5165\u5f0f\u53cd\u9988\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u8377\u5e76\u83b7\u5f97\u66f4\u9ad8\u7684\u504f\u597d\u8bc4\u5206\uff0c\u800c\u5c4f\u5e55\u56fa\u5b9a\u5f0f\u53cd\u9988\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u4fee\u6b63\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u8bbe\u8ba1\u5f20\u529b\uff1a\u5d4c\u5165\u5f0f\u53cd\u9988\u6539\u5584\u4e86\u4f53\u9a8c\uff0c\u4f46\u5f15\u53d1\u4e86\u81ea\u6211\u610f\u8bc6\u548c\u89e3\u91ca\u56f0\u96be\u3002", "conclusion": "\u5c06\u8fd9\u4e9b\u89c1\u89e3\u63d0\u70bc\u4e3a\u9762\u90e8\u8bad\u7ec3\u7cfb\u7edf\u7684\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\uff0c\u89e3\u51b3\u4e86\u5d4c\u5165\u5f0f\u53cd\u9988\u4e0e\u7cbe\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5bf9\u5eb7\u590d\u3001\u8868\u6f14\u8bad\u7ec3\u548c\u8fd0\u52a8\u6280\u80fd\u4e60\u5f97\u5177\u6709\u5e94\u7528\u610f\u4e49\u3002"}}
{"id": "2602.00935", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00935", "abs": "https://arxiv.org/abs/2602.00935", "authors": ["Mohamed Sorour", "Barbara Webb"], "title": "Minimal Footprint Grasping Inspired by Ants", "comment": null, "summary": "Ants are highly capable of grasping objects in clutter, and we have recently observed that this involves substantial use of their forelegs. The forelegs, more specifically the tarsi, have high friction microstructures (setal pads), are covered in hairs, and have a flexible under-actuated tip. Here we abstract these features to test their functional advantages for a novel low-cost gripper design, suitable for bin-picking applications. In our implementation, the gripper legs are long and slim, with high friction gripping pads, low friction hairs and single-segment tarsus-like structure to mimic the insect's setal pads, hairs, and the tarsi's interactive compliance. Experimental evaluation shows this design is highly robust for grasping a wide variety of individual consumer objects, with all grasp attempts successful. In addition, we demonstrate this design is effective for picking single objects from dense clutter, a task at which ants also show high competence. The work advances grasping technology and shed new light on the mechanical importance of hairy structures and tarsal flexibility in insects.", "AI": {"tldr": "\u57fa\u4e8e\u8682\u8681\u524d\u817f\u7ed3\u6784\u4eff\u751f\u7684\u4f4e\u6210\u672c\u6293\u53d6\u5668\u8bbe\u8ba1\uff0c\u5177\u6709\u9ad8\u6469\u64e6\u57ab\u3001\u4f4e\u6469\u64e6\u6bdb\u53d1\u548c\u67d4\u6027\u7ed3\u6784\uff0c\u9002\u7528\u4e8e\u6742\u4e71\u73af\u5883\u4e2d\u7684\u7269\u4f53\u6293\u53d6", "motivation": "\u8682\u8681\u5728\u6742\u4e71\u73af\u5883\u4e2d\u6293\u53d6\u7269\u4f53\u7684\u80fd\u529b\u5f88\u5f3a\uff0c\u7814\u7a76\u53d1\u73b0\u8fd9\u4e3b\u8981\u4f9d\u8d56\u4e8e\u524d\u817f\u7684\u7279\u6b8a\u7ed3\u6784\uff0c\u5305\u62ec\u9ad8\u6469\u64e6\u5fae\u7ed3\u6784\uff08\u521a\u6bdb\u57ab\uff09\u3001\u6bdb\u53d1\u8986\u76d6\u548c\u67d4\u6027\u6b20\u9a71\u52a8\u5c16\u7aef\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5c06\u8fd9\u4e9b\u7279\u5f81\u62bd\u8c61\u5316\uff0c\u5f00\u53d1\u9002\u7528\u4e8e\u6599\u7bb1\u62e3\u9009\u5e94\u7528\u7684\u65b0\u578b\u4f4e\u6210\u672c\u6293\u53d6\u5668\u8bbe\u8ba1\u3002", "method": "\u8bbe\u8ba1\u4eff\u751f\u6293\u53d6\u5668\uff0c\u5176\u817f\u90e8\u7ec6\u957f\uff0c\u5305\u542b\u9ad8\u6469\u64e6\u6293\u53d6\u57ab\uff08\u6a21\u62df\u6606\u866b\u521a\u6bdb\u57ab\uff09\u3001\u4f4e\u6469\u64e6\u6bdb\u53d1\u548c\u5355\u6bb5\u8dd7\u8282\u6837\u7ed3\u6784\uff08\u6a21\u62df\u8dd7\u8282\u7684\u4ea4\u4e92\u67d4\u987a\u6027\uff09\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u8be5\u8bbe\u8ba1\u5bf9\u5404\u79cd\u6d88\u8d39\u54c1\u7684\u6293\u53d6\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u8be5\u8bbe\u8ba1\u5bf9\u591a\u79cd\u5355\u4e2a\u6d88\u8d39\u54c1\u6293\u53d6\u5177\u6709\u9ad8\u5ea6\u9c81\u68d2\u6027\uff0c\u6240\u6709\u6293\u53d6\u5c1d\u8bd5\u5747\u6210\u529f\u3002\u6b64\u5916\uff0c\u8be5\u8bbe\u8ba1\u5728\u4ece\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u6293\u53d6\u5355\u4e2a\u7269\u4f53\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u8fd9\u662f\u8682\u8681\u540c\u6837\u64c5\u957f\u7684\u4efb\u52a1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u8fdb\u4e86\u6293\u53d6\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5e76\u4e3a\u6606\u866b\u6bdb\u53d1\u7ed3\u6784\u548c\u8dd7\u8282\u67d4\u987a\u6027\u7684\u673a\u68b0\u91cd\u8981\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002\u4eff\u751f\u8bbe\u8ba1\u5c55\u793a\u4e86\u5728\u6742\u4e71\u73af\u5883\u4e2d\u9ad8\u6548\u6293\u53d6\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.01061", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01061", "abs": "https://arxiv.org/abs/2602.01061", "authors": ["Linjie Qiu", "Duotun Wang", "Boyu Li", "Jiawei Li", "Yulin Shen", "Zeyu Wang", "Mingming Fan"], "title": "Direct vs. Score-based Selection: Understanding the Heisenberg Effect in Target Acquisition Across Input Modalities in Virtual Reality", "comment": "Accepted by TVCG and IEEE VR'26", "summary": "Target selection is a fundamental interaction in virtual reality (VR). But the act of confirming a selection, such as a button press or pinch, can disturb the tracked pose and shift the intended target, which is referred to as the Heisenberg Effect. Prior research has mainly investigated controller input. However, it remains unclear how the effect manifests in the bare-hand input and how score-based techniques may mitigate the effect in different spatial variations. To fill the gap, we conduct a within-subject study to examine the Heisenberg Effect across two input modalities (i.e., controller and hand) and two selection mechanisms (i.e., direct and score-based). Our results show that hand input is more susceptible to the Heisenberg Effect, with direct selection more influenced by target width and score-based selection more sensitive to target density. Based on previous vote-oriented technique and our temporal analysis, we introduce weighted VOTE, a history-based intention accuracy model for target voting, that reweights recent interaction intent to counteract input disturbances. Our evaluation shows the method improves selection accuracy compared to baseline techniques. Finally, we discuss future directions for adaptive selection methods.", "AI": {"tldr": "\u7814\u7a76VR\u4e2d\u7684\u6d77\u68ee\u5821\u6548\u5e94\uff08\u786e\u8ba4\u9009\u62e9\u65f6\u59ff\u6001\u6270\u52a8\u5bfc\u81f4\u76ee\u6807\u504f\u79fb\uff09\uff0c\u6bd4\u8f83\u63a7\u5236\u5668\u4e0e\u88f8\u624b\u8f93\u5165\uff0c\u63d0\u51fa\u52a0\u6743VOTE\u65b9\u6cd5\u6539\u5584\u9009\u62e9\u7cbe\u5ea6", "motivation": "VR\u76ee\u6807\u9009\u62e9\u4e2d\u7684\u6d77\u68ee\u5821\u6548\u5e94\uff08\u786e\u8ba4\u9009\u62e9\u65f6\u59ff\u6001\u6270\u52a8\u5bfc\u81f4\u76ee\u6807\u504f\u79fb\uff09\u5728\u63a7\u5236\u5668\u8f93\u5165\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u88f8\u624b\u8f93\u5165\u4e2d\u7684\u8868\u73b0\u53ca\u57fa\u4e8e\u5f97\u5206\u7684\u6280\u672f\u5982\u4f55\u7f13\u89e3\u8be5\u6548\u5e94\u5728\u4e0d\u540c\u7a7a\u95f4\u53d8\u5316\u4e2d\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a", "method": "\u91c7\u7528\u88ab\u8bd5\u5185\u8bbe\u8ba1\uff0c\u6bd4\u8f83\u4e24\u79cd\u8f93\u5165\u6a21\u6001\uff08\u63a7\u5236\u5668\u548c\u624b\uff09\u548c\u4e24\u79cd\u9009\u62e9\u673a\u5236\uff08\u76f4\u63a5\u9009\u62e9\u548c\u57fa\u4e8e\u5f97\u5206\u7684\u9009\u62e9\uff09\u3002\u57fa\u4e8e\u5148\u524dvote-oriented\u6280\u672f\u548c\u65f6\u95f4\u5206\u6790\uff0c\u63d0\u51fa\u52a0\u6743VOTE\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u8fd1\u671f\u4ea4\u4e92\u610f\u56fe\u6765\u62b5\u6d88\u8f93\u5165\u6270\u52a8", "result": "\u624b\u8f93\u5165\u66f4\u5bb9\u6613\u53d7\u6d77\u68ee\u5821\u6548\u5e94\u5f71\u54cd\uff0c\u76f4\u63a5\u9009\u62e9\u66f4\u53d7\u76ee\u6807\u5bbd\u5ea6\u5f71\u54cd\uff0c\u57fa\u4e8e\u5f97\u5206\u7684\u9009\u62e9\u5bf9\u76ee\u6807\u5bc6\u5ea6\u66f4\u654f\u611f\u3002\u52a0\u6743VOTE\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u6280\u672f\u63d0\u9ad8\u4e86\u9009\u62e9\u7cbe\u5ea6", "conclusion": "\u88f8\u624b\u8f93\u5165\u4e2d\u7684\u6d77\u68ee\u5821\u6548\u5e94\u9700\u8981\u7279\u522b\u5173\u6ce8\uff0c\u52a0\u6743VOTE\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u9009\u62e9\u7cbe\u5ea6\uff0c\u672a\u6765\u53ef\u7814\u7a76\u81ea\u9002\u5e94\u9009\u62e9\u65b9\u6cd5"}}
{"id": "2602.00937", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00937", "abs": "https://arxiv.org/abs/2602.00937", "authors": ["I-Chun Arthur Liu", "Krzysztof Choromanski", "Sandy Huang", "Connor Schenck"], "title": "CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining", "comment": null, "summary": "Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.", "AI": {"tldr": "CLAMP\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u548c\u591a\u89c6\u89d2\u56fe\u50cf\u76843D\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c063D\u51e0\u4f55\u4fe1\u606f\u4e0e\u673a\u5668\u4eba\u52a8\u4f5c\u6a21\u5f0f\u5173\u8054\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e2D\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u884c\u4e3a\u514b\u9686\u7b56\u7565\u65e0\u6cd5\u6355\u6349\u7269\u4f53\u548c\u573a\u666f\u76843D\u7a7a\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u4e8e\u7cbe\u786e\u64cd\u4f5c\u81f3\u5173\u91cd\u8981", "method": "\u4eceRGB-D\u56fe\u50cf\u548c\u76f8\u673a\u5916\u53c2\u8ba1\u7b97\u5408\u5e76\u70b9\u4e91\uff0c\u91cd\u65b0\u6e32\u67d3\u5305\u542b\u6df1\u5ea6\u548c3D\u5750\u6807\u7684\u591a\u89c6\u89d2\u56db\u901a\u9053\u56fe\u50cf\u89c2\u5bdf\uff08\u5305\u62ec\u52a8\u6001\u624b\u8155\u89c6\u89d2\uff09\uff1b\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u6a21\u62df\u8f68\u8ff9\u4e0a\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u540c\u65f6\u9884\u8bad\u7ec3Diffusion Policy\u521d\u59cb\u5316\u7b56\u7565\u6743\u91cd\uff1b\u6700\u540e\u5728\u5c11\u91cf\u4efb\u52a1\u6f14\u793a\u4e0a\u8fdb\u884c\u5fae\u8c03", "result": "CLAMP\u5728\u516d\u4e2a\u6a21\u62df\u4efb\u52a1\u548c\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u7b56\u7565\u6027\u80fd", "conclusion": "\u63d0\u51fa\u76843D\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e863D\u51e0\u4f55\u4fe1\u606f\u5bf9\u4e8e\u7cbe\u786e\u64cd\u4f5c\u7684\u91cd\u8981\u6027"}}
{"id": "2602.01084", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01084", "abs": "https://arxiv.org/abs/2602.01084", "authors": ["Prasenjit Karmakar", "Manjeet Yadav", "Swayanshu Rout", "Swadhin Pradhan", "Sandip Chakraborty"], "title": "From Invisible to Actionable: Augmented Reality Interactions with Indoor CO2", "comment": "20 pages, 13 figures, 4 tables, ACM CHI 2026", "summary": "Indoor carbon dioxide (CO2) can rapidly accumulate to form invisible pollution hotspots, posing significant health risks due to its odorless and colorless nature. Despite growing interest in wearable or stationary sensors for pollutant detection, effectively visualizing CO2 levels and engaging individuals remains an ongoing challenge. In this paper, we develop a portable wrist-sized pollution sensor that detects CO2 in real time at any indoor location and reveals CO2 bubbles by highlighting sudden spikes. In order to promote better ventilation habits and user awareness, we also develop a smartphone-based augmented reality (AR) game for users to locate and disperse these high-CO2 zones. A user study with 35 participants demonstrated increased engagement and heightened understanding of CO2's health impacts. Our system's usability evaluations yielded a median score of 1.88, indicating its strong practicality.", "AI": {"tldr": "\u5f00\u53d1\u4fbf\u643a\u5f0f\u8155\u6234CO2\u4f20\u611f\u5668\u4e0eAR\u6e38\u620f\u7cfb\u7edf\uff0c\u5b9e\u65f6\u68c0\u6d4b\u5ba4\u5185CO2\u6d53\u5ea6\u5e76\u53ef\u89c6\u5316\"\u6c14\u6ce1\"\u70ed\u70b9\uff0c\u901a\u8fc7\u6e38\u620f\u5316\u65b9\u5f0f\u63d0\u9ad8\u7528\u6237\u5bf9CO2\u6c61\u67d3\u7684\u8ba4\u8bc6\u548c\u901a\u98ce\u4e60\u60ef\u3002", "motivation": "\u5ba4\u5185CO2\u65e0\u8272\u65e0\u5473\uff0c\u6613\u5feb\u901f\u79ef\u7d2f\u5f62\u6210\u6c61\u67d3\u70ed\u70b9\uff0c\u5bf9\u5065\u5eb7\u6784\u6210\u98ce\u9669\u3002\u73b0\u6709\u4f20\u611f\u5668\u96be\u4ee5\u6709\u6548\u53ef\u89c6\u5316CO2\u6c34\u5e73\u5e76\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u9700\u8981\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4fbf\u643a\u5f0f\u8155\u6234\u4f20\u611f\u5668\u5b9e\u65f6\u68c0\u6d4b\u5ba4\u5185CO2\u6d53\u5ea6\uff0c\u8bc6\u522b\u6d53\u5ea6\u7a81\u589e\u533a\u57df\u5e76\u53ef\u89c6\u5316\"\u6c14\u6ce1\"\uff1b\u914d\u5957\u5f00\u53d1\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684AR\u6e38\u620f\uff0c\u8ba9\u7528\u6237\u5b9a\u4f4d\u5e76\u9a71\u6563\u9ad8CO2\u533a\u57df\u3002", "result": "35\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u663e\u793a\u7cfb\u7edf\u63d0\u9ad8\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u5bf9CO2\u5065\u5eb7\u5f71\u54cd\u7684\u7406\u89e3\uff1b\u53ef\u7528\u6027\u8bc4\u4f30\u4e2d\u4f4d\u6570\u5f97\u52061.88\u5206\uff08Likert\u91cf\u8868\uff09\uff0c\u8868\u660e\u7cfb\u7edf\u5b9e\u7528\u6027\u826f\u597d\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u7ed3\u5408\u4fbf\u643a\u4f20\u611f\u5668\u4e0eAR\u6e38\u620f\u5316\uff0c\u6709\u6548\u63d0\u5347\u5ba4\u5185CO2\u6c61\u67d3\u7684\u53ef\u89c6\u5316\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u901a\u98ce\u4e60\u60ef\u548c\u5065\u5eb7\u610f\u8bc6\u3002"}}
{"id": "2602.00980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00980", "abs": "https://arxiv.org/abs/2602.00980", "authors": ["Yichen Cai", "Yuan Gao", "Pengpeng Li", "Wei Wang", "Guibin Sun", "Jinhu L\u00fc"], "title": "Meanshift Shape Formation Control Using Discrete Mass Distribution", "comment": null, "summary": "The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u5206\u5e03\u63a7\u5236\u7b56\u7565\uff0c\u80fd\u591f\u5f62\u6210\u590d\u6742\u5f62\u72b6\u5e76\u9002\u5e94\u7fa4\u4f53\u89c4\u6a21\u53d8\u5316\uff0c\u901a\u8fc7\u79bb\u6563\u8d28\u91cf\u5206\u5e03\u51fd\u6570\u548c\u53bb\u4e2d\u5fc3\u5316\u5747\u503c\u6f02\u79fb\u63a7\u5236\u5b9e\u73b0", "motivation": "\u73b0\u6709\u5bc6\u5ea6\u5206\u5e03\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u590d\u6742\u5f62\u72b6\u8868\u793a\u548c\u53bb\u4e2d\u5fc3\u5316\u5b9e\u73b0\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u5f62\u6210\u590d\u6742\u5f62\u72b6\u53c8\u80fd\u9002\u5e94\u7fa4\u4f53\u89c4\u6a21\u53d8\u5316\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u5206\u5e03\u63a7\u5236\u7b56\u7565", "method": "\u9996\u5148\u63d0\u51fa\u5728\u6837\u672c\u70b9\u96c6\u4e0a\u5b9a\u4e49\u7684\u79bb\u6563\u8d28\u91cf\u5206\u5e03\u51fd\u6570\u6765\u5efa\u6a21\u7fa4\u4f53\u5f62\u6210\uff1b\u5176\u6b21\u8bbe\u8ba1\u53bb\u4e2d\u5fc3\u5316\u5747\u503c\u6f02\u79fb\u63a7\u5236\u5f8b\uff0c\u901a\u8fc7\u53cd\u9988\u8d28\u91cf\u4f30\u8ba1\u534f\u8c03\u7fa4\u4f53\u5168\u5c40\u5206\u5e03\u4ee5\u62df\u5408\u6837\u672c\u70b9\u5206\u5e03\uff1b\u6240\u6709\u6837\u672c\u70b9\u7684\u8d28\u91cf\u4f30\u8ba1\u901a\u8fc7\u8bbe\u8ba1\u7684\u8d28\u91cf\u4f30\u8ba1\u5668\u7531\u673a\u5668\u4eba\u4ee5\u53bb\u4e2d\u5fc3\u5316\u65b9\u5f0f\u5b9e\u73b0", "result": "\u7406\u8bba\u8bc1\u660e\u6837\u672c\u70b9\u7684\u8d28\u91cf\u4f30\u8ba1\u80fd\u591f\u6e10\u8fd1\u6536\u655b\u5230\u771f\u5b9e\u5168\u5c40\u503c\uff1b\u901a\u8fc7\u7efc\u5408\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b56\u7565\u5728\u590d\u6742\u5f62\u72b6\u5f62\u6210\u548c\u9002\u5e94\u7fa4\u4f53\u89c4\u6a21\u53d8\u5316\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u5206\u5e03\u63a7\u5236\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u5f62\u72b6\u8868\u793a\u548c\u53bb\u4e2d\u5fc3\u5316\u5b9e\u73b0\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7fa4\u4f53\u590d\u6742\u5f62\u72b6\u5f62\u6210\u548c\u9002\u5e94\u89c4\u6a21\u53d8\u5316\u7684\u53cc\u91cd\u80fd\u529b"}}
{"id": "2602.02293", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02293", "abs": "https://arxiv.org/abs/2602.02293", "authors": ["Nils Chur", "Thiago Santos de Moura", "Argentina Ortega", "Sven Peldszus", "Thorsten Berger", "Nico Hochgeschwender", "Yannic Noller"], "title": "Before Autonomy Takes Control: Software Testing in Robotics", "comment": null, "summary": "Robotic systems are complex and safety-critical software systems. As such, they need to be tested thoroughly. Unfortunately, robot software is intrinsically hard to test compared to traditional software, mainly since the software needs to closely interact with hardware, account for uncertainty in its operational environment, handle disturbances, and act highly autonomously. However, given the large space in which robots operate, anticipating possible failures when designing tests is challenging. This paper presents a mapping study by considering robotics testing papers and relating them to the software testing theory. We consider 247 robotics testing papers and map them to software testing, discussing the state-of-the-art software testing in robotics with an illustrated example, and discuss current challenges. Forming the basis to introduce both the robotics and software engineering communities to software testing challenges. Finally, we identify open questions and lessons learned.", "AI": {"tldr": "\u5bf9247\u7bc7\u673a\u5668\u4eba\u6d4b\u8bd5\u8bba\u6587\u8fdb\u884c\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\uff0c\u5206\u6790\u673a\u5668\u4eba\u8f6f\u4ef6\u6d4b\u8bd5\u73b0\u72b6\u3001\u6311\u6218\u53ca\u4e0e\u8f6f\u4ef6\u6d4b\u8bd5\u7406\u8bba\u7684\u5173\u8054", "motivation": "\u673a\u5668\u4eba\u7cfb\u7edf\u662f\u590d\u6742\u7684\u5b89\u5168\u5173\u952e\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u9700\u8981\u5f7b\u5e95\u6d4b\u8bd5\u3002\u4f46\u673a\u5668\u4eba\u8f6f\u4ef6\u6d4b\u8bd5\u6bd4\u4f20\u7edf\u8f6f\u4ef6\u6d4b\u8bd5\u66f4\u56f0\u96be\uff0c\u56e0\u4e3a\u9700\u8981\u4e0e\u786c\u4ef6\u7d27\u5bc6\u4ea4\u4e92\u3001\u8003\u8651\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u3001\u5904\u7406\u5e72\u6270\u5e76\u9ad8\u5ea6\u81ea\u4e3b\u8fd0\u884c\u3002\u5728\u8bbe\u8ba1\u6d4b\u8bd5\u65f6\u9884\u6d4b\u53ef\u80fd\u7684\u6545\u969c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u6620\u5c04\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790247\u7bc7\u673a\u5668\u4eba\u6d4b\u8bd5\u8bba\u6587\uff0c\u5c06\u5176\u6620\u5c04\u5230\u8f6f\u4ef6\u6d4b\u8bd5\u7406\u8bba\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u793a\u4f8b\u8bf4\u660e\u673a\u5668\u4eba\u8f6f\u4ef6\u6d4b\u8bd5\u73b0\u72b6\uff0c\u5e76\u8ba8\u8bba\u5f53\u524d\u6311\u6218\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u673a\u5668\u4eba\u8f6f\u4ef6\u6d4b\u8bd5\u7684\u73b0\u72b6\uff0c\u5efa\u7acb\u4e86\u673a\u5668\u4eba\u6d4b\u8bd5\u4e0e\u8f6f\u4ef6\u6d4b\u8bd5\u7406\u8bba\u7684\u5173\u8054\uff0c\u8bc6\u522b\u4e86\u673a\u5668\u4eba\u6d4b\u8bd5\u7279\u6709\u7684\u6311\u6218\u548c\u95ee\u9898\u3002", "conclusion": "\u4e3a\u673a\u5668\u4eba\u5b66\u548c\u8f6f\u4ef6\u5de5\u7a0b\u793e\u533a\u4ecb\u7ecd\u4e86\u8f6f\u4ef6\u6d4b\u8bd5\u6311\u6218\uff0c\u8bc6\u522b\u4e86\u5f00\u653e\u6027\u95ee\u9898\u5e76\u603b\u7ed3\u4e86\u7ecf\u9a8c\u6559\u8bad\uff0c\u4e3a\u4e24\u4e2a\u9886\u57df\u7684\u4ea4\u53c9\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.01341", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.01341", "abs": "https://arxiv.org/abs/2602.01341", "authors": ["Pedro Campon\u00eas", "Hugo Pereira", "Adrian Persaud", "Kevin Gallagher", "Santiago Torres-Arias"], "title": "Privocracy: Online Democracy through Private Voting", "comment": null, "summary": "In traditional access control policies, every access granted and administrative account introduces an additional vulnerability, as a corruption of a high-privilege user can compromise several sensitive files. Privocracy is an access control mechanism that minimizes the need to attribute high privileges by triggering a secure e-voting procedure to run commands that require using sensitive resources. With Privocracy an organization can distribute trust in resource access, minimizing the system vulnerabilities from single points of failure, all while maintaining the high flexibility of discretionary access control policies.\n  The Privocracy voting mechanism achieves everlasting privacy, ensuring votes remain confidential regardless of an adversary's computational power, while addressing the dependability requirements of a practical and secure system. The procedure incorporates useful features such as vote delegation to reduce voter fatigue, rapid voting rounds to enable quick action during emergencies, and selective vote auditing for application-level accountability. Our experimental results demonstrate that Privocracy processes votes efficiently and can be deployed on commodity hardware.", "AI": {"tldr": "Privocracy\u662f\u4e00\u79cd\u8bbf\u95ee\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u5b89\u5168\u7535\u5b50\u6295\u7968\u6765\u6267\u884c\u9700\u8981\u654f\u611f\u8d44\u6e90\u7684\u547d\u4ee4\uff0c\u51cf\u5c11\u9ad8\u6743\u9650\u5206\u914d\uff0c\u5206\u6563\u4fe1\u4efb\uff0c\u540c\u65f6\u4fdd\u6301\u8bbf\u95ee\u63a7\u5236\u7684\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u8bbf\u95ee\u63a7\u5236\u4e2d\uff0c\u6bcf\u4e2a\u6388\u4e88\u7684\u8bbf\u95ee\u6743\u9650\u548c\u7ba1\u7406\u8d26\u6237\u90fd\u4f1a\u5f15\u5165\u989d\u5916\u6f0f\u6d1e\uff0c\u9ad8\u6743\u9650\u7528\u6237\u4e00\u65e6\u88ab\u653b\u7834\u4f1a\u5371\u53ca\u591a\u4e2a\u654f\u611f\u6587\u4ef6\u3002\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u6700\u5c0f\u5316\u9ad8\u6743\u9650\u5206\u914d\uff0c\u5206\u6563\u4fe1\u4efb\uff0c\u51cf\u5c11\u5355\u70b9\u6545\u969c\u3002", "method": "Privocracy\u91c7\u7528\u5b89\u5168\u7535\u5b50\u6295\u7968\u673a\u5236\u6765\u8fd0\u884c\u9700\u8981\u654f\u611f\u8d44\u6e90\u7684\u547d\u4ee4\uff0c\u5b9e\u73b0\u8bbf\u95ee\u63a7\u5236\u3002\u8be5\u673a\u5236\u5177\u6709\u6c38\u6052\u9690\u79c1\u4fdd\u62a4\uff08\u65e0\u8bba\u5bf9\u624b\u8ba1\u7b97\u80fd\u529b\u5982\u4f55\uff0c\u6295\u7968\u90fd\u4fdd\u6301\u673a\u5bc6\uff09\u3001\u6295\u7968\u59d4\u6258\uff08\u51cf\u5c11\u6295\u7968\u75b2\u52b3\uff09\u3001\u5feb\u901f\u6295\u7968\u8f6e\u6b21\uff08\u7d27\u6025\u60c5\u51b5\u4e0b\u5feb\u901f\u884c\u52a8\uff09\u548c\u9009\u62e9\u6027\u6295\u7968\u5ba1\u8ba1\uff08\u5e94\u7528\u7ea7\u95ee\u8d23\uff09\u7b49\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPrivocracy\u80fd\u591f\u9ad8\u6548\u5904\u7406\u6295\u7968\uff0c\u53ef\u4ee5\u5728\u5546\u7528\u786c\u4ef6\u4e0a\u90e8\u7f72\u3002\u8be5\u673a\u5236\u6ee1\u8db3\u4e86\u5b9e\u9645\u5b89\u5168\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u8981\u6c42\u3002", "conclusion": "Privocracy\u901a\u8fc7\u7535\u5b50\u6295\u7968\u673a\u5236\u5b9e\u73b0\u4e86\u8bbf\u95ee\u63a7\u5236\u7684\u4fe1\u4efb\u5206\u6563\uff0c\u6700\u5c0f\u5316\u4e86\u7cfb\u7edf\u6f0f\u6d1e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u7684\u9ad8\u5ea6\u7075\u6d3b\u6027\uff0c\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u8bbf\u95ee\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01114", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01114", "abs": "https://arxiv.org/abs/2602.01114", "authors": ["Sai Keerthana Karnam", "Abhisek Dash", "Krishna Gummadi", "Animesh Mukherjee", "Ingmar Weber", "Savvas Zannettou"], "title": "Bowling with ChatGPT: On the Evolving User Interactions with Conversational AI Systems", "comment": "This work has been accepted at The ACM Web Conference 2026", "summary": "Recent studies have discussed how users are increasingly using conversational AI systems, powered by LLMs, for information seeking, decision support, and even emotional support. However, these macro-level observations offer limited insight into how the purpose of these interactions shifts over time, how users frame their interactions with the system, and how steering dynamics unfold in these human-AI interactions. To examine these evolving dynamics, we gathered and analyzed a unique dataset InVivoGPT: consisting of 825K ChatGPT interactions, donated by 300 users through their GDPR data rights. Our analyses reveal three key findings. First, participants increasingly turn to ChatGPT for a broader range of purposes, including substantial growth in sensitive domains such as health and mental health. Second, interactions become more socially framed: the system anthropomorphizes itself at rising rates, participants more frequently treat it as a companion, and personal data disclosure becomes both more common and more diverse. Third, conversational steering becomes more prominent, especially after the release of GPT-4o, with conversations where the participants followed a model-initiated suggestion quadrupling over the period of our dataset. Overall, our results show that conversational AI systems are shifting from functional tools to social partners, raising important questions about their design and governance.", "AI": {"tldr": "\u7528\u6237\u4e0eChatGPT\u7684\u4ea4\u4e92\u4ece\u529f\u80fd\u6027\u5de5\u5177\u8f6c\u5411\u793e\u4ea4\u4f19\u4f34\uff0c\u4f7f\u7528\u76ee\u7684\u6269\u5c55\u81f3\u654f\u611f\u9886\u57df\uff0c\u793e\u4ea4\u6846\u67b6\u589e\u5f3a\uff0c\u5bf9\u8bdd\u5f15\u5bfc\u66f4\u9891\u7e41", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5b8f\u89c2\u5c42\u9762\u7684\u5bf9\u8bddAI\u4f7f\u7528\u60c5\u51b5\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4ea4\u4e92\u76ee\u7684\u968f\u65f6\u95f4\u6f14\u53d8\u3001\u7528\u6237\u5982\u4f55\u6784\u5efa\u4e0e\u7cfb\u7edf\u7684\u4e92\u52a8\u5173\u7cfb\u3001\u4ee5\u53ca\u4eba\u673a\u4ea4\u4e92\u4e2d\u5f15\u5bfc\u52a8\u6001\u5982\u4f55\u5c55\u5f00\u7684\u6df1\u5165\u7406\u89e3", "method": "\u901a\u8fc7GDPR\u6570\u636e\u6743\u5229\u6536\u96c6300\u540d\u7528\u6237\u6350\u8d60\u7684825K\u6b21ChatGPT\u4ea4\u4e92\u6570\u636e\uff0c\u6784\u5efaInVivoGPT\u6570\u636e\u96c6\uff0c\u5206\u6790\u4ea4\u4e92\u76ee\u7684\u3001\u793e\u4ea4\u6846\u67b6\u548c\u5f15\u5bfc\u52a8\u6001\u7684\u6f14\u53d8\u6a21\u5f0f", "result": "1) \u7528\u6237\u4f7f\u7528ChatGPT\u7684\u76ee\u7684\u8303\u56f4\u6269\u5927\uff0c\u7279\u522b\u662f\u5728\u5065\u5eb7\u548c\u5fc3\u7406\u5065\u5eb7\u7b49\u654f\u611f\u9886\u57df\u663e\u8457\u589e\u957f\uff1b2) \u4ea4\u4e92\u793e\u4ea4\u5316\u589e\u5f3a\uff1a\u7cfb\u7edf\u81ea\u6211\u62df\u4eba\u5316\u7387\u4e0a\u5347\uff0c\u7528\u6237\u66f4\u591a\u5c06\u5176\u89c6\u4e3a\u4f34\u4fa3\uff0c\u4e2a\u4eba\u6570\u636e\u62ab\u9732\u66f4\u666e\u904d\u591a\u6837\uff1b3) GPT-4o\u53d1\u5e03\u540e\u5bf9\u8bdd\u5f15\u5bfc\u66f4\u7a81\u51fa\uff0c\u7528\u6237\u9075\u5faa\u6a21\u578b\u5efa\u8bae\u7684\u5bf9\u8bdd\u6570\u91cf\u589e\u52a0\u56db\u500d", "conclusion": "\u5bf9\u8bddAI\u7cfb\u7edf\u6b63\u4ece\u529f\u80fd\u6027\u5de5\u5177\u8f6c\u53d8\u4e3a\u793e\u4ea4\u4f19\u4f34\uff0c\u8fd9\u4e00\u8f6c\u53d8\u5bf9\u7cfb\u7edf\u8bbe\u8ba1\u548c\u6cbb\u7406\u63d0\u51fa\u4e86\u91cd\u8981\u95ee\u9898"}}
{"id": "2602.00992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00992", "abs": "https://arxiv.org/abs/2602.00992", "authors": ["Phone Thiha Kyaw", "Jonathan Kelly"], "title": "Geometry-Aware Sampling-Based Motion Planning on Riemannian Manifolds", "comment": "Submitted to WAFR 2026 (17th World Symposium on the Algorithmic Foundations of Robotics (WAFR))", "summary": "In many robot motion planning problems, task objectives and physical constraints induce non-Euclidean geometry on the configuration space, yet many planners operate using Euclidean distances that ignore this structure. We address the problem of planning collision-free motions that minimize length under configuration-dependent Riemannian metrics, corresponding to geodesics on the configuration manifold. Conventional numerical methods for computing such paths do not scale well to high-dimensional systems, while sampling-based planners trade scalability for geometric fidelity. To bridge this gap, we propose a sampling-based motion planning framework that operates directly on Riemannian manifolds. We introduce a computationally efficient midpoint-based approximation of the Riemannian geodesic distance and prove that it matches the true Riemannian distance with third-order accuracy. Building on this approximation, we design a local planner that traces the manifold using first-order retractions guided by Riemannian natural gradients. Experiments on a two-link planar arm and a 7-DoF Franka manipulator under a kinetic-energy metric, as well as on rigid-body planning in $\\mathrm{SE}(2)$ with non-holonomic motion constraints, demonstrate that our approach consistently produces lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u76f4\u63a5\u64cd\u4f5c\u7684\u91c7\u6837\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u4e2d\u70b9\u8fd1\u4f3c\u9ece\u66fc\u6d4b\u5730\u8ddd\u79bb\u548c\u57fa\u4e8e\u4e00\u9636\u6536\u7f29\u7684\u5c40\u90e8\u89c4\u5212\u5668\uff0c\u4e3a\u9ad8\u7ef4\u7cfb\u7edf\u751f\u6210\u4f4e\u6210\u672c\u7684\u8f68\u8ff9\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u4e2d\uff0c\u4efb\u52a1\u76ee\u6807\u548c\u7269\u7406\u7ea6\u675f\u5728\u6784\u578b\u7a7a\u95f4\u4e0a\u8bf1\u5bfc\u51fa\u975e\u6b27\u51e0\u4f55\u7ed3\u6784\uff0c\u4f46\u73b0\u6709\u89c4\u5212\u5668\u901a\u5e38\u4f7f\u7528\u5ffd\u7565\u8fd9\u79cd\u7ed3\u6784\u7684\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u3002\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u8ba1\u7b97\u6b64\u7c7b\u8def\u5f84\u96be\u4ee5\u6269\u5c55\u5230\u9ad8\u7ef4\u7cfb\u7edf\uff0c\u800c\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\u5219\u5728\u53ef\u6269\u5c55\u6027\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u4e4b\u95f4\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u76f4\u63a5\u64cd\u4f5c\u7684\u91c7\u6837\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff1a1) \u5f15\u5165\u8ba1\u7b97\u9ad8\u6548\u7684\u4e2d\u70b9\u8fd1\u4f3c\u65b9\u6cd5\u6765\u8fd1\u4f3c\u9ece\u66fc\u6d4b\u5730\u8ddd\u79bb\uff0c\u5e76\u8bc1\u660e\u5176\u5177\u6709\u4e09\u9636\u7cbe\u5ea6\uff1b2) \u57fa\u4e8e\u6b64\u8fd1\u4f3c\u8bbe\u8ba1\u5c40\u90e8\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u9ece\u66fc\u81ea\u7136\u68af\u5ea6\u5f15\u5bfc\u7684\u4e00\u9636\u6536\u7f29\u5728\u6d41\u5f62\u4e0a\u8ffd\u8e2a\u8def\u5f84\u3002", "result": "\u5728\u4e8c\u8fde\u6746\u5e73\u9762\u81c2\u548c7\u81ea\u7531\u5ea6Franka\u673a\u68b0\u81c2\uff08\u4f7f\u7528\u52a8\u80fd\u5ea6\u91cf\uff09\u4ee5\u53ca\u5177\u6709\u975e\u5b8c\u6574\u8fd0\u52a8\u7ea6\u675f\u7684SE(2)\u521a\u4f53\u89c4\u5212\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u6bd4\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u7684\u89c4\u5212\u5668\u548c\u7ecf\u5178\u6570\u503c\u6d4b\u5730\u7ebf\u6c42\u89e3\u5668\u57fa\u7ebf\u4ea7\u751f\u66f4\u4f4e\u6210\u672c\u7684\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u6865\u63a5\u4e86\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u548c\u91c7\u6837\u89c4\u5212\u5668\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u80fd\u591f\u5728\u9ad8\u7ef4\u7cfb\u7edf\u4e2d\u6709\u6548\u5904\u7406\u9ece\u66fc\u51e0\u4f55\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u7b26\u5408\u7269\u7406\u7ea6\u675f\u548c\u4efb\u52a1\u76ee\u6807\u7684\u4f4e\u6210\u672c\u8fd0\u52a8\u8f68\u8ff9\u3002"}}
{"id": "2602.01342", "categories": ["cs.CR", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.01342", "abs": "https://arxiv.org/abs/2602.01342", "authors": ["Poushali Sengupta", "Mayank Raikwar", "Sabita Maharjan", "Frank Eliassen", "Yan Zhang"], "title": "Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization", "comment": "Accepted for presentation at NDSS 2026 - FutureG Workshop, 23 February 2026. (10 pages, 5 figures.)", "summary": "Powerful quantum computers in the future may be able to break the security used for communication between vehicles and other devices (Vehicle-to-Everything, or V2X). New security methods called post-quantum cryptography can help protect these systems, but they often require more computing power and can slow down communication, posing a challenge for fast 6G vehicle networks. In this paper, we propose an adaptive post-quantum cryptography (PQC) framework that predicts short-term mobility and channel variations and dynamically selects suitable lattice-, code-, or hash-based PQC configurations using a predictive multi-objective evolutionary algorithm (APMOEA) to meet vehicular latency and security constraints.However, frequent cryptographic reconfiguration in dynamic vehicular environments introduces new attack surfaces during algorithm transitions. A secure monotonic-upgrade protocol prevents downgrade, replay, and desynchronization attacks during transitions. Theoretical results show decision stability under bounded prediction error, latency boundedness under mobility drift, and correctness under small forecast noise. These results demonstrate a practical path toward quantum-safe cryptography in future 6G vehicular networks. Through extensive experiments based on realistic mobility (LuST), weather (ERA5), and NR-V2X channel traces, we show that the proposed framework reduces end-to-end latency by up to 27\\%, lowers communication overhead by up to 65\\%, and effectively stabilizes cryptographic switching behavior using reinforcement learning. Moreover, under the evaluated adversarial scenarios, the monotonic-upgrade protocol successfully prevents downgrade, replay, and desynchronization attacks.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u540e\u91cf\u5b50\u5bc6\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u79fb\u52a8\u6027\u548c\u4fe1\u9053\u53d8\u5316\uff0c\u52a8\u6001\u9009\u62e9\u5bc6\u7801\u914d\u7f6e\u4ee5\u6ee1\u8db3\u8f66\u8054\u7f51\u5ef6\u8fdf\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u540c\u65f6\u8bbe\u8ba1\u5b89\u5168\u5355\u8c03\u5347\u7ea7\u534f\u8bae\u9632\u6b62\u7b97\u6cd5\u5207\u6362\u671f\u95f4\u7684\u653b\u51fb\u3002", "motivation": "\u672a\u6765\u91cf\u5b50\u8ba1\u7b97\u673a\u53ef\u80fd\u7834\u89e3\u73b0\u6709\u8f66\u8054\u7f51\u5b89\u5168\u673a\u5236\uff0c\u800c\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u901a\u5e38\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\uff0c\u53ef\u80fd\u964d\u4f4e6G\u8f66\u8054\u7f51\u901a\u4fe1\u901f\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u6765\u5e73\u8861\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u540e\u91cf\u5b50\u5bc6\u7801\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u6d4b\u6027\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\u52a8\u6001\u9009\u62e9\u683c\u57fa\u3001\u7f16\u7801\u6216\u54c8\u5e0c\u57fa\u5bc6\u7801\u914d\u7f6e\uff1b\u8bbe\u8ba1\u5b89\u5168\u5355\u8c03\u5347\u7ea7\u534f\u8bae\u9632\u6b62\u5207\u6362\u671f\u95f4\u7684\u964d\u7ea7\u3001\u91cd\u653e\u548c\u53bb\u540c\u6b65\u653b\u51fb\uff1b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7a33\u5b9a\u5bc6\u7801\u5207\u6362\u884c\u4e3a\u3002", "result": "\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u8fbe27%\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c11\u8fbe65%\uff1b\u5728\u8bc4\u4f30\u7684\u5bf9\u6297\u573a\u666f\u4e2d\uff0c\u5355\u8c03\u5347\u7ea7\u534f\u8bae\u6210\u529f\u9632\u6b62\u964d\u7ea7\u3001\u91cd\u653e\u548c\u53bb\u540c\u6b65\u653b\u51fb\uff1b\u7406\u8bba\u5206\u6790\u663e\u793a\u51b3\u7b56\u7a33\u5b9a\u6027\u3001\u5ef6\u8fdf\u6709\u754c\u6027\u548c\u6b63\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u67656G\u8f66\u8054\u7f51\u5b9e\u73b0\u91cf\u5b50\u5b89\u5168\u5bc6\u7801\u5b66\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u914d\u7f6e\u548c\u5b89\u5168\u7684\u7b97\u6cd5\u5207\u6362\u673a\u5236\uff0c\u5728\u52a8\u6001\u8f66\u8f7d\u73af\u5883\u4e2d\u5e73\u8861\u5b89\u5168\u6027\u548c\u6027\u80fd\u8981\u6c42\u3002"}}
{"id": "2602.01201", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01201", "abs": "https://arxiv.org/abs/2602.01201", "authors": ["Lingyu Du", "Xucong Zhang", "Guohao Lan"], "title": "Talk to Me, Not the Slides: A Real-Time Wearable Assistant for Improving Eye Contact in Presentations", "comment": null, "summary": "Effective eye contact is a cornerstone of successful public speaking. It strengthens the speaker's credibility and fosters audience engagement. Yet, managing effective eye contact is a skill that demands extensive training and practice, often posing a significant challenge for novice speakers. In this paper, we present SpeakAssis, the first real-time, in-situ wearable system designed to actively assist speakers in maintaining effective eye contact during live presentations. Leveraging a head-mounted eye tracker for gaze and scene view capture, SpeakAssis continuously monitors and analyzes the speaker's gaze distribution across audience and non-audience regions. When ineffective eye-contact patterns are detected, such as insufficient eye contact, or neglect of certain audience segments, SpeakAssis provides timely, context-aware audio prompts via an earphone to guide the speaker's gaze behavior. We evaluate SpeakAssis through a user study involving eight speakers and 24 audience members. Quantitative results show that SpeakAssis increases speakers' eye-contact duration by 62.5% on average and promotes a more balanced distribution of visual attention. Additionally, statistical analysis based on audience surveys reveals that improvements in speaker's eye-contact behavior significantly enhance the audience's perceived engagement and interactivity during presentations.", "AI": {"tldr": "SpeakAssis\uff1a\u9996\u4e2a\u5b9e\u65f6\u3001\u539f\u4f4d\u53ef\u7a7f\u6234\u7cfb\u7edf\uff0c\u901a\u8fc7\u5934\u6234\u5f0f\u773c\u52a8\u4eea\u76d1\u6d4b\u6f14\u8bb2\u8005\u89c6\u7ebf\u5206\u5e03\uff0c\u5f53\u68c0\u6d4b\u5230\u65e0\u6548\u773c\u795e\u63a5\u89e6\u6a21\u5f0f\u65f6\u63d0\u4f9b\u97f3\u9891\u63d0\u793a\uff0c\u5e2e\u52a9\u6f14\u8bb2\u8005\u7ef4\u6301\u6709\u6548\u773c\u795e\u4ea4\u6d41", "motivation": "\u6709\u6548\u773c\u795e\u4ea4\u6d41\u662f\u6210\u529f\u516c\u5f00\u6f14\u8bb2\u7684\u5173\u952e\uff0c\u80fd\u589e\u5f3a\u6f14\u8bb2\u8005\u53ef\u4fe1\u5ea6\u5e76\u4fc3\u8fdb\u89c2\u4f17\u53c2\u4e0e\u3002\u7136\u800c\uff0c\u7ba1\u7406\u6709\u6548\u773c\u795e\u4ea4\u6d41\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u548c\u5b9e\u8df5\uff0c\u5bf9\u65b0\u624b\u6f14\u8bb2\u8005\u6784\u6210\u91cd\u5927\u6311\u6218", "method": "\u5f00\u53d1SpeakAssis\u7cfb\u7edf\uff0c\u4f7f\u7528\u5934\u6234\u5f0f\u773c\u52a8\u4eea\u6355\u6349\u6f14\u8bb2\u8005\u89c6\u7ebf\u548c\u573a\u666f\u89c6\u56fe\uff0c\u6301\u7eed\u76d1\u6d4b\u5206\u6790\u6f14\u8bb2\u8005\u89c6\u7ebf\u5728\u89c2\u4f17\u533a\u548c\u975e\u89c2\u4f17\u533a\u7684\u5206\u5e03\u3002\u5f53\u68c0\u6d4b\u5230\u65e0\u6548\u773c\u795e\u63a5\u89e6\u6a21\u5f0f\uff08\u5982\u773c\u795e\u4ea4\u6d41\u4e0d\u8db3\u6216\u5ffd\u89c6\u67d0\u4e9b\u89c2\u4f17\u533a\u57df\uff09\u65f6\uff0c\u901a\u8fc7\u8033\u673a\u63d0\u4f9b\u53ca\u65f6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u97f3\u9891\u63d0\u793a\u6765\u5f15\u5bfc\u6f14\u8bb2\u8005\u89c6\u7ebf\u884c\u4e3a", "result": "\u7528\u6237\u7814\u7a76\uff088\u540d\u6f14\u8bb2\u8005\u548c24\u540d\u89c2\u4f17\uff09\u663e\u793a\uff1aSpeakAssis\u4f7f\u6f14\u8bb2\u8005\u773c\u795e\u63a5\u89e6\u6301\u7eed\u65f6\u95f4\u5e73\u5747\u589e\u52a062.5%\uff0c\u4fc3\u8fdb\u66f4\u5e73\u8861\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u5e03\u3002\u57fa\u4e8e\u89c2\u4f17\u8c03\u67e5\u7684\u7edf\u8ba1\u5206\u6790\u8868\u660e\uff0c\u6f14\u8bb2\u8005\u773c\u795e\u4ea4\u6d41\u884c\u4e3a\u7684\u6539\u5584\u663e\u8457\u63d0\u5347\u4e86\u89c2\u4f17\u611f\u77e5\u7684\u53c2\u4e0e\u5ea6\u548c\u4e92\u52a8\u6027", "conclusion": "SpeakAssis\u4f5c\u4e3a\u9996\u4e2a\u5b9e\u65f6\u3001\u539f\u4f4d\u53ef\u7a7f\u6234\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u5e2e\u52a9\u6f14\u8bb2\u8005\u7ef4\u6301\u6709\u6548\u773c\u795e\u4ea4\u6d41\uff0c\u6539\u5584\u6f14\u8bb2\u8868\u73b0\u548c\u89c2\u4f17\u4f53\u9a8c\uff0c\u4e3a\u89e3\u51b3\u65b0\u624b\u6f14\u8bb2\u8005\u773c\u795e\u4ea4\u6d41\u7ba1\u7406\u96be\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00993", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00993", "abs": "https://arxiv.org/abs/2602.00993", "authors": ["Weizhe Tang", "Junwei You", "Jiaxi Liu", "Zhaoyi Wang", "Rui Gan", "Zilin Huang", "Feng Wei", "Bin Ran"], "title": "HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.", "AI": {"tldr": "HERMES\u662f\u4e00\u4e2a\u9762\u5411\u957f\u5c3e\u6df7\u5408\u4ea4\u901a\u573a\u666f\u7684\u98ce\u9669\u611f\u77e5\u7aef\u5230\u7aef\u591a\u6a21\u6001\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u5165\u663e\u5f0f\u957f\u5c3e\u98ce\u9669\u7ebf\u7d22\u6765\u63d0\u5347\u8f68\u8ff9\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u867d\u7136\u53d7\u76ca\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5728\u957f\u5c3e\u6df7\u5408\u4ea4\u901a\u573a\u666f\u4e0b\u786e\u4fdd\u5b89\u5168\u548c\u51c6\u786e\u64cd\u4f5c\u4ecd\u9762\u4e34\u6311\u6218\u3002\u8fd9\u4e9b\u573a\u666f\u4e2d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u4e0e\u5f02\u8d28\u9053\u8def\u4f7f\u7528\u8005\uff08\u5305\u62ec\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u548c\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff09\u5728\u590d\u6742\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u4ea4\u4e92\u3002", "method": "\u63d0\u51faHERMES\u6846\u67b6\uff1a1\uff09\u91c7\u7528\u57fa\u7840\u6a21\u578b\u8f85\u52a9\u7684\u6807\u6ce8\u6d41\u7a0b\u751f\u6210\u7ed3\u6784\u5316\u957f\u5c3e\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u89c4\u5212\u4e0a\u4e0b\u6587\uff0c\u6355\u6349\u5371\u9669\u4e2d\u5fc3\u7ebf\u7d22\u3001\u673a\u52a8\u610f\u56fe\u548c\u5b89\u5168\u504f\u597d\uff1b2\uff09\u5f15\u5165\u4e09\u6a21\u6001\u9a7e\u9a76\u6a21\u5757\uff0c\u878d\u5408\u591a\u89c6\u89d2\u611f\u77e5\u3001\u5386\u53f2\u8fd0\u52a8\u7ebf\u7d22\u548c\u8bed\u4e49\u5f15\u5bfc\uff0c\u5b9e\u73b0\u98ce\u9669\u611f\u77e5\u7684\u51c6\u786e\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u957f\u5c3e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHERMES\u5728\u957f\u5c3e\u6df7\u5408\u4ea4\u901a\u573a\u666f\u4e0b\u6301\u7eed\u4f18\u4e8e\u4ee3\u8868\u6027\u7684\u7aef\u5230\u7aef\u548cVLM\u9a71\u52a8\u57fa\u7ebf\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5173\u952e\u7ec4\u4ef6\u7684\u4e92\u8865\u8d21\u732e\u3002", "conclusion": "HERMES\u901a\u8fc7\u6ce8\u5165\u663e\u5f0f\u957f\u5c3e\u98ce\u9669\u7ebf\u7d22\u548c\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u5728\u957f\u5c3e\u6df7\u5408\u4ea4\u901a\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u548c\u89c4\u5212\u51c6\u786e\u6027\uff0c\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7684\u957f\u5c3e\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.01438", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01438", "abs": "https://arxiv.org/abs/2602.01438", "authors": ["Max Manolov", "Tony Gao", "Siddharth Shukla", "Cheng-Ting Chou", "Ryan Lagasse"], "title": "CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses", "comment": null, "summary": "Large language models (LLMs) are increasingly used to assist developers with code, yet their implementations of cryptographic functionality often contain exploitable flaws. Minor design choices (e.g., static initialization vectors or missing authentication) can silently invalidate security guarantees. We introduce CIPHER(\\textbf{C}ryptographic \\textbf{I}nsecurity \\textbf{P}rofiling via \\textbf{H}ybrid \\textbf{E}valuation of \\textbf{R}esponses), a benchmark for measuring cryptographic vulnerability incidence in LLM-generated Python code under controlled security-guidance conditions. CIPHER uses insecure/neutral/secure prompt variants per task, a cryptography-specific vulnerability taxonomy, and line-level attribution via an automated scoring pipeline. Across a diverse set of widely used LLMs, we find that explicit ``secure'' prompting reduces some targeted issues but does not reliably eliminate cryptographic vulnerabilities overall. The benchmark and reproducible scoring pipeline will be publicly released upon publication.", "AI": {"tldr": "CIPHER\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u751f\u6210Python\u4ee3\u7801\u4e2d\u52a0\u5bc6\u6f0f\u6d1e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5b89\u5168/\u4e2d\u6027/\u4e0d\u5b89\u5168\u63d0\u793a\u53d8\u4f53\u3001\u52a0\u5bc6\u6f0f\u6d1e\u5206\u7c7b\u548c\u81ea\u52a8\u5316\u8bc4\u5206\u7ba1\u9053\u6765\u6d4b\u91cf\u6f0f\u6d1e\u53d1\u751f\u7387", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u8f85\u52a9\u5f00\u53d1\u4eba\u5458\u7f16\u5199\u4ee3\u7801\uff0c\u4f46\u5176\u751f\u6210\u7684\u52a0\u5bc6\u529f\u80fd\u5b9e\u73b0\u7ecf\u5e38\u5305\u542b\u53ef\u5229\u7528\u7684\u7f3a\u9677\u3002\u5fae\u5c0f\u7684\u8bbe\u8ba1\u9009\u62e9\uff08\u5982\u9759\u6001\u521d\u59cb\u5316\u5411\u91cf\u6216\u7f3a\u5c11\u8eab\u4efd\u9a8c\u8bc1\uff09\u53ef\u80fd\u65e0\u58f0\u5730\u7834\u574f\u5b89\u5168\u4fdd\u8bc1\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u7684\u52a0\u5bc6\u5b89\u5168\u6027", "method": "CIPHER\u57fa\u51c6\u6d4b\u8bd5\u4f7f\u7528\u4e09\u79cd\u63d0\u793a\u53d8\u4f53\uff08\u4e0d\u5b89\u5168/\u4e2d\u6027/\u5b89\u5168\uff09\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\uff0c\u91c7\u7528\u52a0\u5bc6\u7279\u5b9a\u7684\u6f0f\u6d1e\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u8bc4\u5206\u7ba1\u9053\u8fdb\u884c\u884c\u7ea7\u5f52\u56e0\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u5728\u53d7\u63a7\u7684\u5b89\u5168\u6307\u5bfc\u6761\u4ef6\u4e0b\u6d4b\u91cfLLM\u751f\u6210\u7684Python\u4ee3\u7801\u4e2d\u7684\u52a0\u5bc6\u6f0f\u6d1e\u53d1\u751f\u7387", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684LLM\u96c6\u5408\u4e0a\u6d4b\u8bd5\u53d1\u73b0\uff0c\u660e\u786e\u7684\"\u5b89\u5168\"\u63d0\u793a\u53ef\u4ee5\u51cf\u5c11\u67d0\u4e9b\u7279\u5b9a\u95ee\u9898\uff0c\u4f46\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u6d88\u9664\u6574\u4f53\u52a0\u5bc6\u6f0f\u6d1e\u3002\u57fa\u51c6\u6d4b\u8bd5\u548c\u53ef\u590d\u73b0\u7684\u8bc4\u5206\u7ba1\u9053\u5c06\u5728\u53d1\u8868\u540e\u516c\u5f00", "conclusion": "LLM\u751f\u6210\u7684\u52a0\u5bc6\u4ee3\u7801\u5b58\u5728\u7cfb\u7edf\u6027\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u3002CIPHER\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u6846\u67b6\u6765\u6d4b\u91cf\u548c\u6bd4\u8f83\u4e0d\u540cLLM\u7684\u52a0\u5bc6\u5b89\u5168\u6027\u80fd\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u5b89\u5168\u7684\u4ee3\u7801\u751f\u6210\u5b9e\u8df5"}}
{"id": "2602.01213", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01213", "abs": "https://arxiv.org/abs/2602.01213", "authors": ["Jungmin Lee", "Inhee Cho", "Youngjae Yoo"], "title": "LeagueBot: A Voice LLM Companion of Cognitive and Emotional Support for Novice Players in Competitive Games", "comment": null, "summary": "Competitive games pose steep learning curves and strong social pressures, often discouraging novice players and limiting sustained engagement. To address these challenges, this study introduces LeagueBot, a large language model-based voice chatbot designed to provide both informational and emotional support during live gameplay in league of legends, one of the most competitive multiplayer online battle arena games. In a within-subjects experiment with 33 novice players, LeagueBot was found to reduce cognitive challenge, performative challenge, and perceived tension. Qualitative analysis further identified three themes: enhanced access to game information, relief from cognitive burden, and practical limitations. Participants noted that LeagueBot offered context-appropriate guidance and emotional support, helping ease the steep learning curve and psychological pressures of competitive gaming. Together, these findings underscore the potential of voice-based LLM companions to assist novice players in competitive environments and highlight their broader applicability for real-time support in other high-pressure contexts.", "AI": {"tldr": "LeagueBot\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u4e3a\u300a\u82f1\u96c4\u8054\u76df\u300b\u65b0\u624b\u73a9\u5bb6\u63d0\u4f9b\u6e38\u620f\u4fe1\u606f\u548c\u60c5\u611f\u652f\u6301\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5b83\u80fd\u964d\u4f4e\u8ba4\u77e5\u6311\u6218\u3001\u8868\u73b0\u6311\u6218\u548c\u611f\u77e5\u538b\u529b\u3002", "motivation": "\u7ade\u4e89\u6027\u6e38\u620f\u5b58\u5728\u9661\u5ced\u7684\u5b66\u4e60\u66f2\u7ebf\u548c\u5f3a\u70c8\u7684\u793e\u4f1a\u538b\u529b\uff0c\u8fd9\u5e38\u5e38\u963b\u788d\u65b0\u624b\u73a9\u5bb6\u53c2\u4e0e\u5e76\u9650\u5236\u6301\u7eed\u53c2\u4e0e\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u6765\u5e2e\u52a9\u65b0\u624b\u73a9\u5bb6\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86LeagueBot\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u5728\u300a\u82f1\u96c4\u8054\u76df\u300b\u5b9e\u65f6\u6e38\u620f\u4e2d\u63d0\u4f9b\u4fe1\u606f\u548c\u60c5\u611f\u652f\u6301\u3002\u7814\u7a76\u91c7\u7528\u7ec4\u5185\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5bf933\u540d\u65b0\u624b\u73a9\u5bb6\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0LeagueBot\u80fd\u964d\u4f4e\u8ba4\u77e5\u6311\u6218\u3001\u8868\u73b0\u6311\u6218\u548c\u611f\u77e5\u538b\u529b\u3002\u5b9a\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u8bc6\u522b\u51fa\u4e09\u4e2a\u4e3b\u9898\uff1a\u589e\u5f3a\u6e38\u620f\u4fe1\u606f\u83b7\u53d6\u3001\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\u4ee5\u53ca\u5b9e\u9645\u9650\u5236\u3002\u53c2\u4e0e\u8005\u6307\u51faLeagueBot\u63d0\u4f9b\u4e86\u60c5\u5883\u9002\u5f53\u7684\u6307\u5bfc\u548c\u60c5\u611f\u652f\u6301\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u57fa\u4e8e\u8bed\u97f3\u7684LLM\u4f34\u4fa3\u5728\u7ade\u4e89\u6027\u73af\u5883\u4e2d\u534f\u52a9\u65b0\u624b\u73a9\u5bb6\u7684\u6f5c\u529b\uff0c\u5e76\u7a81\u51fa\u4e86\u5b83\u4eec\u5728\u5176\u4ed6\u9ad8\u538b\u60c5\u5883\u4e2d\u63d0\u4f9b\u5b9e\u65f6\u652f\u6301\u7684\u66f4\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2602.01018", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01018", "abs": "https://arxiv.org/abs/2602.01018", "authors": ["Chongyu Zhu", "Mithun Vanniasinghe", "Jiayu Chen", "Chi-Guhn Lee"], "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories", "comment": null, "summary": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.", "AI": {"tldr": "LOKI\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u7aef\u5230\u7aef\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u79bb\u7ebf\u591a\u4efb\u52a1\u6f14\u793a\u6570\u636e\u4e2d\u53d1\u73b0\u53ef\u91cd\u7528\u6280\u80fd\u5e76\u8fdb\u884c\u5206\u5c42\u6a21\u4eff\u5b66\u4e60\uff0c\u65e0\u9700\u663e\u5f0f\u5956\u52b1\u6216\u5b50\u4efb\u52a1\u6807\u6ce8\u3002", "motivation": "\u5206\u5c42\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u4ece\u957f\u65f6\u7a0b\u3001\u591a\u4efb\u52a1\u7684\u79bb\u7ebf\u6570\u636e\u4e2d\u53d1\u73b0\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u7f3a\u4e4f\u663e\u5f0f\u5956\u52b1\u6216\u5b50\u4efb\u52a1\u6807\u6ce8\u7684\u6311\u6218\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528\u5bf9\u9f50\u5f3a\u5316\u7684VQ-VAE\u8fdb\u884c\u7c97\u7c92\u5ea6\u4efb\u52a1\u611f\u77e5\u5b8f\u5206\u5272\uff1b2) \u81ea\u76d1\u7763\u5e8f\u5217\u6a21\u578b\u8fdb\u884c\u5fae\u5206\u5272\uff0c\u8fed\u4ee3\u805a\u7c7b\u5de9\u56fa\u6280\u80fd\u8fb9\u754c\uff1b3) \u57fa\u4e8e\u9009\u9879\u6846\u67b6\u6784\u5efa\u5206\u5c42\u7b56\u7565\uff0c\u5b66\u4e60\u663e\u5f0f\u6280\u80fd\u5207\u6362\u7ec8\u6b62\u6761\u4ef6\u3002", "result": "\u5728D4RL Kitchen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9ad8\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u6807\u51c6HIL\u57fa\u7ebf\uff1b\u53d1\u73b0\u7684\u6280\u80fd\u5177\u6709\u8bed\u4e49\u610f\u4e49\uff0c\u7b26\u5408\u4eba\u7c7b\u76f4\u89c9\uff0c\u5e76\u80fd\u901a\u8fc7\u7ec4\u5408\u89e3\u51b3\u672a\u89c1\u4efb\u52a1\u3002", "conclusion": "LOKI\u80fd\u591f\u6709\u6548\u4ece\u65e0\u6807\u6ce8\u79bb\u7ebf\u6570\u636e\u4e2d\u53d1\u73b0\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u5b9e\u73b0\u5206\u5c42\u6a21\u4eff\u5b66\u4e60\uff0c\u5c55\u793a\u51fa\u6280\u80fd\u7684\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u548c\u7ec4\u5408\u6027\u3002"}}
{"id": "2602.01489", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01489", "abs": "https://arxiv.org/abs/2602.01489", "authors": ["Renascence Tarafder Prapty", "Gene Tsudik"], "title": "DuoLungo: Usability Study of Duo 2FA", "comment": null, "summary": "Multi-Factor Authentication (MFA) enhances login security by requiring multiple authentication factors. Its adoption has increased in response to more frequent and sophisticated attacks. Duo is widely used by organizations including Fortune 500 companies and major educational institutions, yet its usability has not been examined thoroughly or recently. Earlier studies focused on technical challenges during initial deployment but did not measure core usability metrics such as task completion time or System Usability Scale (SUS) scores. These results are also outdated, originating from a time when MFA was less familiar to typical users.\n  We conducted a long-term, large-scale Duo usability study at the University of California Irvine during the 2024-2025 academic year, involving 2559 participants. Our analysis uses authentication log data and a survey of 57 randomly selected users. The average overhead of a Duo Push task is nearly 8 seconds, which participants described as short to moderate. Overhead varies with time of day, field of study, and education level. The rate of authentication failures due to incomplete Duo tasks is 4.35 percent, and 43.86 percent of survey respondents reported at least one Duo login failure. The Duo SUS score is 70, indicating good usability. Participants generally find Duo easy to use but somewhat annoying, while also reporting an increased sense of account security. They also described common issues and offered suggestions for improvement.", "AI": {"tldr": "\u5bf9Duo MFA\u7cfb\u7edf\u8fdb\u884c\u7684\u5927\u89c4\u6a21\u53ef\u7528\u6027\u7814\u7a76\uff0c\u53d1\u73b0\u5e73\u5747\u8ba4\u8bc1\u5f00\u9500\u7ea68\u79d2\uff0c\u5931\u8d25\u73874.35%\uff0cSUS\u5f97\u520670\u8868\u660e\u826f\u597d\u53ef\u7528\u6027\uff0c\u7528\u6237\u8ba4\u4e3a\u6613\u7528\u4f46\u6709\u4e9b\u70e6\u4eba", "motivation": "Duo\u4f5c\u4e3a\u5e7f\u6cdb\u4f7f\u7528\u7684MFA\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u53ef\u7528\u6027\u7f3a\u4e4f\u8fd1\u671f\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u90e8\u7f72\u6311\u6218\uff0c\u672a\u6d4b\u91cf\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001SUS\u8bc4\u5206\u7b49\u6838\u5fc3\u53ef\u7528\u6027\u6307\u6807\uff0c\u4e14\u6570\u636e\u5df2\u8fc7\u65f6\uff0c\u5f53\u65f6\u7528\u6237\u5bf9MFA\u719f\u6089\u5ea6\u8f83\u4f4e", "method": "\u5728\u52a0\u5dde\u5927\u5b66\u5c14\u6e7e\u5206\u6821\u8fdb\u884c\u4e3a\u671f\u4e00\u5e74\u7684\u957f\u671f\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u6d89\u53ca2559\u540d\u53c2\u4e0e\u8005\u3002\u7ed3\u5408\u8ba4\u8bc1\u65e5\u5fd7\u6570\u636e\u5206\u6790\uff08\u91cf\u5316\u6307\u6807\uff09\u548c57\u540d\u968f\u673a\u7528\u6237\u7684\u95ee\u5377\u8c03\u67e5\uff08\u5b9a\u6027\u53cd\u9988\uff09\uff0c\u6d4b\u91cf\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u5931\u8d25\u7387\u3001SUS\u8bc4\u5206\u7b49\u6307\u6807", "result": "Duo Push\u4efb\u52a1\u5e73\u5747\u5f00\u9500\u8fd18\u79d2\uff08\u7528\u6237\u63cf\u8ff0\u4e3a\u77ed\u5230\u4e2d\u7b49\uff09\uff1b\u8ba4\u8bc1\u5931\u8d25\u73874.35%\uff0843.86%\u53d7\u8bbf\u8005\u81f3\u5c11\u7ecf\u5386\u8fc7\u4e00\u6b21\u5931\u8d25\uff09\uff1bSUS\u5f97\u520670\uff08\u826f\u597d\u53ef\u7528\u6027\uff09\u3002\u5f00\u9500\u53d7\u65f6\u95f4\u3001\u4e13\u4e1a\u9886\u57df\u3001\u6559\u80b2\u6c34\u5e73\u5f71\u54cd\u3002\u7528\u6237\u8ba4\u4e3a\u7cfb\u7edf\u6613\u7528\u4f46\u6709\u4e9b\u70e6\u4eba\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u8d26\u6237\u5b89\u5168\u611f", "conclusion": "Duo MFA\u7cfb\u7edf\u5177\u6709\u826f\u597d\u53ef\u7528\u6027\uff08SUS 70\uff09\uff0c\u4f46\u5b58\u5728\u663e\u8457\u8ba4\u8bc1\u5f00\u9500\u548c\u5931\u8d25\u7387\u3002\u7528\u6237\u63a5\u53d7\u5ea6\u8f83\u9ad8\uff0c\u8ba4\u4e3a\u6613\u7528\u4e14\u589e\u5f3a\u5b89\u5168\uff0c\u4f46\u4f53\u9a8c\u4e2d\u5b58\u5728\u70e6\u6270\u611f\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u5f53\u524dMFA\u53ef\u7528\u6027\u57fa\u51c6\uff0c\u5e76\u8bc6\u522b\u4e86\u6539\u8fdb\u673a\u4f1a"}}
{"id": "2602.01264", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01264", "abs": "https://arxiv.org/abs/2602.01264", "authors": ["Jonatan Reyes", "Mina Massoumi", "Anil Ufuk Batmaz", "Marta Kersten-Oertel"], "title": "Shades of Uncertainty: How AI Uncertainty Visualizations Affect Trust in Alzheimer's Predictions", "comment": null, "summary": "Artificial intelligence (AI) is increasingly used to support prognosis in Alzheimer's disease (AD), but adoption remains limited due to a lack of transparency and interpretability, particularly for long-term predictions where uncertainty is intrinsic and outcomes may not be known for years. We position uncertainty visualization as an explainable AI (XAI) technique and examine how it shapes trust, confidence, and reliance when users interpret AI-generated forecasts of future cognitive decline transitions. We conducted two studies, one with general participants (N=37) and one with experts in neuroimaging and neurology (N=10), to compare binary (present/absent) and continuous (saturation) uncertainty encodings. Continuous encodings improved perceived reliability and helped users recognize model limitations, while binary encodings increased momentary confidence, revealing expertise-dependent trade-offs in interpreting future predictions under high uncertainty. These findings surface key challenges in designing uncertainty representations for prognostic AI and culminate in a set of empirically grounded guidelines for creating trustworthy, user-appropriate clinical decision support tools.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e8c\u5143\u548c\u8fde\u7eed\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u5bf9\u7528\u6237\u4fe1\u4efb\u3001\u4fe1\u5fc3\u548c\u4f9d\u8d56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8fde\u7eed\u7f16\u7801\u63d0\u5347\u611f\u77e5\u53ef\u9760\u6027\uff0c\u4e8c\u5143\u7f16\u7801\u589e\u52a0\u77ac\u65f6\u4fe1\u5fc3\uff0c\u63ed\u793a\u4e86\u4e13\u4e1a\u4f9d\u8d56\u7684\u6743\u8861", "motivation": "AI\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u540e\u4e2d\u7684\u5e94\u7528\u56e0\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u800c\u53d7\u9650\uff0c\u7279\u522b\u662f\u5728\u957f\u671f\u9884\u6d4b\u4e2d\uff0c\u4e0d\u786e\u5b9a\u6027\u662f\u56fa\u6709\u7684\u4e14\u7ed3\u679c\u53ef\u80fd\u591a\u5e74\u540e\u624d\u77e5\u6653\u3002\u7814\u7a76\u5c06\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u5b9a\u4f4d\u4e3a\u53ef\u89e3\u91caAI\u6280\u672f\uff0c\u63a2\u8ba8\u5176\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9AI\u751f\u6210\u672a\u6765\u8ba4\u77e5\u8870\u9000\u9884\u6d4b\u7684\u4fe1\u4efb\u3001\u4fe1\u5fc3\u548c\u4f9d\u8d56", "method": "\u8fdb\u884c\u4e86\u4e24\u9879\u7814\u7a76\uff1a\u4e00\u9879\u9488\u5bf9\u666e\u901a\u53c2\u4e0e\u8005(N=37)\uff0c\u53e6\u4e00\u9879\u9488\u5bf9\u795e\u7ecf\u5f71\u50cf\u5b66\u548c\u795e\u7ecf\u5b66\u4e13\u5bb6(N=10)\u3002\u6bd4\u8f83\u4e86\u4e8c\u5143(\u5b58\u5728/\u4e0d\u5b58\u5728)\u548c\u8fde\u7eed(\u9971\u548c\u5ea6)\u4e0d\u786e\u5b9a\u6027\u7f16\u7801\u65b9\u5f0f\uff0c\u5206\u6790\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9AI\u9884\u6d4b\u7684\u4fe1\u4efb\u3001\u4fe1\u5fc3\u548c\u4f9d\u8d56", "result": "\u8fde\u7eed\u4e0d\u786e\u5b9a\u6027\u7f16\u7801\u63d0\u9ad8\u4e86\u611f\u77e5\u53ef\u9760\u6027\uff0c\u5e2e\u52a9\u7528\u6237\u8ba4\u8bc6\u5230\u6a21\u578b\u7684\u5c40\u9650\u6027\uff1b\u800c\u4e8c\u5143\u7f16\u7801\u589e\u52a0\u4e86\u77ac\u65f6\u4fe1\u5fc3\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u4e0b\u89e3\u91ca\u672a\u6765\u9884\u6d4b\u65f6\u5b58\u5728\u4e13\u4e1a\u4f9d\u8d56\u7684\u6743\u8861", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u8bbe\u8ba1\u9884\u540eAI\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u6700\u7ec8\u5f62\u6210\u4e86\u4e00\u5957\u57fa\u4e8e\u5b9e\u8bc1\u7684\u6307\u5bfc\u539f\u5219\uff0c\u7528\u4e8e\u521b\u5efa\u53ef\u4fe1\u8d56\u3001\u9002\u5408\u7528\u6237\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177"}}
{"id": "2602.01040", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01040", "abs": "https://arxiv.org/abs/2602.01040", "authors": ["Yuhang Zhang", "Chao Yan", "Jiaxi Yu", "Jiaping Xiao", "Mir Feroskhan"], "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration", "comment": null, "summary": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.", "AI": {"tldr": "CAPO\uff1a\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u63d0\u793a\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u63d0\u793a\u7f16\u6392\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u8de8\u5177\u8eab\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u96f6\u6837\u672c\u9002\u5e94\u80fd\u529b", "motivation": "\u4f20\u7edf\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5206\u79bb\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u4e0e\u9886\u57df\u7279\u5b9a\u53d8\u5316\uff08\u5982\u5149\u7167\u3001\u89c6\u91ce\u3001\u65cb\u8f6c\uff09\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u4e14\u5728\u672a\u89c1\u73af\u5883\u4e2d\u51fa\u73b0\u707e\u96be\u6027\u5931\u8d25\u3002\u9700\u8981\u89e3\u51b3\u8de8\u5177\u8eab\u667a\u80fd\u4f53\u53d8\u4f53\uff08\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u548c\u52a8\u6001\u7279\u6027\uff09\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faContrAstive Prompt Orchestration (CAPO)\uff1a1\uff09\u6df7\u5408\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff1a\u6574\u5408\u89c6\u89c9\u3001\u65f6\u5e8f\u52a8\u4f5c\u548c\u6587\u672c\u76ee\u6807\uff0c\u5efa\u7acb\u53ef\u5b66\u4e60\u63d0\u793a\u6c60\uff0c\u6bcf\u4e2a\u63d0\u793a\u7f16\u7801\u7ec6\u7c92\u5ea6\u9886\u57df\u56e0\u7d20\uff1b2\uff09\u81ea\u9002\u5e94\u63d0\u793a\u7f16\u6392\u673a\u5236\uff1a\u57fa\u4e8e\u5f53\u524d\u89c2\u5bdf\u52a8\u6001\u805a\u5408\u8fd9\u4e9b\u63d0\u793a\uff0c\u81ea\u9002\u5e94\u6784\u5efa\u6700\u4f18\u72b6\u6001\u8868\u793a\u3002", "result": "CAPO\u5728\u6837\u672c\u6548\u7387\u548c\u6e10\u8fdb\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u5177\u6709\u5267\u70c8\u73af\u5883\u53d8\u5316\uff08\u5982\u5149\u7167\uff09\u548c\u7269\u7406\u53d8\u5316\uff08\u5982\u89c6\u91ce\u548c\u65cb\u8f6c\uff09\u7684\u672a\u89c1\u76ee\u6807\u57df\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "CAPO\u901a\u8fc7\u5bf9\u6bd4\u63d0\u793a\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u7f16\u6392\uff0c\u6709\u6548\u5c4f\u853d\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u65e0\u5173\u5e72\u6270\uff0c\u9632\u6b62\u5bf9\u6e90\u57df\u7684\u8fc7\u62df\u5408\uff0c\u4e3a\u8de8\u5177\u8eab\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00129", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00129", "abs": "https://arxiv.org/abs/2602.00129", "authors": ["Yixuan Liang"], "title": "Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models", "comment": "10 pages, 5 figures. Submitted to a conference workshop", "summary": "Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.", "AI": {"tldr": "CodePilot\uff1a\u7ed3\u5408MCTS\u4e0eLLM\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6267\u884c\u5f15\u5bfc\u7684\u7a0b\u5e8f\u4fee\u590d\u89e3\u51b3GitHub\u95ee\u9898\uff0c\u5728SWE-bench Lite\u4e0a\u8fbe\u523024.67%\u7684\u95ee\u9898\u89e3\u51b3\u7387", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u5728\u4ed3\u5e93\u7ea7\u522b\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u957f\u65f6\u7a0b\u63a8\u7406\u9700\u6c42\u548c\u81ea\u56de\u5f52\u89e3\u7801\u7684\u9650\u5236\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6267\u884c\u5f15\u5bfc\u4fee\u590d\u65b9\u6cd5", "method": "\u96c6\u6210\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u8fdb\u884c\u4ece\u4ed3\u5e93\u5230\u6587\u4ef6\u518d\u5230\u51fd\u6570\u7684\u5206\u5c42\u6545\u969c\u5b9a\u4f4d\uff0c\u5229\u7528MCTS\u63a2\u7d22\u591a\u6837\u5316\u8865\u4e01\u8f68\u8ff9\uff0c\u5c06\u6267\u884c\u53cd\u9988\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\u641c\u7d22\u548c\u4f18\u5316\uff0c\u5e76\u91c7\u7528\u7f6e\u4fe1\u5ea6\u6821\u51c6\u751f\u6210\u9009\u62e9\u6027\u4f18\u5316\u4f4e\u7f6e\u4fe1\u5ea6\u8f93\u51fa", "result": "\u5728SWE-bench Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCodePilot\u4f7f\u7528\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u5b9e\u73b0\u4e8624.67%\u7684\u95ee\u9898\u89e3\u51b3\u7387\uff0c\u4f18\u4e8e\u53ef\u6bd4\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u5c06\u7b26\u53f7\u641c\u7d22\u4e0e\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u6267\u884c\u611f\u77e5\u7684\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u7684\u6709\u6548\u7b56\u7565"}}
{"id": "2602.01491", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01491", "abs": "https://arxiv.org/abs/2602.01491", "authors": ["Sahan Sanjaya", "Prabhat Mishra"], "title": "Sleep Reveals the Nonce: Breaking ECDSA using Sleep-Based Power Side-Channel Vulnerability", "comment": null, "summary": "Security of Elliptic Curve Digital Signature Algorithm (ECDSA) depends on the secrecy of the per-signature nonce. Even partial nonce leakage can expose the long-term private key through lattice-based cryptanalysis. In this paper, we introduce a previously unexplored power side-channel vulnerability that exploits sleep-induced power spikes to extract ECDSA nonces. Unlike conventional power-based side-channel attacks, this vulnerability leverages power fluctuations generated during processor context switches invoked by sleep functions. These fluctuations correlate with nonce-dependent operations in scalar multiplication, enabling nonce recovery even under constant-time and masked implementations. We evaluate the attack across multiple cryptographic libraries, RustCrypto, BearSSL, and GoCrypto, and processor architectures, including ARM and RISC-V. Our experiments show that subtle variations in the power envelope during sleep-induced context switches provide sufficient leakage for practical ECDSA nonce extraction, recovering 20 bits of the nonce. These results establish sleep-induced power spikes as a practical cross-platform side-channel threat and highlight the need to reconsider design choices in cryptographic systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u4fa7\u4fe1\u9053\u653b\u51fb\uff1a\u5229\u7528\u7761\u7720\u51fd\u6570\u5f15\u53d1\u7684\u5904\u7406\u5668\u4e0a\u4e0b\u6587\u5207\u6362\u4ea7\u751f\u7684\u529f\u7387\u5c16\u5cf0\u6765\u63d0\u53d6ECDSA\u7b7e\u540d\u4e2d\u7684\u968f\u673a\u6570\uff0c\u5373\u4f7f\u662f\u5728\u6052\u5b9a\u65f6\u95f4\u548c\u63a9\u7801\u5b9e\u73b0\u4e0b\u4e5f\u80fd\u6210\u529f\u3002", "motivation": "ECDSA\u7684\u5b89\u5168\u6027\u4f9d\u8d56\u4e8e\u6bcf\u4e2a\u7b7e\u540d\u968f\u673a\u6570\u7684\u4fdd\u5bc6\u6027\uff0c\u5373\u4f7f\u90e8\u5206\u968f\u673a\u6570\u6cc4\u9732\u4e5f\u53ef\u80fd\u901a\u8fc7\u683c\u57fa\u5bc6\u7801\u5206\u6790\u66b4\u9732\u957f\u671f\u79c1\u94a5\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f20\u7edf\u529f\u7387\u4fa7\u4fe1\u9053\u653b\u51fb\uff0c\u4f46\u5c1a\u672a\u63a2\u7d22\u7761\u7720\u51fd\u6570\u5f15\u53d1\u7684\u5904\u7406\u5668\u4e0a\u4e0b\u6587\u5207\u6362\u4ea7\u751f\u7684\u529f\u7387\u6ce2\u52a8\u4f5c\u4e3a\u4fa7\u4fe1\u9053\u6f0f\u6d1e\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u7761\u7720\u8bf1\u5bfc\u529f\u7387\u5c16\u5cf0\u7684\u65b0\u578b\u4fa7\u4fe1\u9053\u653b\u51fb\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5904\u7406\u5668\u5728\u8c03\u7528\u7761\u7720\u51fd\u6570\u8fdb\u884c\u4e0a\u4e0b\u6587\u5207\u6362\u65f6\u4ea7\u751f\u7684\u529f\u7387\u6ce2\u52a8\uff0c\u8fd9\u4e9b\u6ce2\u52a8\u4e0e\u6807\u91cf\u4e58\u6cd5\u4e2d\u4f9d\u8d56\u4e8e\u968f\u673a\u6570\u7684\u64cd\u4f5c\u76f8\u5173\u3002\u653b\u51fb\u8bc4\u4f30\u4e86\u591a\u4e2a\u5bc6\u7801\u5e93\uff08RustCrypto\u3001BearSSL\u3001GoCrypto\uff09\u548c\u5904\u7406\u5668\u67b6\u6784\uff08ARM\u3001RISC-V\uff09\uff0c\u901a\u8fc7\u5206\u6790\u7761\u7720\u671f\u95f4\u529f\u7387\u5305\u7edc\u7684\u7ec6\u5fae\u53d8\u5316\u6765\u63d0\u53d6\u968f\u673a\u6570\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7761\u7720\u8bf1\u5bfc\u4e0a\u4e0b\u6587\u5207\u6362\u671f\u95f4\u529f\u7387\u5305\u7edc\u7684\u7ec6\u5fae\u53d8\u5316\u63d0\u4f9b\u4e86\u8db3\u591f\u7684\u6cc4\u9732\u4fe1\u606f\uff0c\u80fd\u591f\u5b9e\u9645\u63d0\u53d6ECDSA\u968f\u673a\u6570\uff0c\u6210\u529f\u6062\u590d\u4e8620\u4f4d\u968f\u673a\u6570\u3002\u653b\u51fb\u5728\u6052\u5b9a\u65f6\u95f4\u548c\u63a9\u7801\u5b9e\u73b0\u4e0b\u4ecd\u7136\u6709\u6548\uff0c\u8bc1\u660e\u4e86\u8be5\u6f0f\u6d1e\u7684\u5b9e\u7528\u6027\u548c\u8de8\u5e73\u53f0\u5a01\u80c1\u3002", "conclusion": "\u7761\u7720\u8bf1\u5bfc\u529f\u7387\u5c16\u5cf0\u6784\u6210\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u8de8\u5e73\u53f0\u4fa7\u4fe1\u9053\u5a01\u80c1\uff0c\u66b4\u9732\u4e86\u5bc6\u7801\u7cfb\u7edf\u8bbe\u8ba1\u9009\u62e9\u4e2d\u7684\u65b0\u6f0f\u6d1e\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u9700\u8981\u91cd\u65b0\u8003\u8651\u5bc6\u7801\u7cfb\u7edf\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u4ee5\u9632\u8303\u8fd9\u79cd\u65b0\u578b\u4fa7\u4fe1\u9053\u653b\u51fb\u3002"}}
{"id": "2602.01386", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01386", "abs": "https://arxiv.org/abs/2602.01386", "authors": ["Qing", "Xia", "Marios Constantinides", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "title": "\"If You're Very Clever, No One Knows You've Used It\": The Social Dynamics of Developing Generative AI Literacy in the Workplace", "comment": "CHIWORK 2026", "summary": "Generative AI (GenAI) tools are rapidly transforming knowledge work, making AI literacy a critical priority for organizations. However, research on AI literacy lacks empirical insight into how knowledge workers' beliefs around GenAI literacy are shaped by the social dynamics of the workplace, and how workers learn to apply GenAI tools in these environments. To address this gap, we conducted in-depth interviews with 19 knowledge workers across multiple sectors to examine how they develop GenAI competencies in real-world professional contexts. We found that, while knowledge sharing from colleagues supported learning, the ability to remove cues indicating GenAI use was perceived as validation of domain expertise. These behaviours ultimately reduced opportunities for learning via knowledge sharing and undermined transparency. To advance workplace AI literacy, we argue for fostering open dialogue, increasing visibility of user-generated knowledge, and greater emphasis on the benefits of collaborative learning for navigating rapid technological developments.", "AI": {"tldr": "\u77e5\u8bc6\u5de5\u4f5c\u8005\u5728\u804c\u573a\u4e2d\u901a\u8fc7\u540c\u4e8b\u77e5\u8bc6\u5206\u4eab\u5b66\u4e60GenAI\uff0c\u4f46\u9690\u85cfAI\u4f7f\u7528\u75d5\u8ff9\u88ab\u89c6\u4e3a\u4e13\u4e1a\u9a8c\u8bc1\uff0c\u53cd\u800c\u51cf\u5c11\u4e86\u5b66\u4e60\u673a\u4f1a\u548c\u900f\u660e\u5ea6", "motivation": "\u751f\u6210\u5f0fAI\u5de5\u5177\u6b63\u5728\u5feb\u901f\u6539\u53d8\u77e5\u8bc6\u5de5\u4f5c\uff0c\u4f7fAI\u7d20\u517b\u6210\u4e3a\u7ec4\u7ec7\u7684\u5173\u952e\u4f18\u5148\u4e8b\u9879\u3002\u7136\u800c\uff0c\u73b0\u6709AI\u7d20\u517b\u7814\u7a76\u7f3a\u4e4f\u5bf9\u77e5\u8bc6\u5de5\u4f5c\u8005\u5982\u4f55\u5728\u5de5\u4f5c\u573a\u6240\u793e\u4f1a\u52a8\u6001\u4e2d\u5f62\u6210GenAI\u7d20\u517b\u4fe1\u5ff5\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\u5b66\u4e60\u5e94\u7528GenAI\u5de5\u5177\u7684\u5b9e\u8bc1\u6d1e\u5bdf\u3002", "method": "\u901a\u8fc7\u5bf9\u591a\u4e2a\u884c\u4e1a\u768419\u540d\u77e5\u8bc6\u5de5\u4f5c\u8005\u8fdb\u884c\u6df1\u5ea6\u8bbf\u8c08\uff0c\u7814\u7a76\u4ed6\u4eec\u5728\u771f\u5b9e\u4e13\u4e1a\u73af\u5883\u4e2d\u5982\u4f55\u53d1\u5c55GenAI\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u867d\u7136\u540c\u4e8b\u95f4\u7684\u77e5\u8bc6\u5206\u4eab\u652f\u6301\u5b66\u4e60\uff0c\u4f46\u80fd\u591f\u79fb\u9664\u8868\u660eGenAI\u4f7f\u7528\u7684\u7ebf\u7d22\u88ab\u89c6\u4e3a\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u9a8c\u8bc1\u3002\u8fd9\u4e9b\u884c\u4e3a\u6700\u7ec8\u51cf\u5c11\u4e86\u901a\u8fc7\u77e5\u8bc6\u5206\u4eab\u5b66\u4e60\u7684\u673a\u4f1a\uff0c\u5e76\u7834\u574f\u4e86\u900f\u660e\u5ea6\u3002", "conclusion": "\u4e3a\u63a8\u8fdb\u5de5\u4f5c\u573a\u6240AI\u7d20\u517b\uff0c\u9700\u8981\u4fc3\u8fdb\u5f00\u653e\u5bf9\u8bdd\uff0c\u589e\u52a0\u7528\u6237\u751f\u6210\u77e5\u8bc6\u7684\u53ef\u89c1\u6027\uff0c\u5e76\u66f4\u52a0\u5f3a\u8c03\u534f\u4f5c\u5b66\u4e60\u5728\u5e94\u5bf9\u5feb\u901f\u6280\u672f\u53d1\u5c55\u4e2d\u7684\u76ca\u5904\u3002"}}
{"id": "2602.01067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01067", "abs": "https://arxiv.org/abs/2602.01067", "authors": ["Fanqi Lin", "Kushal Arora", "Jean Mercat", "Haruki Nishimura", "Paarth Shah", "Chen Xu", "Mengchao Zhang", "Mark Zolotas", "Maya Angeles", "Owen Pfannenstiehl", "Andrew Beaulieu", "Jose Barreiros"], "title": "A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation", "comment": null, "summary": "Large behavior models have shown strong dexterous manipulation capabilities by extending imitation learning to large-scale training on multi-task robot data, yet their generalization remains limited by the insufficient robot data coverage. To expand this coverage without costly additional data collection, recent work relies on co-training: jointly learning from target robot data and heterogeneous data modalities. However, how different co-training data modalities and strategies affect policy performance remains poorly understood. We present a large-scale empirical study examining five co-training data modalities: standard vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies. Our study leverages 4,000 hours of robot and human manipulation data and 50M vision-language samples to train vision-language-action policies. We evaluate 89 policies over 58,000 simulation rollouts and 2,835 real-world rollouts. Our results show that co-training with forms of vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following, while discrete action token variants yield no significant benefits. Combining effective modalities produces cumulative gains and enables rapid adaptation to unseen long-horizon dexterous tasks via fine-tuning. Training exclusively on robot data degrades the visiolinguistic understanding of the vision-language model backbone, while co-training with effective modalities restores these capabilities. Explicitly conditioning action generation on chain-of-thought traces learned from co-training data does not improve performance in our simulation benchmark. Together, these results provide practical guidance for building scalable generalist robot policies.", "AI": {"tldr": "\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u63a2\u7d22\u4e94\u79cd\u534f\u540c\u8bad\u7ec3\u6570\u636e\u6a21\u6001\u5bf9\u673a\u5668\u4eba\u7b56\u7565\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u89c6\u89c9-\u8bed\u8a00\u548c\u8de8\u5177\u8eab\u673a\u5668\u4eba\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\u65e0\u663e\u8457\u76ca\u5904\u3002", "motivation": "\u5927\u578b\u884c\u4e3a\u6a21\u578b\u901a\u8fc7\u591a\u4efb\u52a1\u673a\u5668\u4eba\u6570\u636e\u7684\u6a21\u4eff\u5b66\u4e60\u5c55\u73b0\u51fa\u5f3a\u5927\u7075\u5de7\u64cd\u4f5c\u80fd\u529b\uff0c\u4f46\u5176\u6cdb\u5316\u53d7\u9650\u4e8e\u673a\u5668\u4eba\u6570\u636e\u8986\u76d6\u4e0d\u8db3\u3002\u4e3a\u5728\u4e0d\u589e\u52a0\u6570\u636e\u6536\u96c6\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u6269\u5c55\u8986\u76d6\u8303\u56f4\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u534f\u540c\u8bad\u7ec3\u6570\u636e\u6a21\u6001\u548c\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u7b56\u7565\u6027\u80fd\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u8003\u5bdf\u4e94\u79cd\u534f\u540c\u8bad\u7ec3\u6570\u636e\u6a21\u6001\uff1a\u6807\u51c6\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u3001\u673a\u5668\u4eba\u8f68\u8ff9\u7684\u5bc6\u96c6\u8bed\u8a00\u6807\u6ce8\u3001\u8de8\u5177\u8eab\u673a\u5668\u4eba\u6570\u636e\u3001\u4eba\u7c7b\u89c6\u9891\u548c\u79bb\u6563\u673a\u5668\u4eba\u52a8\u4f5c\u6807\u8bb0\uff0c\u91c7\u7528\u5355\u9636\u6bb5\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002\u5229\u75284000\u5c0f\u65f6\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u53ca5000\u4e07\u89c6\u89c9-\u8bed\u8a00\u6837\u672c\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\uff0c\u8bc4\u4f3089\u4e2a\u7b56\u7565\u572858000\u6b21\u6a21\u62df\u8fd0\u884c\u548c2835\u6b21\u771f\u5b9e\u4e16\u754c\u8fd0\u884c\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u89c6\u89c9-\u8bed\u8a00\u548c\u8de8\u5177\u8eab\u673a\u5668\u4eba\u6570\u636e\u7684\u534f\u540c\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u5bf9\u5206\u5e03\u504f\u79fb\u3001\u672a\u89c1\u4efb\u52a1\u548c\u8bed\u8a00\u8ddf\u968f\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\u53d8\u4f53\u65e0\u663e\u8457\u76ca\u5904\uff1b\u6709\u6548\u6a21\u6001\u7ec4\u5408\u4ea7\u751f\u7d2f\u79ef\u589e\u76ca\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u672a\u89c1\u957f\u65f6\u57df\u7075\u5de7\u4efb\u52a1\uff1b\u4ec5\u4f7f\u7528\u673a\u5668\u4eba\u6570\u636e\u8bad\u7ec3\u4f1a\u635f\u5bb3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u7684\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u800c\u6709\u6548\u6a21\u6001\u534f\u540c\u8bad\u7ec3\u53ef\u6062\u590d\u8fd9\u4e9b\u80fd\u529b\uff1b\u57fa\u4e8e\u534f\u540c\u8bad\u7ec3\u6570\u636e\u5b66\u4e60\u7684\u601d\u7ef4\u94fe\u8f68\u8ff9\u663e\u5f0f\u6761\u4ef6\u5316\u52a8\u4f5c\u751f\u6210\u5728\u6a21\u62df\u57fa\u51c6\u4e2d\u672a\u6539\u5584\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff1a\u89c6\u89c9-\u8bed\u8a00\u548c\u8de8\u5177\u8eab\u673a\u5668\u4eba\u6570\u636e\u662f\u6709\u6548\u7684\u534f\u540c\u8bad\u7ec3\u6a21\u6001\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\u7b56\u7565\u6548\u679c\u6709\u9650\uff0c\u6709\u6548\u6a21\u6001\u7ec4\u5408\u53ef\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2602.01544", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01544", "abs": "https://arxiv.org/abs/2602.01544", "authors": ["Sarah Tabassum"], "title": "Are Security Cues Static? Rethinking Warning and Trust Indicators for Life Transitions", "comment": null, "summary": "Security cues, such as warnings and trust signals, are designed as stable interface elements, even though people's lives, contexts, and vulnerabilities change over time. Life transitions including migration, aging, or shifts in institutional environments reshape how risk and trust are understood and acted upon. Yet current systems rarely adapt their security cues to these changing conditions, placing the burden of interpretation on users. In this Works-in-Progress paper, we argue that the static nature of security cues represents a design mismatch with transitional human lives. We draw on prior empirical insights from work on educational migration as a motivating case, and extend the discussion to other life transitions. Building on these insights, we introduce the Transition-Aware Security Cues (TASeC) framework and present speculative design concepts illustrating how security cues might evolve across transition stages. We invite HCI to rethink security cues as longitudinal, life-centered design elements collectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5b89\u5168\u63d0\u793a\uff08\u5982\u8b66\u544a\u548c\u4fe1\u4efb\u4fe1\u53f7\uff09\u5e94\u9002\u5e94\u4eba\u751f\u8fc7\u6e21\u9636\u6bb5\uff08\u5982\u79fb\u6c11\u3001\u8001\u9f84\u5316\uff09\uff0c\u800c\u975e\u4fdd\u6301\u9759\u6001\uff0c\u5e76\u5f15\u5165\u4e86\u8fc7\u6e21\u611f\u77e5\u5b89\u5168\u63d0\u793a\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u5b89\u5168\u63d0\u793a\u8bbe\u8ba1\u4e3a\u9759\u6001\u754c\u9762\u5143\u7d20\uff0c\u4f46\u4eba\u4eec\u7684\u751f\u6d3b\u3001\u80cc\u666f\u548c\u8106\u5f31\u6027\u4f1a\u968f\u65f6\u95f4\u53d8\u5316\u3002\u4eba\u751f\u8fc7\u6e21\uff08\u5982\u79fb\u6c11\u3001\u8001\u9f84\u5316\u3001\u73af\u5883\u53d8\u5316\uff09\u4f1a\u91cd\u5851\u98ce\u9669\u4e0e\u4fe1\u4efb\u7684\u7406\u89e3\u65b9\u5f0f\uff0c\u800c\u73b0\u6709\u7cfb\u7edf\u5f88\u5c11\u6839\u636e\u8fd9\u4e9b\u53d8\u5316\u8c03\u6574\u5b89\u5168\u63d0\u793a\uff0c\u5c06\u89e3\u91ca\u8d1f\u62c5\u8f6c\u5ac1\u7ed9\u7528\u6237\u3002", "method": "\u57fa\u4e8e\u6559\u80b2\u79fb\u6c11\u7684\u5b9e\u8bc1\u7814\u7a76\u4f5c\u4e3a\u6848\u4f8b\uff0c\u6269\u5c55\u5230\u5176\u4ed6\u4eba\u751f\u8fc7\u6e21\u9636\u6bb5\u3002\u63d0\u51fa\u4e86\u8fc7\u6e21\u611f\u77e5\u5b89\u5168\u63d0\u793a\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u63a8\u6d4b\u6027\u8bbe\u8ba1\u6982\u5ff5\u5c55\u793a\u5b89\u5168\u63d0\u793a\u5982\u4f55\u5728\u4e0d\u540c\u8fc7\u6e21\u9636\u6bb5\u6f14\u53d8\u3002", "result": "\u63d0\u51fa\u4e86TASeC\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5b89\u5168\u63d0\u793a\u5982\u4f55\u9002\u5e94\u4eba\u751f\u8fc7\u6e21\u9636\u6bb5\u7684\u8bbe\u8ba1\u6982\u5ff5\uff0c\u547c\u5401HCI\u9886\u57df\u91cd\u65b0\u601d\u8003\u5b89\u5168\u63d0\u793a\u4f5c\u4e3a\u7eb5\u5411\u3001\u4ee5\u751f\u6d3b\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u5143\u7d20\u3002", "conclusion": "\u5b89\u5168\u63d0\u793a\u7684\u9759\u6001\u6027\u8d28\u4e0e\u8fc7\u6e21\u6027\u4eba\u751f\u5b58\u5728\u8bbe\u8ba1\u4e0d\u5339\u914d\uff0c\u9700\u8981\u91cd\u65b0\u6784\u60f3\u5b89\u5168\u63d0\u793a\u4e3a\u80fd\u591f\u9002\u5e94\u4eba\u751f\u53d8\u5316\u7684\u8bbe\u8ba1\u5143\u7d20\uff0c\u4fc3\u8fdbHCI\u9886\u57df\u5bf9\u6b64\u8fdb\u884c\u96c6\u4f53\u53cd\u601d\u3002"}}
{"id": "2602.01387", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01387", "abs": "https://arxiv.org/abs/2602.01387", "authors": ["Ziwen Li", "Ziang Xiao", "Tianshi Li"], "title": "Disclose with Care: Designing Privacy Controls in Interview Chatbots", "comment": "25 pages, 14 figures", "summary": "Collecting data on sensitive topics remains challenging in HCI, as participants often withhold information due to privacy concerns and social desirability bias. While chatbots' perceived anonymity may reduce these barriers, research paradoxically suggests people tend to over-share personal or sensitive information with chatbots. In this work, we explore privacy controls in chatbot interviews to address this problem. The privacy control allows participants to revise their transcripts at the end of the interview, featuring two design variants: free editing and AI-aided editing. In a between-subjects study \\red{($N=188$)}, we compared no-editing, free-editing, and AI-aided editing conditions in a chatbot-based interview on a sensitive topic. Our results confirm the prevalent issue of oversharing in chatbot-based interviews and show that AI-aided editing serves as an effective privacy-control mechanism, reducing PII disclosure while maintaining data quality and user engagement, thereby offering a promising approach to balancing ethical practice and data quality in such interviews.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u804a\u5929\u673a\u5668\u4eba\u8bbf\u8c08\u4e2d\u7684\u9690\u79c1\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u5141\u8bb8\u53c2\u4e0e\u8005\u7f16\u8f91\u8bbf\u8c08\u8bb0\u5f55\u6765\u51cf\u5c11\u654f\u611f\u4fe1\u606f\u8fc7\u5ea6\u5206\u4eab\u95ee\u9898\uff0c\u53d1\u73b0AI\u8f85\u52a9\u7f16\u8f91\u80fd\u6709\u6548\u964d\u4f4e\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u6cc4\u9732\u540c\u65f6\u4fdd\u6301\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u5728HCI\u7814\u7a76\u4e2d\uff0c\u654f\u611f\u8bdd\u9898\u6570\u636e\u6536\u96c6\u9762\u4e34\u6311\u6218\uff0c\u53c2\u4e0e\u8005\u5e38\u56e0\u9690\u79c1\u987e\u8651\u548c\u793e\u4f1a\u671f\u671b\u504f\u5dee\u800c\u9690\u7792\u4fe1\u606f\u3002\u867d\u7136\u804a\u5929\u673a\u5668\u4eba\u611f\u77e5\u7684\u533f\u540d\u6027\u53ef\u80fd\u964d\u4f4e\u8fd9\u4e9b\u969c\u788d\uff0c\u4f46\u7814\u7a76\u8868\u660e\u4eba\u4eec\u53cd\u800c\u503e\u5411\u4e8e\u5411\u804a\u5929\u673a\u5668\u4eba\u8fc7\u5ea6\u5206\u4eab\u4e2a\u4eba\u6216\u654f\u611f\u4fe1\u606f\uff0c\u8fd9\u6784\u6210\u4e86\u7814\u7a76\u52a8\u673a\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7ec4\u95f4\u8bbe\u8ba1\uff08N=188\uff09\uff0c\u6bd4\u8f83\u4e09\u79cd\u6761\u4ef6\uff1a\u65e0\u7f16\u8f91\u3001\u81ea\u7531\u7f16\u8f91\u548cAI\u8f85\u52a9\u7f16\u8f91\u3002\u5728\u804a\u5929\u673a\u5668\u4eba\u8bbf\u8c08\u4e2d\u5f15\u5165\u9690\u79c1\u63a7\u5236\u673a\u5236\uff0c\u5141\u8bb8\u53c2\u4e0e\u8005\u5728\u8bbf\u8c08\u7ed3\u675f\u540e\u4fee\u6539\u8bb0\u5f55\u3002AI\u8f85\u52a9\u7f16\u8f91\u63d0\u4f9b\u81ea\u52a8\u68c0\u6d4b\u548c\u7f16\u8f91\u5efa\u8bae\u529f\u80fd\u3002", "result": "\u7ed3\u679c\u8bc1\u5b9e\u4e86\u804a\u5929\u673a\u5668\u4eba\u8bbf\u8c08\u4e2d\u666e\u904d\u5b58\u5728\u7684\u8fc7\u5ea6\u5206\u4eab\u95ee\u9898\u3002AI\u8f85\u52a9\u7f16\u8f91\u4f5c\u4e3a\u6709\u6548\u7684\u9690\u79c1\u63a7\u5236\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\uff08PII\uff09\u7684\u62ab\u9732\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u636e\u8d28\u91cf\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002\u81ea\u7531\u7f16\u8f91\u6761\u4ef6\u7684\u6548\u679c\u4e0d\u5982AI\u8f85\u52a9\u7f16\u8f91\u3002", "conclusion": "AI\u8f85\u52a9\u7f16\u8f91\u4e3a\u804a\u5929\u673a\u5668\u4eba\u8bbf\u8c08\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u9690\u79c1\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4f26\u7406\u5b9e\u8df5\u548c\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u89e3\u51b3\u654f\u611f\u8bdd\u9898\u6570\u636e\u6536\u96c6\u4e2d\u7684\u9690\u79c1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01085", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01085", "abs": "https://arxiv.org/abs/2602.01085", "authors": ["Qi Jing Chen", "Shilin Shan", "Timothy Bretl", "Quang-Cuong Pham"], "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes", "comment": "7 pages, 4 figures", "summary": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u901a\u8fc7\u89c2\u5bdf\u67d4\u6027\u7ebf\u6027\u7269\u4f53\u5f62\u72b6\u6765\u68c0\u6d4b\u548c\u4f30\u8ba1\u5916\u90e8\u4f5c\u7528\u529b\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba-\u7ebf\u7f06\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u975e\u672b\u7aef\u63a5\u89e6\u573a\u666f\u3002", "motivation": "\u5728\u673a\u5668\u4eba-\u7ebf\u7f06\u4ea4\u4e92\u4efb\u52a1\u4e2d\uff0c\u63a5\u89e6\u901a\u5e38\u53d1\u751f\u5728\u673a\u5668\u4eba\u8eab\u4f53\u7684\u5176\u4ed6\u90e8\u4f4d\u800c\u975e\u672b\u7aef\u6267\u884c\u5668\uff08\u5982\u63a8\u52a8\u64cd\u4f5c\u6216\u7ebf\u7f06\u4f5c\u4e3a\u88ab\u52a8\u969c\u788d\u7269\uff09\u3002\u51c6\u786e\u8bc6\u522b\u8fd9\u4e9b\u4ea4\u4e92\u5bf9\u4e8e\u5b89\u5168\u9ad8\u6548\u7684\u8f68\u8ff9\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u9632\u6b62\u7ebf\u7f06\u635f\u574f\u3001\u907f\u514d\u53d7\u9650\u673a\u5668\u4eba\u8fd0\u52a8\u5e76\u964d\u4f4e\u6f5c\u5728\u5371\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6602\u8d35\u7684\u5916\u90e8\u529b-\u626d\u77e9\u4f20\u611f\u5668\u6216\u5047\u8bbe\u63a5\u89e6\u53d1\u751f\u5728\u672b\u7aef\u6267\u884c\u5668\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u76f8\u673a\u83b7\u53d6\u7684\u7ebf\u7f06\u5f62\u72b6\u4fe1\u606f\uff0c\u5728\u7ebf\u7f06\u5904\u4e8e\u6216\u63a5\u8fd1\u9759\u6001\u5e73\u8861\u7684\u5047\u8bbe\u4e0b\uff0c\u901a\u8fc7\u63a8\u5bfc\u4e00\u81f4\u6027\u6761\u4ef6\u5e76\u6c42\u89e3\u57fa\u4e8e\u7ebf\u7f06\u4e0a\u529b-\u626d\u77e9\u5e73\u8861\u7684\u7ebf\u6027\u65b9\u7a0b\u7ec4\uff0c\u4f30\u8ba1\u5916\u90e8\u4f5c\u7528\u529b\u7684\u4f4d\u7f6e\u548c\u5927\u5c0f\uff0c\u65e0\u9700\u989d\u5916\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u5728\u9009\u5b9a\u7684\u4ea4\u4e92\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u51c6\u786e\u7684\u4f30\u8ba1\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ec5\u901a\u8fc7\u5f62\u72b6\u4fe1\u606f\u68c0\u6d4b\u548c\u4f30\u8ba1\u67d4\u6027\u7ebf\u6027\u7269\u4f53\u4e0a\u5916\u90e8\u4f5c\u7528\u529b\u7684\u6709\u6548\u9014\u5f84\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u6216\u672b\u7aef\u63a5\u89e6\u5047\u8bbe\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5b89\u5168\u4ea4\u4e92\u573a\u666f\u3002"}}
{"id": "2602.01390", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01390", "abs": "https://arxiv.org/abs/2602.01390", "authors": ["Lana Do", "Gio Jung", "Juvenal Francisco Barajas", "Andrew Taylor Scott", "Shasta Ihorn", "Alexander Mario Blum", "Vassilis Athitsos", "Ilmi Yoon"], "title": "How well can VLMs rate audio descriptions: A multi-dimensional quantitative assessment framework", "comment": null, "summary": "Digital video is central to communication, education, and entertainment, but without audio description (AD), blind and low-vision audiences are excluded. While crowdsourced platforms and vision-language-models (VLMs) expand AD production, quality is rarely checked systematically. Existing evaluations rely on NLP metrics and short-clip guidelines, leaving questions about what constitutes quality for full-length content and how to assess it at scale. To address these questions, we first developed a multi-dimensional assessment framework for uninterrupted, full-length video, grounded in professional guidelines and refined by accessibility specialists. Second, we integrated this framework into a comprehensive methodological workflow, utilizing Item Response Theory, to assess the proficiency of VLM and human raters against expert-established ground truth. Findings suggest that while VLMs can approximate ground-truth ratings with high alignment, their reasoning was found to be less reliable and actionable than that of human respondents. These insights show the potential of hybrid evaluation systems that leverage VLMs alongside human oversight, offering a path towards scalable AD quality control.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u9488\u5bf9\u5b8c\u6574\u89c6\u9891\u97f3\u9891\u63cf\u8ff0(AD)\u7684\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5229\u7528\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8005\u5728AD\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "motivation": "\u6570\u5b57\u89c6\u9891\u5728\u6559\u80b2\u3001\u5a31\u4e50\u548c\u901a\u4fe1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u97f3\u9891\u63cf\u8ff0(AD)\u4f1a\u6392\u9664\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u89c2\u4f17\u3002\u867d\u7136\u4f17\u5305\u5e73\u53f0\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u6269\u5c55\u4e86AD\u751f\u4ea7\uff0c\u4f46\u8d28\u91cf\u5f88\u5c11\u88ab\u7cfb\u7edf\u68c0\u67e5\u3002\u73b0\u6709\u8bc4\u4f30\u4f9d\u8d56\u4e8eNLP\u6307\u6807\u548c\u77ed\u89c6\u9891\u6307\u5357\uff0c\u672a\u80fd\u89e3\u51b3\u5b8c\u6574\u89c6\u9891\u5185\u5bb9\u7684\u8d28\u91cf\u6807\u51c6\u548c\u5927\u89c4\u6a21\u8bc4\u4f30\u95ee\u9898\u3002", "method": "1. \u57fa\u4e8e\u4e13\u4e1a\u6307\u5357\u5f00\u53d1\u4e86\u9488\u5bf9\u4e0d\u95f4\u65ad\u5b8c\u6574\u89c6\u9891\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u7531\u65e0\u969c\u788d\u4e13\u5bb6\u5b8c\u5584\uff1b2. \u5c06\u8be5\u6846\u67b6\u6574\u5408\u5230\u7efc\u5408\u65b9\u6cd5\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5229\u7528\u9879\u76ee\u53cd\u5e94\u7406\u8bba(Item Response Theory)\u8bc4\u4f30VLM\u548c\u4eba\u7c7b\u8bc4\u5206\u8005\u76f8\u5bf9\u4e8e\u4e13\u5bb6\u5efa\u7acb\u7684\u5730\u9762\u771f\u503c\u7684\u719f\u7ec3\u7a0b\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136VLMs\u80fd\u591f\u4ee5\u9ad8\u5ea6\u4e00\u81f4\u6027\u8fd1\u4f3c\u5730\u9762\u771f\u503c\u8bc4\u5206\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u6bd4\u4eba\u7c7b\u53d7\u8bbf\u8005\u66f4\u4e0d\u53ef\u9760\u4e14\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u6027\u3002VLMs\u5728\u8bc4\u5206\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u89e3\u91ca\u548c\u63a8\u7406\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u663e\u793a\u4e86\u6df7\u5408\u8bc4\u4f30\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u5373\u5229\u7528VLMs\u4e0e\u4eba\u7c7b\u76d1\u7763\u76f8\u7ed3\u5408\uff0c\u4e3a\u53ef\u6269\u5c55\u7684AD\u8d28\u91cf\u63a7\u5236\u63d0\u4f9b\u4e86\u8def\u5f84\u3002\u9700\u8981\u7ed3\u5408VLM\u7684\u8bc4\u5206\u80fd\u529b\u548c\u4eba\u7c7b\u7684\u63a8\u7406\u5224\u65ad\u6765\u5efa\u7acb\u6709\u6548\u7684\u8d28\u91cf\u8bc4\u4f30\u4f53\u7cfb\u3002"}}
{"id": "2602.01092", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01092", "abs": "https://arxiv.org/abs/2602.01092", "authors": ["Peng Zhou", "Zhongxuan Li", "Jinsong Wu", "Jiaming Qi", "Jun Hu", "David Navarro-Alarcon", "Jia Pan", "Lihua Xie", "Shiyao Zhang", "Zeqing Zhang"], "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance", "comment": null, "summary": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at https://www.youtube.com/watch?v=XDTsvzEkDRE", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fdd\u5b88\u4ef7\u503c\u5b66\u4e60\u7684\u6545\u969c\u611f\u77e5\u53cc\u624b\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u79bb\u7ebf\u6570\u636e\u4e2d\u7684\u6210\u529f\u4e0e\u5931\u8d25\u7ecf\u9a8c\uff0c\u63d0\u4f9b\u7b26\u5408\u6027\u89e6\u89c9\u8f85\u52a9\uff0c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u5e76\u964d\u4f4e\u64cd\u4f5c\u5458\u8d1f\u62c5", "motivation": "\u9ad8\u7cbe\u5ea6\u9065\u64cd\u4f5c\u53d7\u9650\u4e8e\u4e25\u683c\u7684\u6210\u529f\u5bb9\u5dee\u548c\u590d\u6742\u7684\u63a5\u89e6\u52a8\u529b\u5b66\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\uff0c\u64cd\u4f5c\u5458\u96be\u4ee5\u9884\u89c1\u5373\u5c06\u53d1\u751f\u7684\u6545\u969c\u3002\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6f5c\u5728\u5931\u8d25\u7684\u611f\u77e5\u80fd\u529b", "method": "1) \u4ece\u5305\u542b\u6210\u529f\u548c\u5931\u8d25\u6267\u884c\u7684\u5f02\u6784\u79bb\u7ebf\u9065\u64cd\u4f5c\u6570\u636e\u4e2d\u8bad\u7ec3\uff1b2) \u901a\u8fc7\u4fdd\u5b88\u4ef7\u503c\u5b66\u4e60\u5efa\u6a21\u4efb\u52a1\u53ef\u884c\u6027\uff0c\u83b7\u5f97\u98ce\u9669\u654f\u611f\u7684\u6210\u529f\u5206\u6570\u4f30\u8ba1\uff1b3) \u5728\u7ebf\u64cd\u4f5c\u65f6\uff0c\u6210\u529f\u5206\u6570\u8c03\u8282\u8f85\u52a9\u6c34\u5e73\uff0c\u5b66\u4e60\u5230\u7684\u6267\u884c\u5668\u63d0\u4f9b\u7ea0\u6b63\u8fd0\u52a8\u65b9\u5411\uff1b4) \u901a\u8fc7\u4e3b\u7aef\u7684\u5173\u8282\u7a7a\u95f4\u963b\u6297\u63a5\u53e3\u96c6\u6210\uff0c\u63d0\u4f9b\u8fde\u7eed\u5f15\u5bfc", "result": "\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u9065\u64cd\u4f5c\u548c\u5171\u4eab\u81ea\u6cbb\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u64cd\u4f5c\u5458\u5de5\u4f5c\u8d1f\u62c5\u3002\u4fdd\u5b88\u4ef7\u503c\u5b66\u4e60\u4e3a\u53cc\u8fb9\u9065\u64cd\u4f5c\u5d4c\u5165\u6545\u969c\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u673a\u5236", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4fdd\u5b88\u4ef7\u503c\u5b66\u4e60\u7684\u6545\u969c\u611f\u77e5\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u8986\u76d6\u64cd\u4f5c\u5458\u610f\u56fe\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u8fde\u7eed\u5f15\u5bfc\u907f\u514d\u6545\u969c\u503e\u5411\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u9065\u64cd\u4f5c\u6027\u80fd"}}
{"id": "2602.01600", "categories": ["cs.CR", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01600", "abs": "https://arxiv.org/abs/2602.01600", "authors": ["Yen-Shan Chen", "Zhi Rui Tam", "Cheng-Kuang Wu", "Yun-Nung Chen"], "title": "Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs", "comment": null, "summary": "Current evaluations of LLM safety predominantly rely on severity-based taxonomies to assess the harmfulness of malicious queries. We argue that this formulation requires re-examination as it assumes uniform risk across all malicious queries, neglecting Execution Likelihood--the conditional probability of a threat being realized given the model's response. In this work, we introduce Expected Harm, a metric that weights the severity of a jailbreak by its execution likelihood, modeled as a function of execution cost. Through empirical analysis of state-of-the-art models, we reveal a systematic Inverse Risk Calibration: models disproportionately exhibit stronger refusal behaviors for low-likelihood (high-cost) threats while remaining vulnerable to high-likelihood (low-cost) queries. We demonstrate that this miscalibration creates a structural vulnerability: by exploiting this property, we increase the attack success rate of existing jailbreaks by up to $2\\times$. Finally, we trace the root cause of this failure using linear probing, which reveals that while models encode severity in their latent space to drive refusal decisions, they possess no distinguishable internal representation of execution cost, making them \"blind\" to this critical dimension of risk.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faExpected Harm\uff08\u9884\u671f\u5371\u5bb3\uff09\u6307\u6807\uff0c\u5c06\u8d8a\u72f1\u4e25\u91cd\u6027\u52a0\u6743\u6267\u884c\u53ef\u80fd\u6027\uff0c\u63ed\u793aLLM\u5b58\u5728\u7cfb\u7edf\u6027\u9006\u5411\u98ce\u9669\u6821\u51c6\u95ee\u9898\uff1a\u5bf9\u4f4e\u53ef\u80fd\u6027\uff08\u9ad8\u6210\u672c\uff09\u5a01\u80c1\u8fc7\u5ea6\u62d2\u7edd\uff0c\u4f46\u5bf9\u9ad8\u53ef\u80fd\u6027\uff08\u4f4e\u6210\u672c\uff09\u67e5\u8be2\u4fdd\u6301\u8106\u5f31\u3002", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u4e25\u91cd\u6027\u7684\u5206\u7c7b\u6cd5\u8bc4\u4f30\u6076\u610f\u67e5\u8be2\u7684\u5371\u5bb3\u6027\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u6076\u610f\u67e5\u8be2\u98ce\u9669\u5747\u5300\uff0c\u5ffd\u7565\u4e86\u6267\u884c\u53ef\u80fd\u6027\uff08\u5a01\u80c1\u5728\u6a21\u578b\u54cd\u5e94\u540e\u5b9e\u73b0\u7684\u6761\u4ef6\u6982\u7387\uff09\u3002", "method": "\u5f15\u5165Expected Harm\u6307\u6807\uff0c\u5c06\u8d8a\u72f1\u4e25\u91cd\u6027\u6309\u6267\u884c\u53ef\u80fd\u6027\u52a0\u6743\uff0c\u6267\u884c\u53ef\u80fd\u6027\u5efa\u6a21\u4e3a\u6267\u884c\u6210\u672c\u7684\u51fd\u6570\u3002\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u5229\u7528\u7ebf\u6027\u63a2\u6d4b\u8ffd\u6eaf\u5931\u8d25\u6839\u6e90\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u9006\u5411\u98ce\u9669\u6821\u51c6\uff1a\u5bf9\u4f4e\u53ef\u80fd\u6027\uff08\u9ad8\u6210\u672c\uff09\u5a01\u80c1\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u4f46\u5bf9\u9ad8\u53ef\u80fd\u6027\uff08\u4f4e\u6210\u672c\uff09\u67e5\u8be2\u4fdd\u6301\u8106\u5f31\u3002\u5229\u7528\u6b64\u7279\u6027\u53ef\u5c06\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u81f32\u500d\u3002\u7ebf\u6027\u63a2\u6d4b\u663e\u793a\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7f16\u7801\u4e25\u91cd\u6027\u4ee5\u9a71\u52a8\u62d2\u7edd\u51b3\u7b56\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6267\u884c\u6210\u672c\u7684\u53ef\u533a\u5206\u5185\u90e8\u8868\u5f81\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u4e25\u91cd\u6027\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0c\u9700\u8981\u7eb3\u5165\u6267\u884c\u53ef\u80fd\u6027\u6765\u66f4\u51c6\u786e\u8bc4\u4f30\u98ce\u9669\u3002\u6a21\u578b\u7684\"\u76f2\u70b9\"\u5728\u4e8e\u65e0\u6cd5\u5185\u90e8\u8868\u5f81\u6267\u884c\u6210\u672c\uff0c\u5bfc\u81f4\u98ce\u9669\u6821\u51c6\u5931\u8d25\uff0c\u8fd9\u4e3a\u6539\u8fdbLLM\u5b89\u5168\u8bc4\u4f30\u548c\u9632\u5fa1\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01396", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01396", "abs": "https://arxiv.org/abs/2602.01396", "authors": ["Ziheng Huang", "Robin Kar", "Hari Sundaram", "Tal August"], "title": "Living Contracts: Beyond Document-Centric Interaction with Legal Agreements", "comment": null, "summary": "User interaction with legal contracts has been limited to document reading, which is often complicated by complex, ambiguous legal language. We explore possible futures where contract interfaces go beyond single document interfaces to (1) educate users with legal rights not stated in the contract, (2) transform legal language into alternative representations to aid information tasks before, during, and after signing, and (3) proactively supply contractual information at relevant moments. We refer to these future interfaces collectively as Living Contracts. Using residential leases as a case study, we created three design probes representing different possible Living Contracts. A three-part qualitative study (N=18) revealed participants' barriers to interacting with contracts, including interpreting complex language, uncertainty about legal rights, and the pressure to sign quickly. Participants' feedback on the probes highlighted how Living Contracts have the potential to address these challenges and open new design opportunities for human-contract interactions beyond document reading.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u8d85\u8d8a\u4f20\u7edf\u6587\u6863\u9605\u8bfb\u7684\u5408\u540c\u4ea4\u4e92\u754c\u9762\uff0c\u63d0\u51fa\"\u6d3b\u5408\u540c\"\u6982\u5ff5\uff0c\u901a\u8fc7\u6559\u80b2\u7528\u6237\u3001\u8f6c\u6362\u6cd5\u5f8b\u8bed\u8a00\u548c\u4e3b\u52a8\u63d0\u4f9b\u4fe1\u606f\u6765\u6539\u5584\u5408\u540c\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u7528\u6237\u4e0e\u6cd5\u5f8b\u5408\u540c\u7684\u4ea4\u4e92\u901a\u5e38\u4ec5\u9650\u4e8e\u6587\u6863\u9605\u8bfb\uff0c\u800c\u590d\u6742\u7684\u6cd5\u5f8b\u8bed\u8a00\u3001\u672a\u660e\u786e\u9648\u8ff0\u7684\u6cd5\u5f8b\u6743\u5229\u4ee5\u53ca\u7b7e\u7ea6\u538b\u529b\u7b49\u95ee\u9898\u963b\u788d\u4e86\u6709\u6548\u7684\u5408\u540c\u7406\u89e3\u548c\u4f7f\u7528\u3002", "method": "\u4ee5\u4f4f\u5b85\u79df\u8d41\u5408\u540c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u521b\u5efa\u4e86\u4e09\u4e2a\u4ee3\u8868\u4e0d\u540c\"\u6d3b\u5408\u540c\"\u53ef\u80fd\u6027\u7684\u8bbe\u8ba1\u63a2\u9488\uff0c\u5e76\u901a\u8fc7\u4e09\u90e8\u5206\u5b9a\u6027\u7814\u7a76\uff08N=18\uff09\u63a2\u7d22\u53c2\u4e0e\u8005\u7684\u5408\u540c\u4ea4\u4e92\u969c\u788d\u548c\u5bf9\u63a2\u9488\u7684\u53cd\u9988\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53c2\u4e0e\u8005\u9762\u4e34\u7684\u4e3b\u8981\u969c\u788d\u5305\u62ec\uff1a\u89e3\u91ca\u590d\u6742\u8bed\u8a00\u3001\u5bf9\u6cd5\u5f8b\u6743\u5229\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u5feb\u901f\u7b7e\u7ea6\u7684\u538b\u529b\u3002\u53c2\u4e0e\u8005\u5bf9\u8bbe\u8ba1\u63a2\u9488\u7684\u53cd\u9988\u8868\u660e\uff0c\"\u6d3b\u5408\u540c\"\u6709\u6f5c\u529b\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u5e76\u4e3a\u8d85\u8d8a\u6587\u6863\u9605\u8bfb\u7684\u4eba\u673a\u5408\u540c\u4ea4\u4e92\u5f00\u8f9f\u65b0\u7684\u8bbe\u8ba1\u673a\u4f1a\u3002", "conclusion": "\"\u6d3b\u5408\u540c\"\u6982\u5ff5\u5c55\u793a\u4e86\u5408\u540c\u754c\u9762\u8d85\u8d8a\u5355\u4e00\u6587\u6863\u7684\u53ef\u80fd\u6027\uff0c\u901a\u8fc7\u6559\u80b2\u7528\u6237\u3001\u8f6c\u6362\u6cd5\u5f8b\u8bed\u8a00\u548c\u4e3b\u52a8\u63d0\u4f9b\u4fe1\u606f\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u5408\u540c\u4ea4\u4e92\u4f53\u9a8c\uff0c\u4e3a\u672a\u6765\u5408\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.01100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01100", "abs": "https://arxiv.org/abs/2602.01100", "authors": ["Hang Wu", "Tongqing Chen", "Jiasen Wang", "Xiaotao Li", "Lu Fang"], "title": "StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating", "comment": null, "summary": "Long-horizon robotic manipulation requires bridging the gap between high-level planning (System 2) and low-level control (System 1). Current Vision-Language-Action (VLA) models often entangle these processes, performing redundant multimodal reasoning at every timestep, which leads to high latency and goal instability. To address this, we present StreamVLA, a dual-system architecture that unifies textual task decomposition, visual goal imagination, and continuous action generation within a single parameter-efficient backbone. We introduce a \"Lock-and-Gated\" mechanism to intelligently modulate computation: only when a sub-task transition is detected, the model triggers slow thinking to generate a textual instruction and imagines the specific visual completion state, rather than generic future frames. Crucially, this completion state serves as a time-invariant goal anchor, making the policy robust to execution speed variations. During steady execution, these high-level intents are locked to condition a Flow Matching action head, allowing the model to bypass expensive autoregressive decoding for 72% of timesteps. This hierarchical abstraction ensures sub-goal focus while significantly reducing inference latency. Extensive evaluations demonstrate that StreamVLA achieves state-of-the-art performance, with a 98.5% success rate on the LIBERO benchmark and robust recovery in real-world interference scenarios, achieving a 48% reduction in latency compared to full-reasoning baselines.", "AI": {"tldr": "StreamVLA\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\"\u9501\u5b9a\u4e0e\u95e8\u63a7\"\u673a\u5236\u5206\u79bb\u9ad8\u5c42\u6b21\u4efb\u52a1\u5206\u89e3\u4e0e\u4f4e\u5c42\u6b21\u52a8\u4f5c\u751f\u6210\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u5e76\u63d0\u5347\u76ee\u6807\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b58\u5728\u7cfb\u7edf2\uff08\u9ad8\u5c42\u6b21\u89c4\u5212\uff09\u4e0e\u7cfb\u7edf1\uff08\u4f4e\u5c42\u6b21\u63a7\u5236\uff09\u7684\u8026\u5408\u95ee\u9898\uff0c\u5bfc\u81f4\u6bcf\u4e2a\u65f6\u95f4\u6b65\u90fd\u8fdb\u884c\u5197\u4f59\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4ea7\u751f\u9ad8\u5ef6\u8fdf\u548c\u76ee\u6807\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faStreamVLA\u53cc\u7cfb\u7edf\u67b6\u6784\uff1a1\uff09\u5f15\u5165\"\u9501\u5b9a\u4e0e\u95e8\u63a7\"\u673a\u5236\uff0c\u4ec5\u5728\u68c0\u6d4b\u5230\u5b50\u4efb\u52a1\u8f6c\u6362\u65f6\u89e6\u53d1\u6162\u601d\u8003\uff0c\u751f\u6210\u6587\u672c\u6307\u4ee4\u5e76\u60f3\u8c61\u5177\u4f53\u7684\u89c6\u89c9\u5b8c\u6210\u72b6\u6001\uff1b2\uff09\u5b8c\u6210\u72b6\u6001\u4f5c\u4e3a\u65f6\u95f4\u4e0d\u53d8\u7684\u76ee\u6807\u951a\u70b9\uff1b3\uff09\u7a33\u6001\u6267\u884c\u65f6\u9501\u5b9a\u9ad8\u5c42\u6b21\u610f\u56fe\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u52a8\u4f5c\u5934\u751f\u6210\u52a8\u4f5c\uff0c\u7ed5\u8fc772%\u65f6\u95f4\u6b65\u7684\u81ea\u56de\u5f52\u89e3\u7801\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523098.5%\u7684\u6210\u529f\u7387\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5e72\u6270\u573a\u666f\u4e2d\u8868\u73b0\u9c81\u68d2\u6062\u590d\u80fd\u529b\uff0c\u76f8\u6bd4\u5b8c\u5168\u63a8\u7406\u57fa\u7ebf\u5ef6\u8fdf\u964d\u4f4e48%\u3002", "conclusion": "StreamVLA\u901a\u8fc7\u5c42\u6b21\u5316\u62bd\u8c61\u6709\u6548\u5206\u79bb\u4efb\u52a1\u5206\u89e3\u4e0e\u52a8\u4f5c\u751f\u6210\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u4e3a\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u53cc\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02079", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02079", "abs": "https://arxiv.org/abs/2602.02079", "authors": ["Daniil Orel", "Dilshod Azizov", "Indraneil Paul", "Yuxia Wang", "Iryna Gurevych", "Preslav Nakov"], "title": "AICD Bench: A Challenging Benchmark for AI-Generated Code Detection", "comment": null, "summary": "Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\\emph{2M examples}$, $\\emph{77 models}$ across $\\emph{11 families}$, and $\\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\\emph{i}$)~$\\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\\emph{ii}$)~$\\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\\emph{iii}$)~$\\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.", "AI": {"tldr": "AICD Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684AI\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u57fa\u51c6\uff0c\u5305\u542b200\u4e07\u6837\u672c\u300177\u4e2a\u6a21\u578b\u300111\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c9\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5f15\u5165\u4e09\u79cd\u73b0\u5b9e\u68c0\u6d4b\u4efb\u52a1\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u68c0\u6d4b\u5668\u6027\u80fd\u8fdc\u672a\u8fbe\u5230\u5b9e\u7528\u6c34\u5e73\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u529f\u80fd\u6027\u6e90\u4ee3\u7801\u7684\u80fd\u529b\u589e\u5f3a\uff0c\u5f15\u53d1\u4e86\u5173\u4e8e\u4f5c\u8005\u8eab\u4efd\u3001\u8d23\u4efb\u548c\u5b89\u5168\u7684\u62c5\u5fe7\u3002\u73b0\u6709AI\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u8fc7\u4e8e\u72ed\u7a84\uff0c\u901a\u5e38\u4ec5\u9650\u4e8e\u5206\u5e03\u5185\u8bbe\u7f6e\u7684\u4e8c\u5143\u4eba\u673a\u5206\u7c7b\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u6784\u5efaAICD Bench\u57fa\u51c6\uff0c\u5305\u542b200\u4e07\u4e2a\u793a\u4f8b\u300177\u4e2a\u6a21\u578b\uff08\u6db5\u76d611\u4e2a\u6a21\u578b\u5bb6\u65cf\uff09\u548c9\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5305\u62ec\u6700\u65b0\u7684\u63a8\u7406\u6a21\u578b\u3002\u5f15\u5165\u4e09\u79cd\u73b0\u5b9e\u68c0\u6d4b\u4efb\u52a1\uff1a1\uff09\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u4e8c\u5143\u5206\u7c7b\uff1b2\uff09\u6a21\u578b\u5bb6\u65cf\u5f52\u5c5e\uff0c\u6309\u67b6\u6784\u8c31\u7cfb\u5bf9\u751f\u6210\u5668\u5206\u7ec4\uff1b3\uff09\u7ec6\u7c92\u5ea6\u4eba\u673a\u5206\u7c7b\uff0c\u6db5\u76d6\u4eba\u7c7b\u3001\u673a\u5668\u3001\u6df7\u5408\u548c\u5bf9\u6297\u6027\u4ee3\u7801\u3002", "result": "\u5bf9\u795e\u7ecf\u548c\u7ecf\u5178\u68c0\u6d4b\u5668\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u6027\u80fd\u8fdc\u4f4e\u4e8e\u5b9e\u9645\u53ef\u7528\u6c34\u5e73\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4ee5\u53ca\u5bf9\u6df7\u5408\u6216\u5bf9\u6297\u6027\u4ee3\u7801\u7684\u68c0\u6d4b\u65b9\u9762\u3002\u57fa\u51c6\u4f5c\u4e3a\u7edf\u4e00\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\u5957\u4ef6\u53d1\u5e03\u3002", "conclusion": "AICD Bench\u662f\u4e00\u4e2a\u5168\u9762\u4e14\u5177\u6709\u6311\u6218\u6027\u7684AI\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u8868\u73b0\u4e0d\u8db3\uff0c\u65e8\u5728\u63a8\u52a8\u4e0b\u4e00\u4ee3\u9c81\u68d2\u68c0\u6d4b\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01621", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01621", "abs": "https://arxiv.org/abs/2602.01621", "authors": ["Hanjun Park", "Byeong-Seo Min", "Jiheon Woo", "Min-Wook Jeong", "Jongho Shin", "Yongwoo Lee", "Young-Sik Kim", "Yongjune Kim"], "title": "Efficient Softmax Reformulation for Homomorphic Encryption via Moment Generating Function", "comment": null, "summary": "Homomorphic encryption (HE) is a prominent framework for privacy-preserving machine learning, enabling inference directly on encrypted data. However, evaluating softmax, a core component of transformer architectures, remains particularly challenging in HE due to its multivariate structure, the large dynamic range induced by exponential functions, and the need for accurate division during normalization. In this paper, we propose MGF-softmax, a novel softmax reformulation based on the moment generating function (MGF) that replaces the softmax denominator with its moment-based counterpart. This reformulation substantially reduces multiplicative depth while preserving key properties of softmax and asymptotically converging to the exact softmax as the number of input tokens increases. Extensive experiments on Vision Transformers and large language models show that MGF-softmax provides an efficient and accurate approximation of softmax in encrypted inference. In particular, it achieves inference accuracy close to that of high-depth exact methods, while requiring substantially lower computational cost through reduced multiplicative depth.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e9\u751f\u6210\u51fd\u6570\u7684MGF-softmax\u65b9\u6cd5\uff0c\u7528\u4e8e\u540c\u6001\u52a0\u5bc6\u4e2d\u7684softmax\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u4e58\u6cd5\u6df1\u5ea6\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027", "motivation": "\u540c\u6001\u52a0\u5bc6\u4e2d\u7684softmax\u8ba1\u7b97\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u591a\u5143\u7ed3\u6784\u3001\u6307\u6570\u51fd\u6570\u5bfc\u81f4\u7684\u5927\u52a8\u6001\u8303\u56f4\u3001\u5f52\u4e00\u5316\u9700\u8981\u7cbe\u786e\u9664\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u8fd1\u4f3c\u65b9\u6848", "method": "\u63d0\u51faMGF-softmax\u65b9\u6cd5\uff0c\u5229\u7528\u77e9\u751f\u6210\u51fd\u6570\u91cd\u65b0\u8868\u8ff0softmax\uff0c\u5c06\u5206\u6bcd\u66ff\u6362\u4e3a\u57fa\u4e8e\u77e9\u7684\u5bf9\u5e94\u9879\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301softmax\u5173\u952e\u7279\u6027\uff0c\u968f\u7740\u8f93\u5165token\u6570\u91cf\u589e\u52a0\u6e10\u8fd1\u6536\u655b\u5230\u7cbe\u786esoftmax", "result": "\u5728Vision Transformers\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMGF-softmax\u5728\u52a0\u5bc6\u63a8\u7406\u4e2d\u63d0\u4f9b\u9ad8\u6548\u51c6\u786e\u7684softmax\u8fd1\u4f3c\uff0c\u63a5\u8fd1\u9ad8\u6df1\u5ea6\u7cbe\u786e\u65b9\u6cd5\u7684\u63a8\u7406\u7cbe\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u51cf\u5c11\u4e58\u6cd5\u6df1\u5ea6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "conclusion": "MGF-softmax\u662f\u540c\u6001\u52a0\u5bc6\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u4e2dsoftmax\u8ba1\u7b97\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8etransformer\u67b6\u6784\u7684\u52a0\u5bc6\u63a8\u7406"}}
{"id": "2602.01405", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01405", "abs": "https://arxiv.org/abs/2602.01405", "authors": ["Nikhil Sharma", "Zheng Zhang", "Daniel Lee", "Namita Krishnan", "Guang-Jie Ren", "Ziang Xiao", "Yunyao Li"], "title": "Feedback by Design: Understanding and Overcoming User Feedback Barriers in Conversational Agents", "comment": "Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems, 23 pages, 3 figures", "summary": "High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap motivates a critical examination of human feedback during interactions with AIs. To understand and overcome the challenges preventing users from giving high-quality feedback, we conducted two studies examining feedback dynamics between humans and conversational agents (CAs). Our formative study, through the lens of Grice's maxims, identified four Feedback Barriers -- Common Ground, Verifiability, Communication, and Informativeness -- that prevent high-quality feedback by users. Building on these findings, we derive three design desiderata and show that systems incorporating scaffolds aligned with these desiderata enabled users to provide higher-quality feedback. Finally, we detail a call for action to the broader AI community for advances in Large Language Models capabilities to overcome Feedback Barriers.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4eba\u673a\u4ea4\u4e92\u4e2d\u9ad8\u8d28\u91cf\u53cd\u9988\u7684\u969c\u788d\uff0c\u901a\u8fc7\u4e24\u4e2a\u7814\u7a76\u53d1\u73b0\u56db\u79cd\u53cd\u9988\u969c\u788d\uff0c\u63d0\u51fa\u4e09\u79cd\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u5c55\u793a\u652f\u6301\u8fd9\u4e9b\u539f\u5219\u7684\u7cfb\u7edf\u80fd\u5e2e\u52a9\u7528\u6237\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u53cd\u9988", "motivation": "\u9ad8\u8d28\u91cf\u53cd\u9988\u5bf9\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7528\u6237\u53cd\u9988\u5f80\u5f80\u9891\u7387\u4f4e\u3001\u8d28\u91cf\u5dee\u3002\u8fd9\u4e00\u5dee\u8ddd\u4fc3\u4f7f\u7814\u7a76\u8005\u6df1\u5165\u63a2\u7a76\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u53cd\u9988\u969c\u788d\uff0c\u4ee5\u7406\u89e3\u5e76\u514b\u670d\u7528\u6237\u63d0\u4f9b\u9ad8\u8d28\u91cf\u53cd\u9988\u7684\u6311\u6218", "method": "\u901a\u8fc7\u4e24\u4e2a\u7814\u7a76\uff1a1) \u57fa\u4e8e\u683c\u8d56\u65af\u51c6\u5219\u7684\u63a2\u7d22\u6027\u7814\u7a76\uff0c\u8bc6\u522b\u51fa\u56db\u79cd\u53cd\u9988\u969c\u788d\uff1b2) \u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u51fa\u4e09\u79cd\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u9a8c\u8bc1\u652f\u6301\u8fd9\u4e9b\u539f\u5219\u7684\u7cfb\u7edf\u80fd\u5426\u5e2e\u52a9\u7528\u6237\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u53cd\u9988", "result": "\u8bc6\u522b\u51fa\u56db\u79cd\u53cd\u9988\u969c\u788d\uff1a\u5171\u540c\u57fa\u7840\u969c\u788d\u3001\u53ef\u9a8c\u8bc1\u6027\u969c\u788d\u3001\u6c9f\u901a\u969c\u788d\u548c\u4fe1\u606f\u6027\u969c\u788d\u3002\u63d0\u51fa\u4e09\u79cd\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u8bc1\u660e\u652f\u6301\u8fd9\u4e9b\u539f\u5219\u7684\u7cfb\u7edf\u80fd\u663e\u8457\u63d0\u9ad8\u7528\u6237\u53cd\u9988\u8d28\u91cf", "conclusion": "\u4eba\u673a\u4ea4\u4e92\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u53cd\u9988\u969c\u788d\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u8bbe\u8ba1\u53ef\u4ee5\u514b\u670d\u8fd9\u4e9b\u969c\u788d\u3002\u7814\u7a76\u547c\u5401AI\u793e\u533a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4ee5\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u53cd\u9988\u969c\u788d\u95ee\u9898"}}
{"id": "2602.01663", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01663", "abs": "https://arxiv.org/abs/2602.01663", "authors": ["David Condrey"], "title": "Witnessd: Proof-of-process via Adversarial Collapse", "comment": null, "summary": "Digital signatures prove key possession, not authorship. An author who generates text with AI, constructs intermediate document states post-hoc, and signs each hash produces a signature chain indistinguishable from genuine composition. We address this gap between cryptographic integrity and process provenance. We introduce proof-of-process, a primitive category for evidence that a physical process, not merely a signing key, produced a digital artifact. Our construction, the jitter seal, injects imperceptible microsecond delays derived via HMAC from a session secret, keystroke ordinal, and cumulative document hash. Valid evidence requires that real keystrokes produced the document through those intermediate states. We propose the Adversarial Collapse Principle as an evaluation criterion: evidence systems should be judged by whether disputing them requires a conjunction of specific, testable allegations against components with independent trust assumptions. We present Witnessd, an architecture combining jitter seals with Verifiable Delay Functions, external timestamp anchors, dual-source keystroke validation, and optional hardware attestation. Each layer forces allegations at different capability levels; disputing authentic evidence requires coordinated claims across independent trust boundaries. The system does not prevent forgery: a kernel-level adversary can defeat it, and typing AI-generated content produces valid evidence. The contribution is converting vague doubt into falsifiable allegations. We evaluate across 31,000 verification trials with deterministic rejection of invalid proofs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u8fc7\u7a0b\u8bc1\u660e\"\u6982\u5ff5\uff0c\u901a\u8fc7\"\u6296\u52a8\u5c01\u5370\"\u6280\u672f\u8bb0\u5f55\u6253\u5b57\u8fc7\u7a0b\u7684\u5fae\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u5c06\u6a21\u7cca\u7684\u6000\u7591\u8f6c\u5316\u4e3a\u53ef\u8bc1\u4f2a\u7684\u6307\u63a7\uff0c\u89e3\u51b3AI\u751f\u6210\u6587\u672c\u7684\u7b7e\u540d\u9a8c\u8bc1\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6570\u5b57\u7b7e\u540d\u53ea\u80fd\u8bc1\u660e\u5bc6\u94a5\u6301\u6709\uff0c\u65e0\u6cd5\u8bc1\u660e\u521b\u4f5c\u8fc7\u7a0b\u3002AI\u751f\u6210\u6587\u672c\u540e\uff0c\u4f5c\u8005\u53ef\u4ee5\u4e8b\u540e\u6784\u9020\u4e2d\u95f4\u6587\u6863\u72b6\u6001\u5e76\u7b7e\u540d\uff0c\u4ea7\u751f\u4e0e\u771f\u5b9e\u521b\u4f5c\u8fc7\u7a0b\u65e0\u6cd5\u533a\u5206\u7684\u7b7e\u540d\u94fe\u3002\u9700\u8981\u586b\u8865\u5bc6\u7801\u5b66\u5b8c\u6574\u6027\u4e0e\u8fc7\u7a0b\u6eaf\u6e90\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u63d0\u51fa\"\u8fc7\u7a0b\u8bc1\u660e\"\u539f\u8bed\u7c7b\u522b\uff0c\u5f15\u5165\"\u6296\u52a8\u5c01\u5370\"\u6280\u672f\uff1a\u57fa\u4e8eHMAC\u4ece\u4f1a\u8bdd\u5bc6\u94a5\u3001\u51fb\u952e\u5e8f\u53f7\u548c\u7d2f\u79ef\u6587\u6863\u54c8\u5e0c\u751f\u6210\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u3002\u8bbe\u8ba1Witnessd\u67b6\u6784\uff0c\u7ed3\u5408\u6296\u52a8\u5c01\u5370\u3001\u53ef\u9a8c\u8bc1\u5ef6\u8fdf\u51fd\u6570\u3001\u5916\u90e8\u65f6\u95f4\u6233\u951a\u70b9\u3001\u53cc\u6e90\u51fb\u952e\u9a8c\u8bc1\u548c\u53ef\u9009\u786c\u4ef6\u8bc1\u660e\u3002", "result": "\u572831,000\u6b21\u9a8c\u8bc1\u8bd5\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u65e0\u6548\u8bc1\u660e\u7684\u786e\u5b9a\u6027\u62d2\u7edd\u3002\u7cfb\u7edf\u4e0d\u9632\u6b62\u4f2a\u9020\uff08\u5185\u6838\u7ea7\u653b\u51fb\u8005\u53ef\u7834\u89e3\uff09\uff0c\u4f46\u5c06\u6a21\u7cca\u6000\u7591\u8f6c\u5316\u4e3a\u9700\u8981\u8de8\u72ec\u7acb\u4fe1\u4efb\u8fb9\u754c\u534f\u8c03\u6307\u63a7\u7684\u53ef\u8bc1\u4f2a\u4e3b\u5f20\u3002", "conclusion": "\u63d0\u51fa\"\u5bf9\u6297\u6027\u5d29\u6e83\u539f\u5219\"\u4f5c\u4e3a\u8bc4\u4f30\u6807\u51c6\uff1a\u8bc1\u636e\u7cfb\u7edf\u5e94\u901a\u8fc7\u8d28\u7591\u5b83\u662f\u5426\u9700\u8981\u9488\u5bf9\u5177\u6709\u72ec\u7acb\u4fe1\u4efb\u5047\u8bbe\u7ec4\u4ef6\u7684\u5177\u4f53\u3001\u53ef\u6d4b\u8bd5\u6307\u63a7\u6765\u5224\u65ad\u3002\u8d21\u732e\u5728\u4e8e\u5c06\u6a21\u7cca\u6000\u7591\u8f6c\u5316\u4e3a\u53ef\u8bc1\u4f2a\u7684\u6307\u63a7\uff0c\u800c\u975e\u5b8c\u5168\u9632\u6b62\u4f2a\u9020\u3002"}}
{"id": "2602.01423", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01423", "abs": "https://arxiv.org/abs/2602.01423", "authors": ["Matt Gottsacker", "Yahya Hmaiti", "Mykola Maslych", "Hiroshi Furuya", "Jasmine Joyce DeGuzman", "Gerd Bruder", "Gregory F. Welch", "Joseph J. LaViola"], "title": "From One World to Another: Interfaces for Efficiently Transitioning Between Virtual Environments", "comment": null, "summary": "Personal computers and handheld devices provide keyboard shortcuts and swipe gestures to enable users to efficiently switch between applications, whereas today's virtual reality (VR) systems do not. In this work, we present an exploratory study on user interface aspects to support efficient switching between worlds in VR. We created eight interfaces that afford previewing and selecting from the available virtual worlds, including methods using portals and worlds-in-miniature (WiMs). To evaluate these methods, we conducted a controlled within-subjects empirical experiment (N=22) where participants frequently transitioned between six different environments to complete an object collection task. Our quantitative and qualitative results show that WiMs supported rapid acquisition of high-level spatial information while searching and were deemed most efficient by participants while portals provided fast pre-orientation. Finally, we present insights into the applicability, usability, and effectiveness of the VR world switching methods we explored, and provide recommendations for their application and future context/world switching techniques and interfaces.", "AI": {"tldr": "\u7814\u7a76VR\u73af\u5883\u4e2d\u9ad8\u6548\u5207\u6362\u865a\u62df\u4e16\u754c\u7684\u754c\u9762\u8bbe\u8ba1\uff0c\u6bd4\u8f83\u95e8\u6237\u548c\u5fae\u578b\u4e16\u754c\u4e24\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u5fae\u578b\u4e16\u754c\u5728\u83b7\u53d6\u7a7a\u95f4\u4fe1\u606f\u65b9\u9762\u66f4\u4f18\uff0c\u95e8\u6237\u5728\u9884\u5b9a\u5411\u65b9\u9762\u66f4\u5feb\u3002", "motivation": "\u4e2a\u4eba\u7535\u8111\u548c\u624b\u6301\u8bbe\u5907\u6709\u952e\u76d8\u5feb\u6377\u952e\u548c\u6ed1\u52a8\u624b\u52bf\u652f\u6301\u5e94\u7528\u5feb\u901f\u5207\u6362\uff0c\u800c\u5f53\u524d\u7684VR\u7cfb\u7edf\u7f3a\u4e4f\u7c7b\u4f3c\u7684\u9ad8\u6548\u4e16\u754c\u5207\u6362\u673a\u5236\uff0c\u9700\u8981\u63a2\u7d22\u652f\u6301VR\u4e2d\u9ad8\u6548\u4e16\u754c\u5207\u6362\u7684\u7528\u6237\u754c\u9762\u3002", "method": "\u8bbe\u8ba1\u4e868\u79cd\u652f\u6301\u9884\u89c8\u548c\u9009\u62e9\u53ef\u7528\u865a\u62df\u4e16\u754c\u7684\u754c\u9762\uff0c\u5305\u62ec\u95e8\u6237\u548c\u5fae\u578b\u4e16\u754c\u65b9\u6cd5\u3002\u901a\u8fc7\u53d7\u63a7\u7684\u7ec4\u5185\u5b9e\u9a8c\uff08N=22\uff09\uff0c\u8ba9\u53c2\u4e0e\u8005\u57286\u4e2a\u4e0d\u540c\u73af\u5883\u4e2d\u9891\u7e41\u5207\u6362\u5b8c\u6210\u5bf9\u8c61\u6536\u96c6\u4efb\u52a1\uff0c\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "\u5fae\u578b\u4e16\u754c\u5728\u641c\u7d22\u65f6\u652f\u6301\u5feb\u901f\u83b7\u53d6\u9ad8\u5c42\u7ea7\u7a7a\u95f4\u4fe1\u606f\uff0c\u88ab\u53c2\u4e0e\u8005\u8ba4\u4e3a\u6700\u6709\u6548\u7387\uff1b\u95e8\u6237\u5219\u63d0\u4f9b\u5feb\u901f\u7684\u9884\u5b9a\u5411\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8eVR\u4e16\u754c\u5207\u6362\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3001\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\u7684\u89c1\u89e3\u3002", "conclusion": "\u63d0\u51fa\u4e86VR\u4e16\u754c\u5207\u6362\u65b9\u6cd5\u7684\u9002\u7528\u6027\u5efa\u8bae\u548c\u672a\u6765\u4e0a\u4e0b\u6587/\u4e16\u754c\u5207\u6362\u6280\u672f\u4e0e\u754c\u9762\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u4e3aVR\u7cfb\u7edf\u8bbe\u8ba1\u9ad8\u6548\u7684\u4e16\u754c\u5207\u6362\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2602.01153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01153", "abs": "https://arxiv.org/abs/2602.01153", "authors": ["Zhuo Chen", "Fei Ni", "Kaiyao Luo", "Zhiyuan Wu", "Xuyang Zhang", "Emmanouil Spyrakos-Papastavridis", "Lorenzo Jamone", "Nathan F. Lepora", "Jiankang Deng", "Shan Luo"], "title": "UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors", "comment": null, "summary": "Force sensing is essential for dexterous robot manipulation, but scaling force-aware policy learning is hindered by the heterogeneity of tactile sensors. Differences in sensing principles (e.g., optical vs. magnetic), form factors, and materials typically require sensor-specific data collection, calibration, and model training, thereby limiting generalisability. We propose UniForce, a novel unified tactile representation learning framework that learns a shared latent force space across diverse tactile sensors. UniForce reduces cross-sensor domain shift by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image), constrained by force equilibrium and image reconstruction losses to produce force-grounded representations. To avoid reliance on expensive external force/torque (F/T) sensors, we exploit static equilibrium and collect force-paired data via direct sensor--object--sensor interactions, enabling cross-sensor alignment with contact force. The resulting universal tactile encoder can be plugged into downstream force-aware robot manipulation tasks with zero-shot transfer, without retraining or finetuning. Extensive experiments on heterogeneous tactile sensors including GelSight, TacTip, and uSkin, demonstrate consistent improvements in force estimation over prior methods, and enable effective cross-sensor coordination in Vision-Tactile-Language-Action (VTLA) models for a robotic wiping task. Code and datasets will be released.", "AI": {"tldr": "UniForce\uff1a\u4e00\u4e2a\u7edf\u4e00\u7684\u89e6\u89c9\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u8de8\u591a\u79cd\u89e6\u89c9\u4f20\u611f\u5668\u7684\u5171\u4eab\u6f5c\u5728\u529b\u7a7a\u95f4\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u7684\u529b\u611f\u77e5\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u89e6\u89c9\u4f20\u611f\u5668\u5728\u539f\u7406\u3001\u5f62\u6001\u548c\u6750\u6599\u4e0a\u7684\u5f02\u8d28\u6027\u5bfc\u81f4\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4f20\u611f\u5668\u8fdb\u884c\u6570\u636e\u6536\u96c6\u3001\u6821\u51c6\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u529b\u611f\u77e5\u7b56\u7565\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faUniForce\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u9006\u52a8\u529b\u5b66\uff08\u56fe\u50cf\u5230\u529b\uff09\u548c\u6b63\u52a8\u529b\u5b66\uff08\u529b\u5230\u56fe\u50cf\uff09\uff0c\u5229\u7528\u529b\u5e73\u8861\u548c\u56fe\u50cf\u91cd\u5efa\u635f\u5931\u7ea6\u675f\uff0c\u5b66\u4e60\u8de8\u4f20\u611f\u5668\u7684\u5171\u4eab\u6f5c\u5728\u529b\u8868\u793a\u3002\u5229\u7528\u9759\u6001\u5e73\u8861\u539f\u7406\uff0c\u901a\u8fc7\u4f20\u611f\u5668-\u7269\u4f53-\u4f20\u611f\u5668\u76f4\u63a5\u4ea4\u4e92\u6536\u96c6\u529b\u914d\u5bf9\u6570\u636e\uff0c\u907f\u514d\u4f9d\u8d56\u6602\u8d35\u7684\u5916\u90e8\u529b/\u529b\u77e9\u4f20\u611f\u5668\u3002", "result": "\u5728GelSight\u3001TacTip\u548cuSkin\u7b49\u591a\u79cd\u5f02\u8d28\u89e6\u89c9\u4f20\u611f\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUniForce\u5728\u529b\u4f30\u8ba1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u5728\u89c6\u89c9-\u89e6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u4f20\u611f\u5668\u534f\u8c03\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u64e6\u62ed\u4efb\u52a1\u3002", "conclusion": "UniForce\u901a\u8fc7\u5b66\u4e60\u7edf\u4e00\u7684\u89e6\u89c9\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u89e6\u89c9\u4f20\u611f\u5668\u5f02\u8d28\u6027\u5e26\u6765\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u7684\u529b\u611f\u77e5\u64cd\u4f5c\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u89e6\u89c9\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01765", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01765", "abs": "https://arxiv.org/abs/2602.01765", "authors": ["Bingzheng Wang", "Xiaoyan Gu", "Hongbo Xu", "Hongcheng Li", "Zimo Yu", "Jiang Zhou", "Weiping Wang"], "title": "Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency", "comment": null, "summary": "Diffusion models have been widely deployed in AIGC services; however, their reliance on opaque training data and procedures exposes a broad attack surface for backdoor injection. In practical auditing scenarios, due to the protection of intellectual property and commercial confidentiality, auditors are typically unable to access model parameters, rendering existing white-box or query-intensive detection methods impractical. More importantly, even after the backdoor is detected, existing detoxification approaches are often trapped in a dilemma between detoxification effectiveness and generation quality.\n  In this work, we identify a previously unreported phenomenon called temporal noise unconsistency, where the noise predictions between adjacent diffusion timesteps is disrupted in specific temporal segments when the input is triggered, while remaining stable under clean inputs. Leveraging this finding, we propose Temporal Noise Consistency Defense (TNC-Defense), a unified framework for backdoor detection and detoxification. The framework first uses the adjacent timestep noise consistency to design a gray-box detection module, for identifying and locating anomalous diffusion timesteps. Furthermore, the framework uses the identified anomalous timesteps to construct a trigger-agnostic, timestep-aware detoxification module, which directly corrects the backdoor generation path. This effectively suppresses backdoor behavior while significantly reducing detoxification costs.\n  We evaluate the proposed method under five representative backdoor attack scenarios and compare it with state-of-the-art defenses. The results show that TNC-Defense improves the average detection accuracy by $11\\%$ with negligible additional overhead, and invalidates an average of $98.5\\%$ of triggered samples with only a mild degradation in generation quality.", "AI": {"tldr": "TNC-Defense\uff1a\u4e00\u79cd\u57fa\u4e8e\u65f6\u5e8f\u566a\u58f0\u4e0d\u4e00\u81f4\u6027\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u6269\u6563\u6a21\u578b\u540e\u95e8\u68c0\u6d4b\u4e0e\u53bb\u6bd2\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u53c2\u6570\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u9632\u5fa1\u540e\u95e8\u653b\u51fb\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728AIGC\u670d\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u4f9d\u8d56\u4e0d\u900f\u660e\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6d41\u7a0b\u5b58\u5728\u540e\u95e8\u6ce8\u5165\u98ce\u9669\u3002\u5b9e\u9645\u5ba1\u8ba1\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u77e5\u8bc6\u4ea7\u6743\u548c\u5546\u4e1a\u673a\u5bc6\u4fdd\u62a4\uff0c\u5ba1\u8ba1\u8005\u901a\u5e38\u65e0\u6cd5\u8bbf\u95ee\u6a21\u578b\u53c2\u6570\uff0c\u4f7f\u73b0\u6709\u767d\u76d2\u6216\u9ad8\u67e5\u8be2\u91cf\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u5b9e\u7528\u3002\u73b0\u6709\u53bb\u6bd2\u65b9\u6cd5\u5728\u53bb\u6bd2\u6548\u679c\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u4e24\u96be\u56f0\u5883\u3002", "method": "\u63d0\u51faTNC-Defense\u6846\u67b6\uff1a1) \u57fa\u4e8e\u76f8\u90bb\u65f6\u95f4\u6b65\u566a\u58f0\u4e00\u81f4\u6027\u7684\u7070\u76d2\u68c0\u6d4b\u6a21\u5757\uff0c\u8bc6\u522b\u548c\u5b9a\u4f4d\u5f02\u5e38\u6269\u6563\u65f6\u95f4\u6b65\uff1b2) \u5229\u7528\u8bc6\u522b\u7684\u5f02\u5e38\u65f6\u95f4\u6b65\u6784\u5efa\u89e6\u53d1\u65e0\u5173\u3001\u65f6\u95f4\u6b65\u611f\u77e5\u7684\u53bb\u6bd2\u6a21\u5757\uff0c\u76f4\u63a5\u4fee\u6b63\u540e\u95e8\u751f\u6210\u8def\u5f84\u3002", "result": "\u5728\u4e94\u79cd\u4ee3\u8868\u6027\u540e\u95e8\u653b\u51fb\u573a\u666f\u4e0b\u8bc4\u4f30\uff0cTNC-Defense\u5c06\u5e73\u5747\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u534711%\uff0c\u989d\u5916\u5f00\u9500\u53ef\u5ffd\u7565\uff1b\u4f7f98.5%\u7684\u89e6\u53d1\u6837\u672c\u5931\u6548\uff0c\u4ec5\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u8f7b\u5fae\u4e0b\u964d\u3002", "conclusion": "TNC-Defense\u901a\u8fc7\u5229\u7528\u65f6\u5e8f\u566a\u58f0\u4e0d\u4e00\u81f4\u6027\u73b0\u8c61\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6a21\u578b\u53c2\u6570\u8bbf\u95ee\u7684\u7edf\u4e00\u540e\u95e8\u9632\u5fa1\u6846\u67b6\uff0c\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u53bb\u6bd2\u6548\u679c\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2602.01166", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01166", "abs": "https://arxiv.org/abs/2602.01166", "authors": ["Shuanghao Bai", "Jing Lyu", "Wanqi Zhou", "Zhe Li", "Dakai Wang", "Lei Xing", "Xiaoguang Zhao", "Pengwei Wang", "Zhongyuan Wang", "Cheng Chi", "Badong Chen", "Shanghang Zhang"], "title": "Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \\href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.", "code_url": "https://loveju1y.github.io/Latent-Reasoning-VLA", "AI": {"tldr": "LaRA-VLA\u5c06\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u5185\u5316\u4e3a\u8fde\u7eed\u6f5c\u5728\u8868\u793a\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e90%", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u5f00\u9500\u9ad8\u3001\u79bb\u6563\u8868\u793a\u4e0e\u8fde\u7eed\u611f\u77e5\u63a7\u5236\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u8303\u5f0f", "method": "\u63d0\u51faLaRA-VLA\u6846\u67b6\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u7edf\u4e00\u63a8\u7406\u548c\u9884\u6d4b\uff1b\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ece\u663e\u5f0f\u6587\u672c/\u89c6\u89c9CoT\u76d1\u7763\u8fc7\u6e21\u5230\u6f5c\u5728\u63a8\u7406\uff0c\u518d\u9002\u914d\u5230\u52a8\u4f5c\u751f\u6210", "result": "\u5728\u4eff\u771f\u57fa\u51c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709VLA\u65b9\u6cd5\uff0c\u63a8\u7406\u5ef6\u8fdf\u76f8\u6bd4\u663e\u5f0fCoT\u65b9\u6cd5\u964d\u4f4e\u8fbe90%", "conclusion": "\u6f5c\u5728\u63a8\u7406\u662f\u5b9e\u73b0\u5b9e\u65f6\u5177\u8eab\u63a7\u5236\u7684\u6709\u6548\u9ad8\u6548\u8303\u5f0f\uff0cLaRA-VLA\u901a\u8fc7\u5185\u5316\u63a8\u7406\u5230\u8fde\u7eed\u6f5c\u5728\u8868\u793a\u89e3\u51b3\u4e86\u73b0\u6709CoT\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2602.01795", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01795", "abs": "https://arxiv.org/abs/2602.01795", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long", "Kwok-Yan Lam"], "title": "RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse", "comment": "under review", "summary": "Large Language Models (LLMs) are increasingly vulnerable to Prompt Injection (PI) attacks, where adversarial instructions hidden within retrieved contexts hijack the model's execution flow. Current defenses typically face a critical trade-off: prevention-based fine-tuning often degrades general utility via the \"alignment tax\", while detection-based filtering incurs prohibitive latency and memory costs. To bridge this gap, we propose RedVisor, a unified framework that synthesizes the explainability of detection systems with the seamless integration of prevention strategies. To the best of our knowledge, RedVisor is the first approach to leverage fine-grained reasoning paths to simultaneously detect attacks and guide the model's safe response. We implement this via a lightweight, removable adapter positioned atop the frozen backbone. This adapter serves a dual function: it first generates an explainable analysis that precisely localizes the injection and articulates the threat, which then explicitly conditions the model to reject the malicious command. Uniquely, the adapter is active only during this reasoning phase and is effectively muted during the subsequent response generation. This architecture yields two distinct advantages: (1) it mathematically preserves the backbone's original utility on benign inputs; and (2) it enables a novel KV Cache Reuse strategy, eliminating the redundant prefill computation inherent to decoupled pipelines. We further pioneer the integration of this defense into the vLLM serving engine with custom kernels. Experiments demonstrate that RedVisor outperforms state-of-the-art defenses in detection accuracy and throughput while incurring negligible utility loss.", "AI": {"tldr": "RedVisor\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53ef\u79fb\u9664\u9002\u914d\u5668\u540c\u65f6\u5b9e\u73b0\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u68c0\u6d4b\u548c\u9884\u9632\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u539f\u59cb\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5b89\u5168\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u9762\u4e34\u5173\u952e\u6743\u8861\uff1a\u57fa\u4e8e\u9884\u9632\u7684\u5fae\u8c03\u4f1a\u56e0\"\u5bf9\u9f50\u7a0e\"\u964d\u4f4e\u901a\u7528\u6027\u80fd\uff0c\u800c\u57fa\u4e8e\u68c0\u6d4b\u7684\u8fc7\u6ee4\u5219\u5e26\u6765\u9ad8\u5ef6\u8fdf\u548c\u5185\u5b58\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u68c0\u6d4b\u653b\u51fb\u53c8\u80fd\u6307\u5bfc\u5b89\u5168\u54cd\u5e94\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRedVisor\u6846\u67b6\uff0c\u5728\u51bb\u7ed3\u7684\u4e3b\u5e72\u6a21\u578b\u4e0a\u90e8\u7f72\u8f7b\u91cf\u7ea7\u53ef\u79fb\u9664\u9002\u914d\u5668\u3002\u8be5\u9002\u914d\u5668\u9996\u5148\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5206\u6790\u6765\u7cbe\u786e\u5b9a\u4f4d\u6ce8\u5165\u5e76\u63cf\u8ff0\u5a01\u80c1\uff0c\u7136\u540e\u663e\u5f0f\u6307\u5bfc\u6a21\u578b\u62d2\u7edd\u6076\u610f\u6307\u4ee4\u3002\u9002\u914d\u5668\u4ec5\u5728\u63a8\u7406\u9636\u6bb5\u6fc0\u6d3b\uff0c\u5728\u540e\u7eed\u54cd\u5e94\u751f\u6210\u65f6\u9759\u9ed8\u3002\u91c7\u7528KV\u7f13\u5b58\u91cd\u7528\u7b56\u7565\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\uff0c\u5e76\u96c6\u6210\u5230vLLM\u670d\u52a1\u5f15\u64ce\u3002", "result": "RedVisor\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u541e\u5410\u91cf\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u540c\u65f6\u5e26\u6765\u53ef\u5ffd\u7565\u7684\u6027\u80fd\u635f\u5931\u3002\u6570\u5b66\u4e0a\u4fdd\u7559\u4e86\u4e3b\u5e72\u6a21\u578b\u5728\u826f\u6027\u8f93\u5165\u4e0a\u7684\u539f\u59cb\u6027\u80fd\u3002", "conclusion": "RedVisor\u901a\u8fc7\u7edf\u4e00\u68c0\u6d4b\u548c\u9884\u9632\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u63d0\u793a\u6ce8\u5165\u9632\u5fa1\u4e2d\u7684\u5173\u952e\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u5e76\u901a\u8fc7KV\u7f13\u5b58\u91cd\u7528\u5b9e\u73b0\u4e86\u9ad8\u6548\u90e8\u7f72\u3002"}}
{"id": "2602.01481", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01481", "abs": "https://arxiv.org/abs/2602.01481", "authors": ["Yunhao Luo", "Arthur Caetano", "Avinash Ajit Nargund", "Tobias H\u00f6llerer", "Misha Sra"], "title": "How Users Perceive Mixed-Initiative AI: Attitudes Toward Assistance in Problem Solving", "comment": "ACM IUI '26 | 31st International Conference on Intelligent User Interfaces", "summary": "In mixed-initiative systems, the mode of AI assistance delivery can be as consequential as the assistance itself. We investigated two assistance delivery modes: on-demand help (users request via Button) and pre-scheduled help (assistance delivered at user-selected intervals, with user actions resetting the Timer). To evaluate these modes, we selected Rush Hour puzzles as the human-AI collaborative task because they capture elements of real-world problem solving such as analysis, resource management, and decision-making under constraints. To enhance ecological validity, we imposed monetary costs for both time and AI assistance, simulating scenarios where people must balance implicit or explicit trade-offs such as time pressure, financial limitations, or opportunity costs. Although task performance was comparable across modes, participants who used the pre-scheduled (Timer) mode reported more positive perceptions of the AI, even when their ending budget was low. This suggests that assistance delivery mode can shape user experience independent of task outcomes, indicating that human-AI systems may need to consider how AI assistance is delivered alongside improving task performance.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u6df7\u5408\u4e3b\u52a8\u7cfb\u7edf\u4e2d\u4e24\u79cdAI\u8f85\u52a9\u4ea4\u4ed8\u6a21\u5f0f\uff1a\u6309\u9700\u5e2e\u52a9\uff08\u6309\u94ae\u8bf7\u6c42\uff09\u548c\u9884\u5b9a\u65f6\u5e2e\u52a9\uff08\u5b9a\u65f6\u5668\u95f4\u9694\u4ea4\u4ed8\uff09\uff0c\u53d1\u73b0\u5728\u4efb\u52a1\u6027\u80fd\u76f8\u4f3c\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9a\u65f6\u5668\u6a21\u5f0f\u7684\u7528\u6237\u5bf9AI\u611f\u77e5\u66f4\u79ef\u6781", "motivation": "\u5728\u6df7\u5408\u4e3b\u52a8\u7cfb\u7edf\u4e2d\uff0cAI\u8f85\u52a9\u7684\u4ea4\u4ed8\u65b9\u5f0f\u53ef\u80fd\u548c\u8f85\u52a9\u672c\u8eab\u4e00\u6837\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u4ea4\u4ed8\u6a21\u5f0f\u5982\u4f55\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548c\u611f\u77e5\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u6743\u8861\u65f6\u95f4\u6210\u672c\u3001\u8d22\u52a1\u9650\u5236\u6216\u673a\u4f1a\u6210\u672c\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u3002", "method": "\u4f7f\u7528Rush Hour\u62fc\u56fe\u4f5c\u4e3a\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e86\u73b0\u5b9e\u95ee\u9898\u89e3\u51b3\u7684\u5143\u7d20\uff08\u5206\u6790\u3001\u8d44\u6e90\u7ba1\u7406\u3001\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u51b3\u7b56\uff09\u3002\u8bbe\u7f6e\u4e24\u79cd\u8f85\u52a9\u4ea4\u4ed8\u6a21\u5f0f\uff1a\u6309\u9700\u5e2e\u52a9\uff08\u6309\u94ae\u8bf7\u6c42\uff09\u548c\u9884\u5b9a\u65f6\u5e2e\u52a9\uff08\u7528\u6237\u9009\u62e9\u95f4\u9694\u65f6\u95f4\uff0c\u7528\u6237\u64cd\u4f5c\u91cd\u7f6e\u5b9a\u65f6\u5668\uff09\u3002\u4e3a\u589e\u5f3a\u751f\u6001\u6548\u5ea6\uff0c\u5bf9\u65f6\u95f4\u548cAI\u8f85\u52a9\u90fd\u65bd\u52a0\u8d27\u5e01\u6210\u672c\uff0c\u6a21\u62df\u4eba\u4eec\u5fc5\u987b\u5e73\u8861\u65f6\u95f4\u538b\u529b\u3001\u8d22\u52a1\u9650\u5236\u6216\u673a\u4f1a\u6210\u672c\u7684\u573a\u666f\u3002", "result": "\u5c3d\u7ba1\u4efb\u52a1\u6027\u80fd\u5728\u4e0d\u540c\u6a21\u5f0f\u95f4\u76f8\u5f53\uff0c\u4f46\u4f7f\u7528\u9884\u5b9a\u65f6\uff08\u5b9a\u65f6\u5668\uff09\u6a21\u5f0f\u7684\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u5bf9AI\u66f4\u79ef\u6781\u7684\u611f\u77e5\uff0c\u5373\u4f7f\u4ed6\u4eec\u7684\u6700\u7ec8\u9884\u7b97\u8f83\u4f4e\u3002\u8fd9\u8868\u660e\u8f85\u52a9\u4ea4\u4ed8\u6a21\u5f0f\u53ef\u4ee5\u72ec\u7acb\u4e8e\u4efb\u52a1\u7ed3\u679c\u5851\u9020\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "AI\u8f85\u52a9\u7684\u4ea4\u4ed8\u65b9\u5f0f\u80fd\u591f\u5f71\u54cd\u7528\u6237\u5bf9AI\u7684\u611f\u77e5\uff0c\u72ec\u7acb\u4e8e\u4efb\u52a1\u6027\u80fd\u7ed3\u679c\u3002\u8fd9\u8868\u660e\u4eba\u673a\u7cfb\u7edf\u4e0d\u4ec5\u9700\u8981\u6539\u8fdb\u4efb\u52a1\u6027\u80fd\uff0c\u8fd8\u9700\u8981\u8003\u8651\u5982\u4f55\u4ea4\u4ed8AI\u8f85\u52a9\uff0c\u4ee5\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2602.01932", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01932", "abs": "https://arxiv.org/abs/2602.01932", "authors": ["Kristopher Alex Schlett", "Bela Genge", "Savio Sciancalepore"], "title": "Things that Matter -- Identifying Interactions and IoT Device Types in Encrypted Matter Traffic", "comment": "11 pages, 1 figure, 12 tables", "summary": "Matter is the most recent application-layer standard for the Internet of Things (IoT). As one of its major selling points, Matter's design imposes particular attention to security and privacy: it provides validated secure session establishment protocols, and it uses robust security algorithms to secure communications between IoT devices and Matter controllers. However, to our knowledge, there is no systematic analysis investigating the extent to which a passive attacker, in possession of lower layer keys or exploiting security misconfiguration at those layers, could infer information by passively analyzing encrypted Matter traffic. In this paper, we fill this gap by analyzing the robustness of the Matter IoT standard to encrypted traffic analysis performed by a passive eavesdropper. By using various datasets collected from real-world testbeds and simulated setups, we identify patterns in metadata of the encrypted Matter traffic that allow inferring the specific interactions occurring between end devices and controllers. Moreover, we associate patterns in sequences of interactions to specific types of IoT devices. These patterns can be used to create fingerprints that allow a passive attacker to infer the type of devices used in the network, constituting a serious breach of users privacy. Our results reveal that we can identify specific Matter interactions that occur in encrypted traffic with over $95\\%$ accuracy also in the presence of packet losses and delays. Moreover, we can identify Matter device types with a minimum accuracy of $88\\%$. The CSA acknowledged our findings, and expressed the willingness to address such vulnerabilities in the next releases of the standard.", "AI": {"tldr": "Matter\u7269\u8054\u7f51\u6807\u51c6\u5b58\u5728\u52a0\u5bc6\u6d41\u91cf\u5206\u6790\u6f0f\u6d1e\uff0c\u88ab\u52a8\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u5206\u6790\u52a0\u5bc6\u6d41\u91cf\u5143\u6570\u636e\u6a21\u5f0f\u63a8\u65ad\u8bbe\u5907\u4ea4\u4e92\u548c\u8bbe\u5907\u7c7b\u578b\uff0c\u51c6\u786e\u7387\u5206\u522b\u8d85\u8fc795%\u548c88%\uff0c\u6784\u6210\u4e25\u91cd\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "motivation": "Matter\u4f5c\u4e3a\u6700\u65b0\u7684\u7269\u8054\u7f51\u5e94\u7528\u5c42\u6807\u51c6\uff0c\u867d\u7136\u8bbe\u8ba1\u4e0a\u7279\u522b\u5173\u6ce8\u5b89\u5168\u9690\u79c1\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u88ab\u52a8\u653b\u51fb\u8005\u901a\u8fc7\u52a0\u5bc6\u6d41\u91cf\u5206\u6790\u63a8\u65ad\u4fe1\u606f\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30Matter\u6807\u51c6\u5bf9\u52a0\u5bc6\u6d41\u91cf\u5206\u6790\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u5e73\u53f0\u548c\u6a21\u62df\u8bbe\u7f6e\u6536\u96c6\u7684\u5404\u79cd\u6570\u636e\u96c6\uff0c\u5206\u6790\u52a0\u5bc6Matter\u6d41\u91cf\u7684\u5143\u6570\u636e\u6a21\u5f0f\u3002\u8bc6\u522b\u5141\u8bb8\u63a8\u65ad\u7ec8\u7aef\u8bbe\u5907\u4e0e\u63a7\u5236\u5668\u4e4b\u95f4\u7279\u5b9a\u4ea4\u4e92\u7684\u6a21\u5f0f\uff0c\u5e76\u5c06\u4ea4\u4e92\u5e8f\u5217\u6a21\u5f0f\u4e0e\u7279\u5b9a\u7269\u8054\u7f51\u8bbe\u5907\u7c7b\u578b\u5173\u8054\uff0c\u521b\u5efa\u53ef\u7528\u4e8e\u8bbe\u5907\u7c7b\u578b\u63a8\u65ad\u7684\u6307\u7eb9\u3002", "result": "\u5373\u4f7f\u5728\u5b58\u5728\u4e22\u5305\u548c\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u4ee5\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\u8bc6\u522b\u52a0\u5bc6\u6d41\u91cf\u4e2d\u7684\u7279\u5b9aMatter\u4ea4\u4e92\u3002\u80fd\u591f\u4ee5\u6700\u4f4e88%\u7684\u51c6\u786e\u7387\u8bc6\u522bMatter\u8bbe\u5907\u7c7b\u578b\u3002\u8fde\u63a5\u6807\u51c6\u8054\u76df(CSA)\u5df2\u786e\u8ba4\u8fd9\u4e9b\u53d1\u73b0\uff0c\u5e76\u8868\u793a\u613f\u610f\u5728\u6807\u51c6\u7684\u4e0b\u4e00\u4e2a\u7248\u672c\u4e2d\u89e3\u51b3\u8fd9\u4e9b\u6f0f\u6d1e\u3002", "conclusion": "Matter\u7269\u8054\u7f51\u6807\u51c6\u5b58\u5728\u4e25\u91cd\u7684\u52a0\u5bc6\u6d41\u91cf\u5206\u6790\u6f0f\u6d1e\uff0c\u88ab\u52a8\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u5206\u6790\u52a0\u5bc6\u6d41\u91cf\u5143\u6570\u636e\u63a8\u65ad\u8bbe\u5907\u4ea4\u4e92\u548c\u8bbe\u5907\u7c7b\u578b\uff0c\u6784\u6210\u91cd\u5927\u9690\u79c1\u98ce\u9669\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u6807\u51c6\u5728\u62b5\u5fa1\u88ab\u52a8\u6d41\u91cf\u5206\u6790\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u8bbe\u8ba1\u4ee5\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2602.01494", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01494", "abs": "https://arxiv.org/abs/2602.01494", "authors": ["Yuqi Hang"], "title": "Draw2Learn: A Human-AI Collaborative Tool for Drawing-Based Science Learning", "comment": null, "summary": "Drawing supports learning by externalizing mental models, but providing timely feedback at scale remains challenging. We present Draw2Learn, a system that explores how AI can act as a supportive teammate during drawing-based learning. The design translates learning principles into concrete interaction patterns: AI generates structured drawing quests, provides optional visual scaffolds, monitors progress, and delivers multidimensional feedback. We collected formative user feedback during system development and open-ended comments. Feedback showed positive ratings for usability, usefulness, and user experience, with themes highlighting AI scaffolding value and learner autonomy. This work contributes a design framework for teammate-oriented AI in generative learning and identifies key considerations for future research.", "AI": {"tldr": "Draw2Learn\uff1a\u4e00\u4e2aAI\u8f85\u52a9\u7ed8\u56fe\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7ed8\u56fe\u4efb\u52a1\u3001\u89c6\u89c9\u652f\u67b6\u3001\u8fdb\u5ea6\u76d1\u63a7\u548c\u591a\u7ef4\u53cd\u9988\u6765\u652f\u6301\u5b66\u4e60\uff0c\u7528\u6237\u53cd\u9988\u663e\u793a\u7cfb\u7edf\u5728\u53ef\u7528\u6027\u3001\u6709\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u7ed8\u56fe\u901a\u8fc7\u5916\u5316\u5fc3\u7406\u6a21\u578b\u6765\u652f\u6301\u5b66\u4e60\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u5b66\u4e60\u4e2d\u63d0\u4f9b\u53ca\u65f6\u53cd\u9988\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\u5982\u4f55\u4f5c\u4e3a\u652f\u6301\u6027\u961f\u53cb\u5728\u57fa\u4e8e\u7ed8\u56fe\u7684\u5b66\u4e60\u4e2d\u53d1\u6325\u4f5c\u7528\u3002", "method": "\u5f00\u53d1\u4e86Draw2Learn\u7cfb\u7edf\uff0c\u5c06\u5b66\u4e60\u539f\u5219\u8f6c\u5316\u4e3a\u5177\u4f53\u4ea4\u4e92\u6a21\u5f0f\uff1aAI\u751f\u6210\u7ed3\u6784\u5316\u7ed8\u56fe\u4efb\u52a1\u3001\u63d0\u4f9b\u53ef\u9009\u89c6\u89c9\u652f\u67b6\u3001\u76d1\u63a7\u5b66\u4e60\u8fdb\u5ea6\u3001\u5e76\u63d0\u4f9b\u591a\u7ef4\u53cd\u9988\u3002\u5728\u7cfb\u7edf\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u6536\u96c6\u4e86\u5f62\u6210\u6027\u7528\u6237\u53cd\u9988\u548c\u5f00\u653e\u5f0f\u8bc4\u8bba\u3002", "result": "\u7528\u6237\u53cd\u9988\u663e\u793a\u7cfb\u7edf\u5728\u53ef\u7528\u6027\u3001\u6709\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u83b7\u5f97\u79ef\u6781\u8bc4\u4ef7\uff0c\u4e3b\u9898\u5206\u6790\u7a81\u51fa\u4e86AI\u652f\u67b6\u7684\u4ef7\u503c\u548c\u5b66\u4e60\u8005\u81ea\u4e3b\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u751f\u6210\u5f0f\u5b66\u4e60\u4e2d\u7684\u961f\u53cb\u5bfc\u5411AI\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5e76\u786e\u5b9a\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u5c55\u793a\u4e86AI\u4f5c\u4e3a\u5b66\u4e60\u652f\u6301\u4f19\u4f34\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.01226", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01226", "abs": "https://arxiv.org/abs/2602.01226", "authors": ["Aditya Shibu", "Marah Saleh", "Mohamed Al-Musleh", "Nidhal Abdulaziz"], "title": "SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., \"Form a circle\") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.", "AI": {"tldr": "SkySim\u662f\u4e00\u4e2a\u57fa\u4e8eROS2\u548cGazebo\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u4eff\u771f\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u5c42\u89c4\u5212\uff0c\u7ed3\u5408\u4eba\u5de5\u52bf\u573a\u5b89\u5168\u8fc7\u6ee4\u5668\u786e\u4fdd\u8f68\u8ff9\u5b89\u5168\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u65e0\u4eba\u673a\u96c6\u7fa4\u3002", "motivation": "\u65e0\u4eba\u673a\u96c6\u7fa4\u5728\u7269\u6d41\u3001\u519c\u4e1a\u548c\u76d1\u63a7\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u9002\u5e94\u6027\u6709\u9650\u3002\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u63a7\u5236\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u57fa\u7840\u5bfc\u81f4\u751f\u6210\u4e0d\u5b89\u5168\u8f68\u8ff9\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528LLM\u8ba4\u77e5\u80fd\u529b\u53c8\u80fd\u786e\u4fdd\u5b89\u5168\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SkySim\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1a1) \u4f7f\u7528Gemini 3.5 Pro\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u7528\u6237\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff08\u5982\"\u5f62\u6210\u5706\u5f62\"\uff09\u8f6c\u6362\u4e3a\u7a7a\u95f4\u822a\u70b9\uff1b2) \u57fa\u4e8e\u5b9e\u65f6\u65e0\u4eba\u673a\u72b6\u6001\u4fe1\u606f\uff1b3) \u4eba\u5de5\u52bf\u573a\u5b89\u5168\u8fc7\u6ee4\u5668\u4ee520Hz\u9891\u7387\u6267\u884c\u6700\u5c0f\u8c03\u6574\uff0c\u907f\u514d\u78b0\u649e\u3001\u9075\u5b88\u8fd0\u52a8\u5b66\u9650\u5236\u548c\u5730\u7406\u56f4\u680f\uff1b4) \u5728ROS2\u548cGazebo\u4e2d\u5b9e\u73b0\u4eff\u771f\u6846\u67b6\u3002", "result": "\u4f7f\u75283\u300110\u548c30\u67b6Crazyflie\u65e0\u4eba\u673a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff1a1) \u7a7a\u95f4\u63a8\u7406\u51c6\u786e\u7387\u8fbe\u5230100%\uff08\u6d4b\u8bd5\u6240\u6709\u51e0\u4f55\u57fa\u5143\uff09\uff1b2) \u5b9e\u65f6\u78b0\u649e\u9884\u9632\u6709\u6548\uff1b3) \u7cfb\u7edf\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002\u6846\u67b6\u4f7f\u975e\u4e13\u5bb6\u7528\u6237\u80fd\u591f\u8fed\u4ee3\u4f18\u5316\u884c\u4e3a\u3002", "conclusion": "SkySim\u6210\u529f\u5c06AI\u8ba4\u77e5\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u5b89\u5168\u8981\u6c42\u76f8\u7ed3\u5408\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u63a7\u5236\u63d0\u4f9b\u4e86\u5b89\u5168\u53ef\u9760\u7684\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u805a\u7126\u4e8e\u786c\u4ef6\u96c6\u6210\u3002"}}
{"id": "2602.01517", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01517", "abs": "https://arxiv.org/abs/2602.01517", "authors": ["ATM Mizanur Rahman", "Syed Ishtiaque Ahmed", "Sharifa Sultana"], "title": "Data Repair", "comment": null, "summary": "This paper investigates data repair practices through a six-month-long ethnographic study in Bangladesh. Our interviews and field observations with data repairers and related stakeholders found that, alongside the scarcity of high-precision machinery and access to advanced software, data repair work is constrained by cross-language learning resources and the protective nature of documenting, curating, and sharing the experiences and knowledge among local peers. Repairers turning to external resources such as foreign forums and LLMs also revealed their frustrating experiences and the postcolonial ethical tensions they encountered. We noted that both anticipated technical labor and the emotionality of data were taken into account for pricing the data repair job, which contributed to their market sustainability strategies. Engaging with repair, infrastructure, and data poverty discourse, we argue that data repair practices represent a crucial challenge and opportunity for HCI in advancing global efforts toward data equity.", "AI": {"tldr": "\u901a\u8fc7\u5b5f\u52a0\u62c9\u56fd6\u4e2a\u6708\u7684\u6c11\u65cf\u5fd7\u7814\u7a76\u53d1\u73b0\uff0c\u6570\u636e\u4fee\u590d\u5de5\u4f5c\u9762\u4e34\u8bbe\u5907\u7a00\u7f3a\u3001\u8de8\u8bed\u8a00\u5b66\u4e60\u8d44\u6e90\u4e0d\u8db3\u3001\u77e5\u8bc6\u5171\u4eab\u4fdd\u62a4\u6027\u7b49\u7ea6\u675f\uff0c\u540c\u65f6\u4fee\u590d\u8005\u4f7f\u7528\u5916\u90e8\u8d44\u6e90\u65f6\u906d\u9047\u632b\u8d25\u611f\u548c\u540e\u6b96\u6c11\u4f26\u7406\u5f20\u529b\uff0c\u5b9a\u4ef7\u7b56\u7565\u8003\u8651\u6280\u672f\u52b3\u52a8\u548c\u60c5\u611f\u56e0\u7d20\uff0c\u5bf9HCI\u63a8\u8fdb\u6570\u636e\u516c\u5e73\u5177\u6709\u91cd\u8981\u542f\u793a\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6570\u636e\u4fee\u590d\u5b9e\u8df5\u4e2d\u7684\u73b0\u5b9e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5982\u4f55\u5f00\u5c55\u6570\u636e\u4fee\u590d\u5de5\u4f5c\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u5b9e\u8df5\u5bf9\u5168\u7403\u6570\u636e\u516c\u5e73\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u75286\u4e2a\u6708\u7684\u6c11\u65cf\u5fd7\u7814\u7a76\uff0c\u5305\u62ec\u5bf9\u6570\u636e\u4fee\u590d\u8005\u53ca\u76f8\u5173\u5229\u76ca\u76f8\u5173\u8005\u7684\u8bbf\u8c08\u548c\u5b9e\u5730\u89c2\u5bdf\u3002", "result": "\u53d1\u73b0\u6570\u636e\u4fee\u590d\u5de5\u4f5c\u53d7\u9ad8\u7cbe\u5ea6\u8bbe\u5907\u7a00\u7f3a\u3001\u5148\u8fdb\u8f6f\u4ef6\u8bbf\u95ee\u6709\u9650\u3001\u8de8\u8bed\u8a00\u5b66\u4e60\u8d44\u6e90\u4e0d\u8db3\u3001\u672c\u5730\u540c\u884c\u95f4\u77e5\u8bc6\u5171\u4eab\u4fdd\u62a4\u6027\u7b49\u56e0\u7d20\u5236\u7ea6\uff1b\u4fee\u590d\u8005\u4f7f\u7528\u5916\u56fd\u8bba\u575b\u548cLLMs\u7b49\u5916\u90e8\u8d44\u6e90\u65f6\u7ecf\u5386\u632b\u8d25\u611f\u548c\u540e\u6b96\u6c11\u4f26\u7406\u5f20\u529b\uff1b\u5b9a\u4ef7\u7b56\u7565\u540c\u65f6\u8003\u8651\u9884\u671f\u6280\u672f\u52b3\u52a8\u548c\u6570\u636e\u60c5\u611f\u6027\uff0c\u5f62\u6210\u5e02\u573a\u53ef\u6301\u7eed\u6027\u7b56\u7565\u3002", "conclusion": "\u6570\u636e\u4fee\u590d\u5b9e\u8df5\u4ee3\u8868\u4e86HCI\u5728\u63a8\u8fdb\u5168\u7403\u6570\u636e\u516c\u5e73\u52aa\u529b\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u673a\u9047\uff0c\u9700\u8981\u4e0e\u4fee\u590d\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u6570\u636e\u8d2b\u56f0\u7b49\u8bdd\u8bed\u8fdb\u884c\u5bf9\u8bdd\u3002"}}
{"id": "2602.01266", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01266", "abs": "https://arxiv.org/abs/2602.01266", "authors": ["Grzegorz Malczyk", "Mihir Kulkarni", "Kostas Alexis"], "title": "Reinforcement Learning for Active Perception in Autonomous Navigation", "comment": "Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "This paper addresses the challenge of active perception within autonomous navigation in complex, unknown environments. Revisiting the foundational principles of active perception, we introduce an end-to-end reinforcement learning framework in which a robot must not only reach a goal while avoiding obstacles, but also actively control its onboard camera to enhance situational awareness. The policy receives observations comprising the robot state, the current depth frame, and a particularly local geometry representation built from a short history of depth readings. To couple collision-free motion planning with information-driven active camera control, we augment the navigation reward with a voxel-based information metric. This enables an aerial robot to learn a robust policy that balances goal-directed motion with exploratory sensing. Extensive evaluation demonstrates that our strategy achieves safer flight compared to using fixed, non-actuated camera baselines while also inducing intrinsic exploratory behaviors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u673a\u5668\u4eba\u540c\u65f6\u8fdb\u884c\u907f\u969c\u5bfc\u822a\u548c\u4e3b\u52a8\u76f8\u673a\u63a7\u5236\u4ee5\u589e\u5f3a\u73af\u5883\u611f\u77e5\u80fd\u529b", "motivation": "\u89e3\u51b3\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u7684\u4e3b\u52a8\u611f\u77e5\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5c06\u5bfc\u822a\u548c\u611f\u77e5\u5206\u79bb\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8026\u5408\u8fd0\u52a8\u89c4\u5212\u548c\u4e3b\u52a8\u76f8\u673a\u63a7\u5236\u7684\u7edf\u4e00\u6846\u67b6", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7b56\u7565\u63a5\u6536\u673a\u5668\u4eba\u72b6\u6001\u3001\u5f53\u524d\u6df1\u5ea6\u5e27\u548c\u5c40\u90e8\u51e0\u4f55\u8868\u793a\uff1b\u901a\u8fc7\u4f53\u7d20\u5316\u4fe1\u606f\u5ea6\u91cf\u589e\u5f3a\u5bfc\u822a\u5956\u52b1\uff0c\u5b9e\u73b0\u78b0\u649e\u89c4\u907f\u4e0e\u4fe1\u606f\u9a71\u52a8\u76f8\u673a\u63a7\u5236\u7684\u8026\u5408", "result": "\u76f8\u6bd4\u56fa\u5b9a\u76f8\u673a\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u7684\u98de\u884c\uff0c\u5e76\u8bf1\u5bfc\u51fa\u5185\u5728\u7684\u63a2\u7d22\u884c\u4e3a\uff0c\u5728\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027", "conclusion": "\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6210\u529f\u5730\u5c06\u4e3b\u52a8\u611f\u77e5\u4e0e\u81ea\u4e3b\u5bfc\u822a\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u4fe1\u606f\u9a71\u52a8\u7684\u76f8\u673a\u63a7\u5236\u589e\u5f3a\u4e86\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02147", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02147", "abs": "https://arxiv.org/abs/2602.02147", "authors": ["Jiayao Wang", "Yang Song", "Zhendong Zhao", "Jiale Zhang", "Qilin Wu", "Wenliang Yuan", "Junwu Zhu", "Dongfang Zhao"], "title": "HPE: Hallucinated Positive Entanglement for Backdoor Attacks in Federated Self-Supervised Learning", "comment": null, "summary": "Federated self-supervised learning (FSSL) enables collaborative training of self-supervised representation models without sharing raw unlabeled data. While it serves as a crucial paradigm for privacy-preserving learning, its security remains vulnerable to backdoor attacks, where malicious clients manipulate local training to inject targeted backdoors. Existing FSSL attack methods, however, often suffer from low utilization of poisoned samples, limited transferability, and weak persistence. To address these limitations, we propose a new backdoor attack method for FSSL, namely Hallucinated Positive Entanglement (HPE). HPE first employs hallucination-based augmentation using synthetic positive samples to enhance the encoder's embedding of backdoor features. It then introduces feature entanglement to enforce tight binding between triggers and backdoor samples in the representation space. Finally, selective parameter poisoning and proximity-aware updates constrain the poisoned model within the vicinity of the global model, enhancing its stability and persistence. Experimental results on several FSSL scenarios and datasets show that HPE significantly outperforms existing backdoor attack methods in performance and exhibits strong robustness under various defense mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHPE\u7684\u65b0\u578b\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e7b\u89c9\u589e\u5f3a\u3001\u7279\u5f81\u7ea0\u7f20\u548c\u9009\u62e9\u6027\u53c2\u6570\u4e2d\u6bd2\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u4e2d\u6bd2\u6837\u672c\u5229\u7528\u7387\u4f4e\u3001\u53ef\u8fc1\u79fb\u6027\u6709\u9650\u548c\u6301\u4e45\u6027\u5f31\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u63d0\u5347\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u5b89\u5168\u6027\u3002", "method": "HPE\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u4f7f\u7528\u5408\u6210\u6b63\u6837\u672c\u8fdb\u884c\u5e7b\u89c9\u589e\u5f3a\uff0c\u63d0\u5347\u7f16\u7801\u5668\u5bf9\u540e\u95e8\u7279\u5f81\u7684\u5d4c\u5165\u80fd\u529b\uff1b2) \u5f15\u5165\u7279\u5f81\u7ea0\u7f20\u6280\u672f\uff0c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5f3a\u5236\u89e6\u53d1\u5668\u548c\u540e\u95e8\u6837\u672c\u7d27\u5bc6\u7ed3\u5408\uff1b3) \u91c7\u7528\u9009\u62e9\u6027\u53c2\u6570\u4e2d\u6bd2\u548c\u90bb\u8fd1\u611f\u77e5\u66f4\u65b0\uff0c\u5c06\u4e2d\u6bd2\u6a21\u578b\u7ea6\u675f\u5728\u5168\u5c40\u6a21\u578b\u9644\u8fd1\uff0c\u589e\u5f3a\u7a33\u5b9a\u6027\u548c\u6301\u4e45\u6027\u3002", "result": "\u5728\u591a\u4e2a\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u573a\u666f\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHPE\u5728\u653b\u51fb\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u9632\u5fa1\u673a\u5236\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "HPE\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u5e7b\u89c9\u589e\u5f3a\u548c\u7279\u5f81\u7ea0\u7f20\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u540e\u95e8\u653b\u51fb\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8bc4\u4f30\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u5de5\u5177\u3002"}}
{"id": "2602.01525", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01525", "abs": "https://arxiv.org/abs/2602.01525", "authors": ["Jingyue Zhang", "J. D. Zamfirescu-Pereira", "Elena L. Glassman", "Damien Masson", "Ian Arawjo"], "title": "How Notations Evolve: A Historical Analysis with Implications for Supporting User-Defined Abstractions", "comment": "23 pages, 4 figures", "summary": "Traditional human-computer interaction takes place through formally-specified systems like structured UIs and programming languages. Recent AI systems promise a new set of informal interactions with computers through natural language and other notational forms. These informal interactions can then lead to formal representations, but depend upon pre-existing formalisms known to both humans and AI. What about novel formalisms and notations? How are new abstractions created, evolved, and incrementally formalized over time -- and how might new systems, in turn, be explicitly designed to support these processes? We conduct a comparative historical analysis of notation development to identify some relevant characteristics. These include three social stages of notation development: invention & incubation, dispersion & divergence, and institutionalization & sanctification, as well as three functional stages: descriptive, generative, and evaluative. Within and across these stages, we detail several patterns, such as the role of linking and grounding metaphors, dimensions of meaningful variation, and analogical alignment. Finally, we offer some implications for design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6bd4\u8f83\u5386\u53f2\u5206\u6790\u63a2\u8ba8\u4e86\u65b0\u578b\u5f62\u5f0f\u7cfb\u7edf\u548c\u7b26\u53f7\u5982\u4f55\u88ab\u521b\u9020\u3001\u6f14\u8fdb\u548c\u9010\u6b65\u5f62\u5f0f\u5316\uff0c\u63d0\u51fa\u4e86\u7b26\u53f7\u53d1\u5c55\u7684\u4e09\u4e2a\u9636\u6bb5\u548c\u529f\u80fd\u9636\u6bb5\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u542f\u793a\u3002", "motivation": "\u4f20\u7edf\u4eba\u673a\u4ea4\u4e92\u901a\u8fc7\u7ed3\u6784\u5316UI\u548c\u7f16\u7a0b\u8bed\u8a00\u7b49\u6b63\u5f0f\u7cfb\u7edf\u8fdb\u884c\uff0c\u800c\u65b0\u5174AI\u7cfb\u7edf\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7b49\u975e\u6b63\u5f0f\u5f62\u5f0f\u5b9e\u73b0\u4ea4\u4e92\u3002\u8fd9\u4e9b\u975e\u6b63\u5f0f\u4ea4\u4e92\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6b63\u5f0f\u8868\u793a\uff0c\u4f46\u4f9d\u8d56\u4e8e\u4eba\u7c7b\u548cAI\u90fd\u5df2\u77e5\u7684\u73b0\u6709\u5f62\u5f0f\u7cfb\u7edf\u3002\u8bba\u6587\u5173\u6ce8\u7684\u662f\u65b0\u578b\u5f62\u5f0f\u7cfb\u7edf\u548c\u7b26\u53f7\u5982\u4f55\u88ab\u521b\u9020\u3001\u6f14\u8fdb\u548c\u9010\u6b65\u5f62\u5f0f\u5316\uff0c\u4ee5\u53ca\u5982\u4f55\u8bbe\u8ba1\u65b0\u7cfb\u7edf\u6765\u652f\u6301\u8fd9\u4e9b\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u6bd4\u8f83\u5386\u53f2\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u7b26\u53f7\u53d1\u5c55\u7684\u5386\u53f2\u6848\u4f8b\uff0c\u8bc6\u522b\u76f8\u5173\u7279\u5f81\u548c\u6a21\u5f0f\u3002\u5206\u6790\u5305\u62ec\u7b26\u53f7\u53d1\u5c55\u7684\u4e09\u4e2a\u9636\u6bb5\uff08\u53d1\u660e\u4e0e\u5b75\u5316\u3001\u4f20\u64ad\u4e0e\u5206\u5316\u3001\u5236\u5ea6\u5316\u4e0e\u795e\u5723\u5316\uff09\u548c\u4e09\u4e2a\u529f\u80fd\u9636\u6bb5\uff08\u63cf\u8ff0\u6027\u3001\u751f\u6210\u6027\u3001\u8bc4\u4f30\u6027\uff09\uff0c\u4ee5\u53ca\u5728\u8fd9\u4e9b\u9636\u6bb5\u4e2d\u7684\u5177\u4f53\u6a21\u5f0f\u3002", "result": "\u8bc6\u522b\u51fa\u7b26\u53f7\u53d1\u5c55\u7684\u4e09\u4e2a\u793e\u4f1a\u9636\u6bb5\uff1a\u53d1\u660e\u4e0e\u5b75\u5316\u3001\u4f20\u64ad\u4e0e\u5206\u5316\u3001\u5236\u5ea6\u5316\u4e0e\u795e\u5723\u5316\uff1b\u4ee5\u53ca\u4e09\u4e2a\u529f\u80fd\u9636\u6bb5\uff1a\u63cf\u8ff0\u6027\u3001\u751f\u6210\u6027\u3001\u8bc4\u4f30\u6027\u3002\u8be6\u7ec6\u63cf\u8ff0\u4e86\u5728\u8fd9\u4e9b\u9636\u6bb5\u4e2d\u7684\u591a\u79cd\u6a21\u5f0f\uff0c\u5305\u62ec\u94fe\u63a5\u548c\u57fa\u7840\u9690\u55bb\u7684\u4f5c\u7528\u3001\u6709\u610f\u4e49\u53d8\u5316\u7684\u7ef4\u5ea6\u3001\u7c7b\u6bd4\u5bf9\u9f50\u7b49\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u5386\u53f2\u5206\u6790\u63ed\u793a\u4e86\u65b0\u578b\u5f62\u5f0f\u7cfb\u7edf\u548c\u7b26\u53f7\u53d1\u5c55\u7684\u8fc7\u7a0b\u548c\u6a21\u5f0f\uff0c\u4e3a\u8bbe\u8ba1\u652f\u6301\u8fd9\u4e9b\u8fc7\u7a0b\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u9645\u542f\u793a\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5982\u4f55\u521b\u9020\u548c\u6f14\u8fdb\u65b0\u7684\u62bd\u8c61\u8868\u793a\u5f62\u5f0f\u3002"}}
{"id": "2602.01385", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01385", "abs": "https://arxiv.org/abs/2602.01385", "authors": ["Xiangyu Li", "Mingwei Lai", "Mengke Zhang", "Junxiao Lin", "Tiancheng Lai", "Junping Zhi", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design", "comment": null, "summary": "Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot's dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot's multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u4e09\u6816\u673a\u5668\u4eba\uff0c\u91c7\u7528\u56db\u65cb\u7ffc\u7ed3\u6784\u914d\u88ab\u52a8\u8f6e\uff0c\u901a\u8fc7\u504f\u5fc3\u91cd\u5fc3\u8bbe\u8ba1\u548c\u7edf\u4e00\u63a8\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u7a7a\u4e2d\u3001\u9646\u5730\u3001\u6c34\u4e0b\u7684\u9ad8\u6548\u591a\u57df\u8fd0\u52a8\u4e0e\u8de8\u57df\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709\u4e09\u6816\u673a\u5668\u4eba\u8bbe\u8ba1\u4e3b\u8981\u5173\u6ce8\u53cc\u6a21\u5f0f\u5e73\u53f0\uff0c\u5b58\u5728\u673a\u68b0\u590d\u6742\u5ea6\u9ad8\u6216\u63a8\u8fdb\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5b9e\u73b0\u7a7a\u4e2d\u3001\u9646\u5730\u3001\u6c34\u4e0b\u591a\u57df\u8fd0\u52a8\u4e0e\u8de8\u57df\u8f6c\u6362\u7684\u673a\u5668\u4eba\u3002", "method": "1. \u91c7\u7528\u6781\u7b80\u8bbe\u8ba1\uff1a\u56db\u65cb\u7ffc\u7ed3\u6784\u914d\u4e24\u4e2a\u88ab\u52a8\u8f6e\uff0c\u65e0\u9700\u989d\u5916\u6267\u884c\u5668\uff1b2. \u5f15\u5165\u504f\u5fc3\u91cd\u5fc3\u8bbe\u8ba1\uff0c\u4f7f\u63a8\u529b\u4e0e\u8fd0\u52a8\u65b9\u5411\u81ea\u7136\u5bf9\u9f50\uff0c\u63d0\u9ad8\u5730\u9762/\u6d77\u5e95\u652f\u6491\u8fd0\u52a8\u6548\u7387\uff1b3. \u5f00\u53d1\u57fa\u4e8e\u78c1\u573a\u5b9a\u5411\u63a7\u5236\u7684\u7edf\u4e00\u63a8\u8fdb\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e0d\u540c\u4ecb\u8d28\u4e2d\u626d\u77e9\u5339\u914d\u95ee\u9898\uff1b4. \u63d0\u51fa\u6df7\u5408\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236-PID\u63a7\u5236\u7cfb\u7edf\uff0c\u786e\u4fdd\u7a33\u5b9a\u591a\u57df\u8fd0\u52a8\u4e0e\u65e0\u7f1d\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u673a\u5668\u4eba\u5177\u5907\u591a\u57df\u8fd0\u52a8\u548c\u8de8\u6a21\u5f0f\u8f6c\u6362\u80fd\u529b\uff0c\u6240\u63d0\u51fa\u7684\u63a8\u8fdb\u7cfb\u7edf\u5728\u6548\u7387\u548c\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u504f\u5fc3\u91cd\u5fc3\u8bbe\u8ba1\u663e\u8457\u63d0\u9ad8\u4e86\u5730\u9762\u652f\u6491\u8fd0\u52a8\u6548\u7387\uff0c\u7edf\u4e00\u63a8\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4e0d\u540c\u4ecb\u8d28\u4e2d\u7684\u7cbe\u786e\u5feb\u901f\u53cc\u5411\u63a8\u529b\u63a7\u5236\u3002", "conclusion": "\u901a\u8fc7\u6781\u7b80\u7684\u673a\u68b0\u8bbe\u8ba1\u3001\u521b\u65b0\u7684\u504f\u5fc3\u91cd\u5fc3\u5e03\u5c40\u548c\u7edf\u4e00\u7684\u63a8\u8fdb\u63a7\u5236\u7cfb\u7edf\uff0c\u6210\u529f\u5f00\u53d1\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u4e09\u6816\u673a\u5668\u4eba\uff0c\u80fd\u591f\u5728\u7a7a\u4e2d\u3001\u9646\u5730\u548c\u6c34\u4e0b\u73af\u5883\u4e2d\u7a33\u5b9a\u8fd0\u52a8\u5e76\u5b9e\u73b0\u65e0\u7f1d\u8de8\u57df\u8f6c\u6362\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2602.01527", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01527", "abs": "https://arxiv.org/abs/2602.01527", "authors": ["Brian Keith-Norambuena"], "title": "Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition", "comment": "Preprint submitted to IEEE TVCG on February 2026", "summary": "Visualization's design knowledge-effectiveness rankings, encoding guidelines, color models, preattentive processing rules -- derives from six decades of psychophysical studies of human vision. Yet vision-language models (VLMs) increasingly consume chart images in automated analysis pipelines, and a growing body of benchmark evidence indicates that this human-centered knowledge base does not straightforwardly transfer to machine audiences. Machines exhibit different encoding performance patterns, process images through patch-based tokenization rather than holistic perception, and fail on design patterns that pose no difficulty for humans-while occasionally succeeding where humans struggle. Current approaches address this gap primarily by bypassing vision entirely, converting charts to data tables or structured text. We argue that this response forecloses a more fundamental question: what visual representations would actually serve machine cognition well? This paper makes the case that the visualization field needs to investigate machine-oriented visual design as a distinct research problem. We synthesize evidence from VLM benchmarks, visual reasoning research, and visualization literacy studies to show that the human-machine perceptual divergence is qualitative, not merely quantitative, and critically examine the prevailing bypassing approach. We propose a conceptual distinction between human-oriented and machine-oriented visualization-not as an engineering architecture but as a recognition that different audiences may require fundamentally different design foundations-and outline a research agenda for developing the empirical foundations the field currently lacks: the beginnings of a \"machine Bertin\" to complement the human-centered knowledge the field already possesses.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u53ef\u89c6\u5316\u9886\u57df\u9700\u8981\u5c06\u9762\u5411\u673a\u5668\u7684\u89c6\u89c9\u8bbe\u8ba1\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u7814\u7a76\u95ee\u9898\u8fdb\u884c\u7814\u7a76\uff0c\u56e0\u4e3a\u57fa\u4e8e\u4eba\u7c7b\u89c6\u89c9\u5fc3\u7406\u7269\u7406\u7814\u7a76\u7684\u8bbe\u8ba1\u77e5\u8bc6\u4e0d\u80fd\u76f4\u63a5\u8fc1\u79fb\u5230\u673a\u5668\u89c6\u89c9\uff0c\u9700\u8981\u5efa\u7acb\u673a\u5668\u5bfc\u5411\u7684\u53ef\u89c6\u5316\u8bbe\u8ba1\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u7684\u53ef\u89c6\u5316\u8bbe\u8ba1\u77e5\u8bc6\u4e3b\u8981\u57fa\u4e8e60\u5e74\u7684\u4eba\u7c7b\u89c6\u89c9\u5fc3\u7406\u7269\u7406\u7814\u7a76\uff0c\u4f46\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u81ea\u52a8\u5316\u5206\u6790\u6d41\u7a0b\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u5904\u7406\u56fe\u8868\u56fe\u50cf\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u4ee5\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u77e5\u8bc6\u4e0d\u80fd\u76f4\u63a5\u8fc1\u79fb\u5230\u673a\u5668\u53d7\u4f17\uff0c\u673a\u5668\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u7f16\u7801\u6027\u80fd\u6a21\u5f0f\uff0c\u901a\u8fc7\u57fa\u4e8e\u8865\u4e01\u7684\u6807\u8bb0\u5316\u800c\u975e\u6574\u4f53\u611f\u77e5\u5904\u7406\u56fe\u50cf\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u4eba\u7c7b\u65e0\u56f0\u96be\u7684\u8bbe\u8ba1\u6a21\u5f0f\u4e0a\u5931\u8d25\uff0c\u5076\u5c14\u5728\u4eba\u7c7b\u56f0\u96be\u7684\u5730\u65b9\u6210\u529f\u3002\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u5b8c\u5168\u7ed5\u8fc7\u89c6\u89c9\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5c06\u56fe\u8868\u8f6c\u6362\u4e3a\u6570\u636e\u8868\u6216\u7ed3\u6784\u5316\u6587\u672c\uff0c\u4f46\u8fd9\u56de\u907f\u4e86\u4e00\u4e2a\u66f4\u6839\u672c\u7684\u95ee\u9898\uff1a\u4ec0\u4e48\u6837\u7684\u89c6\u89c9\u8868\u793a\u5b9e\u9645\u4e0a\u9002\u5408\u673a\u5668\u8ba4\u77e5\uff1f", "method": "\u672c\u6587\u7efc\u5408\u4e86\u6765\u81eaVLM\u57fa\u51c6\u6d4b\u8bd5\u3001\u89c6\u89c9\u63a8\u7406\u7814\u7a76\u548c\u53ef\u89c6\u5316\u7d20\u517b\u7814\u7a76\u7684\u8bc1\u636e\uff0c\u8868\u660e\u4eba\u673a\u611f\u77e5\u5dee\u5f02\u662f\u8d28\u6027\u7684\u800c\u975e\u4ec5\u4ec5\u662f\u91cf\u6027\u7684\u3002\u4f5c\u8005\u6279\u5224\u6027\u5730\u5ba1\u89c6\u4e86\u5f53\u524d\u6d41\u884c\u7684\u7ed5\u8fc7\u89c6\u89c9\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4eba\u7c7b\u5bfc\u5411\u548c\u673a\u5668\u5bfc\u5411\u53ef\u89c6\u5316\u4e4b\u95f4\u7684\u6982\u5ff5\u533a\u5206\uff0c\u5e76\u6982\u8ff0\u4e86\u4e00\u4e2a\u7814\u7a76\u8bae\u7a0b\uff0c\u65e8\u5728\u5f00\u53d1\u8be5\u9886\u57df\u76ee\u524d\u7f3a\u4e4f\u7684\u7ecf\u9a8c\u57fa\u7840\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u673a\u5668\u4e0e\u4eba\u7c7b\u5728\u89c6\u89c9\u5904\u7406\u4e0a\u5b58\u5728\u6839\u672c\u6027\u5dee\u5f02\uff1a\u673a\u5668\u4f7f\u7528\u57fa\u4e8e\u8865\u4e01\u7684\u6807\u8bb0\u5316\u800c\u975e\u6574\u4f53\u611f\u77e5\uff0c\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u7f16\u7801\u6027\u80fd\u6a21\u5f0f\uff0c\u5728\u67d0\u4e9b\u4eba\u7c7b\u65e0\u56f0\u96be\u7684\u8bbe\u8ba1\u6a21\u5f0f\u4e0a\u5931\u8d25\uff0c\u5076\u5c14\u5728\u4eba\u7c7b\u56f0\u96be\u7684\u5730\u65b9\u6210\u529f\u3002\u8fd9\u79cd\u5dee\u5f02\u662f\u8d28\u6027\u7684\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u673a\u5668\u8ba4\u77e5\u7684\u89c6\u89c9\u8bbe\u8ba1\u7814\u7a76\u3002", "conclusion": "\u53ef\u89c6\u5316\u9886\u57df\u9700\u8981\u5c06\u673a\u5668\u5bfc\u5411\u7684\u89c6\u89c9\u8bbe\u8ba1\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u7814\u7a76\u95ee\u9898\u8fdb\u884c\u7814\u7a76\uff0c\u5efa\u7acb\u7c7b\u4f3c\u4e8e\"\u673a\u5668Bertin\"\u7684\u7ecf\u9a8c\u57fa\u7840\u6765\u8865\u5145\u73b0\u6709\u7684\u4eba\u7c7b\u4e2d\u5fc3\u77e5\u8bc6\u3002\u8fd9\u4e0d\u4ec5\u4ec5\u662f\u5de5\u7a0b\u67b6\u6784\u95ee\u9898\uff0c\u800c\u662f\u8ba4\u8bc6\u5230\u4e0d\u540c\u53d7\u4f17\u53ef\u80fd\u9700\u8981\u6839\u672c\u4e0d\u540c\u7684\u8bbe\u8ba1\u57fa\u7840\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u7814\u7a76\u8bae\u7a0b\uff0c\u65e8\u5728\u5f00\u53d1\u8be5\u9886\u57df\u76ee\u524d\u7f3a\u4e4f\u7684\u673a\u5668\u5bfc\u5411\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u7ecf\u9a8c\u57fa\u7840\u3002"}}
{"id": "2602.01579", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01579", "abs": "https://arxiv.org/abs/2602.01579", "authors": ["Chuyang Zhang", "Bin Yu", "Yuchao Wang", "Mansi Yuan", "Wanqi Wang", "Seungwoo Je", "Pengcheng An"], "title": "ASafePlace: User-Led Personalization of VR Relaxation via an Art Therapy Activity", "comment": null, "summary": "To overcome the lack of deep personalization in standard biofeedback methods, we introduce ASafePlace, a system utilizing an AI-powered, art-therapy-inspired exercise called The Safe Place, to create a personalized VR biofeedback experience. In our system, users sketch a personal sanctuary from memory, which is then transformed into a customized 360 virtual environment with personalized audio guidance for relaxation training. A study with 52 participants showed this approach effectively reduced anxiety and increased user presence, while the integration of art-therapy-inspired activity and biofeedback produced strong improvements in physiological relaxation, measured by heart rate variability and respiration rate. Qualitative results showed how participants' sense of familiarity and presence was enhanced by the symbolic elements and natural sanctuaries created from their autobiographical memories. Our findings demonstrate that art-therapy-inspired activity is a powerful tool for creating highly effective and individualized relaxation experiences, naturally connecting the virtual environment to a user's core memories and emotions.", "AI": {"tldr": "ASafePlace\u7cfb\u7edf\u901a\u8fc7AI\u5c06\u7528\u6237\u624b\u7ed8\u7684\u4e2a\u4eba\u5e87\u62a4\u6240\u8bb0\u5fc6\u8f6c\u5316\u4e3a\u4e2a\u6027\u5316VR\u73af\u5883\uff0c\u7ed3\u5408\u827a\u672f\u6cbb\u7597\u548c\u751f\u7269\u53cd\u9988\u6280\u672f\uff0c\u6709\u6548\u964d\u4f4e\u7126\u8651\u3001\u589e\u5f3a\u4e34\u573a\u611f\uff0c\u5b9e\u73b0\u6df1\u5ea6\u4e2a\u6027\u5316\u653e\u677e\u8bad\u7ec3\u3002", "motivation": "\u6807\u51c6\u751f\u7269\u53cd\u9988\u65b9\u6cd5\u7f3a\u4e4f\u6df1\u5ea6\u4e2a\u6027\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u8fde\u63a5\u7528\u6237\u7684\u4e2a\u4eba\u8bb0\u5fc6\u548c\u60c5\u611f\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5229\u7528\u7528\u6237\u81ea\u4f20\u4f53\u8bb0\u5fc6\u521b\u9020\u4e2a\u6027\u5316\u653e\u677e\u4f53\u9a8c\u7684\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1ASafePlace\u7cfb\u7edf\uff1a1) \u7528\u6237\u624b\u7ed8\u4e2a\u4eba\u5e87\u62a4\u6240\u8bb0\u5fc6\uff1b2) AI\u5c06\u624b\u7ed8\u8f6c\u5316\u4e3a\u5b9a\u5236\u5316360\u5ea6VR\u73af\u5883\uff1b3) \u52a0\u5165\u4e2a\u6027\u5316\u97f3\u9891\u5f15\u5bfc\u8fdb\u884c\u653e\u677e\u8bad\u7ec3\uff1b4) \u6574\u5408\u827a\u672f\u6cbb\u7597\u6d3b\u52a8\u4e0e\u751f\u7269\u53cd\u9988\uff1b5) \u901a\u8fc7\u5fc3\u7387\u53d8\u5f02\u6027\u548c\u547c\u5438\u7387\u6d4b\u91cf\u751f\u7406\u653e\u677e\u6548\u679c\u3002", "result": "52\u540d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u663e\u793a\uff1a1) \u6709\u6548\u964d\u4f4e\u7126\u8651\u6c34\u5e73\uff1b2) \u589e\u5f3a\u7528\u6237\u4e34\u573a\u611f\uff1b3) \u751f\u7406\u653e\u677e\u6307\u6807\u663e\u8457\u6539\u5584\uff08\u5fc3\u7387\u53d8\u5f02\u6027\u63d0\u9ad8\u3001\u547c\u5438\u7387\u964d\u4f4e\uff09\uff1b4) \u5b9a\u6027\u7ed3\u679c\u663e\u793a\u53c2\u4e0e\u8005\u719f\u6089\u611f\u548c\u4e34\u573a\u611f\u56e0\u7b26\u53f7\u5143\u7d20\u548c\u81ea\u7136\u5e87\u62a4\u6240\u800c\u589e\u5f3a\u3002", "conclusion": "\u827a\u672f\u6cbb\u7597\u542f\u53d1\u7684\u6d3b\u52a8\u662f\u521b\u5efa\u9ad8\u6548\u4e2a\u6027\u5316\u653e\u677e\u4f53\u9a8c\u7684\u6709\u529b\u5de5\u5177\uff0c\u80fd\u591f\u81ea\u7136\u5730\u5c06\u865a\u62df\u73af\u5883\u4e0e\u7528\u6237\u7684\u6838\u5fc3\u8bb0\u5fc6\u548c\u60c5\u611f\u8fde\u63a5\u8d77\u6765\uff0c\u4e3a\u751f\u7269\u53cd\u9988\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6df1\u5ea6\u4e2a\u6027\u5316\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.01429", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01429", "abs": "https://arxiv.org/abs/2602.01429", "authors": ["Gonzalo Olguin", "Javier Ruiz-del-Solar"], "title": "Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors", "comment": "8 pages, 5 figures", "summary": "This work presents a mapless global navigation approach for outdoor applications. It combines the exploratory capacity of conditional variational autoencoders (CVAEs) to generate trajectories and the semantic segmentation capabilities of a lightweight visual language model (VLM) to select the trajectory to execute. Open-vocabulary segmentation is used to score and select the generated trajectories based on natural language, and a state-of-the-art local planner executes velocity commands. One of the key features of the proposed approach is its ability to generate a large variability of trajectories and to select them and navigate in real-time. The approach was validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods. A video showing an experimental run of the system can be found in https://www.youtube.com/watch?v=i3R5ey5O2yk.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5730\u56fe\u7684\u6237\u5916\u5168\u5c40\u5bfc\u822a\u65b9\u6cd5\uff0c\u7ed3\u5408CVAE\u751f\u6210\u8f68\u8ff9\u548c\u8f7b\u91cf\u7ea7VLM\u8bed\u4e49\u5206\u5272\u9009\u62e9\u8f68\u8ff9\uff0c\u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u8bc4\u5206\u9009\u62e9\u8f68\u8ff9\uff0c\u7531\u5148\u8fdb\u5c40\u90e8\u89c4\u5212\u5668\u6267\u884c\u901f\u5ea6\u6307\u4ee4\u3002", "motivation": "\u89e3\u51b3\u6237\u5916\u73af\u5883\u4e2d\u65e0\u9700\u9884\u5efa\u5730\u56fe\u7684\u5168\u5c40\u5bfc\u822a\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u8f68\u8ff9\u751f\u6210\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5b9e\u73b0\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u5b9e\u65f6\u5bfc\u822a\u3002", "method": "1. \u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE)\u751f\u6210\u591a\u6837\u5316\u7684\u8f68\u8ff9\uff1b2. \u91c7\u7528\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u8fdb\u884c\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff1b3. \u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5bf9\u751f\u6210\u8f68\u8ff9\u8fdb\u884c\u8bc4\u5206\u548c\u9009\u62e9\uff1b4. \u4f7f\u7528\u5148\u8fdb\u5c40\u90e8\u89c4\u5212\u5668\u6267\u884c\u901f\u5ea6\u6307\u4ee4\u3002", "result": "\u5728\u771f\u5b9e\u6237\u5916\u5bfc\u822a\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u6027\u80fd\uff0c\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u591a\u6837\u5316\u8f68\u8ff9\u5e76\u8fdb\u884c\u9009\u62e9\u5bfc\u822a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u5730\u56fe\u7684\u6237\u5916\u5168\u5c40\u5bfc\u822a\uff0c\u7ed3\u5408\u8f68\u8ff9\u751f\u6210\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u5b9e\u65f6\u6237\u5916\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02243", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02243", "abs": "https://arxiv.org/abs/2602.02243", "authors": ["Dakshina Tharindu", "Aruna Jayasena", "Prabhat Mishra"], "title": "SysFuSS: System-Level Firmware Fuzzing with Selective Symbolic Execution", "comment": null, "summary": "Firmware serves as the critical interface between hardware and software in computing systems, making any bugs or vulnerabilities particularly dangerous as they can cause catastrophic system failures. While fuzzing is a promising approach for identifying design flaws and security vulnerabilities, traditional fuzzers are ineffective at detecting firmware vulnerabilities. For example, existing fuzzers focus on user-level fuzzing, which is not suitable for detecting kernel-level vulnerabilities. Existing fuzzers also face a coverage plateau problem when dealing with complex interactions between firmware and hardware. In this paper, we present an efficient firmware verification framework, SysFuSS, that integrates system-level fuzzing with selective symbolic execution. Our approach leverages system-level emulation for initial fuzzing, and automatically transitions to symbolic execution when coverage reaches a plateau. This strategy enables us to generate targeted test cases that can trigger previously unexplored regions in firmware designs. We have evaluated SysFuSS on real-world embedded firmware, including OpenSSL, WolfBoot, WolfMQTT, HTSlib, MXML, and libIEC. Experimental evaluation demonstrates that SysFuSS significantly outperforms state-of-the-art fuzzers in terms of both branch coverage and detection of firmware vulnerabilities. Specifically, SysFuSS can detect 118 known vulnerabilities while state-of-the-art can cover only 13 of them. Moreover, SysFuSS takes significantly less time (up to 3.3X, 1.7X on average) to activate these vulnerabilities.", "AI": {"tldr": "SysFuSS\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u56fa\u4ef6\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7cfb\u7edf\u7ea7\u6a21\u7cca\u6d4b\u8bd5\u4e0e\u9009\u62e9\u6027\u7b26\u53f7\u6267\u884c\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u7cca\u6d4b\u8bd5\u5728\u68c0\u6d4b\u56fa\u4ef6\u6f0f\u6d1e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u652f\u8986\u76d6\u7387\u548c\u6f0f\u6d1e\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u56fa\u4ef6\u4f5c\u4e3a\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u5173\u952e\u63a5\u53e3\uff0c\u5176\u6f0f\u6d1e\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u7cfb\u7edf\u6545\u969c\u3002\u4f20\u7edf\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u5728\u68c0\u6d4b\u56fa\u4ef6\u6f0f\u6d1e\u65b9\u9762\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u5668\u4e3b\u8981\u5173\u6ce8\u7528\u6237\u7ea7\u6d4b\u8bd5\uff0c\u4e0d\u9002\u5408\u68c0\u6d4b\u5185\u6838\u7ea7\u6f0f\u6d1e\uff1b2) \u5728\u5904\u7406\u56fa\u4ef6\u4e0e\u786c\u4ef6\u590d\u6742\u4ea4\u4e92\u65f6\u9762\u4e34\u8986\u76d6\u7387\u5e73\u53f0\u95ee\u9898\u3002", "method": "SysFuSS\u6846\u67b6\u6574\u5408\u4e86\u7cfb\u7edf\u7ea7\u6a21\u7cca\u6d4b\u8bd5\u548c\u9009\u62e9\u6027\u7b26\u53f7\u6267\u884c\u3002\u9996\u5148\u5229\u7528\u7cfb\u7edf\u7ea7\u4eff\u771f\u8fdb\u884c\u521d\u59cb\u6a21\u7cca\u6d4b\u8bd5\uff0c\u5f53\u8986\u76d6\u7387\u8fbe\u5230\u5e73\u53f0\u671f\u65f6\u81ea\u52a8\u5207\u6362\u5230\u7b26\u53f7\u6267\u884c\u3002\u8fd9\u79cd\u7b56\u7565\u80fd\u591f\u751f\u6210\u9488\u5bf9\u6027\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u89e6\u53d1\u56fa\u4ef6\u8bbe\u8ba1\u4e2d\u5148\u524d\u672a\u63a2\u7d22\u7684\u533a\u57df\u3002", "result": "\u5728\u771f\u5b9e\u5d4c\u5165\u5f0f\u56fa\u4ef6\uff08\u5305\u62ecOpenSSL\u3001WolfBoot\u3001WolfMQTT\u3001HTSlib\u3001MXML\u548clibIEC\uff09\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cSysFuSS\u5728\u5206\u652f\u8986\u76d6\u7387\u548c\u56fa\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u7cca\u6d4b\u8bd5\u5668\u3002\u5177\u4f53\u6765\u8bf4\uff0cSysFuSS\u80fd\u591f\u68c0\u6d4b118\u4e2a\u5df2\u77e5\u6f0f\u6d1e\uff0c\u800c\u6700\u5148\u8fdb\u65b9\u6cd5\u53ea\u80fd\u8986\u76d6\u5176\u4e2d\u768413\u4e2a\u3002\u6b64\u5916\uff0cSysFuSS\u6fc0\u6d3b\u8fd9\u4e9b\u6f0f\u6d1e\u6240\u9700\u7684\u65f6\u95f4\u663e\u8457\u51cf\u5c11\uff08\u6700\u591a3.3\u500d\uff0c\u5e73\u57471.7\u500d\uff09\u3002", "conclusion": "SysFuSS\u901a\u8fc7\u7cfb\u7edf\u7ea7\u6a21\u7cca\u6d4b\u8bd5\u4e0e\u9009\u62e9\u6027\u7b26\u53f7\u6267\u884c\u7684\u96c6\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fa\u4ef6\u9a8c\u8bc1\u4e2d\u7684\u8986\u76d6\u7387\u5e73\u53f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u6548\u7387\u548c\u80fd\u529b\uff0c\u4e3a\u56fa\u4ef6\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01448", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01448", "abs": "https://arxiv.org/abs/2602.01448", "authors": ["Harshith Jella", "Pejman Kheradmand", "Joseph Klein", "Behnam Moradkhani", "Yash Chitalia"], "title": "Towards a Novel Wearable Robotic Vest for Hemorrhage Suppression", "comment": null, "summary": "This paper introduces a novel robotic system designed to manage severe bleeding in emergency scenarios, including unique environments like space stations. The robot features a shape-adjustable \"ring mechanism\", transitioning from a circular to an elliptical configuration to adjust wound coverage across various anatomical regions. We developed various arms for this ring mechanism with varying flexibilities to improve adaptability when applied to non-extremities of the body (abdomen, back, neck, etc.). To apply equal and constant pressure across the wound, we developed an inflatable ring and airbag balloon that are compatible with this shape-changing ring mechanism. A series of experiments focused on evaluating various ring arm configurations to characterize their bending stiffness. Subsequent experiments measured the force exerted by the airbag balloon system using a digital scale. Despite its promising performance, certain limitations related to coverage area are identified. The shape-changing effect of the device is limited to scenarios involving partially inflated or deflated airbag balloons, and cannot fully conform to complex anatomical regions. Finally, the device was tested on casualty simulation kits, where it successfully demonstrated its ability to control simulated bleeding.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u7d27\u6025\u6b62\u8840\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u91c7\u7528\u5f62\u72b6\u53ef\u8c03\u7684\u73af\u5f62\u673a\u5236\uff0c\u914d\u5907\u4e0d\u540c\u67d4\u97e7\u6027\u7684\u81c2\u548c\u5145\u6c14\u6c14\u56ca\uff0c\u80fd\u5728\u592a\u7a7a\u7ad9\u7b49\u7279\u6b8a\u73af\u5883\u4e0b\u81ea\u9002\u5e94\u4e0d\u540c\u89e3\u5256\u90e8\u4f4d\u65bd\u52a0\u5747\u5300\u538b\u529b\u3002", "motivation": "\u9488\u5bf9\u7d27\u6025\u573a\u666f\uff08\u5305\u62ec\u592a\u7a7a\u7ad9\u7b49\u7279\u6b8a\u73af\u5883\uff09\u4e2d\u4e25\u91cd\u51fa\u8840\u7684\u7ba1\u7406\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89e3\u5256\u90e8\u4f4d\u3001\u65bd\u52a0\u5747\u5300\u6052\u5b9a\u538b\u529b\u7684\u81ea\u52a8\u5316\u6b62\u8840\u7cfb\u7edf\uff0c\u4ee5\u5f25\u8865\u4f20\u7edf\u6b62\u8840\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bbe\u8ba1\u5f62\u72b6\u53ef\u8c03\u7684\u73af\u5f62\u673a\u5236\uff08\u53ef\u4ece\u5706\u5f62\u53d8\u4e3a\u692d\u5706\u5f62\uff09\uff0c\u914d\u5907\u4e0d\u540c\u67d4\u97e7\u6027\u7684\u81c2\u4ee5\u63d0\u9ad8\u5bf9\u975e\u56db\u80a2\u90e8\u4f4d\uff08\u8179\u90e8\u3001\u80cc\u90e8\u3001\u9888\u90e8\u7b49\uff09\u7684\u9002\u5e94\u6027\uff1b\u5f00\u53d1\u4e0e\u5f62\u72b6\u53d8\u5316\u673a\u5236\u517c\u5bb9\u7684\u5145\u6c14\u73af\u5f62\u6c14\u56ca\u7cfb\u7edf\uff0c\u786e\u4fdd\u5bf9\u4f24\u53e3\u65bd\u52a0\u5747\u5300\u6052\u5b9a\u538b\u529b\uff1b\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e0d\u540c\u81c2\u914d\u7f6e\u7684\u5f2f\u66f2\u521a\u5ea6\uff0c\u6d4b\u91cf\u6c14\u56ca\u7cfb\u7edf\u7684\u65bd\u529b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u6210\u529f\u8868\u5f81\u4e86\u4e0d\u540c\u81c2\u914d\u7f6e\u7684\u5f2f\u66f2\u521a\u5ea6\uff0c\u6d4b\u91cf\u4e86\u6c14\u56ca\u7cfb\u7edf\u7684\u65bd\u529b\u80fd\u529b\uff1b\u5728\u6a21\u62df\u4f24\u5458\u5957\u4ef6\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63a7\u5236\u6a21\u62df\u51fa\u8840\uff1b\u4f46\u5b58\u5728\u8986\u76d6\u9762\u79ef\u9650\u5236\uff0c\u5f62\u72b6\u53d8\u5316\u6548\u679c\u4ec5\u5728\u6c14\u56ca\u90e8\u5206\u5145\u6c14\u6216\u653e\u6c14\u65f6\u6709\u6548\uff0c\u65e0\u6cd5\u5b8c\u5168\u8d34\u5408\u590d\u6742\u89e3\u5256\u533a\u57df\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u7d27\u6025\u6b62\u8840\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u592a\u7a7a\u7ad9\u7b49\u7279\u6b8a\u73af\u5883\uff1b\u5f62\u72b6\u53ef\u8c03\u673a\u5236\u548c\u6c14\u56ca\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u89e3\u5256\u90e8\u4f4d\u5e76\u65bd\u52a0\u5747\u5300\u538b\u529b\uff0c\u4f46\u5b58\u5728\u8986\u76d6\u9762\u79ef\u548c\u590d\u6742\u89e3\u5256\u533a\u57df\u8d34\u5408\u5ea6\u7684\u9650\u5236\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002"}}
{"id": "2602.02412", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.02412", "abs": "https://arxiv.org/abs/2602.02412", "authors": ["Apoorv Mohit", "Bhavya Aggarwal", "Chinmay Gondhalekar"], "title": "Provenance Verification of AI-Generated Images via a Perceptual Hash Registry Anchored on Blockchain", "comment": null, "summary": "The rapid advancement of artificial intelligence has made the generation of synthetic images widely accessible, increasing concerns related to misinformation, digital forgery, and content authenticity on large-scale online platforms. This paper proposes a blockchain-backed framework for verifying AI-generated images through a registry-based provenance mechanism. Each AI-generated image is assigned a digital fingerprint that preserves similarity using perceptual hashing and is registered at creation time by participating generation platforms. The hashes are stored on a hybrid on-chain/off-chain public blockchain using a Merkle Patricia Trie for tamper-resistant storage (on-chain) and a Burkhard-Keller tree (off-chain) to enable efficient similarity search over large image registries. Verification is performed when images are re-uploaded to digital platforms such as social media services, enabling identification of previously registered AI-generated images even after benign transformations or partial modifications. The proposed system does not aim to universally detect all synthetic images, but instead focuses on verifying the provenance of AI-generated content that has been registered at creation time. By design, this approach complements existing watermarking and learning-based detection methods, providing a platform-agnostic, tamper-proof mechanism for scalable content provenance and authenticity verification at the point of large-scale online distribution.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u533a\u5757\u94fe\u7684AI\u751f\u6210\u56fe\u50cf\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u518c\u673a\u5236\u8ffd\u8e2a\u56fe\u50cf\u6765\u6e90\uff0c\u4f7f\u7528\u611f\u77e5\u54c8\u5e0c\u548c\u6df7\u5408\u94fe\u4e0a/\u94fe\u4e0b\u5b58\u50a8\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u9632\u7be1\u6539\u9a8c\u8bc1\u3002", "motivation": "AI\u751f\u6210\u56fe\u50cf\u7684\u666e\u53ca\u5e26\u6765\u4e86\u865a\u5047\u4fe1\u606f\u3001\u6570\u5b57\u4f2a\u9020\u548c\u5185\u5bb9\u771f\u5b9e\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5728\u5927\u89c4\u6a21\u5728\u7ebf\u5e73\u53f0\u4e0a\u5efa\u7acb\u53ef\u9a8c\u8bc1\u7684\u5185\u5bb9\u6765\u6e90\u673a\u5236\u3002", "method": "\u91c7\u7528\u533a\u5757\u94fe\u652f\u6301\u7684\u6ce8\u518c\u5236\u9a8c\u8bc1\u6846\u67b6\uff1a1) \u4e3aAI\u751f\u6210\u56fe\u50cf\u5206\u914d\u57fa\u4e8e\u611f\u77e5\u54c8\u5e0c\u7684\u6570\u5b57\u6307\u7eb9\uff1b2) \u751f\u6210\u5e73\u53f0\u5728\u521b\u5efa\u65f6\u6ce8\u518c\u54c8\u5e0c\uff1b3) \u4f7f\u7528\u6df7\u5408\u5b58\u50a8\u67b6\u6784\uff08\u94fe\u4e0aMerkle Patricia Trie\u9632\u7be1\u6539\u5b58\u50a8\uff0c\u94fe\u4e0bBurkhard-Keller\u6811\u5b9e\u73b0\u9ad8\u6548\u76f8\u4f3c\u6027\u641c\u7d22\uff09\uff1b4) \u5728\u793e\u4ea4\u5a92\u4f53\u7b49\u5e73\u53f0\u91cd\u65b0\u4e0a\u4f20\u65f6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u8bc6\u522b\u7ecf\u8fc7\u826f\u6027\u53d8\u6362\u6216\u90e8\u5206\u4fee\u6539\u7684\u5df2\u6ce8\u518cAI\u751f\u6210\u56fe\u50cf\uff0c\u63d0\u4f9b\u5e73\u53f0\u65e0\u5173\u3001\u9632\u7be1\u6539\u7684\u5185\u5bb9\u6765\u6e90\u9a8c\u8bc1\u673a\u5236\uff0c\u4e0e\u73b0\u6709\u6c34\u5370\u548c\u5b66\u4e60\u68c0\u6d4b\u65b9\u6cd5\u4e92\u8865\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u751f\u6210\u56fe\u50cf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6765\u6e90\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\uff0c\u4e13\u6ce8\u4e8e\u6ce8\u518c\u65f6\u9a8c\u8bc1\u800c\u975e\u901a\u7528\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5728\u7ebf\u5206\u53d1\u573a\u666f\u7684\u5185\u5bb9\u771f\u5b9e\u6027\u9a8c\u8bc1\u3002"}}
{"id": "2602.01694", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01694", "abs": "https://arxiv.org/abs/2602.01694", "authors": ["Ningjing Tang", "Alice Qian", "Qiaosi Wang", "Esther Howe", "Blake Bullwinkel", "Paola Pedrelli", "Jina Suh", "Hoda Heidari", "Hong Shen"], "title": "Beyond the Single Turn: Reframing Refusals as Dynamic Experiences Embedded in the Context of Mental Health Support Interactions with LLMs", "comment": null, "summary": "Content Warning: This paper contains participant quotes and discussions related to mental health challenges, emotional distress, and suicidal ideation.\n  Large language models (LLMs) are increasingly used for mental health support, yet the model safeguards -- particularly refusals to engage with sensitive content -- remain poorly understood from the perspectives of users and mental health professionals (MHPs) and have been reported to cause real-world harms. This paper presents findings from a sequential mixed-methods study examining how LLM refusals are experienced and interpreted in mental health support interactions. Through surveys (N=53) and in-depth interviews (N=16) with individuals using LLMs for mental health support and MHPs, we reveal that refusals are not isolated, single-turn system behaviors, but rather constitute dynamic, multi-phase experiences: pre-refusal expectation formation, refusal triggering and encounter, refusal message framing, resource referral provision, and post-refusal outcomes. We contribute a multi-phase framework for evaluating refusals beyond binary policy compliance accuracy and design recommendations for future refusal mechanisms. These findings suggest that understanding LLM refusals requires moving beyond single-turn interactions toward recognizing them as holistic experiential processes embedded within the entire LLM design pipeline and the broader realities of mental health access.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u8c03\u67e5\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u4e2d\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u53d1\u73b0\u62d2\u7edd\u4e0d\u662f\u5355\u6b21\u7cfb\u7edf\u884c\u4e3a\uff0c\u800c\u662f\u5305\u542b\u591a\u4e2a\u9636\u6bb5\u7684\u52a8\u6001\u4f53\u9a8c\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u62d2\u7edd\u7684\u591a\u9636\u6bb5\u6846\u67b6\u548c\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u652f\u6301\uff0c\u4f46\u6a21\u578b\u7684\u5b89\u5168\u4fdd\u969c\u673a\u5236\uff08\u7279\u522b\u662f\u5bf9\u654f\u611f\u5185\u5bb9\u7684\u62d2\u7edd\u56de\u5e94\uff09\u4ece\u7528\u6237\u548c\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u58eb\u7684\u89d2\u5ea6\u6765\u770b\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u4e14\u5df2\u62a5\u544a\u9020\u6210\u5b9e\u9645\u4f24\u5bb3\u3002\u9700\u8981\u4ece\u7528\u6237\u4f53\u9a8c\u89d2\u5ea6\u6df1\u5165\u7406\u89e3LLM\u62d2\u7edd\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u987a\u5e8f\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff1a\u9996\u5148\u5bf953\u540d\u4f7f\u7528LLM\u8fdb\u884c\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7684\u4e2a\u4eba\u548c\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u7136\u540e\u5bf9\u5176\u4e2d16\u4eba\u8fdb\u884c\u6df1\u5ea6\u8bbf\u8c08\uff0c\u5206\u6790LLM\u62d2\u7edd\u884c\u4e3a\u7684\u4f53\u9a8c\u548c\u89e3\u91ca\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u62d2\u7edd\u4e0d\u662f\u5b64\u7acb\u7684\u5355\u8f6e\u7cfb\u7edf\u884c\u4e3a\uff0c\u800c\u662f\u6784\u6210\u52a8\u6001\u7684\u591a\u9636\u6bb5\u4f53\u9a8c\uff1a\u62d2\u7edd\u524d\u9884\u671f\u5f62\u6210\u3001\u62d2\u7edd\u89e6\u53d1\u4e0e\u906d\u9047\u3001\u62d2\u7edd\u4fe1\u606f\u6846\u67b6\u3001\u8d44\u6e90\u8f6c\u4ecb\u63d0\u4f9b\u3001\u62d2\u7edd\u540e\u7ed3\u679c\u3002\u63d0\u51fa\u4e86\u8d85\u8d8a\u4e8c\u5143\u653f\u7b56\u5408\u89c4\u51c6\u786e\u6027\u7684\u591a\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u7406\u89e3LLM\u62d2\u7edd\u884c\u4e3a\u9700\u8981\u8d85\u8d8a\u5355\u8f6e\u4ea4\u4e92\uff0c\u5c06\u5176\u89c6\u4e3a\u5d4c\u5165\u6574\u4e2aLLM\u8bbe\u8ba1\u6d41\u7a0b\u548c\u5fc3\u7406\u5065\u5eb7\u83b7\u53d6\u73b0\u5b9e\u80cc\u666f\u4e2d\u7684\u6574\u4f53\u4f53\u9a8c\u8fc7\u7a0b\u3002\u7814\u7a76\u4e3a\u672a\u6765\u62d2\u7edd\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5efa\u8bae\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2602.01501", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01501", "abs": "https://arxiv.org/abs/2602.01501", "authors": ["Minwoo Jung", "Nived Chebrolu", "Lucas Carvalho de Lima", "Haedam Oh", "Maurice Fallon", "Ayoung Kim"], "title": "TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching", "comment": "An 8-page paper with 7 tables and 8 figures, accepted to ICRA 2026", "summary": "Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.", "code_url": "https://github.com/minwoo0611/TreeLoc", "code_stars": 0, "code_last_update": "2026-02-01", "AI": {"tldr": "TreeLoc\u662f\u4e00\u4e2a\u7528\u4e8e\u68ee\u6797\u73af\u5883\u7684LiDAR\u5168\u5c40\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u5e72\u7279\u5f81\u8bc6\u522b\u548c6\u81ea\u7531\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u89e3\u51b3GPS\u4fe1\u53f7\u5f31\u3001LiDAR\u6570\u636e\u91cd\u590d\u4e14\u7ed3\u6784\u590d\u6742\u7684\u95ee\u9898\u3002", "motivation": "\u68ee\u6797\u73af\u5883\u4e2dGPS\u4fe1\u53f7\u8870\u51cf\uff0cLiDAR\u6d4b\u91cf\u5177\u6709\u91cd\u590d\u6027\u3001\u906e\u6321\u548c\u7ed3\u6784\u590d\u6742\u6027\uff0c\u4f20\u7edf\u57ce\u5e02\u5b9a\u4f4d\u65b9\u6cd5\u5047\u8bbe\u7279\u5f81\u6765\u81ea\u72ec\u7279\u7ed3\u6784\u6a21\u5f0f\uff0c\u5728\u68ee\u6797\u4e2d\u5931\u6548\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6811\u5e72\u53ca\u5176\u80f8\u5f84(DBH)\u8868\u793a\u573a\u666f\uff0c\u901a\u8fc7\u6811\u5e72\u8f74\u5bf9\u9f50\u5230\u5171\u540c\u53c2\u8003\u7cfb\uff0c\u7528\u6811\u5206\u5e03\u76f4\u65b9\u56fe(TDH)\u8fdb\u884c\u7c97\u5339\u914d\uff0c\u518d\u75282D\u4e09\u89d2\u5f62\u63cf\u8ff0\u7b26\u8fdb\u884c\u7cbe\u7ec6\u5339\u914d\uff0c\u6700\u540e\u901a\u8fc7\u4e24\u6b65\u51e0\u4f55\u9a8c\u8bc1\u5b9e\u73b0\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u6837\u5316\u68ee\u6797\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTreeLoc\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7cbe\u786e\u5b9a\u4f4d\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u8d21\u732e\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u7d27\u51d1\u5168\u5c40\u6811\u6728\u6570\u636e\u5e93\u63cf\u8ff0\u7b26\u7684\u957f\u671f\u68ee\u6797\u7ba1\u7406\u5e94\u7528\u3002", "conclusion": "TreeLoc\u4e3a\u68ee\u6797\u73af\u5883\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684LiDAR\u5168\u5c40\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6811\u5e72\u7279\u5f81\u8868\u793a\u548c\u5206\u5c42\u5339\u914d\u7b56\u7565\u6709\u6548\u5e94\u5bf9\u68ee\u6797\u73af\u5883\u7684\u72ec\u7279\u6311\u6218\uff0c\u5df2\u5f00\u6e90\u4f9b\u673a\u5668\u4eba\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2602.01729", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01729", "abs": "https://arxiv.org/abs/2602.01729", "authors": ["Seoyoung Kang", "Seokhwan Yang", "Hail Song", "Boram Yoon", "Jinwook Kim", "Kangsoo Kim", "Woontack Woo"], "title": "Streamlined Facial Data Collection based on Utterance and Emotional Data for Human-to-Avatar Reconstruction", "comment": "Accepted as an IEEE TVCG paper at IEEE VR 2026 (journal track)", "summary": "This study explores a streamlined facial data collection method for conversational contexts, addressing the limitations of existing approaches that often require extensive datasets and prioritize technical metrics over user perception and experience. We systematically investigate which facial expression data are essential for reconstructing photorealistic avatars and how they can be captured efficiently. Our research employs a two-phase methodology to identify efficient facial data collection strategies and evaluate their effectiveness. In the first phase, we conduct facial data acquisition and evaluate reconstruction performance using utterance data and emotional data. In the second phase, we carry out a comprehensive user evaluation comparing three progressive conditions: utterance only, utterance and emotional data, and a control condition involving extensive data. Findings from 24 participants engaged in simulated face-to-face conversations reveal that targeted utterance and emotional data achieve comparable levels of perceived realism, naturalness, and telepresence, while reducing training time and data usage when compared to the extensive data collection approach. These results demonstrate that targeted data inputs can enable efficient avatar face reconstruction, offering practical guidelines for real-time applications such as AR/VR telepresence and highlighting the trade-off between data quantity and perceived quality.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5bf9\u8bdd\u573a\u666f\u7684\u7b80\u5316\u9762\u90e8\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u91c7\u96c6\u8bed\u97f3\u548c\u60c5\u611f\u6570\u636e\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u771f\u5b9e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6570\u636e\u91cf\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u9762\u90e8\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\uff0c\u4e14\u8fc7\u4e8e\u5173\u6ce8\u6280\u672f\u6307\u6807\u800c\u5ffd\u89c6\u7528\u6237\u611f\u77e5\u548c\u4f53\u9a8c\u3002\u5728\u5bf9\u8bdd\u573a\u666f\u4e2d\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u91cd\u5efa\u903c\u771f\u865a\u62df\u5316\u8eab\uff0c\u7279\u522b\u662f\u9762\u5411AR/VR\u4e34\u573a\u611f\u7b49\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u9762\u90e8\u6570\u636e\u91c7\u96c6\uff0c\u4f7f\u7528\u8bed\u97f3\u6570\u636e\u548c\u60c5\u611f\u6570\u636e\u8bc4\u4f30\u91cd\u5efa\u6027\u80fd\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u5168\u9762\u7684\u7528\u6237\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e09\u79cd\u6e10\u8fdb\u6761\u4ef6\uff1a\u4ec5\u8bed\u97f3\u6570\u636e\u3001\u8bed\u97f3+\u60c5\u611f\u6570\u636e\u3001\u4ee5\u53ca\u4f7f\u7528\u5927\u91cf\u6570\u636e\u7684\u63a7\u5236\u6761\u4ef6\u3002", "result": "24\u540d\u53c2\u4e0e\u8005\u5728\u6a21\u62df\u9762\u5bf9\u9762\u5bf9\u8bdd\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u9488\u5bf9\u6027\u7684\u8bed\u97f3\u548c\u60c5\u611f\u6570\u636e\u5728\u611f\u77e5\u771f\u5b9e\u6027\u3001\u81ea\u7136\u5ea6\u548c\u4e34\u573a\u611f\u65b9\u9762\u4e0e\u5927\u91cf\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u6570\u636e\u4f7f\u7528\u91cf\u3002", "conclusion": "\u9488\u5bf9\u6027\u6570\u636e\u8f93\u5165\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u865a\u62df\u5316\u8eab\u9762\u90e8\u91cd\u5efa\uff0c\u4e3aAR/VR\u4e34\u573a\u611f\u7b49\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u63ed\u793a\u4e86\u6570\u636e\u91cf\u4e0e\u611f\u77e5\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2602.01515", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01515", "abs": "https://arxiv.org/abs/2602.01515", "authors": ["Humphrey Munn", "Brendan Tidd", "Peter Bohm", "Marcus Gallagher", "David Howard"], "title": "RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots", "comment": null, "summary": "Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.", "AI": {"tldr": "RAPT\uff1a\u4e00\u79cd\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba50Hz\u63a7\u5236\u7684\u8f7b\u91cf\u7ea7\u81ea\u76d1\u7763\u90e8\u7f72\u65f6\u76d1\u63a7\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u4eff\u771f\u4e2d\u7684\u6807\u79f0\u6267\u884c\u6982\u7387\u65f6\u7a7a\u6d41\u5f62\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u5728\u7ebfOOD\u68c0\u6d4b\u548c\u53ef\u89e3\u91ca\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u4e0d\u5339\u914d\u5ea6\u91cf\uff0c\u5e76\u5305\u542b\u81ea\u52a8\u6839\u56e0\u5206\u6790\u7ba1\u9053\u3002", "motivation": "\u5c06\u5b66\u4e60\u5230\u7684\u63a7\u5236\u7b56\u7565\u90e8\u7f72\u5230\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5177\u6709\u6311\u6218\u6027\uff1a\u4eff\u771f\u4e2d\u770b\u4f3c\u9c81\u68d2\u7684\u7b56\u7565\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u540e\u53ef\u80fd\u5728\u5206\u5e03\u5916\u72b6\u6001\u4e0b\u81ea\u4fe1\u6267\u884c\uff0c\u5bfc\u81f4\u53ef\u80fd\u5bfc\u81f4\u786c\u4ef6\u635f\u574f\u7684\u9759\u9ed8\u6545\u969c\u3002\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4e0e\u9ad8\u901f\u7387\u63a7\u5236\u4e0d\u517c\u5bb9\uff0c\u5728\u6781\u4f4e\u8bef\u62a5\u7387\u8981\u6c42\u4e0b\u6821\u51c6\u4e0d\u4f73\uff0c\u6216\u4f5c\u4e3a\u9ed1\u76d2\u4ec5\u63d0\u4f9b\u4e8c\u8fdb\u5236\u505c\u6b62\u4fe1\u53f7\u800c\u4e0d\u89e3\u91ca\u673a\u5668\u4eba\u504f\u79bb\u6807\u79f0\u884c\u4e3a\u7684\u539f\u56e0\u3002", "method": "RAPT\u5b66\u4e60\u4eff\u771f\u4e2d\u6807\u79f0\u6267\u884c\u7684\u6982\u7387\u65f6\u7a7a\u6d41\u5f62\uff0c\u8bc4\u4f30\u6267\u884c\u65f6\u9884\u6d4b\u504f\u5dee\u4f5c\u4e3a\u6821\u51c6\u7684\u6bcf\u7ef4\u5ea6\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a(1) \u57fa\u4e8e\u91cd\u5efa\u76ee\u6807\u7684\u68af\u5ea6\u65f6\u95f4\u663e\u8457\u6027\uff1b(2) \u7ed3\u5408\u663e\u8457\u6027\u548c\u5173\u8282\u8fd0\u52a8\u5b66\u7684LLM\u63a8\u7406\uff1b(3) \u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8bed\u4e49\u6545\u969c\u8bca\u65ad\u81ea\u52a8\u6839\u56e0\u5206\u6790\u7ba1\u9053\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u56db\u4e2a\u590d\u6742\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0c\u5927\u89c4\u6a21\u4eff\u771f\u663e\u793aRAPT\u5728\u56fa\u5b9a0.5%\u7684episode\u7ea7\u8bef\u62a5\u7387\u4e0b\uff0c\u771f\u9633\u6027\u7387\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u9ad837%\u3002\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\uff0cRAPT\u5b9e\u73b012.5%\u7684\u771f\u9633\u6027\u7387\u63d0\u5347\uff0c\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u77e5\u6570\u636e\u572816\u4e2a\u771f\u5b9e\u4e16\u754c\u6545\u969c\u4e2d\u8fbe\u523075%\u7684\u6839\u56e0\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "RAPT\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u81ea\u76d1\u7763\u7684\u90e8\u7f72\u65f6\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u5728\u7ebfOOD\u68c0\u6d4b\u548c\u4e25\u683c\u8bef\u62a5\u7ea6\u675f\u4e0b\u7684\u6821\u51c6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u4e0d\u5339\u914d\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u6839\u56e0\u5206\u6790\u7ba1\u9053\u4ea7\u751f\u8bed\u4e49\u6545\u969c\u8bca\u65ad\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.00318", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00318", "abs": "https://arxiv.org/abs/2602.00318", "authors": ["Kunal Mukherjee", "Zulfikar Alom", "Tran Gia Bao Ngo", "Cuneyt Gurcan Akcora", "Murat Kantarcioglu"], "title": "Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection", "comment": null, "summary": "The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.\n  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.", "AI": {"tldr": "BOCLOAK\uff1a\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8f7b\u91cf\u7ea7\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u8bc4\u4f30GNN\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709GNN\u673a\u5668\u4eba\u68c0\u6d4b\u5668\u5728\u73b0\u5b9e\u653b\u51fb\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u4e0d\u8db3\uff0c\u653b\u51fb\u8005\u9762\u4e34\u9886\u57df\u7279\u5b9a\u548c\u65f6\u95f4\u7ea6\u675f\uff0c\u9700\u8981\u5f00\u53d1\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5", "method": "BOCLOAK\u6784\u5efa\u65f6\u7a7a\u90bb\u5c45\u7279\u5f81\u7684\u6982\u7387\u5ea6\u91cf\uff0c\u5b66\u4e60\u5206\u79bb\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u884c\u4e3a\u7684\u6700\u4f18\u4f20\u8f93\u51e0\u4f55\uff0c\u5c06\u4f20\u8f93\u8ba1\u5212\u89e3\u7801\u4e3a\u7a00\u758f\u3001\u5408\u7406\u7684\u8fb9\u7f16\u8f91\uff0c\u5728\u9075\u5b88\u73b0\u5b9e\u7ea6\u675f\u7684\u540c\u65f6\u89c4\u907f\u68c0\u6d4b", "result": "\u5728\u4e09\u4e2a\u793e\u4ea4\u673a\u5668\u4eba\u6570\u636e\u96c6\u3001\u4e94\u4e2aSOTA\u68c0\u6d4b\u5668\u3001\u4e09\u4e2a\u5bf9\u6297\u9632\u5fa1\u548c\u56db\u4e2a\u57fa\u7ebf\u653b\u51fb\u65b9\u6cd5\u4e0a\u8bc4\u4f30\uff0cBOCLOAK\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad880.13%\uff0cGPU\u5185\u5b58\u4f7f\u7528\u51cf\u5c1199.80%", "conclusion": "\u6700\u4f18\u4f20\u8f93\u4e3a\u8fde\u63a5\u5bf9\u6297\u653b\u51fb\u4e0e\u73b0\u5b9e\u673a\u5668\u4eba\u68c0\u6d4b\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u539f\u5219\u6027\u6846\u67b6\uff0cBOCLOAK\u5c55\u793a\u4e86\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u8bc4\u4f30GNN\u68c0\u6d4b\u5668\u9c81\u68d2\u6027\u7684\u6709\u6548\u6027"}}
{"id": "2602.01774", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01774", "abs": "https://arxiv.org/abs/2602.01774", "authors": ["Thomas Langerak", "Renate Zhang", "Ziyuan Wang", "Per Ola Kristensson", "Antti Oulasvirta"], "title": "Cost-Aware Bayesian Optimization for Prototyping Interactive Devices", "comment": null, "summary": "Deciding which idea is worth prototyping is a central concern in iterative design. A prototype should be produced when the expected improvement is high and the cost is low. However, this is hard to decide, because costs can vary drastically: a simple parameter tweak may take seconds, while fabricating hardware consumes material and energy. Such asymmetries, can discourage a designer from exploring the design space. In this paper, we present an extension of cost-aware Bayesian optimization to account for diverse prototyping costs. The method builds on the power of Bayesian optimization and requires only a minimal modification to the acquisition function. The key idea is to use designer-estimated costs to guide sampling toward more cost-effective prototypes. In technical evaluations, the method achieved comparable utility to a cost-agnostic baseline while requiring only ${\\approx}70\\%$ of the cost; under strict budgets, it outperformed the baseline threefold. A within-subjects study with 12 participants in a realistic joystick design task demonstrated similar benefits. These results show that accounting for prototyping costs can make Bayesian optimization more compatible with real-world design projects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u6210\u672c\u611f\u77e5\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u591a\u6837\u5316\u7684\u539f\u578b\u5236\u4f5c\u6210\u672c\u6765\u6307\u5bfc\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u5728\u4fdd\u6301\u76f8\u4f3c\u6548\u7528\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u5728\u8fed\u4ee3\u8bbe\u8ba1\u4e2d\uff0c\u51b3\u5b9a\u54ea\u4e9b\u60f3\u6cd5\u503c\u5f97\u5236\u4f5c\u539f\u578b\u662f\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u3002\u539f\u578b\u5236\u4f5c\u6210\u672c\u5dee\u5f02\u5de8\u5927\uff08\u4ece\u7b80\u5355\u7684\u53c2\u6570\u8c03\u6574\u5230\u786c\u4ef6\u5236\u9020\uff09\uff0c\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u4f1a\u963b\u788d\u8bbe\u8ba1\u5e08\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u591a\u6837\u5316\u7684\u539f\u578b\u5236\u4f5c\u6210\u672c\u3002", "method": "\u6269\u5c55\u6210\u672c\u611f\u77e5\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5e08\u4f30\u8ba1\u7684\u6210\u672c\u6765\u6307\u5bfc\u91c7\u6837\uff0c\u9009\u62e9\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u539f\u578b\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u4ec5\u9700\u5bf9\u91c7\u96c6\u51fd\u6570\u8fdb\u884c\u6700\u5c0f\u4fee\u6539\u3002", "result": "\u6280\u672f\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4e0e\u6210\u672c\u65e0\u5173\u57fa\u7ebf\u76f8\u4f3c\u6548\u7528\u7684\u540c\u65f6\uff0c\u4ec5\u9700\u7ea670%\u7684\u6210\u672c\uff1b\u5728\u4e25\u683c\u9884\u7b97\u4e0b\uff0c\u6027\u80fd\u6bd4\u57fa\u7ebf\u9ad8\u51fa\u4e09\u500d\u3002\u572812\u540d\u53c2\u4e0e\u8005\u53c2\u4e0e\u7684\u6447\u6746\u8bbe\u8ba1\u4efb\u52a1\u4e2d\uff0c\u7528\u6237\u7814\u7a76\u4e5f\u663e\u793a\u51fa\u7c7b\u4f3c\u4f18\u52bf\u3002", "conclusion": "\u8003\u8651\u539f\u578b\u5236\u4f5c\u6210\u672c\u53ef\u4ee5\u4f7f\u8d1d\u53f6\u65af\u4f18\u5316\u66f4\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u8bbe\u8ba1\u9879\u76ee\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u5728\u6210\u672c\u7ea6\u675f\u4e0b\u66f4\u6709\u6548\u5730\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u3002"}}
{"id": "2602.01535", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01535", "abs": "https://arxiv.org/abs/2602.01535", "authors": ["Huzaifa Mustafa Unjhawala", "Khizar Shaikh", "Luning Bakke", "Radu Serban", "Dan Negrut"], "title": "Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations", "comment": "19 pages, 15 figures", "summary": "While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u6708\u7403\u8f66\u8f66\u8f6e\u51e0\u4f55\u5f62\u72b6\u548c\u8f6c\u5411\u63a7\u5236\u5668\u53c2\u6570\uff0c\u4f7f\u7528\u8fde\u7eed\u4ecb\u8d28\u8868\u793a\u6a21\u578b\u8fdb\u884c\u9ad8\u4fdd\u771f\u5168\u8f66\u95ed\u73af\u4eff\u771f\uff0c\u76f8\u6bd4\u4f20\u7edf\u79bb\u6563\u5143\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387", "motivation": "\u4f20\u7edf\u79bb\u6563\u5143\u65b9\u6cd5\u4eff\u771f\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5168\u8f66\u7cfb\u7edf\u7814\u7a76\uff0c\u65e0\u6cd5\u5b9e\u73b0\u673a\u68b0\u8bbe\u8ba1\u4e0e\u63a7\u5236\u7684\u8054\u5408\u4f18\u5316\uff0c\u963b\u788d\u4e86\u8d8a\u91ce\u81ea\u4e3b\u79fb\u52a8\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u63d0\u5347", "method": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u8fde\u7eed\u4ecb\u8d28\u8868\u793a\u6a21\u578b\u8fdb\u884c\u9ad8\u4fdd\u771f\u5168\u8f66\u95ed\u73af\u4eff\u771f\uff0c\u540c\u65f6\u4f18\u5316\u8f66\u8f6e\u53c2\u6570\uff08\u534a\u5f84\u3001\u5bbd\u5ea6\u3001\u6293\u5730\u9f7f\u7279\u5f81\uff09\u548c\u8f6c\u5411PID\u589e\u76ca\uff0c\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u5e73\u8861\u884c\u9a76\u901f\u5ea6\u3001\u8ddf\u8e2a\u8bef\u5dee\u548c\u80fd\u8017", "result": "\u5b8c\u62103000\u6b21\u5168\u8f66\u4eff\u771f\u4ec5\u97005-9\u5929\uff0c\u76f8\u6bd4\u4f20\u7edfDEM\u65b9\u6cd5\u6570\u6708\u65f6\u95f4\u5927\u5e45\u63d0\u5347\u6548\u7387\uff1b\u786c\u4ef6\u521d\u6b65\u9a8c\u8bc1\u663e\u793a\u4eff\u771f\u4f18\u5316\u8bbe\u8ba1\u5728\u7269\u7406\u7cfb\u7edf\u4e0a\u4fdd\u6301\u76f8\u5bf9\u6027\u80fd\u8d8b\u52bf", "conclusion": "\u53ef\u6269\u5c55\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u80fd\u591f\u5b9e\u73b0\u8d8a\u91ce\u8f66\u8f86\u8f66\u8f6e\u8bbe\u8ba1\u4e0e\u63a7\u5236\u7684\u5b9e\u7528\u8054\u5408\u4f18\u5316\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684DEM\u7814\u7a76\uff1b\u5f00\u6e90\u4eff\u771f\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u53ef\u91cd\u590d\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76"}}
{"id": "2602.01796", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01796", "abs": "https://arxiv.org/abs/2602.01796", "authors": ["Xiaojiao Chen", "Jiahuan Zhou", "Yunfeng Shu", "Ruihan Wang", "Qinghua Liu"], "title": "CritiqueCrew: Orchestrating Multi-Perspective Conversational Design Critique", "comment": "20 pages, 10 figures; Accepted to CHI 2026", "summary": "UI designers face growing cognitive load and cross functional friction at the intersection of user needs, business goals, and engineering constraints. Existing automated tools often deliver static \"problem lists\", lacking actionable repair paths and disrupting creative flow. We introduce CritiqueCrew, a Figma tool that supports designers through conversational critique. CritiqueCrew generates multi-faceted insights by implementing a multi-perspective orchestration of distinct expert roles (UX, PM, Engineer). It translates abstract critiques into concrete actions via in context feedback and interactive remediation. Across two independent controlled studies (Total N=48), CritiqueCrew significantly improved both design quality and subjective experience compared to a traditional static checker. Furthermore, our results confirm that the structured orchestration of expert roles-rather than a unified model-is key to fostering trust and creativity support. Our work demonstrates how AI can shift from a \"problem auditor\" to a \"solution co-creator\" by integrating multi-perspective dialogue with interactive repair, offering design implications for future creative tools.", "AI": {"tldr": "CritiqueCrew\uff1a\u57fa\u4e8e\u591a\u4e13\u5bb6\u89d2\u8272\u534f\u540c\u7684Figma\u8bbe\u8ba1\u5de5\u5177\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u6279\u8bc4\u5c06AI\u4ece\"\u95ee\u9898\u5ba1\u8ba1\u5458\"\u8f6c\u53d8\u4e3a\"\u89e3\u51b3\u65b9\u6848\u5171\u521b\u8005\"", "motivation": "UI\u8bbe\u8ba1\u5e08\u9762\u4e34\u7528\u6237\u9700\u6c42\u3001\u4e1a\u52a1\u76ee\u6807\u548c\u5de5\u7a0b\u7ea6\u675f\u4ea4\u53c9\u70b9\u4e0a\u7684\u8ba4\u77e5\u8d1f\u8377\u548c\u8de8\u804c\u80fd\u6469\u64e6\u3002\u73b0\u6709\u81ea\u52a8\u5316\u5de5\u5177\u901a\u5e38\u63d0\u4f9b\u9759\u6001\"\u95ee\u9898\u5217\u8868\"\uff0c\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u7684\u4fee\u590d\u8def\u5f84\uff0c\u4e14\u4f1a\u4e2d\u65ad\u521b\u610f\u6d41\u7a0b\u3002", "method": "\u5f15\u5165CritiqueCrew\uff0c\u4e00\u4e2aFigma\u5de5\u5177\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u6279\u8bc4\u652f\u6301\u8bbe\u8ba1\u5e08\u3002\u8be5\u5de5\u5177\u901a\u8fc7\u5b9e\u65bd\u591a\u89c6\u89d2\u534f\u540c\u673a\u5236\uff0c\u534f\u8c03\u4e0d\u540c\u4e13\u5bb6\u89d2\u8272\uff08UX\u8bbe\u8ba1\u5e08\u3001\u4ea7\u54c1\u7ecf\u7406\u3001\u5de5\u7a0b\u5e08\uff09\u751f\u6210\u591a\u7ef4\u5ea6\u6d1e\u5bdf\uff0c\u5c06\u62bd\u8c61\u6279\u8bc4\u8f6c\u5316\u4e3a\u5177\u4f53\u884c\u52a8\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u53cd\u9988\u548c\u4ea4\u4e92\u5f0f\u4fee\u590d\u3002", "result": "\u5728\u4e24\u9879\u72ec\u7acb\u5bf9\u7167\u7814\u7a76\uff08\u603bN=48\uff09\u4e2d\uff0c\u4e0e\u4f20\u7edf\u9759\u6001\u68c0\u67e5\u5668\u76f8\u6bd4\uff0cCritiqueCrew\u663e\u8457\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u8d28\u91cf\u548c\u4e3b\u89c2\u4f53\u9a8c\u3002\u7ed3\u679c\u8bc1\u5b9e\uff0c\u7ed3\u6784\u5316\u4e13\u5bb6\u89d2\u8272\u534f\u540c\uff08\u800c\u975e\u7edf\u4e00\u6a21\u578b\uff09\u662f\u5efa\u7acb\u4fe1\u4efb\u548c\u652f\u6301\u521b\u610f\u7684\u5173\u952e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5c06\u591a\u89c6\u89d2\u5bf9\u8bdd\u4e0e\u4ea4\u4e92\u5f0f\u4fee\u590d\u76f8\u7ed3\u5408\uff0cAI\u53ef\u4ee5\u4ece\"\u95ee\u9898\u5ba1\u8ba1\u5458\"\u8f6c\u53d8\u4e3a\"\u89e3\u51b3\u65b9\u6848\u5171\u521b\u8005\"\uff0c\u4e3a\u672a\u6765\u521b\u610f\u5de5\u5177\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u542f\u793a\u3002"}}
{"id": "2602.01536", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01536", "abs": "https://arxiv.org/abs/2602.01536", "authors": ["Shuai Liu", "Siheng Ren", "Xiaoyao Zhu", "Quanmin Liang", "Zefeng Li", "Qiang Li", "Xin Hu", "Kai Huang"], "title": "UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning", "comment": null, "summary": "Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.", "code_url": "https://github.com/Say2L/UniDWM", "AI": {"tldr": "UniDWM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u65b9\u9762\u7684\u8868\u5f81\u5b66\u4e60\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u80fd\u529b\uff0c\u6784\u5efa\u7ed3\u6784\u548c\u52a8\u6001\u611f\u77e5\u7684\u6f5c\u5728\u4e16\u754c\u8868\u793a\uff0c\u652f\u6301\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u7684\u4e00\u81f4\u6027\u63a8\u7406\u3002", "motivation": "\u5728\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u9ad8\u6548\u7684\u89c4\u5212\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u63a8\u7406\u573a\u666f\u51e0\u4f55\u3001\u5916\u89c2\u548c\u52a8\u6001\u7684\u6a21\u578b\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u7f3a\u4e4f\u7edf\u4e00\u7684\u4e16\u754c\u8868\u793a\u6765\u534f\u8c03\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u3002", "method": "UniDWM\u6784\u5efa\u7ed3\u6784\u548c\u52a8\u6001\u611f\u77e5\u7684\u6f5c\u5728\u4e16\u754c\u8868\u793a\u4f5c\u4e3a\u7269\u7406\u57fa\u7840\u7684\u72b6\u6001\u7a7a\u95f4\u3002\u91c7\u7528\u8054\u5408\u91cd\u5efa\u8def\u5f84\u5b66\u4e60\u6062\u590d\u573a\u666f\u7ed3\u6784\uff08\u51e0\u4f55\u548c\u89c6\u89c9\u7eb9\u7406\uff09\uff0c\u534f\u4f5c\u751f\u6210\u6846\u67b6\u5229\u7528\u6761\u4ef6\u6269\u6563\u53d8\u6362\u5668\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9884\u6d4b\u672a\u6765\u4e16\u754c\u6f14\u5316\u3002\u7406\u8bba\u5206\u6790\u8868\u660eUniDWM\u53ef\u89c6\u4e3aVAE\u7684\u53d8\u4f53\uff0c\u4e3a\u591a\u65b9\u9762\u8868\u5f81\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eUniDWM\u5728\u8f68\u8ff9\u89c4\u5212\u30014D\u91cd\u5efa\u548c\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7a81\u663e\u591a\u65b9\u9762\u4e16\u754c\u8868\u793a\u4f5c\u4e3a\u7edf\u4e00\u9a7e\u9a76\u667a\u80fd\u57fa\u7840\u7684\u6f5c\u529b\u3002", "conclusion": "UniDWM\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u65b9\u9762\u4e16\u754c\u8868\u793a\u63a8\u8fdb\u81ea\u52a8\u9a7e\u9a76\uff0c\u4e3a\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u63d0\u4f9b\u4e00\u81f4\u7684\u63a8\u7406\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u7edf\u4e00\u9a7e\u9a76\u667a\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.01824", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01824", "abs": "https://arxiv.org/abs/2602.01824", "authors": ["Daniel Mwesigwa", "Steven J. Jackson", "Christopher Csikszentmihalyi"], "title": "Risk, Data, Alignment: Making Credit Scoring Work in Kenya", "comment": "Conditionally accepted to ACM CHI 2026, Barcelona, Spain", "summary": "Credit scoring is an increasingly central and contested domain of data and AI governance, frequently framed as a neutral and objective method of assessing risk across diverse economic and political contexts. Based on a nine-month ethnography of credit scoring practices in Nairobi, Kenya, we examined the sociotechnical and institutional work of data science in digital lending. While established regional telcos and banks are leveraging proprietary data to develop digital loan products, algorithmic credit scoring is being transformed by new actors, techniques, and shifting regulations. Our findings show how practitioners construct alternative data using technical and legal workarounds, formulate risk through multiple interpretations, and negotiate model performance via technical and political means. We argue that algorithmic credit scoring is accomplished through the ongoing work of alignment that stabilizes risk under conditions of persistent uncertainty, taking epistemic, modeling, and contextual forms. Extending work on alignment in HCI, we show how it operates as a two-way translation, where models are made \"safe for worlds\" while those worlds are reshaped to be \"safe for models.\"", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u80af\u5c3c\u4e9a\u5185\u7f57\u6bd5\u7684\u4e5d\u4e2a\u6708\u6c11\u65cf\u5fd7\u8c03\u67e5\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u4fe1\u7528\u8bc4\u5206\u5982\u4f55\u901a\u8fc7\u6301\u7eed\u7684\u5bf9\u9f50\u5de5\u4f5c\u6765\u7a33\u5b9a\u98ce\u9669\uff0c\u6d89\u53ca\u6280\u672f\u3001\u6cd5\u5f8b\u548c\u653f\u6cbb\u624b\u6bb5\u7684\u590d\u6742\u4e92\u52a8\u3002", "motivation": "\u4fe1\u7528\u8bc4\u5206\u4f5c\u4e3a\u6570\u636e\u548cAI\u6cbb\u7406\u7684\u6838\u5fc3\u9886\u57df\uff0c\u5e38\u88ab\u63cf\u7ed8\u4e3a\u4e2d\u7acb\u5ba2\u89c2\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5728\u6570\u5b57\u501f\u8d37\u5feb\u901f\u53d1\u5c55\u7684\u80cc\u666f\u4e0b\uff0c\u7279\u522b\u662f\u5728\u80af\u5c3c\u4e9a\u8fd9\u6837\u7684\u65b0\u5174\u5e02\u573a\uff0c\u9700\u8981\u7406\u89e3\u7b97\u6cd5\u4fe1\u7528\u8bc4\u5206\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u793e\u4f1a\u6280\u672f\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u5b83\u5982\u4f55\u5728\u4e0d\u540c\u5236\u5ea6\u73af\u5883\u4e0b\u88ab\u6784\u5efa\u548c\u5b9e\u65bd\u3002", "method": "\u91c7\u7528\u4e5d\u4e2a\u6708\u7684\u6c11\u65cf\u5fd7\u7814\u7a76\u65b9\u6cd5\uff0c\u6df1\u5165\u8c03\u67e5\u5185\u7f57\u6bd5\u7684\u4fe1\u7528\u8bc4\u5206\u5b9e\u8df5\u3002\u901a\u8fc7\u5b9e\u5730\u89c2\u5bdf\u548c\u8bbf\u8c08\uff0c\u5206\u6790\u6570\u636e\u79d1\u5b66\u5728\u6570\u5b57\u501f\u8d37\u4e2d\u7684\u793e\u4f1a\u6280\u672f\u548c\u5236\u5ea6\u5de5\u4f5c\uff0c\u5173\u6ce8\u4e0d\u540c\u53c2\u4e0e\u8005\uff08\u533a\u57df\u7535\u4fe1\u516c\u53f8\u3001\u94f6\u884c\u3001\u65b0\u8fdb\u5165\u8005\uff09\u5982\u4f55\u5229\u7528\u4e13\u6709\u6570\u636e\u3001\u5f00\u53d1\u6280\u672f\u6cd5\u5f8b\u53d8\u901a\u65b9\u6848\uff0c\u4ee5\u53ca\u5e94\u5bf9\u76d1\u7ba1\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u4ece\u4e1a\u8005\u901a\u8fc7\u6280\u672f\u548c\u6cd5\u5f8b\u53d8\u901a\u65b9\u6848\u6784\u5efa\u66ff\u4ee3\u6570\u636e\uff1b2\uff09\u98ce\u9669\u901a\u8fc7\u591a\u79cd\u89e3\u91ca\u65b9\u5f0f\u88ab\u6784\u5efa\uff1b3\uff09\u6a21\u578b\u6027\u80fd\u901a\u8fc7\u6280\u672f\u548c\u653f\u6cbb\u624b\u6bb5\u8fdb\u884c\u534f\u5546\u3002\u7b97\u6cd5\u4fe1\u7528\u8bc4\u5206\u662f\u901a\u8fc7\u6301\u7eed\u7684\u5bf9\u9f50\u5de5\u4f5c\u5b9e\u73b0\u7684\uff0c\u8fd9\u79cd\u5bf9\u9f50\u91c7\u53d6\u8ba4\u77e5\u3001\u5efa\u6a21\u548c\u60c5\u5883\u4e09\u79cd\u5f62\u5f0f\uff0c\u5728\u6301\u7eed\u4e0d\u786e\u5b9a\u7684\u6761\u4ef6\u4e0b\u7a33\u5b9a\u98ce\u9669\u3002", "conclusion": "\u7b97\u6cd5\u4fe1\u7528\u8bc4\u5206\u662f\u901a\u8fc7\u53cc\u5411\u7ffb\u8bd1\u7684\u5bf9\u9f50\u8fc7\u7a0b\u5b9e\u73b0\u7684\uff1a\u4e00\u65b9\u9762\u4f7f\u6a21\u578b\"\u9002\u5e94\u4e16\u754c\"\uff0c\u53e6\u4e00\u65b9\u9762\u91cd\u5851\u4e16\u754c\u4ee5\"\u9002\u5e94\u6a21\u578b\"\u3002\u8fd9\u4e00\u53d1\u73b0\u6269\u5c55\u4e86HCI\u4e2d\u5bf9\u9f50\u5de5\u4f5c\u7684\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u4fe1\u7528\u8bc4\u5206\u5b9e\u8df5\u4e2d\u590d\u6742\u7684\u793e\u4f1a\u6280\u672f\u534f\u5546\u8fc7\u7a0b\uff0c\u6311\u6218\u4e86\u4fe1\u7528\u8bc4\u5206\u4f5c\u4e3a\u4e2d\u7acb\u5ba2\u89c2\u6280\u672f\u7684\u7b80\u5355\u53d9\u4e8b\u3002"}}
{"id": "2602.01632", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01632", "abs": "https://arxiv.org/abs/2602.01632", "authors": ["Chuizheng Kong", "Yunho Cho", "Wonsuhk Jung", "Idris Wibowo", "Parth Shinde", "Sundhar Vinodh-Sangeetha", "Long Kiu Chung", "Zhenyang Chen", "Andrew Mattei", "Advaith Nidumukkala", "Alexander Elias", "Danfei Xu", "Taylor Higgins", "Shreyas Kousik"], "title": "A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation", "comment": "Project page at https://sew-mimic.com/", "summary": "Retargeting human motion to robot poses is a practical approach for teleoperating bimanual humanoid robot arms, but existing methods can be suboptimal and slow, often causing undesirable motion or latency. This is due to optimizing to match robot end-effector to human hand position and orientation, which can also limit the robot's workspace to that of the human. Instead, this paper reframes retargeting as an orientation alignment problem, enabling a closed-form, geometric solution algorithm with an optimality guarantee. The key idea is to align a robot arm to a human's upper and lower arm orientations, as identified from shoulder, elbow, and wrist (SEW) keypoints; hence, the method is called SEW-Mimic. The method has fast inference (3 kHz) on standard commercial CPUs, leaving computational overhead for downstream applications; an example in this paper is a safety filter to avoid bimanual self-collision. The method suits most 7-degree-of-freedom robot arms and humanoids, and is agnostic to input keypoint source. Experiments show that SEW-Mimic outperforms other retargeting methods in computation time and accuracy. A pilot user study suggests that the method improves teleoperation task success. Preliminary analysis indicates that data collected with SEW-Mimic improves policy learning due to being smoother. SEW-Mimic is also shown to be a drop-in way to accelerate full-body humanoid retargeting. Finally, hardware demonstrations illustrate SEW-Mimic's practicality. The results emphasize the utility of SEW-Mimic as a fundamental building block for bimanual robot manipulation and humanoid robot teleoperation.", "AI": {"tldr": "SEW-Mimic\uff1a\u4e00\u79cd\u57fa\u4e8e\u65b9\u5411\u5bf9\u9f50\u7684\u5feb\u901f\u95ed\u5f0f\u51e0\u4f55\u7b97\u6cd5\uff0c\u7528\u4e8e\u5c06\u4eba\u4f53\u8fd0\u52a8\u91cd\u5b9a\u5411\u5230\u673a\u5668\u4eba\u59ff\u6001\uff0c\u901a\u8fc7\u80a9-\u8098-\u8155\u5173\u952e\u70b9\u5bf9\u9f50\u5b9e\u73b03kHz\u63a8\u7406\u901f\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u8fd0\u52a8\u5230\u673a\u5668\u4eba\u59ff\u6001\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u6548\u679c\u4e0d\u4f73\u3001\u901f\u5ea6\u6162\u3001\u4ea7\u751f\u4e0d\u671f\u671b\u8fd0\u52a8\u6216\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4f18\u5316\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u4e0e\u4eba\u7c7b\u624b\u90e8\u4f4d\u7f6e\u548c\u65b9\u5411\u5339\u914d\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u5de5\u4f5c\u7a7a\u95f4\u3002", "method": "\u5c06\u91cd\u5b9a\u5411\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u65b9\u5411\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51faSEW-Mimic\u65b9\u6cd5\uff1a\u901a\u8fc7\u80a9\u3001\u8098\u3001\u8155\u5173\u952e\u70b9\u8bc6\u522b\u4eba\u7c7b\u4e0a\u81c2\u548c\u4e0b\u81c2\u65b9\u5411\uff0c\u7136\u540e\u4f7f\u7528\u95ed\u5f0f\u51e0\u4f55\u7b97\u6cd5\u5c06\u673a\u5668\u4eba\u624b\u81c2\u4e0e\u8fd9\u4e9b\u65b9\u5411\u5bf9\u9f50\uff0c\u4fdd\u8bc1\u6700\u4f18\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u6807\u51c6\u5546\u7528CPU\u4e0a\u8fbe\u52303kHz\u63a8\u7406\u901f\u5ea6\uff0c\u8ba1\u7b97\u65f6\u95f4\u77ed\u4e14\u7cbe\u5ea6\u9ad8\uff1b\u7528\u6237\u7814\u7a76\u8868\u660e\u80fd\u63d0\u9ad8\u9065\u64cd\u4f5c\u4efb\u52a1\u6210\u529f\u7387\uff1b\u6536\u96c6\u7684\u6570\u636e\u66f4\u5e73\u6ed1\uff0c\u6709\u5229\u4e8e\u7b56\u7565\u5b66\u4e60\uff1b\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u65b9\u6848\u52a0\u901f\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba\u91cd\u5b9a\u5411\u3002", "conclusion": "SEW-Mimic\u4f5c\u4e3a\u4e00\u79cd\u57fa\u7840\u6784\u5efa\u6a21\u5757\uff0c\u5728\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u5176\u5feb\u901f\u3001\u51c6\u786e\u7684\u7279\u70b9\u4e3a\u4e0b\u6e38\u5e94\u7528\u7559\u51fa\u4e86\u8ba1\u7b97\u4f59\u91cf\u3002"}}
{"id": "2602.01846", "categories": ["cs.HC", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01846", "abs": "https://arxiv.org/abs/2602.01846", "authors": ["Changyang He", "Parnian Jahangirirad", "Lin Kyi", "Asia J. Biega"], "title": "When Feasibility of Fairness Audits Relies on Willingness to Share Data: Examining User Acceptance of Multi-Party Computation Protocols for Fairness Monitoring", "comment": "34 pages, 5 figures. Conditionally accepted to CHI 2026", "summary": "Fairness monitoring is critical for detecting algorithmic bias, as mandated by the EU AI Act. Since such monitoring requires sensitive user data (e.g., ethnicity), the AI Act permits its processing only with strict privacy measures, such as multi-party computation (MPC), in compliance with the GDPR. However, the effectiveness of such secure monitoring protocols ultimately depends on people's willingness to share their data. Little is known about how different MPC protocol designs shape user acceptance. To address this, we conducted an online survey with 833 participants in Europe, examining user acceptance of various MPC protocol designs for fairness monitoring. Findings suggest that users prioritized risk-related attributes (e.g., privacy protection mechanism) in direct evaluation but benefit-related attributes (e.g., fairness objective) in simulated choices, with acceptance shaped by their fairness and privacy orientations. We derive implications for deploying and communicating privacy-preserving protocols in ways that foster informed consent and align with user expectations.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86\u7528\u6237\u5bf9\u7528\u4e8e\u516c\u5e73\u6027\u76d1\u6d4b\u7684\u591a\u65b9\u8ba1\u7b97\u534f\u8bae\u8bbe\u8ba1\u7684\u63a5\u53d7\u5ea6\uff0c\u53d1\u73b0\u7528\u6237\u8bc4\u4f30\u65f6\u5173\u6ce8\u98ce\u9669\u5c5e\u6027\uff0c\u9009\u62e9\u65f6\u5173\u6ce8\u6536\u76ca\u5c5e\u6027\uff0c\u63a5\u53d7\u5ea6\u53d7\u516c\u5e73\u548c\u9690\u79c1\u53d6\u5411\u5f71\u54cd\u3002", "motivation": "\u6b27\u76dfAI\u6cd5\u6848\u8981\u6c42\u8fdb\u884c\u516c\u5e73\u6027\u76d1\u6d4b\uff0c\u8fd9\u9700\u8981\u5904\u7406\u654f\u611f\u7528\u6237\u6570\u636e\uff0c\u6cd5\u6848\u5141\u8bb8\u5728\u4e25\u683c\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u4e0b\u5904\u7406\u6b64\u7c7b\u6570\u636e\u3002\u7136\u800c\uff0c\u5b89\u5168\u76d1\u6d4b\u534f\u8bae\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u7528\u6237\u5206\u4eab\u6570\u636e\u7684\u610f\u613f\uff0c\u76ee\u524d\u5bf9MPC\u534f\u8bae\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u7528\u6237\u63a5\u53d7\u5ea6\u4e86\u89e3\u751a\u5c11\u3002", "method": "\u5728\u6b27\u6d32\u5bf9833\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e86\u5728\u7ebf\u8c03\u67e5\uff0c\u7814\u7a76\u4e86\u7528\u6237\u5bf9\u7528\u4e8e\u516c\u5e73\u6027\u76d1\u6d4b\u7684\u5404\u79cdMPC\u534f\u8bae\u8bbe\u8ba1\u7684\u63a5\u53d7\u5ea6\uff0c\u5206\u6790\u4e86\u76f4\u63a5\u8bc4\u4f30\u548c\u6a21\u62df\u9009\u62e9\u4e2d\u7684\u4e0d\u540c\u504f\u597d\u6a21\u5f0f\u3002", "result": "\u7528\u6237\u5728\u8fdb\u884c\u76f4\u63a5\u8bc4\u4f30\u65f6\u4f18\u5148\u8003\u8651\u98ce\u9669\u76f8\u5173\u5c5e\u6027\uff08\u5982\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff09\uff0c\u4f46\u5728\u6a21\u62df\u9009\u62e9\u65f6\u66f4\u5173\u6ce8\u6536\u76ca\u76f8\u5173\u5c5e\u6027\uff08\u5982\u516c\u5e73\u6027\u76ee\u6807\uff09\u3002\u7528\u6237\u7684\u63a5\u53d7\u5ea6\u53d7\u5230\u5176\u516c\u5e73\u53d6\u5411\u548c\u9690\u79c1\u53d6\u5411\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u90e8\u7f72\u548c\u4f20\u8fbe\u9690\u79c1\u4fdd\u62a4\u534f\u8bae\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u4ee5\u4fc3\u8fdb\u77e5\u60c5\u540c\u610f\u548c\u7b26\u5408\u7528\u6237\u671f\u671b\u7684\u65b9\u5f0f\u6765\u8bbe\u8ba1\u548c\u6c9f\u901a\u8fd9\u4e9b\u534f\u8bae\u3002"}}
{"id": "2602.01679", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01679", "abs": "https://arxiv.org/abs/2602.01679", "authors": ["Raghavasimhan Sankaranarayanan", "Paul Stuart", "Nicholas Ahn", "Arno Sungarian", "Yash Chitalia"], "title": "Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications", "comment": "7 pages, 9 figures, 2026 International Symposium on Medical Robotics", "summary": "The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.", "AI": {"tldr": "\u63d0\u51fa\u5168\u81ea\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u7528\u4e8e\u624b\u672f\u5668\u68b0\u7684\u5206\u7c7b\u548c\u7ed3\u6784\u5305\u88c5\uff0c\u66ff\u4ee3SPD\u90e8\u95e8\u4eba\u5de5\u64cd\u4f5c\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u5c11\u6c61\u67d3\u548c\u635f\u574f\u98ce\u9669\u3002", "motivation": "SPD\u90e8\u95e8\u4eba\u5de5\u68c0\u67e5\u548c\u51c6\u5907\u5668\u68b0\u6258\u76d8\u8017\u65f6\u3001\u6613\u51fa\u9519\uff0c\u5bb9\u6613\u5bfc\u81f4\u6c61\u67d3\u548c\u5668\u68b0\u635f\u574f\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u6df7\u5408\u611f\u77e5\u7ba1\u9053\uff08YOLO12\u68c0\u6d4b+\u7ea7\u8054ResNet\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff09\u30016\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u3001\u5b9a\u5236\u53cc\u7535\u78c1\u5939\u5177\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5305\u88c5\u7b97\u6cd5\u7684\u5168\u81ea\u52a8\u7cfb\u7edf\uff0c\u4f7f\u75283D\u6253\u5370\u5206\u9694\u5668\u548c\u56fa\u5b9a\u5668\u7269\u7406\u9694\u79bb\u5668\u68b0\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u611f\u77e5\u7cbe\u5ea6\uff0c\u4e0e\u4eba\u5de5\u7ec4\u88c5\u6258\u76d8\u76f8\u6bd4\uff0c\u5728\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u51cf\u5c11\u4e86\u5668\u68b0\u95f4\u78b0\u649e\uff0c\u63d0\u9ad8\u4e86\u5305\u88c5\u8d28\u91cf\u548c\u8fd0\u8f93\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u81ea\u52a8\u5316SPD\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7b2c\u4e00\u6b65\uff0c\u6539\u5584\u4e86\u624b\u672f\u51c6\u5907\u7684\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86SPD\u5904\u7406\u65f6\u95f4\u3002"}}
{"id": "2602.01428", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01428", "abs": "https://arxiv.org/abs/2602.01428", "authors": ["Weiqing He", "Xiang Li", "Li Shen", "Weijie Su", "Qi Long"], "title": "Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models", "comment": "Accepted at ICLR 2026", "summary": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u63a8\u6d4b\u91c7\u6837\u4e0e\u6c34\u5370\u6280\u672f\u76f8\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u8349\u7a3f-\u4ee4\u724c\u63a5\u53d7\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u4f2a\u968f\u673a\u6027\uff0c\u5728\u4fdd\u6301\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u7684\u540c\u65f6\u6700\u5927\u5316\u6c34\u5370\u5f3a\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a5\u53d7\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u6280\u672f\u5728\u5b9e\u8df5\u90e8\u7f72\u4e2d\u5b58\u5728\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002\u63a8\u6d4b\u91c7\u6837\u867d\u7136\u80fd\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8868\u660e\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a5\u53d7\u7387\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff1a\u66f4\u9ad8\u7684\u6c34\u5370\u5f3a\u5ea6\u4f1a\u964d\u4f4e\u63a5\u53d7\u7387\uff0c\u963b\u788d\u4e24\u8005\u540c\u65f6\u5b9e\u73b0\u3002\u4f5c\u8005\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u6743\u8861\uff0c\u63a2\u7d22\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u548c\u5f3a\u6c34\u5370\u7684\u53ef\u80fd\u6027\u3002", "method": "1. \u5f15\u5165\u4e86\u4e00\u4e2a\u91cf\u5316\u6c34\u5370\u5f3a\u5ea6\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u8be5\u6807\u51c6\u63a7\u5236\u7edf\u8ba1\u53ef\u68c0\u6d4b\u6027\uff0c\u5e76\u5728\u4ee4\u724c\u662f\u4f2a\u968f\u673a\u6570\u7684\u786e\u5b9a\u6027\u51fd\u6570\u65f6\u8fbe\u5230\u6700\u5927\u5316\u3002\n2. \u5c06\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a5\u53d7\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5b8c\u5168\u63cf\u8ff0\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4e3a\u4e24\u79cd\u73b0\u6709\u6c34\u5370\u65b9\u6848\u63a8\u5bfc\u51fa\u663e\u5f0f\u7684\u5e15\u7d2f\u6258\u66f2\u7ebf\u3002\n3. \u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u5219\u6027\u673a\u5236\uff0c\u5411\u8349\u7a3f-\u4ee4\u724c\u63a5\u53d7\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u4f2a\u968f\u673a\u6027\uff0c\u786e\u4fdd\u5728\u4fdd\u6301\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u7684\u540c\u65f6\u6700\u5927\u5316\u6c34\u5370\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u53ef\u68c0\u6d4b\u6027\u3002\u901a\u8fc7\u5411\u63a5\u53d7\u8fc7\u7a0b\u6ce8\u5165\u4f2a\u968f\u673a\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u6700\u5927\u6c34\u5370\u5f3a\u5ea6\uff0c\u540c\u65f6\u7ef4\u6301\u63a8\u6d4b\u91c7\u6837\u7684\u6548\u7387\u4f18\u52bf\u3002\u8be5\u65b9\u6cd5\u4e3a\u4e24\u79cd\u73b0\u6709\u6c34\u5370\u65b9\u6848\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a5\u53d7\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5e76\u975e\u7edd\u5bf9\uff0c\u53ef\u4ee5\u901a\u8fc7\u5411\u8349\u7a3f-\u4ee4\u724c\u63a5\u53d7\u8fc7\u7a0b\u6ce8\u5165\u4f2a\u968f\u673a\u6027\u7684\u539f\u5219\u6027\u673a\u5236\u6765\u514b\u670d\u3002\u8be5\u65b9\u6cd5\u7edf\u4e00\u4e86\u63a8\u6d4b\u91c7\u6837\u4e0e\u6c34\u5370\u6280\u672f\uff0c\u4e3a\u4e24\u8005\u7684\u9ad8\u6548\u5b9e\u7528\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u727a\u7272\u63a8\u7406\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u6c34\u5370\u53ef\u68c0\u6d4b\u6027\u7684\u76ee\u6807\u3002"}}
{"id": "2602.01959", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01959", "abs": "https://arxiv.org/abs/2602.01959", "authors": ["Ezequiel Lopez-Lopez", "Christoph M. Abels", "Philipp Lorenz-Spreen", "Stephan Lewandowsky", "Stefan M. Herzog"], "title": "Boosting metacognition in entangled human-AI interaction to navigate cognitive-behavioral drift", "comment": null, "summary": "People navigate complex environments using cues, heuristics, and other strategies, which are often adaptive in stable settings. However, as AI increasingly permeates society's information environments, those become more adaptive and evolving: LLM-based chatbots participate in extended interaction, maintain conversational histories, mirror social cues, and can hypercustomize responses, thereby shaping not only what information is accessed but how questions are framed, how evidence is interpreted, and when action feels warranted. Here we propose a framework for sustained human-AI interaction that rests on invariant features of human cognition and human--AI interaction and centers on three interlinked phenomena: entanglement between users and AI systems, the emergence of cognitive and behavioral drift over repeated interactions, and the role of metacognition in the awareness and regulation of these dynamics. As conversational agents provide cues (e.g., fluency, coherence, responsiveness) that people treat as informative, subjective confidence and action readiness may increase without corresponding gains in epistemic reliability, making drift difficult to detect and correct. We describe these dynamics across micro-, meso-, and macro-levels. The framework identifies four metacognitive intervention points and psychologically informed interventions that provide metacognitive scaffolding (boosting and self-nudging). Finally, we outline a long-horizon research agenda for scientific foresight.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u4eba\u7c7b\u4e0eAI\u6301\u7eed\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u4eba\u7c7b\u8ba4\u77e5\u4e0eAI\u7cfb\u7edf\u4e4b\u95f4\u7684\u7ea0\u7f20\u3001\u8ba4\u77e5\u884c\u4e3a\u6f02\u79fb\u4ee5\u53ca\u5143\u8ba4\u77e5\u5728\u8c03\u8282\u8fd9\u4e9b\u52a8\u6001\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u968f\u7740AI\uff08\u7279\u522b\u662fLLM\u804a\u5929\u673a\u5668\u4eba\uff09\u65e5\u76ca\u6e17\u900f\u793e\u4f1a\u4fe1\u606f\u73af\u5883\uff0c\u8fd9\u4e9b\u73af\u5883\u53d8\u5f97\u66f4\u52a0\u9002\u5e94\u6027\u548c\u52a8\u6001\u5316\u3002AI\u7cfb\u7edf\u80fd\u591f\u53c2\u4e0e\u6269\u5c55\u4ea4\u4e92\u3001\u7ef4\u62a4\u5bf9\u8bdd\u5386\u53f2\u3001\u6a21\u4eff\u793e\u4ea4\u7ebf\u7d22\u5e76\u8d85\u4e2a\u6027\u5316\u54cd\u5e94\uff0c\u8fd9\u4e0d\u4ec5\u5f71\u54cd\u4fe1\u606f\u83b7\u53d6\uff0c\u8fd8\u5f71\u54cd\u95ee\u9898\u6846\u67b6\u3001\u8bc1\u636e\u89e3\u91ca\u548c\u884c\u52a8\u51b3\u7b56\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u4e3b\u89c2\u4fe1\u5fc3\u548c\u884c\u52a8\u51c6\u5907\u5ea6\u589e\u52a0\uff0c\u800c\u8ba4\u77e5\u53ef\u9760\u6027\u5e76\u672a\u76f8\u5e94\u63d0\u5347\uff0c\u4f7f\u5f97\u8ba4\u77e5\u6f02\u79fb\u96be\u4ee5\u68c0\u6d4b\u548c\u7ea0\u6b63\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u548c\u4eba\u7c7b-AI\u4ea4\u4e92\u4e0d\u53d8\u7279\u5f81\u7684\u6301\u7eed\u4ea4\u4e92\u6846\u67b6\u3002\u8be5\u6846\u67b6\u56f4\u7ed5\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u73b0\u8c61\u6784\u5efa\uff1a\u7528\u6237\u4e0eAI\u7cfb\u7edf\u4e4b\u95f4\u7684\u7ea0\u7f20\u3001\u91cd\u590d\u4ea4\u4e92\u4e2d\u8ba4\u77e5\u548c\u884c\u4e3a\u6f02\u79fb\u7684\u51fa\u73b0\u3001\u4ee5\u53ca\u5143\u8ba4\u77e5\u5728\u8fd9\u4e9b\u52a8\u6001\u4e2d\u7684\u610f\u8bc6\u548c\u8c03\u8282\u4f5c\u7528\u3002\u4ece\u5fae\u89c2\u3001\u4e2d\u89c2\u548c\u5b8f\u89c2\u4e09\u4e2a\u5c42\u9762\u63cf\u8ff0\u8fd9\u4e9b\u52a8\u6001\uff0c\u5e76\u8bc6\u522b\u56db\u4e2a\u5143\u8ba4\u77e5\u5e72\u9884\u70b9\uff0c\u63d0\u51fa\u5fc3\u7406\u4fe1\u606f\u5e72\u9884\u63aa\u65bd\uff08\u5982\u5143\u8ba4\u77e5\u652f\u67b6\u3001\u52a9\u63a8\u548c\u81ea\u6211\u52a9\u63a8\uff09\u3002", "result": "\u6846\u67b6\u8bc6\u522b\u4e86\u4eba\u7c7b-AI\u4ea4\u4e92\u4e2d\u7684\u5173\u952e\u52a8\u6001\u673a\u5236\uff0c\u5305\u62ec\uff1a1\uff09\u7528\u6237\u4e0eAI\u7cfb\u7edf\u4e4b\u95f4\u7684\u8ba4\u77e5\u7ea0\u7f20\uff1b2\uff09\u91cd\u590d\u4ea4\u4e92\u5bfc\u81f4\u7684\u8ba4\u77e5\u548c\u884c\u4e3a\u6f02\u79fb\uff1b3\uff09\u5143\u8ba4\u77e5\u5728\u8c03\u8282\u8fd9\u4e9b\u52a8\u6001\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002\u63d0\u51fa\u4e86\u56db\u4e2a\u5143\u8ba4\u77e5\u5e72\u9884\u70b9\u548c\u76f8\u5e94\u7684\u5fc3\u7406\u4fe1\u606f\u5e72\u9884\u7b56\u7565\uff0c\u4e3a\u7406\u89e3\u548c\u7ba1\u7406\u4eba\u7c7b-AI\u4ea4\u4e92\u4e2d\u7684\u8ba4\u77e5\u98ce\u9669\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u7ba1\u7406\u4eba\u7c7b\u4e0eAI\u7cfb\u7edf\u6301\u7eed\u4ea4\u4e92\u4e2d\u7684\u8ba4\u77e5\u52a8\u6001\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002\u901a\u8fc7\u5173\u6ce8\u5143\u8ba4\u77e5\u5e72\u9884\u70b9\uff0c\u53ef\u4ee5\u5f00\u53d1\u6709\u6548\u7684\u5e72\u9884\u63aa\u65bd\u6765\u589e\u5f3a\u7528\u6237\u5bf9\u8ba4\u77e5\u6f02\u79fb\u7684\u610f\u8bc6\u548c\u8c03\u8282\u80fd\u529b\u3002\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u957f\u671f\u7814\u7a76\u8bae\u7a0b\uff0c\u65e8\u5728\u9884\u6d4b\u548c\u5e94\u5bf9AI\u6e17\u900f\u793e\u4f1a\u4fe1\u606f\u73af\u5883\u5e26\u6765\u7684\u8ba4\u77e5\u6311\u6218\u3002"}}
{"id": "2602.01693", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01693", "abs": "https://arxiv.org/abs/2602.01693", "authors": ["Kewei Hu", "Michael Zhang", "Wei Ying", "Tianhao Liu", "Guoqiang Hao", "Zimeng Li", "Wanchan Yu", "Jiajian Jing", "Fangwen Chen", "Hanwen Kang"], "title": "GSR: Learning Structured Reasoning for Embodied Manipulation", "comment": null, "summary": "Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.", "AI": {"tldr": "\u63d0\u51faGSR\uff08Grounded Scene-graph Reasoning\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u57fa\u4e8e\u8bed\u4e49\u573a\u666f\u56fe\u7684\u4e16\u754c\u72b6\u6001\u6f14\u5316\uff0c\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u56e0\u679c\u4f9d\u8d56\u548c\u76ee\u6807\u7ea6\u675f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u4efb\u52a1\u63a8\u7406\u9690\u5f0f\u5d4c\u5165\u9ad8\u7ef4\u6f5c\u5728\u8868\u793a\uff0c\u96be\u4ee5\u5206\u79bb\u4efb\u52a1\u7ed3\u6784\u4e0e\u611f\u77e5\u53d8\u5f02\u6027\uff0c\u5bfc\u81f4\u5177\u8eab\u667a\u80fd\u4f53\u5728\u9700\u8981\u4fdd\u6301\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u56e0\u679c\u4f9d\u8d56\u548c\u76ee\u6807\u7ea6\u675f\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165GSR\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\uff0c\u5c06\u4e16\u754c\u72b6\u6001\u6f14\u5316\u5efa\u6a21\u4e3a\u8bed\u4e49\u573a\u666f\u56fe\u4e0a\u7684\u8f6c\u6362\u8fc7\u7a0b\uff0c\u901a\u8fc7\u9010\u6b65\u63a8\u7406\u5bf9\u8c61\u72b6\u6001\u548c\u7a7a\u95f4\u5173\u7cfb\uff08\u800c\u975e\u76f4\u63a5\u4ece\u611f\u77e5\u6620\u5c04\u5230\u52a8\u4f5c\uff09\uff0c\u5728\u7269\u7406\u63a5\u5730\u7a7a\u95f4\u4e2d\u663e\u5f0f\u63a8\u7406\u52a8\u4f5c\u524d\u63d0\u6761\u4ef6\u3001\u540e\u679c\u548c\u76ee\u6807\u6ee1\u8db3\u5ea6\u3002\u6784\u5efaManip-Cognition-1.6M\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8054\u5408\u76d1\u7763\u4e16\u754c\u7406\u89e3\u3001\u52a8\u4f5c\u89c4\u5212\u548c\u76ee\u6807\u89e3\u91ca\u3002", "result": "\u5728RLBench\u3001LIBERO\u3001GSR-benchmark\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cGSR\u76f8\u6bd4\u57fa\u4e8e\u63d0\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u5b8c\u6210\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u663e\u5f0f\u4e16\u754c\u72b6\u6001\u8868\u793a\u662f\u5177\u8eab\u63a8\u7406\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u5f52\u7eb3\u504f\u7f6e\uff0c\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\u80fd\u591f\u6709\u6548\u63d0\u5347\u5177\u8eab\u667a\u80fd\u4f53\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2602.01979", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01979", "abs": "https://arxiv.org/abs/2602.01979", "authors": ["Fabio Stano", "Max L Wilson", "Christof Weinhardt", "Michael T Knierim"], "title": "Hacking Flow: From Lived Practices to Innovation", "comment": null, "summary": "In digital knowledge work, flow promises not just productivity; it offers a pathway to well-being. Yet despite decades of flow research in HCI, we know little about how to design digital interventions that support it. In this work, we foreground lived interventions - everyday practices workers already use to foster flow - to uncover overlooked opportunities and chart new directions for digital intervention design. Specifically, we report findings from two studies: (1) a reflexive thematic analysis of open-ended survey responses (n = 160), surfacing 38 lived interventions across four categories: environment, organization, task shaping, and personal readiness; and (2) a quantitative online survey (n = 121) that validates this repertoire, identifies which interventions are broadly endorsed versus polarizing, and elicits visions of technological support. We contribute empirical insights into how digital workers cultivate flow, situate these lived interventions within existing literature, and derive design opportunities for future digital flow interventions.", "AI": {"tldr": "\u7814\u7a76\u6570\u5b57\u77e5\u8bc6\u5de5\u4f5c\u8005\u5982\u4f55\u901a\u8fc7\u65e5\u5e38\u5b9e\u8df5\u57f9\u517b\u5fc3\u6d41\u72b6\u6001\uff0c\u5e76\u63a2\u7d22\u6570\u5b57\u5e72\u9884\u8bbe\u8ba1\u673a\u4f1a", "motivation": "\u5c3d\u7ba1HCI\u9886\u57df\u5bf9\u5fc3\u6d41\u7814\u7a76\u5df2\u6709\u6570\u5341\u5e74\uff0c\u4f46\u5bf9\u5982\u4f55\u8bbe\u8ba1\u652f\u6301\u5fc3\u6d41\u7684\u6570\u5b57\u5e72\u9884\u63aa\u65bd\u77e5\u4e4b\u751a\u5c11\u3002\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5de5\u4f5c\u8005\u5df2\u6709\u65e5\u5e38\u5b9e\u8df5\u7684\u5173\u6ce8\uff0c\u800c\u8fd9\u4e9b\u5b9e\u8df5\u53ef\u80fd\u4e3a\u6570\u5b57\u5e72\u9884\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u65b9\u5411\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a1) \u5bf9160\u4efd\u5f00\u653e\u5f0f\u8c03\u67e5\u56de\u590d\u8fdb\u884c\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\uff0c\u8bc6\u522b\u51fa38\u79cd\u65e5\u5e38\u5e72\u9884\u5b9e\u8df5\uff0c\u5206\u4e3a\u73af\u5883\u3001\u7ec4\u7ec7\u3001\u4efb\u52a1\u5851\u9020\u548c\u4e2a\u4eba\u51c6\u5907\u56db\u7c7b\uff1b2) \u5bf9121\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u5b9a\u91cf\u5728\u7ebf\u8c03\u67e5\uff0c\u9a8c\u8bc1\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u7684\u6709\u6548\u6027\uff0c\u8bc6\u522b\u5e7f\u6cdb\u8ba4\u53ef\u4e0e\u5b58\u5728\u4e89\u8bae\u7684\u5b9e\u8df5\uff0c\u5e76\u6536\u96c6\u5bf9\u6280\u672f\u652f\u6301\u7684\u613f\u666f\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6570\u5b57\u5de5\u4f5c\u8005\u57f9\u517b\u5fc3\u6d41\u7684\u5177\u4f53\u5b9e\u8df5\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65e5\u5e38\u5e72\u9884\u63aa\u65bd\u7684\u6709\u6548\u6027\uff0c\u533a\u5206\u4e86\u666e\u904d\u8ba4\u53ef\u548c\u5b58\u5728\u5206\u6b67\u7684\u5b9e\u8df5\uff0c\u5e76\u6536\u96c6\u4e86\u5173\u4e8e\u6280\u672f\u652f\u6301\u7684\u672a\u6765\u613f\u666f\u3002", "conclusion": "\u901a\u8fc7\u5173\u6ce8\u5de5\u4f5c\u8005\u5df2\u6709\u7684\u65e5\u5e38\u5b9e\u8df5\uff08\"\u751f\u6d3b\u5316\u5e72\u9884\"\uff09\uff0c\u7814\u7a76\u4e3a\u6570\u5b57\u5fc3\u6d41\u5e72\u9884\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u8bc1\u89c1\u89e3\u548c\u8bbe\u8ba1\u673a\u4f1a\uff0c\u5c06\u73b0\u6709\u6587\u732e\u4e0e\u5177\u4f53\u5b9e\u8df5\u76f8\u7ed3\u5408\uff0c\u4e3a\u672a\u6765\u6570\u5b57\u5e72\u9884\u8bbe\u8ba1\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.01700", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01700", "abs": "https://arxiv.org/abs/2602.01700", "authors": ["Ruoyu Wang", "Xuchen Liu", "Zongzhou Wu", "Zixuan Guo", "Wendi Ding", "Ben M. Chen"], "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels", "comment": "8 pages, 10 figures", "summary": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.", "AI": {"tldr": "Tilt-Ropter\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u7a7a\u4e2d-\u5730\u9762\u8f66\u8f86\uff0c\u901a\u8fc7\u503e\u659c\u65cb\u7ffc\u4e0e\u88ab\u52a8\u8f6e\u7ed3\u5408\u5b9e\u73b0\u8282\u80fd\u591a\u6a21\u5f0f\u8fd0\u52a8\uff0c\u76f8\u6bd4\u73b0\u6709\u6b20\u9a71\u52a8\u8bbe\u8ba1\u5177\u6709\u5b8c\u5168\u9a71\u52a8\u80fd\u529b\uff0c\u91c7\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5b9e\u73b0\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u5730\u9762\u8fd0\u52a8\u529f\u8017\u964d\u4f4e92.8%\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u7a7a\u4e2d-\u5730\u9762\u8f66\u8f86\u591a\u4e3a\u6b20\u9a71\u52a8\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5176\u8fd0\u52a8\u80fd\u529b\u548c\u73af\u5883\u9002\u5e94\u6027\u3002\u9700\u8981\u4e00\u79cd\u5b8c\u5168\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5b9e\u73b0\u89e3\u8026\u7684\u529b\u548c\u626d\u77e9\u63a7\u5236\uff0c\u63d0\u9ad8\u79fb\u52a8\u6027\u548c\u80fd\u91cf\u6548\u7387\uff0c\u4ee5\u9002\u5e94\u5927\u89c4\u6a21\u3001\u80fd\u91cf\u53d7\u9650\u73af\u5883\u4e2d\u7684\u957f\u65f6\u95f4\u4efb\u52a1\u3002", "method": "1) \u8bbe\u8ba1\u5b8c\u5168\u9a71\u52a8\u7684Tilt-Ropter\u7cfb\u7edf\uff0c\u7ed3\u5408\u503e\u659c\u65cb\u7ffc\u548c\u88ab\u52a8\u8f6e\uff1b2) \u5f00\u53d1\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u5904\u7406\u8f68\u8ff9\u8ddf\u8e2a\u548c\u63a5\u89e6\u7ea6\u675f\uff1b3) \u8bbe\u8ba1\u4e13\u7528\u63a7\u5236\u5206\u914d\u6a21\u5757\u5229\u7528\u9a71\u52a8\u5197\u4f59\u5b9e\u73b0\u8282\u80fd\u63a7\u5236\uff1b4) \u5f15\u5165\u5916\u90e8\u529b\u77e9\u4f30\u8ba1\u7b97\u6cd5\u5b9e\u65f6\u4f30\u8ba1\u73af\u5883\u4ea4\u4e92\u529b\uff0c\u589e\u5f3a\u5730\u9762\u63a5\u89e6\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86\u65e0\u7f1d\u7684\u7a7a\u5730\u8f6c\u6362\u548c\u8f68\u8ff9\u8ddf\u8e2a\u3002\u7ed3\u679c\u663e\u793a\u4e24\u79cd\u6a21\u5f0f\u4e0b\u8ddf\u8e2a\u8bef\u5dee\u90fd\u5f88\u4f4e\uff0c\u5730\u9762\u8fd0\u52a8\u529f\u8017\u964d\u4f4e\u4e8692.8%\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u3001\u80fd\u91cf\u53d7\u9650\u73af\u5883\u4e2d\u6267\u884c\u957f\u65f6\u95f4\u4efb\u52a1\u7684\u6f5c\u529b\u3002", "conclusion": "Tilt-Ropter\u4f5c\u4e3a\u4e00\u79cd\u5b8c\u5168\u9a71\u52a8\u7684\u6df7\u5408\u7a7a\u4e2d-\u5730\u9762\u8f66\u8f86\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u503e\u659c\u65cb\u7ffc\u8bbe\u8ba1\u3001\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548c\u8282\u80fd\u63a7\u5236\u5206\u914d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u79fb\u52a8\u6027\u3001\u73af\u5883\u9002\u5e94\u6027\u548c\u80fd\u91cf\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u3001\u80fd\u91cf\u53d7\u9650\u73af\u5883\u4e2d\u7684\u957f\u65f6\u95f4\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01986", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01986", "abs": "https://arxiv.org/abs/2602.01986", "authors": ["Shreyan Biswas", "Alexander Erlei", "Ujwal Gadiraju"], "title": "Belief Updating and Delegation in Multi-Task Human-AI Interaction: Evidence from Controlled Simulations", "comment": null, "summary": "Large language models (LLMs) increasingly support heterogeneous tasks within a single interface, requiring users to form, update, and act upon beliefs about one system across domains with different reliability profiles. Understanding how such beliefs transfer across tasks and shape delegation is therefore critical for the design of multipurpose AI systems. We report a preregistered experiment (N=240; 7,200 trials) in which participants interacted with a controlled AI simulation across grammar checking, travel planning, and visual question answering, each with fixed, domain-typical accuracy levels. Delegation was operationalized as a binary reliance decision: accepting the AI's output versus acting independently, and belief dynamics were evaluated against Bayesian benchmarks. We find three main results. First, participants do not reset beliefs between tasks: priors in a new task depend on posteriors from the previous task, with a 10-point increase predicting a 3-4 point higher subsequent prior. Second, within tasks, belief updating follows the Bayesian direction but is substantially conservative, proceeding at roughly half the normative Bayesian rate. Third, delegation is driven primarily by subjective beliefs about AI accuracy rather than self-confidence, though confidence independently reduces reliance when beliefs are held constant. Together, these findings show that users form global, path-dependent expectations about multipurpose AI systems, update them conservatively, and rely on AI primarily based on subjective beliefs rather than objective performance. We discuss implications for expectation calibration, reliance design, and the risks of belief spillovers in deployed LLM-based interfaces.", "AI": {"tldr": "\u7528\u6237\u5728\u591a\u4efb\u52a1AI\u7cfb\u7edf\u4e2d\u5f62\u6210\u5168\u5c40\u3001\u8def\u5f84\u4f9d\u8d56\u7684\u4fe1\u5ff5\uff0c\u4ee5\u4fdd\u5b88\u65b9\u5f0f\u66f4\u65b0\uff0c\u5e76\u57fa\u4e8e\u4e3b\u89c2\u4fe1\u5ff5\u800c\u975e\u5ba2\u89c2\u6027\u80fd\u8fdb\u884c\u59d4\u6258\u51b3\u7b56", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u4e00\u754c\u9762\u4e2d\u652f\u6301\u5f02\u6784\u4efb\u52a1\uff0c\u7528\u6237\u9700\u8981\u5728\u4e0d\u540c\u53ef\u9760\u6027\u9886\u57df\u4e2d\u5bf9\u540c\u4e00\u7cfb\u7edf\u5f62\u6210\u3001\u66f4\u65b0\u4fe1\u5ff5\u5e76\u636e\u6b64\u884c\u52a8\u3002\u7406\u89e3\u8fd9\u4e9b\u4fe1\u5ff5\u5982\u4f55\u8de8\u4efb\u52a1\u8f6c\u79fb\u5e76\u5f71\u54cd\u59d4\u6258\u51b3\u7b56\uff0c\u5bf9\u591a\u7528\u9014AI\u7cfb\u7edf\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u9884\u6ce8\u518c\u5b9e\u9a8c\u8bbe\u8ba1(N=240; 7,200\u6b21\u8bd5\u9a8c)\uff0c\u53c2\u4e0e\u8005\u4e0e\u53d7\u63a7AI\u6a21\u62df\u7cfb\u7edf\u5728\u8bed\u6cd5\u68c0\u67e5\u3001\u65c5\u884c\u89c4\u5212\u548c\u89c6\u89c9\u95ee\u7b54\u4e09\u4e2a\u4efb\u52a1\u4e2d\u4ea4\u4e92\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5177\u6709\u56fa\u5b9a\u7684\u9886\u57df\u5178\u578b\u51c6\u786e\u7387\u3002\u59d4\u6258\u64cd\u4f5c\u5316\u4e3a\u4e8c\u5143\u4f9d\u8d56\u51b3\u7b56\uff1a\u63a5\u53d7AI\u8f93\u51favs\u72ec\u7acb\u884c\u52a8\uff0c\u4fe1\u5ff5\u52a8\u6001\u8bc4\u4f30\u4ee5\u8d1d\u53f6\u65af\u57fa\u51c6\u4e3a\u53c2\u7167\u3002", "result": "1) \u53c2\u4e0e\u8005\u4e0d\u5728\u4efb\u52a1\u95f4\u91cd\u7f6e\u4fe1\u5ff5\uff1a\u65b0\u4efb\u52a1\u7684\u5148\u9a8c\u53d6\u51b3\u4e8e\u524d\u4e00\u4efb\u52a1\u7684\u540e\u9a8c\uff0c10\u70b9\u589e\u52a0\u9884\u6d4b\u540e\u7eed\u5148\u9a8c\u63d0\u9ad83-4\u70b9\uff1b2) \u4efb\u52a1\u5185\u4fe1\u5ff5\u66f4\u65b0\u9075\u5faa\u8d1d\u53f6\u65af\u65b9\u5411\u4f46\u663e\u8457\u4fdd\u5b88\uff0c\u66f4\u65b0\u901f\u7387\u7ea6\u4e3a\u89c4\u8303\u8d1d\u53f6\u65af\u901f\u7387\u7684\u4e00\u534a\uff1b3) \u59d4\u6258\u4e3b\u8981\u7531\u5bf9AI\u51c6\u786e\u7387\u7684\u4e3b\u89c2\u4fe1\u5ff5\u9a71\u52a8\u800c\u975e\u81ea\u4fe1\u5ea6\uff0c\u4f46\u5f53\u4fe1\u5ff5\u6052\u5b9a\u65f6\uff0c\u81ea\u4fe1\u5ea6\u72ec\u7acb\u51cf\u5c11\u4f9d\u8d56\u3002", "conclusion": "\u7528\u6237\u5bf9\u591a\u7528\u9014AI\u7cfb\u7edf\u5f62\u6210\u5168\u5c40\u3001\u8def\u5f84\u4f9d\u8d56\u7684\u671f\u671b\uff0c\u4ee5\u4fdd\u5b88\u65b9\u5f0f\u66f4\u65b0\uff0c\u5e76\u4e3b\u8981\u57fa\u4e8e\u4e3b\u89c2\u4fe1\u5ff5\u800c\u975e\u5ba2\u89c2\u6027\u80fd\u4f9d\u8d56AI\u3002\u8fd9\u5bf9\u671f\u671b\u6821\u51c6\u3001\u4f9d\u8d56\u8bbe\u8ba1\u4ee5\u53ca\u5df2\u90e8\u7f72LLM\u754c\u9762\u4e2d\u4fe1\u5ff5\u6ea2\u51fa\u7684\u98ce\u9669\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2602.01789", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01789", "abs": "https://arxiv.org/abs/2602.01789", "authors": ["Entong Su", "Tyler Westenbroek", "Anusha Nagabandi", "Abhishek Gupta"], "title": "RFS: Reinforcement learning with Residual flow steering for dexterous manipulation", "comment": null, "summary": "Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors. We propose Residual Flow Steering(RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy. We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning in both simulation and real-world settings when adapting pretrained base policies. Project website:https://weirdlabuw.github.io/rfs.", "code_url": "https://weirdlabuw.github.io/rfs", "AI": {"tldr": "RFS\u662f\u4e00\u79cd\u7528\u4e8e\u5fae\u8c03\u9884\u8bad\u7ec3\u751f\u6210\u7b56\u7565\u7684\u6570\u636e\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6b8b\u5dee\u52a8\u4f5c\u548c\u6f5c\u5728\u566a\u58f0\u5206\u5e03\uff0c\u5b9e\u73b0\u5c40\u90e8\u7cbe\u70bc\u548c\u5168\u5c40\u63a2\u7d22\u7684\u4e92\u8865", "motivation": "\u6a21\u4eff\u5b66\u4e60\u867d\u7136\u5728\u9ad8\u7ef4\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9884\u8bad\u7ec3\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u989d\u5916\u5fae\u8c03\u624d\u80fd\u83b7\u5f97\u9c81\u68d2\u6027\u80fd\u3002\u8fd9\u79cd\u9002\u5e94\u9700\u8981\u4fdd\u7559\u9884\u8bad\u7ec3\u7684\u5168\u5c40\u63a2\u7d22\u4f18\u52bf\uff0c\u540c\u65f6\u80fd\u591f\u5feb\u901f\u7ea0\u6b63\u5c40\u90e8\u6267\u884c\u9519\u8bef", "method": "\u63d0\u51fa\u6b8b\u5dee\u6d41\u5f15\u5bfc(RFS)\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6b8b\u5dee\u52a8\u4f5c\u548c\u6f5c\u5728\u566a\u58f0\u5206\u5e03\u6765\u5f15\u5bfc\u9884\u8bad\u7ec3\u7684\u6d41\u5339\u914d\u7b56\u7565\u3002\u6b8b\u5dee\u4fee\u6b63\u5b9e\u73b0\u5c40\u90e8\u7cbe\u70bc\uff0c\u6f5c\u5728\u7a7a\u95f4\u8c03\u5236\u5b9e\u73b0\u5168\u5c40\u63a2\u7d22\uff0c\u5728\u4fdd\u7559\u9884\u8bad\u7ec3\u7b56\u7565\u8868\u8fbe\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94", "result": "\u5728\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86RFS\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\uff0c\u5f53\u9002\u5e94\u9884\u8bad\u7ec3\u57fa\u7840\u7b56\u7565\u65f6\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u5fae\u8c03", "conclusion": "RFS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u751f\u6210\u7b56\u7565\uff0c\u5728\u4fdd\u7559\u9884\u8bad\u7ec3\u63a2\u7d22\u4f18\u52bf\u7684\u540c\u65f6\u5b9e\u73b0\u5c40\u90e8\u9519\u8bef\u7ea0\u6b63\uff0c\u9002\u7528\u4e8e\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u7684\u90e8\u7f72\u9002\u5e94"}}
{"id": "2602.01811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01811", "abs": "https://arxiv.org/abs/2602.01811", "authors": ["Wentao Zhang", "Aolan Sun", "Wentao Mo", "Xiaoyang Qu", "Yuxin Zheng", "Jianzong Wang"], "title": "From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models", "comment": "Accepted to 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)", "summary": "While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.", "AI": {"tldr": "\u63d0\u51faVLA-SCT\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u52a8\u4f5c\u7cbe\u70bc\u548c\u6761\u4ef6\u903b\u8f91\u7ec8\u6b62\u673a\u5236\uff0c\u89e3\u51b3VLA\u6a21\u578b\u5728\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u504f\u5dee\u548c\u4efb\u52a1\u5b8c\u6210\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5177\u8eab\u667a\u80fd\u4f53\u5e94\u7528\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a1) \u6293\u53d6\u4efb\u52a1\u4e2d\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u52a8\u4f5ctoken\u5b58\u5728\u7ec6\u5fae\u7a7a\u95f4\u504f\u5dee\u5bfc\u81f4\u6293\u53d6\u5931\u8d25\uff1b2) \u7f3a\u4e4f\u53ef\u9760\u7684\u4efb\u52a1\u5b8c\u6210\u8bc6\u522b\u80fd\u529b\uff0c\u5bfc\u81f4\u5197\u4f59\u52a8\u4f5c\u548c\u8d85\u65f6\u9519\u8bef\u3002\u9700\u8981\u589e\u5f3aVLA\u6a21\u578b\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8bad\u7ec3\u7684VLA-SCT\u6846\u67b6\uff0c\u4f5c\u4e3a\u81ea\u6821\u6b63\u63a7\u5236\u5faa\u73af\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7684\u52a8\u4f5c\u7cbe\u70bc\u548c\u6761\u4ef6\u903b\u8f91\u7ec8\u6b62\u673a\u5236\u3002\u901a\u8fc7\u52a8\u4f5c\u7cbe\u70bc\u7ea0\u6b63\u7a7a\u95f4\u504f\u5dee\uff0c\u901a\u8fc7\u6761\u4ef6\u903b\u8f91\u51c6\u786e\u5224\u65ad\u4efb\u52a1\u5b8c\u6210\u72b6\u6001\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u7684\u6240\u6709\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5747\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u786e\u4fdd\u51c6\u786e\u7684\u4efb\u52a1\u5b8c\u6210\uff0c\u4fc3\u8fdb\u4e86\u66f4\u53ef\u9760\u7684VLA\u667a\u80fd\u4f53\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "conclusion": "VLA-SCT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u504f\u5dee\u548c\u4efb\u52a1\u5b8c\u6210\u8bc6\u522b\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u6821\u6b63\u63a7\u5236\u5faa\u73af\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u9760VLA\u667a\u80fd\u4f53\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02063", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02063", "abs": "https://arxiv.org/abs/2602.02063", "authors": ["Ding Xia", "Xinyue Gui", "Mark Colley", "Fan Gao", "Zhongyi Zhou", "Dongyuan Li", "Renhe Jiang", "Takeo Igarashi"], "title": "See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers", "comment": "Under Review", "summary": "Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer's outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design.", "AI": {"tldr": "See2Refine\u662f\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u95ed\u73af\u6846\u67b6\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u611f\u77e5\u8bc4\u4f30\u4f5c\u4e3a\u81ea\u52a8\u89c6\u89c9\u53cd\u9988\uff0c\u6765\u6539\u8fdb\u57fa\u4e8eLLM\u7684eHMI\u52a8\u4f5c\u8bbe\u8ba1\u5668\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7f3a\u4e4f\u4e0e\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u81ea\u7136\u6c9f\u901a\u6e20\u9053\uff0c\u5916\u90e8\u4eba\u673a\u754c\u9762\uff08eHMI\uff09\u5bf9\u4e8e\u4f20\u8fbe\u610f\u56fe\u548c\u5728\u5171\u4eab\u73af\u5883\u4e2d\u4fdd\u6301\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5927\u591a\u6570eHMI\u7814\u7a76\u4f9d\u8d56\u4e8e\u5f00\u53d1\u8005\u624b\u5de5\u5236\u4f5c\u7684\u6d88\u606f-\u52a8\u4f5c\u5bf9\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u548c\u52a8\u6001\u7684\u4ea4\u901a\u73af\u5883\u3002\u867d\u7136\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u52a8\u4f5c\u8bbe\u8ba1\u5668\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u8fd9\u7c7b\u8bbe\u8ba1\u5668\u7f3a\u4e4f\u611f\u77e5\u9a8c\u8bc1\uff0c\u5e76\u4e14\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u63d0\u793a\u6216\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u53cd\u9988\u8fdb\u884c\u6539\u8fdb\u3002", "method": "\u63d0\u51faSee2Refine\u6846\u67b6\uff1a1\uff09\u7ed9\u5b9a\u9a7e\u9a76\u4e0a\u4e0b\u6587\u548c\u5019\u9009eHMI\u52a8\u4f5c\uff0c\u4f7f\u7528VLM\u8bc4\u4f30\u52a8\u4f5c\u7684\u611f\u77e5\u9002\u5f53\u6027\uff1b2\uff09\u5c06\u6b64\u53cd\u9988\u7528\u4e8e\u8fed\u4ee3\u4fee\u8ba2\u8bbe\u8ba1\u5668\u7684\u8f93\u51fa\uff1b3\uff09\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u7684\u7cfb\u7edf\u6027\u4f18\u5316\u3002\u6846\u67b6\u5728\u4e09\u79cdeHMI\u6a21\u6001\uff08\u706f\u6761\u3001\u773c\u775b\u548c\u624b\u81c2\uff09\u548c\u591a\u79cdLLM\u6a21\u578b\u89c4\u6a21\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\uff0c\u8be5\u6846\u67b6\u5728VLM\u6307\u6807\u548c\u4eba\u7c7b\u88ab\u8bd5\u8bc4\u4f30\u65b9\u9762\u5747\u4e00\u81f4\u4f18\u4e8e\u4ec5\u4f7f\u7528\u63d0\u793a\u7684LLM\u8bbe\u8ba1\u5668\u548c\u624b\u52a8\u6307\u5b9a\u7684\u57fa\u7ebf\u3002\u7ed3\u679c\u8868\u660e\u6539\u8fdb\u6548\u679c\u5728\u4e0d\u540c\u6a21\u6001\u95f4\u5177\u6709\u6cdb\u5316\u6027\uff0c\u4e14VLM\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u504f\u597d\u826f\u597d\u5bf9\u9f50\uff0c\u652f\u6301See2Refine\u5728\u53ef\u6269\u5c55\u52a8\u4f5c\u8bbe\u8ba1\u65b9\u9762\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "See2Refine\u901a\u8fc7VLM\u611f\u77e5\u8bc4\u4f30\u63d0\u4f9b\u81ea\u52a8\u89c6\u89c9\u53cd\u9988\uff0c\u5b9e\u73b0\u4e86LLM-based eHMI\u52a8\u4f5c\u8bbe\u8ba1\u5668\u7684\u7cfb\u7edf\u6027\u6539\u8fdb\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u3002\u8be5\u6846\u67b6\u5728\u591a\u79cd\u6a21\u6001\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u8868\u73b0\u51fa\u8272\uff0cVLM\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u7684eHMI\u52a8\u4f5c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01834", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01834", "abs": "https://arxiv.org/abs/2602.01834", "authors": ["Siqi Wen", "Shu Yang", "Shaopeng Fu", "Jingfeng Zhang", "Lijie Hu", "Di Wang"], "title": "Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models", "comment": null, "summary": "Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6982\u5ff5\u5b57\u5178\u5b66\u4e60\u7684\u63a8\u7406\u65f6\u5b89\u5168\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u9632\u5fa1VLA\u6a21\u578b\u7684\u7269\u7406\u5b89\u5168\u98ce\u9669", "motivation": "VLA\u6a21\u578b\u5c06\u591a\u6a21\u6001\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u884c\u4e3a\u7684\u80fd\u529b\u653e\u5927\u4e86\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5e72\u9884\u65f6\u673a\u4e0d\u5f53\u6216\u6a21\u6001\u9519\u8bef\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u878d\u5408\u8868\u793a\u4e2d\u7684\u6f0f\u6d1e", "method": "\u57fa\u4e8e\u6982\u5ff5\u5b57\u5178\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u9690\u85cf\u6fc0\u6d3b\u4e2d\u6784\u5efa\u7a00\u758f\u53ef\u89e3\u91ca\u5b57\u5178\uff0c\u8bc6\u522b\u6709\u5bb3\u6982\u5ff5\u65b9\u5411\uff0c\u5e94\u7528\u57fa\u4e8e\u9608\u503c\u7684\u5e72\u9884\u6765\u6291\u5236\u6216\u963b\u6b62\u4e0d\u5b89\u5168\u6fc0\u6d3b", "result": "\u5728Libero-Harm\u3001BadRobot\u3001RoboPair\u548cIS-Bench\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u6027\u80fd\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u8d85\u8fc770%\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u9762\u5411\u5177\u8eab\u7cfb\u7edf\u7684\u63a8\u7406\u65f6\u6982\u5ff5\u5b89\u5168\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u53ef\u5373\u63d2\u5373\u7528\uff0c\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u90e8\u7f72\u80fd\u529b"}}
{"id": "2602.02233", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.02233", "abs": "https://arxiv.org/abs/2602.02233", "authors": ["Jonas Hummel", "Maximilian Burzer", "Felix Schlotter", "Michael K\u00fcttner", "Tobias King", "Qiang Yang", "Cecilia Mascolo", "Michael Beigl", "Tobias R\u00f6ddiger"], "title": "CHOMP: Multimodal Chewing Side Detection with Earphones", "comment": null, "summary": "Chewing side preference (CSP) has been identified both as a risk factor for temporomandibular disorders (TMD) and behavioral manifestation. Despite TMDs affecting roughly one third of the global population, assessment mainly relies on clinical examinations and self-reports, offering limited insight into everyday jaw function. Continuous CSP monitoring could provide an objective proxy for functional asymmetries. Prior wearable approaches, however, mostly use specialized form factors and demonstrate limited performance. We therefore present CHOMP, the first system for chewing side detection using earphones. Employing OpenEarable 2.0, we collected data from 20 participants with microphones, a bone-conduction microphone, IMU, PPG, and a pressure sensor across eleven foods, five non-chewing activities, and three noise conditions. We apply the Continuous Wavelet Transform to each sensing modality and use the resulting multi-channel scalograms as inputs to CNN-based classifiers. Microphones achieve the strongest single-sensor unit performance, with median F1 scores of 94.5% in leave-one-food-out (LOFO) and 92.6% in leave-one-subject-out (LOSO) cross-validations. Fusing sensing modalities further improves performance to 97.7% for LOFO and 95.4% for LOSO, with additional evaluations under noise interference indicating robust performance. Our results establish earphones as a practical platform for continuous CSP monitoring, enabling clinicians and patients to assess jaw function in everyday life.", "AI": {"tldr": "CHOMP\u7cfb\u7edf\u9996\u6b21\u5229\u7528\u8033\u585e\u5f0f\u8bbe\u5907\u5b9e\u73b0\u5480\u56bc\u4fa7\u504f\u597d\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u548cCNN\u5206\u7c7b\u5668\uff0c\u5728\u5608\u6742\u73af\u5883\u4e0b\u4e5f\u80fd\u8fbe\u523097.7%\u7684\u51c6\u786e\u7387\uff0c\u4e3a\u65e5\u5e38\u5480\u56bc\u529f\u80fd\u76d1\u6d4b\u63d0\u4f9b\u5b9e\u7528\u5e73\u53f0\u3002", "motivation": "\u5480\u56bc\u4fa7\u504f\u597d\u65e2\u662f\u989e\u4e0b\u988c\u5173\u8282\u7d0a\u4e71\u7684\u98ce\u9669\u56e0\u7d20\u53c8\u662f\u884c\u4e3a\u8868\u73b0\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e34\u5e8a\u68c0\u67e5\u548c\u81ea\u6211\u62a5\u544a\uff0c\u7f3a\u4e4f\u5bf9\u65e5\u5e38\u4e0b\u988c\u529f\u80fd\u7684\u5ba2\u89c2\u76d1\u6d4b\u3002\u73b0\u6709\u53ef\u7a7f\u6234\u8bbe\u5907\u591a\u91c7\u7528\u7279\u6b8a\u5f62\u6001\u4e14\u6027\u80fd\u6709\u9650\u3002", "method": "\u4f7f\u7528OpenEarable 2.0\u8033\u585e\u8bbe\u5907\uff0c\u91c7\u96c620\u540d\u53c2\u4e0e\u8005\u7684\u9ea6\u514b\u98ce\u3001\u9aa8\u4f20\u5bfc\u9ea6\u514b\u98ce\u3001IMU\u3001PPG\u548c\u538b\u529b\u4f20\u611f\u5668\u6570\u636e\uff0c\u6db5\u76d611\u79cd\u98df\u7269\u30015\u79cd\u975e\u5480\u56bc\u6d3b\u52a8\u548c3\u79cd\u566a\u58f0\u6761\u4ef6\u3002\u5e94\u7528\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u751f\u6210\u591a\u901a\u9053\u5c3a\u5ea6\u56fe\uff0c\u4f5c\u4e3aCNN\u5206\u7c7b\u5668\u7684\u8f93\u5165\u3002", "result": "\u9ea6\u514b\u98ce\u5728\u5355\u4f20\u611f\u5668\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u7559\u4e00\u98df\u7269\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u4f4d\u6570F1\u5206\u657094.5%\uff0c\u7559\u4e00\u53d7\u8bd5\u8005\u4ea4\u53c9\u9a8c\u8bc192.6%\u3002\u591a\u4f20\u611f\u5668\u878d\u5408\u540e\u6027\u80fd\u63d0\u5347\u81f397.7%\uff08LOFO\uff09\u548c95.4%\uff08LOSO\uff09\uff0c\u566a\u58f0\u5e72\u6270\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u8033\u585e\u5f0f\u8bbe\u5907\u53ef\u4f5c\u4e3a\u8fde\u7eed\u5480\u56bc\u4fa7\u504f\u597d\u76d1\u6d4b\u7684\u5b9e\u7528\u5e73\u53f0\uff0c\u4f7f\u4e34\u5e8a\u533b\u751f\u548c\u60a3\u8005\u80fd\u591f\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u8bc4\u4f30\u4e0b\u988c\u529f\u80fd\uff0c\u4e3a\u989e\u4e0b\u988c\u5173\u8282\u7d0a\u4e71\u7684\u5ba2\u89c2\u76d1\u6d4b\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.01860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01860", "abs": "https://arxiv.org/abs/2602.01860", "authors": ["Filip Nov\u00e1k", "Mat\u011bj Petrl\u00edk", "Matej Novosad", "Parakh M. Gupta", "Robert P\u011bni\u010dka", "Martin Saska"], "title": "Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach", "comment": "Visit our webpage for more details: https://mrs.fel.cvut.cz/papers/vision-only-uav-state-estimation", "summary": "Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u901f\u65e0\u4eba\u673a\u5728GNSS\u62d2\u6b62\u73af\u5883\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u878d\u5408\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u3001\u5730\u6807\u76f8\u673a\u6d4b\u91cf\u548cIMU\u6570\u636e\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6f02\u79fb\u6a21\u578b\u6821\u6b63VIO\u6f02\u79fb\uff0c\u5b9e\u73b0\u7cbe\u786e\u72b6\u6001\u4f30\u8ba1\u3002", "motivation": "\u5728GNSS\u62d2\u6b62\u7684\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u901f\u673a\u52a8\u98de\u884c\u9700\u8981\u5feb\u901f\u3001\u53ef\u9760\u3001\u51c6\u786e\u7684\u65e0\u4eba\u673a\u72b6\u6001\u4f30\u8ba1\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u66f4\u590d\u6742\u7684\u786c\u4ef6\uff08\u5982\u7acb\u4f53\u76f8\u673a\u6216\u6d4b\u8ddd\u4eea\uff09\uff0c\u5e76\u4f7f\u7528\u672a\u6821\u6b63\u7684\u6f02\u79fbVIO\u901f\u5ea6\u3001\u59ff\u6001\u548c\u89d2\u901f\u7387\uff0c\u5bfc\u81f4\u5feb\u901f\u673a\u52a8\u65f6\u4ea7\u751f\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u673a\u8f7d\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u878d\u5408\u5355\u76eeRGB\u76f8\u673a\u548cIMU\u6570\u636e\u3002\u65b9\u6cd5\u7ed3\u5408\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1(VIO)\u3001\u57fa\u4e8e\u5730\u6807\u7684\u673a\u8f7d\u76f8\u673a\u6d4b\u91cf\u7cfb\u7edf\u548cIMU\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6570\u5b66\u6f02\u79fb\u6a21\u578b\u4f30\u8ba1\u5e76\u8865\u507fVIO\u6f02\u79fb\uff0c\u6821\u6b63\u6240\u6709VIO\u72b6\u6001\uff08\u4f4d\u7f6e\u3001\u59ff\u6001\u3001\u7ebf\u901f\u5ea6\u548c\u89d2\u901f\u5ea6\uff09\u3002", "result": "\u901a\u8fc71600\u6b21\u4eff\u771f\u548c\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5728A2RL\u65e0\u4eba\u673a\u7ade\u901f\u6311\u6218\u8d5b2025\u4e2d\uff0c\u56e2\u961f\u4ece210\u652f\u961f\u4f0d\u4e2d\u664b\u7ea7\u524d\u56db\u5e76\u83b7\u5f97\u5956\u724c\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u5728\u9ad8\u901f\u52a8\u6001\u8fd0\u52a8\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728GNSS\u62d2\u6b62\u73af\u5883\u4e2d\u4e3a\u9ad8\u901f\u65e0\u4eba\u673a\u63d0\u4f9b\u51c6\u786e\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u5373\u4f7f\u5728\u5feb\u901f\u52a8\u6001\u8fd0\u52a8\u4e2d\u4e5f\u80fd\u4fdd\u6301\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u4f9d\u8d56\u590d\u6742\u786c\u4ef6\u548c\u4f7f\u7528\u672a\u6821\u6b63VIO\u6f02\u79fb\u7684\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2602.02298", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.02298", "abs": "https://arxiv.org/abs/2602.02298", "authors": ["Romy M\u00fcller", "Wiebke Klausing"], "title": "When more precision is worse: Do people recognize inadequate scene representations in concept-based explainable AI?", "comment": null, "summary": "Explainable artificial intelligence (XAI) aims to help uncover flaws in an AI model's internal representations. But do people draw the right conclusions from its explanations? Specifically, do they recognize an AI's inability to distinguish between relevant and irrelevant features? In the present study, a simulated AI classified images of railway trespassers as dangerous or not. To explain which features it has used, other images from the dataset were shown that activate the AI in a similar way. These concept images varied in three relevant features (i.e., a person's distance to the tracks, direction, and action) and in an irrelevant feature (i.e., scene background). When the AI uses a feature in its decision, this feature is retained in the concept images, otherwise the images randomize over it (e.g., same distance, varied backgrounds). Participants rated the AI more favorably when it retained relevant features. For the irrelevant feature, they did not mind in general, and sometimes even preferred it to be retained. This suggests that people may not recognize it when an AI model relies on irrelevant features to make its decisions.", "AI": {"tldr": "\u53c2\u4e0e\u8005\u672a\u80fd\u8bc6\u522bAI\u6a21\u578b\u4f9d\u8d56\u65e0\u5173\u7279\u5f81\u8fdb\u884c\u51b3\u7b56\u7684\u95ee\u9898\uff0c\u5373\u4f7f\u89e3\u91ca\u7cfb\u7edf\u660e\u786e\u5c55\u793a\u4e86AI\u5bf9\u65e0\u5173\u7279\u5f81\uff08\u573a\u666f\u80cc\u666f\uff09\u7684\u4f9d\u8d56\uff0c\u53c2\u4e0e\u8005\u4ecd\u503e\u5411\u4e8e\u8ba4\u4e3aAI\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u7814\u7a76\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4eba\u4eec\u662f\u5426\u80fd\u4eceXAI\u7684\u89e3\u91ca\u4e2d\u6b63\u786e\u8bc6\u522bAI\u6a21\u578b\u662f\u5426\u4f9d\u8d56\u65e0\u5173\u7279\u5f81\u8fdb\u884c\u51b3\u7b56\u3002\u5f53\u524dXAI\u65e8\u5728\u63ed\u793aAI\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u7f3a\u9677\uff0c\u4f46\u4eba\u4eec\u662f\u5426\u80fd\u6b63\u786e\u7406\u89e3\u8fd9\u4e9b\u89e3\u91ca\u5e76\u8bc6\u522bAI\u5bf9\u65e0\u5173\u7279\u5f81\u7684\u4f9d\u8d56\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u4f7f\u7528\u6a21\u62dfAI\u5bf9\u94c1\u8def\u4fb5\u5165\u8005\u56fe\u50cf\u8fdb\u884c\u5371\u9669\u5206\u7c7b\u3002\u901a\u8fc7\u5c55\u793a\u6fc0\u6d3bAI\u76f8\u4f3c\u65b9\u5f0f\u7684\u6570\u636e\u96c6\u56fe\u50cf\u6765\u89e3\u91caAI\u4f7f\u7528\u7684\u7279\u5f81\u3002\u6982\u5ff5\u56fe\u50cf\u5728\u4e09\u4e2a\u76f8\u5173\u7279\u5f81\uff08\u4eba\u4e0e\u8f68\u9053\u7684\u8ddd\u79bb\u3001\u65b9\u5411\u3001\u52a8\u4f5c\uff09\u548c\u4e00\u4e2a\u65e0\u5173\u7279\u5f81\uff08\u573a\u666f\u80cc\u666f\uff09\u4e0a\u53d8\u5316\u3002\u5f53AI\u4f7f\u7528\u67d0\u4e2a\u7279\u5f81\u65f6\uff0c\u8be5\u7279\u5f81\u5728\u6982\u5ff5\u56fe\u50cf\u4e2d\u4fdd\u7559\uff1b\u5426\u5219\u968f\u673a\u5316\u8be5\u7279\u5f81\u3002\u53c2\u4e0e\u8005\u8bc4\u4f30AI\u8868\u73b0\u3002", "result": "\u5f53AI\u4fdd\u7559\u76f8\u5173\u7279\u5f81\u65f6\uff0c\u53c2\u4e0e\u8005\u5bf9AI\u8bc4\u4ef7\u66f4\u9ad8\u3002\u5bf9\u4e8e\u65e0\u5173\u7279\u5f81\uff0c\u53c2\u4e0e\u8005\u901a\u5e38\u4e0d\u5728\u610f\uff0c\u6709\u65f6\u751a\u81f3\u504f\u597d\u4fdd\u7559\u8be5\u7279\u5f81\u3002\u8fd9\u8868\u660e\u4eba\u4eec\u53ef\u80fd\u65e0\u6cd5\u8bc6\u522bAI\u6a21\u578b\u4f9d\u8d56\u65e0\u5173\u7279\u5f81\u8fdb\u884c\u51b3\u7b56\u7684\u60c5\u51b5\u3002", "conclusion": "\u5f53\u524dXAI\u89e3\u91ca\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u5e2e\u52a9\u7528\u6237\u8bc6\u522bAI\u6a21\u578b\u5bf9\u65e0\u5173\u7279\u5f81\u7684\u4f9d\u8d56\uff0c\u9700\u8981\u6539\u8fdb\u89e3\u91ca\u8bbe\u8ba1\u4ee5\u66f4\u597d\u5730\u63ed\u793aAI\u51b3\u7b56\u4e2d\u7684\u7f3a\u9677\u3002"}}
{"id": "2602.02375", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.02375", "abs": "https://arxiv.org/abs/2602.02375", "authors": ["Julian Berger", "Pantelis P. Analytis", "Frederik Andersen", "Kristian P. Lorenzen", "Ville Satop\u00e4\u00e4", "Ralf HJM Kurvers"], "title": "The hybrid confirmation tree: A robust strategy for hybrid intelligence", "comment": null, "summary": "Combining human and artificial intelligence (AI) is a potentially powerful approach to boost decision accuracy. However, few such approaches exist that effectively integrate both types of intelligence while maintaining human agency. Here, we introduce and evaluate the hybrid confirmation tree, a simple aggregation strategy that compares the independent decisions of both a human and AI, with disagreements triggering a second human tiebreaker. Through analytical derivations, we show that the hybrid confirmation tree can match and exceed the accuracy of a three-person human majority vote while requiring fewer human inputs, particularly when AI accuracy is comparable to or exceeds human accuracy. We analytically demonstrate that the hybrid confirmation tree's ability to achieve complementarity -- outperforming individual humans, AI, and the majority vote -- is maximized when human and AI accuracies are similar and their decisions are not overly correlated. Empirical reanalysis of six real-world datasets (covering skin cancer diagnosis, deepfake detection, geopolitical forecasting, and criminal rearrest) validates these findings, showing that the hybrid confirmation tree improves accuracy over the majority vote by up to 10 percentage points while reducing the cost of decision making by 28--44$\\%$. Furthermore, the hybrid confirmation tree provides greater flexibility in navigating true and false positive trade-offs compared to fixed human-only heuristics like hierarchies and polyarchies. The hybrid confirmation tree emerges as a practical, efficient, and robust strategy for hybrid collective intelligence that maintains human agency.", "AI": {"tldr": "\u6df7\u5408\u786e\u8ba4\u6811\u662f\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u4e0eAI\u51b3\u7b56\u7684\u805a\u5408\u7b56\u7565\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e24\u8005\u7684\u72ec\u7acb\u51b3\u7b56\u5e76\u5728\u5206\u6b67\u65f6\u5f15\u5165\u7b2c\u4e8c\u4e2a\u4eba\u7c7b\u4ef2\u88c1\u8005\uff0c\u80fd\u5728\u51cf\u5c11\u4eba\u7c7b\u8f93\u5165\u7684\u540c\u65f6\u8fbe\u5230\u6216\u8d85\u8fc7\u4e09\u4eba\u591a\u6570\u6295\u7968\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u6574\u5408\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u5e76\u4fdd\u6301\u4eba\u7c7b\u81ea\u4e3b\u6027\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u5229\u7528AI\u4f18\u52bf\u53c8\u80fd\u7ef4\u6301\u4eba\u7c7b\u51b3\u7b56\u6743\u7684\u6df7\u5408\u667a\u80fd\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u786e\u8ba4\u6811\u7b56\u7565\uff1a\u6bd4\u8f83\u4eba\u7c7b\u4e0eAI\u7684\u72ec\u7acb\u51b3\u7b56\uff0c\u5f53\u4e24\u8005\u4e00\u81f4\u65f6\u76f4\u63a5\u91c7\u7eb3\uff0c\u5f53\u51fa\u73b0\u5206\u6b67\u65f6\u5f15\u5165\u7b2c\u4e8c\u4e2a\u4eba\u7c7b\u4f5c\u4e3a\u4ef2\u88c1\u8005\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u52bf\uff0c\u5e76\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u6df7\u5408\u786e\u8ba4\u6811\u80fd\u5728AI\u51c6\u786e\u7387\u63a5\u8fd1\u6216\u8d85\u8fc7\u4eba\u7c7b\u65f6\uff0c\u4ee5\u66f4\u5c11\u7684\u4eba\u7c7b\u8f93\u5165\u8fbe\u5230\u6216\u8d85\u8fc7\u4e09\u4eba\u591a\u6570\u6295\u7968\u7684\u51c6\u786e\u7387\u3002\u5b9e\u8bc1\u5206\u6790\u663e\u793a\u8be5\u7b56\u7565\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e2d\u80fd\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u6700\u591a10\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u964d\u4f4e\u51b3\u7b56\u6210\u672c28-44%\u3002", "conclusion": "\u6df7\u5408\u786e\u8ba4\u6811\u662f\u4e00\u79cd\u5b9e\u7528\u3001\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u6df7\u5408\u96c6\u4f53\u667a\u80fd\u7b56\u7565\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4eba\u7c7b\u81ea\u4e3b\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4eba\u7c7b\u4e0eAI\u7684\u4e92\u8865\u6027\uff0c\u5728\u4e24\u8005\u51c6\u786e\u7387\u76f8\u4f3c\u4e14\u51b3\u7b56\u76f8\u5173\u6027\u4e0d\u9ad8\u65f6\u6548\u679c\u6700\u4f73\u3002"}}
{"id": "2602.01880", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01880", "abs": "https://arxiv.org/abs/2602.01880", "authors": ["Giulio Antonio Abbo", "Senne Lenaerts", "Tony Belpaeme"], "title": "Multimodal Large Language Models for Real-Time Situated Reasoning", "comment": "Submitted to the interactivity track of the 21st ACM/IEEE International Conference on Human-Robot Interaction on December 2025, accepted January 2026", "summary": "In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.", "AI": {"tldr": "\u5c06GPT-4o\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0eTurtleBot 4\u673a\u5668\u4eba\u5e73\u53f0\u7ed3\u5408\uff0c\u6a21\u62df\u667a\u80fd\u5438\u5c18\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u65f6\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u4ef7\u503c\u611f\u77e5\u7684\u51b3\u7b56", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u652f\u6301\u5b9e\u65f6\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u4ef7\u503c\u611f\u77e5\u7684\u51b3\u7b56\u5236\u5b9a\uff0c\u7279\u522b\u662f\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u673a\u5668\u4eba\u9700\u8981\u7406\u89e3\u793e\u4f1a\u89c4\u8303\u3001\u7528\u6237\u504f\u597d\u548c\u4ef7\u503c\u89c2\u7684\u573a\u666f", "method": "\u5c06GPT-4o\u8bed\u8a00\u6a21\u578b\u4e0eTurtleBot 4\u5e73\u53f0\u96c6\u6210\uff0c\u6a21\u62df\u667a\u80fd\u5438\u5c18\u673a\u5668\u4eba\uff1b\u6a21\u578b\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u8bc4\u4f30\u73af\u5883\uff0c\u5224\u65ad\u662f\u5426\u9002\u5408\u542f\u52a8\u6e05\u6d01\u4efb\u52a1\uff1b\u7cfb\u7edf\u80fd\u591f\u63a8\u7406\u5bb6\u5ead\u6d3b\u52a8\u3001\u793e\u4f1a\u89c4\u8303\u548c\u7528\u6237\u504f\u597d", "result": "\u5728\u771f\u5b9e\u5bb6\u5ead\u73af\u5883\u4e2d\u6f14\u793a\u4e86\u7cfb\u7edf\u4ece\u6709\u9650\u89c6\u89c9\u8f93\u5165\u63a8\u65ad\u4e0a\u4e0b\u6587\u548c\u4ef7\u503c\u89c2\u7684\u80fd\u529b\uff1b\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u589e\u5f3a\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u548c\u60c5\u5883\u611f\u77e5\u65b9\u9762\u7684\u6f5c\u529b", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u5347\u673a\u5668\u4eba\u81ea\u4e3b\u51b3\u7b56\u65b9\u9762\u5177\u6709\u524d\u666f\uff0c\u4f46\u4e5f\u9762\u4e34\u4e00\u81f4\u6027\u3001\u504f\u89c1\u548c\u5b9e\u65f6\u6027\u80fd\u7b49\u6311\u6218"}}
{"id": "2602.01916", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01916", "abs": "https://arxiv.org/abs/2602.01916", "authors": ["Keyu Chen", "Wenchao Sun", "Hao Cheng", "Zheng Fu", "Sifa Zheng"], "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning", "comment": "Accepted by ICRA 2026", "summary": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/", "code_url": "https://currychen77.github.io/ForSim", "AI": {"tldr": "ForSim\u662f\u4e00\u79cd\u9010\u6b65\u95ed\u73af\u524d\u5411\u4eff\u771f\u8303\u5f0f\uff0c\u901a\u8fc7\u65f6\u7a7a\u5339\u914d\u53c2\u8003\u8f68\u8ff9\u548c\u7269\u7406\u8fd0\u52a8\u52a8\u529b\u5b66\u6765\u4fdd\u6301\u591a\u6a21\u6001\u884c\u4e3a\u591a\u6837\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u6a21\u6001\u5185\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u4ea4\u901a\u4eff\u771f\u4e2d\u7684\u534f\u53d8\u91cf\u504f\u79fb\u548c\u4ea4\u4e92\u4e0d\u771f\u5b9e\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4ea4\u901a\u4eff\u771f\u9762\u4e34\u4e24\u4e2a\u57fa\u672c\u6311\u6218\uff1a\u5f00\u73af\u6a21\u4eff\u5b66\u4e60\u5f15\u5165\u7684\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u4ee5\u53ca\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u591a\u6a21\u6001\u884c\u4e3a\u7684\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709\u6846\u67b6\u5982RIFT\u867d\u7136\u901a\u8fc7\u7ec4\u76f8\u5bf9\u4f18\u5316\u90e8\u5206\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5176\u524d\u5411\u4eff\u771f\u8fc7\u7a0b\u4ecd\u7136\u57fa\u672c\u662f\u975e\u53cd\u5e94\u5f0f\u7684\uff0c\u5bfc\u81f4\u865a\u62df\u57df\u4e2d\u667a\u80fd\u4f53\u4ea4\u4e92\u4e0d\u771f\u5b9e\uff0c\u6700\u7ec8\u9650\u5236\u4e86\u4eff\u771f\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faForSim\u9010\u6b65\u95ed\u73af\u524d\u5411\u4eff\u771f\u8303\u5f0f\uff1a\u5728\u6bcf\u4e2a\u865a\u62df\u65f6\u95f4\u6b65\uff0c\u4ea4\u901a\u667a\u80fd\u4f53\u901a\u8fc7\u7269\u7406\u57fa\u7840\u7684\u8fd0\u52a8\u52a8\u529b\u5b66\u4f20\u64ad\u65f6\u7a7a\u5339\u914d\u53c2\u8003\u8f68\u8ff9\u7684\u6700\u4f73\u865a\u62df\u5019\u9009\u8f68\u8ff9\uff0c\u4ece\u800c\u4fdd\u6301\u591a\u6a21\u6001\u884c\u4e3a\u591a\u6837\u6027\u540c\u65f6\u786e\u4fdd\u6a21\u6001\u5185\u4e00\u81f4\u6027\u3002\u5176\u4ed6\u667a\u80fd\u4f53\u901a\u8fc7\u9010\u6b65\u9884\u6d4b\u66f4\u65b0\uff0c\u4ea7\u751f\u8fde\u8d2f\u4e14\u4ea4\u4e92\u611f\u77e5\u7684\u6f14\u5316\u3002\u5f53\u6574\u5408\u5230RIFT\u4ea4\u901a\u4eff\u771f\u6846\u67b6\u65f6\uff0cForSim\u4e0e\u7ec4\u76f8\u5bf9\u4f18\u5316\u534f\u540c\u5de5\u4f5c\u4ee5\u5fae\u8c03\u4ea4\u901a\u7b56\u7565\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u8fd9\u79cd\u6574\u5408\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6548\u7387\u3001\u771f\u5b9e\u6027\u548c\u8212\u9002\u6027\u3002\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u524d\u5411\u4eff\u771f\u4e2d\u5efa\u6a21\u95ed\u73af\u591a\u6a21\u6001\u4ea4\u4e92\u7684\u91cd\u8981\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u4eff\u771f\u7684\u4fdd\u771f\u5ea6\u548c\u53ef\u9760\u6027\u3002", "conclusion": "ForSim\u901a\u8fc7\u9010\u6b65\u95ed\u73af\u524d\u5411\u4eff\u771f\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u901a\u4eff\u771f\u4e2d\u7684\u534f\u53d8\u91cf\u504f\u79fb\u548c\u4ea4\u4e92\u4e0d\u771f\u5b9e\u95ee\u9898\uff0c\u4e0e\u7ec4\u76f8\u5bf9\u4f18\u5316\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u7684\u5b89\u5168\u6027\u3001\u4fdd\u771f\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u95ed\u73af\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u4eff\u771f\u73af\u5883\u3002"}}
{"id": "2602.01930", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01930", "abs": "https://arxiv.org/abs/2602.01930", "authors": ["Felix Igelbrink", "Lennart Niecksch", "Marian Renz", "Martin G\u00fcnther", "Martin Atzmueller"], "title": "LIEREx: Language-Image Embeddings for Robotic Exploration", "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in KI - K\u00fcnstliche Intelligenz, and is available online at https://doi.org/10.1007/s13218-026-00902-6", "summary": "Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.", "AI": {"tldr": "LIEREx\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u4e0e3D\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u90e8\u5206\u672a\u77e5\u73af\u5883\u4e2d\u7684\u76ee\u6807\u5bfc\u5411\u63a2\u7d22", "motivation": "\u4f20\u7edf\u8bed\u4e49\u5730\u56fe\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u7b26\u53f7\u8bcd\u6c47\uff0c\u65e0\u6cd5\u5904\u7406\u8bbe\u8ba1\u65f6\u672a\u5b9a\u4e49\u7684\u5206\u5e03\u5916\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027", "method": "\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u4e0e3D\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u5c06\u5bf9\u8c61\u7f16\u7801\u4e3a\u9ad8\u7ef4\u5d4c\u5165\u800c\u975e\u56fa\u5b9a\u6807\u7b7e\uff0c\u5b9e\u73b0\u5f00\u653e\u96c6\u6620\u5c04", "result": "\u5f00\u53d1\u4e86LIEREx\u7cfb\u7edf\uff0c\u4f7f\u81ea\u4e3b\u673a\u5668\u4eba\u80fd\u591f\u5728\u90e8\u5206\u672a\u77e5\u73af\u5883\u4e2d\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u63a2\u7d22\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8bcd\u6c47\u9650\u5236", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u4e0e\u8bed\u4e49\u573a\u666f\u56fe\u7684\u7ed3\u5408\u4e3a\u673a\u5668\u4eba\u8bed\u4e49\u5730\u56fe\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5f00\u653e\u73af\u5883\u4e2d\u7684\u667a\u80fd\u63a2\u7d22"}}
{"id": "2602.01939", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01939", "abs": "https://arxiv.org/abs/2602.01939", "authors": ["Yuxin He", "Ruihao Zhang", "Tianao Shen", "Cheng Liu", "Qiang Nie"], "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy", "comment": "ICRA 2026", "summary": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.", "AI": {"tldr": "\u63d0\u51fa\u63a2\u7d22\u6027\u4e0e\u805a\u7126\u6027\u64cd\u4f5c\uff08EFM\uff09\u95ee\u9898\uff0c\u5efa\u7acbEFM-10\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f00\u53d1\u53cc\u624b\u4e3b\u52a8\u611f\u77e5\uff08BAP\uff09\u7b56\u7565\uff0c\u5e76\u901a\u8fc7BAPData\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u906e\u6321\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u6e90\u4e8e\u5934\u90e8\u4e3b\u6444\u50cf\u5934\u89c6\u89d2\u53d7\u9650\uff0c\u5bfc\u81f4\u4efb\u52a1\u5b8c\u6210\u6240\u9700\u4fe1\u606f\u7f3a\u5931\u3002\u4f5c\u8005\u5c06\u95ee\u9898\u672c\u8d28\u5f52\u7ed3\u4e3a\u4fe1\u606f\u7f3a\u5931\uff0c\u8fdb\u800c\u63d0\u51fa\u66f4\u57fa\u7840\u7684\u63a2\u7d22\u6027\u4e0e\u805a\u7126\u6027\u64cd\u4f5c\u95ee\u9898\u3002", "method": "1. \u5efa\u7acbEFM-10\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b4\u7c7b\u517110\u4e2a\u4efb\u52a1\uff1b2. \u63d0\u51fa\u53cc\u624b\u4e3b\u52a8\u611f\u77e5\uff08BAP\uff09\u7b56\u7565\uff1a\u4e00\u53ea\u624b\u81c2\u63d0\u4f9b\u4e3b\u52a8\u89c6\u89c9\u611f\u77e5\uff0c\u53e6\u4e00\u53ea\u624b\u81c2\u5728\u64cd\u4f5c\u65f6\u63d0\u4f9b\u529b\u89c9\u611f\u77e5\uff1b3. \u6536\u96c6BAPData\u6570\u636e\u96c6\uff1b4. \u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u65b9\u5f0f\u9a8c\u8bc1BAP\u7b56\u7565\u6709\u6548\u6027\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86BAP\u7b56\u7565\u5728EFM-10\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002EFM-10\u57fa\u51c6\u6d4b\u8bd5\u548cBAP\u7b56\u7565\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\u3002", "conclusion": "EFM-10\u57fa\u51c6\u6d4b\u8bd5\u548cBAP\u7b56\u7565\u4e3a\u89e3\u51b3\u63a2\u7d22\u6027\u4e0e\u805a\u7126\u6027\u64cd\u4f5c\u95ee\u9898\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u65b9\u5411\u672a\u6765\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2602.02026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02026", "abs": "https://arxiv.org/abs/2602.02026", "authors": ["Zhenwei Niu", "Xiaoyi Chen", "Jiayu Hu", "Zhaoyang Liu", "Xiaozu Ju"], "title": "Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp", "comment": null, "summary": "We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u8f7b\u67d4\u6293\u53d6\u6846\u67b6\uff0c\u5c06\u5b9e\u65f6\u6469\u64e6\u7cfb\u6570\u4f30\u8ba1\u4e0e\u81ea\u9002\u5e94\u6293\u53d6\u63a7\u5236\u534f\u540c\u8026\u5408\uff0c\u901a\u8fc7\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u5b9e\u73b0\u5b9e\u65f6\u6469\u64e6\u4f30\u8ba1\uff0c\u5e76\u96c6\u6210\u5230\u53cd\u5e94\u5f0f\u63a7\u5236\u5668\u4e2d\u52a8\u6001\u8c03\u8282\u6293\u53d6\u529b", "motivation": "\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5b9e\u73b0\u8f7b\u67d4\u4e14\u7a33\u5b9a\u6293\u53d6\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u65f6\u6469\u64e6\u4f30\u8ba1\u4e0e\u81ea\u9002\u5e94\u63a7\u5236\u7684\u534f\u540c\u8026\u5408\uff0c\u65e0\u6cd5\u5b9e\u73b0\u9ad8\u5ea6\u54cd\u5e94\u548c\u9c81\u68d2\u7684\u611f\u77e5\u8fd0\u52a8\u5faa\u73af", "method": "\u63d0\u51fa\u57fa\u4e8e\u7c92\u5b50\u6ee4\u6ce2\u7684\u5b9e\u65f6\u6469\u64e6\u7cfb\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff1b\u5c06\u4f30\u8ba1\u7ed3\u679c\u65e0\u7f1d\u96c6\u6210\u5230\u53cd\u5e94\u5f0f\u63a7\u5236\u5668\u4e2d\uff0c\u52a8\u6001\u8c03\u8282\u6293\u53d6\u529b\uff1b\u4e24\u4e2a\u8fc7\u7a0b\u5728\u95ed\u73af\u4e2d\u540c\u6b65\u8fd0\u884c\uff0c\u5f62\u6210\u611f\u77e5\u8fd0\u52a8\u5faa\u73af", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b8c\u6574\u6846\u67b6\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u54cd\u5e94\u548c\u9c81\u68d2\u7684\u4f20\u611f\u5668\u8fd0\u52a8\u5faa\u73af", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6469\u64e6\u4f30\u8ba1\u4e0e\u81ea\u9002\u5e94\u6293\u53d6\u63a7\u5236\u7684\u534f\u540c\u8026\u5408\uff0c\u4e3a\u673a\u5668\u4eba\u8f7b\u67d4\u6293\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02038", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02038", "abs": "https://arxiv.org/abs/2602.02038", "authors": ["Etienne M\u00e9nager", "Justin Carpentier"], "title": "Frictional Contact Solving for Material Point Method", "comment": null, "summary": "Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9690\u5f0f\u7269\u8d28\u70b9\u6cd5\uff08MPM\uff09\u7684\u7cbe\u786e\u9c81\u68d2\u6469\u64e6\u63a5\u89e6\u5904\u7406\u6d41\u7a0b\uff0c\u901a\u8fc7\u7c92\u5b50\u4e2d\u5fc3\u51e0\u4f55\u57fa\u5143\u8fdb\u884c\u63a5\u89e6\u70b9\u68c0\u6d4b\uff0c\u5c06\u6469\u64e6\u63a5\u89e6\u5efa\u6a21\u4e3a\u975e\u7ebf\u6027\u4e92\u8865\u95ee\u9898\u5e76\u7528ADMM\u6c42\u89e3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u63a5\u89e6\u5904\u7406\u3002", "motivation": "\u5728\u7269\u8d28\u70b9\u6cd5\u4e2d\uff0c\u51c6\u786e\u5904\u7406\u5e26\u6469\u64e6\u7684\u63a5\u89e6\u4e00\u76f4\u662f\u4e00\u4e2a\u6838\u5fc3\u74f6\u9888\u95ee\u9898\uff0c\u5305\u62ec\u53ef\u9760\u7684\u63a5\u89e6\u70b9\u68c0\u6d4b\u548c\u6267\u884c\u6469\u64e6\u63a5\u89e6\u5b9a\u5f8b\uff08\u975e\u7a7f\u900f\u3001\u5e93\u4ed1\u6469\u64e6\u548c\u6700\u5927\u8017\u6563\u539f\u7406\uff09\u3002", "method": "1. \u78b0\u649e\u68c0\u6d4b\u9636\u6bb5\uff1a\u4f7f\u7528\u7c92\u5b50\u4e2d\u5fc3\u51e0\u4f55\u57fa\u5143\u8fdb\u884c\u63a5\u89e6\u70b9\u5b9a\u4f4d\uff1b2. \u63a5\u89e6\u89e3\u6790\u9636\u6bb5\uff1a\u5c06\u6469\u64e6\u63a5\u89e6\u5efa\u6a21\u4e3a\u63a5\u89e6\u51b2\u91cf\u7684\u975e\u7ebf\u6027\u4e92\u8865\u95ee\u9898\uff0c\u91c7\u7528\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\u6c42\u89e3\uff1b3. \u5173\u952e\u521b\u65b0\uff1a\u91cd\u7528\u9690\u5f0fMPM\u7ebf\u6027\u5316\uff0c\u786e\u4fdd\u6548\u7387\u548c\u6570\u503c\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4e03\u4e2a\u4ee3\u8868\u6027\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6db5\u76d6\u5f39\u6027\u548c\u5f39\u5851\u6027\u54cd\u5e94\u3001\u7b80\u5355\u548c\u590d\u6742\u53ef\u53d8\u5f62\u51e0\u4f55\u5f62\u72b6\u4ee5\u53ca\u5e7f\u6cdb\u7684\u63a5\u89e6\u6761\u4ef6\u3002\u65b9\u6cd5\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u63a5\u89e6\u5b9a\u4f4d\u3001\u53ef\u9760\u7684\u6469\u64e6\u5904\u7406\u548c\u5e7f\u6cdb\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8eMPM\u7684\u673a\u5668\u4eba\u53ca\u76f8\u5173\u9886\u57df\u4eff\u771f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u7684\u63a5\u89e6\u5b9a\u4f4d\u3001\u53ef\u9760\u7684\u6469\u64e6\u5904\u7406\u548c\u5e7f\u6cdb\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2602.02142", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02142", "abs": "https://arxiv.org/abs/2602.02142", "authors": ["Ruiteng Zhao", "Wenshuo Wang", "Yicheng Ma", "Xiaocong Li", "Francis E. H. Tay", "Marcelo H. Ang", "Haiyue Zhu"], "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation", "comment": null, "summary": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.", "AI": {"tldr": "\u63d0\u51faForce-Distilled VLA\u6846\u67b6\uff0c\u901a\u8fc7Force Distillation Module\u4ece\u89c6\u89c9\u548c\u673a\u5668\u4eba\u72b6\u6001\u4e2d\u84b8\u998f\u51fa\u529b\u4fe1\u53f7\uff0c\u65e0\u9700\u7269\u7406\u529b\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u7684\u529b\u611f\u77e5\u64cd\u4f5c", "motivation": "\u529b\u611f\u77e5\u5bf9\u4e8eVLA\u6846\u67b6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bb8\u591a\u673a\u5668\u4eba\u7f3a\u4e4f\u6602\u8d35\u6216\u6613\u635f\u7684\u529b-\u529b\u77e9\u4f20\u611f\u5668\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u7269\u7406\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u529b\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u548c\u590d\u6742\u5ea6", "method": "\u63d0\u51faForce Distillation Module\uff0c\u5c06\u53ef\u5b66\u4e60\u7684\u67e5\u8be2\u4ee4\u724c\u6620\u5c04\u5230\u9884\u6d4b\u7684\u529b\u4ee4\u724c\uff0c\u8be5\u4ee4\u724c\u4e0e\u5b9e\u9645\u529b\u4fe1\u53f7\u7684\u6f5c\u5728\u8868\u793a\u5bf9\u9f50\u3002\u63a8\u7406\u65f6\uff0c\u5c06\u84b8\u998f\u7684\u529b\u4ee4\u724c\u6ce8\u5165\u9884\u8bad\u7ec3\u7684VLM\u4e2d\uff0c\u5b9e\u73b0\u529b\u611f\u77e5\u63a8\u7406", "result": "\u7269\u7406\u5b9e\u9a8c\u8868\u660e\uff0c\u84b8\u998f\u7684\u529b\u4ee4\u724c\u6027\u80fd\u4f18\u4e8e\u76f4\u63a5\u4f20\u611f\u5668\u529b\u6d4b\u91cf\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u529b\u84b8\u998fVLA\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "FD-VLA\u6846\u67b6\u6210\u529f\u5c06\u529b\u611f\u77e5\u96c6\u6210\u5230\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4e2d\uff0c\u65e0\u9700\u7269\u7406\u529b\u4f20\u611f\u5668\uff0c\u540c\u65f6\u901a\u8fc7\u529b-\u89c6\u89c9-\u72b6\u6001\u878d\u5408\u63d0\u9ad8\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u611f\u77e5-\u52a8\u4f5c\u9c81\u68d2\u6027"}}
{"id": "2602.02331", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02331", "abs": "https://arxiv.org/abs/2602.02331", "authors": ["Shaoting Zhu", "Baijun Ye", "Jiaxuan Wang", "Jiakang Chen", "Ziwen Zhuang", "Linzhan Mou", "Runhan Huang", "Hang Zhao"], "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour", "comment": "Project Page: https://ttt-parkour.github.io/", "summary": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u771f\u5b9e-\u4eff\u771f-\u771f\u5b9e\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u589e\u5f3a\u4eba\u5f62\u673a\u5668\u4eba\u7a7f\u8d8a\u590d\u6742\u672a\u77e5\u5730\u5f62\u7684\u80fd\u529b\uff0c\u6574\u4e2a\u6d41\u7a0b\u572810\u5206\u949f\u5185\u5b8c\u6210", "motivation": "\u73b0\u6709\u901a\u7528\u8fd0\u52a8\u7b56\u7565\u5728\u5e7f\u6cdb\u5730\u5f62\u5206\u5e03\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4efb\u610f\u4e14\u9ad8\u5ea6\u6311\u6218\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5728\u672a\u89c1\u590d\u6742\u5730\u5f62\u4e0a\u7684\u52a8\u6001\u8dd1\u9177\u80fd\u529b", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7aef\u5230\u7aef\u5b66\u4e60\u8303\u5f0f\uff1a\u5148\u5728\u7a0b\u5e8f\u751f\u6210\u7684\u5730\u5f62\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u4ece\u771f\u5b9e\u4e16\u754c\u6355\u83b7\u91cd\u5efa\u7684\u9ad8\u4fdd\u771f\u7f51\u683c\u4e0a\u8fdb\u884c\u5feb\u901f\u5fae\u8c03\uff1b\u5f00\u53d1\u57fa\u4e8eRGB-D\u8f93\u5165\u7684\u524d\u9988\u9ad8\u6548\u9ad8\u4fdd\u771f\u51e0\u4f55\u91cd\u5efa\u6d41\u7a0b", "result": "TTT-Parkour\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u638c\u63e1\u590d\u6742\u969c\u788d\u7269\uff08\u6954\u5f62\u3001\u6869\u3001\u7bb1\u5b50\u3001\u68af\u5f62\u3001\u7a84\u6881\u7b49\uff09\uff1b\u6574\u4e2a\u6355\u83b7\u3001\u91cd\u5efa\u548c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6d41\u7a0b\u5728\u5927\u591a\u6570\u5730\u5f62\u4e0a\u5c11\u4e8e10\u5206\u949f\uff1b\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u540e\u7684\u7b56\u7565\u5c55\u73b0\u51fa\u9c81\u68d2\u7684\u96f6\u6837\u672c\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u80fd\u529b", "conclusion": "\u63d0\u51fa\u7684\u771f\u5b9e-\u4eff\u771f-\u771f\u5b9e\u6846\u67b6\u901a\u8fc7\u5feb\u901f\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u663e\u8457\u589e\u5f3a\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u51e0\u4f55\u5730\u5f62\u4e0a\u7684\u7a7f\u8d8a\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u672a\u77e5\u5730\u5f62\u9002\u5e94"}}
{"id": "2602.02389", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02389", "abs": "https://arxiv.org/abs/2602.02389", "authors": ["Marina Ruediger", "Ashis G. Banerjee"], "title": "Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures", "comment": "This paper will appear in the proceedings of the 2026 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eSLAM\u6570\u636e\u7684\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u68c0\u6d4b\u4efb\u52a1\u751f\u6210\u65b9\u6cd5\uff0c\u65e0\u9700\u5148\u9a8c\u51e0\u4f55\u4fe1\u606f\uff0c\u901a\u8fc7\u786c\u4ef6\u53c2\u6570\u548c\u73af\u5883\u6761\u4ef6\u4f18\u5316\u4efb\u52a1\u96c6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5177\u9002\u5e94\u6027", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u68c0\u6d4b\u4e2d\u7f3a\u4e4f\u5148\u9a8c\u51e0\u4f55\u4fe1\u606f\u65f6\u7684\u4efb\u52a1\u751f\u6210\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u73af\u5883\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u5f80\u5f80\u7f3a\u4e4f\u6b64\u7c7b\u4fe1\u606f", "method": "\u5229\u7528SLAM\u7f51\u683c\u6570\u636e\u751f\u6210\u4efb\u52a1\u96c6\uff0c\u7ed3\u5408\u786c\u4ef6\u53c2\u6570\u548c\u73af\u5883\u6761\u4ef6\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u671f\u671b\u5173\u952e\u70b9\u8bc4\u5206\u548c\u57fa\u4e8e\u8ddd\u79bb\u7684\u526a\u679d\u4f18\u5316\u4efb\u52a1\u5206\u5e03\uff0c\u8fdb\u884c\u6c34\u4e0b\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u6c34\u4e0b\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u6709\u6548\u6027\u5e76\u786e\u5b9a\u4e86\u5408\u9002\u53c2\u6570\uff0c\u76f8\u6bd4\u6a21\u62df\u7684Voronoi\u5206\u533a\u548cBoustrophedon\u6a21\u5f0f\uff0c\u8be5\u65b9\u6cd5\u5728\u6d4b\u8bd5\u73af\u5883\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u68c0\u6d4b\u8986\u76d6\u7387", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u53d1\u73b0\u65b9\u6cd5\u5177\u6709\u9002\u5e94\u610f\u5916\u51e0\u4f55\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u80fd\u591f\u4fdd\u6301\u8986\u76d6\u8303\u56f4\u7684\u540c\u65f6\u91cd\u70b9\u5173\u6ce8\u53ef\u80fd\u51fa\u73b0\u7f3a\u9677\u6216\u635f\u574f\u7684\u533a\u57df\uff0c\u4f18\u4e8e\u4f20\u7edf\u5206\u533a\u65b9\u6cd5"}}
{"id": "2602.02402", "categories": ["cs.RO", "cs.AI", "cs.CV", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.02402", "abs": "https://arxiv.org/abs/2602.02402", "authors": ["Mu Huang", "Hui Wang", "Kerui Ren", "Linning Xu", "Yunsong Zhou", "Mulin Yu", "Bo Dai", "Jiangmiao Pang"], "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation", "comment": "Project page: https://city-super.github.io/SoMA/", "summary": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.", "AI": {"tldr": "SoMA\uff1a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u8f6f\u4f53\u64cd\u4f5c\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u7edf\u4e00\u6f5c\u5728\u795e\u7ecf\u7a7a\u95f4\u8026\u5408\u53d8\u5f62\u52a8\u529b\u5b66\u3001\u73af\u5883\u529b\u548c\u673a\u5668\u4eba\u5173\u8282\u52a8\u4f5c\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u771f\u5b9e\u5230\u6a21\u62df\u4eff\u771f", "motivation": "\u73b0\u6709\u6a21\u62df\u5668\u4f9d\u8d56\u9884\u5b9a\u4e49\u7269\u7406\u6a21\u578b\u6216\u6570\u636e\u9a71\u52a8\u52a8\u529b\u5b66\uff0c\u7f3a\u4e4f\u673a\u5668\u4eba\u6761\u4ef6\u63a7\u5236\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\uff0c\u96be\u4ee5\u5b9e\u73b0\u771f\u5b9e\u5230\u6a21\u62df\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4eff\u771f", "method": "\u63d0\u51faSoMA\u6846\u67b6\uff0c\u5728\u7edf\u4e00\u6f5c\u5728\u795e\u7ecf\u7a7a\u95f4\u4e2d\u8026\u5408\u53d8\u5f62\u52a8\u529b\u5b66\u3001\u73af\u5883\u529b\u548c\u673a\u5668\u4eba\u5173\u8282\u52a8\u4f5c\uff1b\u57fa\u4e8e\u5b66\u4e60\u7684\u9ad8\u65af\u6cfc\u6e85\u5efa\u6a21\u4ea4\u4e92\uff0c\u652f\u6301\u53ef\u63a7\u3001\u7a33\u5b9a\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u7269\u7406\u6a21\u578b", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4e0a\uff0cSoMA\u5c06\u91cd\u6a21\u62df\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u63d0\u534720%\uff1b\u80fd\u591f\u7a33\u5b9a\u6a21\u62df\u590d\u6742\u4efb\u52a1\u5982\u957f\u65f6\u7a0b\u5e03\u6599\u6298\u53e0\uff0c\u8d85\u8d8a\u89c2\u5bdf\u8f68\u8ff9\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "SoMA\u901a\u8fc7\u795e\u7ecf\u6f5c\u5728\u7a7a\u95f4\u7edf\u4e00\u5efa\u6a21\u53d8\u5f62\u4f53\u52a8\u529b\u5b66\u4e0e\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u7a33\u5b9a\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u771f\u5b9e\u5230\u6a21\u62df\u4eff\u771f\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02411", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02411", "abs": "https://arxiv.org/abs/2602.02411", "authors": ["Hanwen Ren", "Junyong Kim", "Aathman Tharmasanthiran", "Ahmed H. Qureshi"], "title": "Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces", "comment": null, "summary": "Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.", "AI": {"tldr": "CAM-MCTS\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7269\u4f53\u91cd\u6392\u89c4\u5212\u7684\u96c6\u4e2d\u5f0f\u5f02\u6b65\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6846\u67b6\uff0c\u5728\u6742\u4e71\u73af\u5883\u4e2d\u4f18\u5316\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u901a\u5e38\u662f\u975e\u5355\u8c03\u7684\uff08\u7269\u4f53\u76f8\u4e92\u963b\u6321\u9700\u8981\u4e34\u65f6\u91cd\u5b9a\u4f4d\uff09\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u5355\u8c03\u5b9e\u4f8b\u3002\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u4f46\u9700\u8981\u6709\u6548\u7684\u534f\u8c03\u673a\u5236\u6765\u907f\u514d\u7a7a\u95f2\u65f6\u95f4\u548c\u540c\u6b65\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faCAM-MCTS\u6846\u67b6\uff0c\u7ed3\u5408\u96c6\u4e2d\u5f0f\u4efb\u52a1\u5206\u914d\uff08\u667a\u80fd\u4f53\u4e86\u89e3\u5f7c\u6b64\u610f\u56fe\u4ee5\u5b9e\u73b0\u5168\u5c40\u4f18\u5316\u89c4\u5212\uff09\u548c\u5f02\u6b65\u4efb\u52a1\u6267\u884c\u7b56\u7565\uff08\u667a\u80fd\u4f53\u5728\u9002\u5f53\u65f6\u95f4\u6b65\u63a5\u53d7\u65b0\u4efb\u52a1\u800c\u975e\u7b49\u5f85\u5176\u4ed6\u667a\u80fd\u4f53\uff09\uff0c\u901a\u8fc7\u4e00\u6b65\u524d\u77bb\u6210\u672c\u4f30\u8ba1\u6307\u5bfc\u51b3\u7b56\uff0c\u6700\u5c0f\u5316\u7a7a\u95f2\u65f6\u95f4\u5e76\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u540c\u6b65\u5ef6\u8fdf\u3002", "result": "\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5355\u8c03\u548c\u975e\u5355\u8c03\u4efb\u52a1\u4e0a\u8bc4\u4f30CAM-MCTS\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002\u5728\u4e0d\u540c\u914d\u7f6e\u7684\u771f\u5b9e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "CAM-MCTS\u662f\u4e00\u79cd\u901a\u7528\u3001\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u7269\u4f53\u91cd\u6392\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u4e2d\u5f0f\u89c4\u5212\u548c\u5f02\u6b65\u6267\u884c\u7684\u7ed3\u5408\uff0c\u5728\u590d\u6742\u6742\u4e71\u73af\u5883\u4e2d\u663e\u8457\u4f18\u5316\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u9002\u7528\u4e8e\u4ed3\u5e93\u3001\u5bb6\u5ead\u548c\u6551\u63f4\u73b0\u573a\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.02430", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02430", "abs": "https://arxiv.org/abs/2602.02430", "authors": ["Pierre-Yves Lajoie", "Benjamin Ramtoula", "Daniele De Martini", "Giovanni Beltrame"], "title": "3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM", "comment": null, "summary": "Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u75283D\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5cSLAM\u95ed\u73af\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u5904\u7406\u5927\u89c6\u89d2\u5dee\u5f02\u7684\u56fe\u50cf\u5339\u914d\uff0c\u7ed3\u5408\u9c81\u68d2\u5f02\u5e38\u503c\u6291\u5236\u548c\u4e13\u95e8\u7684\u4f4d\u59ff\u56fe\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u591a\u673a\u5668\u4eba\u5efa\u56fe\u6027\u80fd\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5cSLAM\u7cfb\u7edf\u5728\u5927\u89c6\u89d2\u5dee\u5f02\u4e0b\u96be\u4ee5\u8bc6\u522b\u5730\u56fe\u91cd\u53e0\u533a\u57df\uff0c\u800c\u8fd1\u671f3D\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u5927\u89c6\u89d2\u5dee\u5f02\u56fe\u50cf\u914d\u51c6\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u8fd9\u4e3a\u89e3\u51b3\u8be5\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "1) \u5c063D\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230\u73b0\u6709SLAM\u6d41\u7a0b\u4e2d\uff0c\u4ece\u5355\u76ee\u56fe\u50cf\u5bf9\u53ef\u9760\u4f30\u8ba1\u76f8\u5bf9\u4f4d\u59ff\uff1b2) \u5f15\u5165\u9c81\u68d2\u7684\u5f02\u5e38\u503c\u6291\u5236\u6280\u672f\uff1b3) \u5f00\u53d1\u4e13\u95e8\u7684\u4f4d\u59ff\u56fe\u4f18\u5316\u516c\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u5c3a\u5ea6\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5b9a\u4f4d\u548c\u5efa\u56fe\u7cbe\u5ea6\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u83b7\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u591a\u673a\u5668\u4eba\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u5229\u75283D\u57fa\u7840\u6a21\u578b\u5904\u7406\u5927\u89c6\u89d2\u5dee\u5f02\u7684\u80fd\u529b\uff0c\u7ed3\u5408\u9c81\u68d2\u4f18\u5316\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5cSLAM\u7cfb\u7edf\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02454", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02454", "abs": "https://arxiv.org/abs/2602.02454", "authors": ["Ansh Kumar Sharma", "Yixiang Sun", "Ninghao Lu", "Yunzhe Zhang", "Jiarao Liu", "Sherry Yang"], "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model", "comment": "https://world-gymnast.github.io/", "summary": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.", "AI": {"tldr": "World-Gymnast\uff1a\u901a\u8fc7\u5728\u52a8\u4f5c\u6761\u4ef6\u89c6\u9891\u4e16\u754c\u6a21\u578b\u4e2d\u6267\u884cRL\u5fae\u8c03\uff0c\u4f7f\u7528VLM\u5956\u52b1\u8f68\u8ff9\uff0c\u663e\u8457\u8d85\u8d8aSFT\u548c\u8f6f\u4ef6\u6a21\u62df\u5668\uff0c\u5b9e\u73b018\u500d\u548c2\u500d\u7684\u6027\u80fd\u63d0\u5347", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u53d7\u9650\u4e8e\u7269\u7406\u4ea4\u4e92\u6210\u672c\uff0c\u4e13\u5bb6\u6f14\u793a\u7684\u76d1\u7763\u5fae\u8c03\u53d7\u6570\u636e\u91cf\u9650\u5236\uff0c\u8f6f\u4ef6\u6a21\u62df\u5668\u5b58\u5728sim-to-real\u5dee\u8ddd\u3002\u4e16\u754c\u6a21\u578b\u7684\u51fa\u73b0\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff1a\u5728\u4e16\u754c\u6a21\u578b\u4e2d\u8bad\u7ec3\u7b56\u7565\u662f\u5426\u6bd4\u76d1\u7763\u5b66\u4e60\u6216\u8f6f\u4ef6\u6a21\u62df\u66f4\u6709\u6548\uff1f", "method": "\u63d0\u51faWorld-Gymnast\u65b9\u6cd5\uff1a1\uff09\u5728\u52a8\u4f5c\u6761\u4ef6\u89c6\u9891\u4e16\u754c\u6a21\u578b\u4e2d\u8fdb\u884c\u7b56\u7565rollout\uff1b2\uff09\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8bc4\u4f30\u8f68\u8ff9\u5e76\u7ed9\u4e88\u5956\u52b1\uff1b3\uff09\u5bf9\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u7b56\u7565\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03", "result": "\u5728Bridge\u673a\u5668\u4eba\u8bbe\u7f6e\u4e2d\uff1a1\uff09\u6bd4\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe18\u500d\uff1b2\uff09\u6bd4\u8f6f\u4ef6\u6a21\u62df\u5668\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe2\u500d\uff1b3\uff09\u5c55\u793a\u4e86\u4e16\u754c\u6a21\u578bRL\u7684\u72ec\u7279\u80fd\u529b\uff1a\u591a\u6837\u5316\u8bed\u8a00\u6307\u4ee4\u8bad\u7ec3\u3001\u65b0\u573a\u666f\u8bad\u7ec3\u3001\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u3001\u5728\u7ebf\u8fed\u4ee3\u6539\u8fdb", "conclusion": "\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u5e76\u5728\u4e91\u7aef\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u53ef\u80fd\u662f\u5f25\u5408\u6f14\u793a\u673a\u5668\u4eba\u4e0e\u5bb6\u5ead\u5b9e\u7528\u673a\u5668\u4eba\u4e4b\u95f4\u5dee\u8ddd\u7684\u5173\u952e\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u8303\u5f0f"}}
{"id": "2602.02456", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02456", "abs": "https://arxiv.org/abs/2602.02456", "authors": ["Albert Gassol Puigjaner", "Angelos Zacharia", "Kostas Alexis"], "title": "Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning", "comment": "ICRA 2026, 8 pages", "summary": "Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.", "AI": {"tldr": "\u63d0\u51fa\u589e\u5f3a\u578b\u5206\u5c423D\u573a\u666f\u56fe\uff0c\u96c6\u6210\u5f00\u653e\u8bcd\u6c47\u7279\u5f81\u5e76\u652f\u6301\u5bf9\u8c61\u5173\u7cfb\u63a8\u7406\uff0c\u7ed3\u5408VLM\u548cLLM\u5b9e\u73b0\u4efb\u52a1\u63a8\u7406\uff0c\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1", "motivation": "\u4f20\u7edfSLAM\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u5c42\u6b21\u62bd\u8c61\u548c\u5173\u7cfb\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u66f4\u7ed3\u6784\u5316\u76843D\u73af\u5883\u8868\u793a\u6765\u652f\u6301\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u5bfc\u822a\u548c\u63a8\u7406", "method": "\u63d0\u51fa\u589e\u5f3a\u578b\u5206\u5c423D\u573a\u666f\u56fe\uff0c\u96c6\u6210\u5f00\u653e\u8bcd\u6c47\u7279\u5f81\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u8bed\u4e49\u5173\u7cfb\uff0c\u5f15\u5165\u7ed3\u5408LLM\u548cVLM\u7684\u4efb\u52a1\u63a8\u7406\u6a21\u5757", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u5728\u591a\u79cd\u73af\u5883\u548c\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b", "conclusion": "\u589e\u5f3a\u578b3D\u573a\u666f\u56fe\u7ed3\u5408VLM\u548cLLM\u80fd\u591f\u6709\u6548\u652f\u6301\u667a\u80fd\u4f53\u5bf9\u73af\u5883\u7684\u8bed\u4e49\u548c\u5173\u7cfb\u63a8\u7406\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u80fd\u529b"}}
{"id": "2602.02459", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02459", "abs": "https://arxiv.org/abs/2602.02459", "authors": ["Zhiyu Huang", "Yun Zhang", "Johnson Liu", "Rui Song", "Chen Tang", "Jiaqi Ma"], "title": "TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments", "comment": null, "summary": "Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/", "code_url": "https://ucla-mobility.github.io/TIC-VLA", "AI": {"tldr": "TIC-VLA\u662f\u4e00\u4e2a\u5ef6\u8fdf\u611f\u77e5\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5ef6\u8fdf\u7684\u8bed\u4e49\u63a8\u7406\u6765\u8865\u507f\u5f02\u6b65\u63a8\u7406\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5047\u8bbe\u65f6\u95f4\u5bf9\u9f50\u7684\u63a8\u7406\u548c\u63a7\u5236\uff0c\u4f46\u8bed\u4e49\u63a8\u7406\u76f8\u5bf9\u4e8e\u5b9e\u65f6\u52a8\u4f5c\u5b58\u5728\u56fa\u6709\u5ef6\u8fdf\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u53cd\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5ef6\u8fdf\u8bed\u4e49-\u63a7\u5236\u63a5\u53e3\uff0c\u5c06\u5ef6\u8fdf\u7684\u89c6\u89c9-\u8bed\u8a00\u8bed\u4e49\u72b6\u6001\u548c\u663e\u5f0f\u5ef6\u8fdf\u5143\u6570\u636e\u4e0e\u5f53\u524d\u89c2\u6d4b\u4e00\u8d77\u4f5c\u4e3a\u52a8\u4f5c\u751f\u6210\u6761\u4ef6\uff1b\u8bbe\u8ba1\u5ef6\u8fdf\u4e00\u81f4\u6027\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5728\u6a21\u4eff\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6ce8\u5165\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cTIC-VLA\u5728\u4fdd\u6301\u591a\u79d2\u63a8\u7406\u5ef6\u8fdf\u4e0b\u7a33\u5065\u5b9e\u65f6\u63a7\u5236\u7684\u540c\u65f6\uff0c\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u7684VLA\u6a21\u578b\u3002", "conclusion": "TIC-VLA\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5ef6\u8fdf\u8bed\u4e49\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u4e2d\u63a8\u7406\u4e0e\u63a7\u5236\u7684\u5f02\u6b65\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8bed\u8a00\u5f15\u5bfc\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02473", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02473", "abs": "https://arxiv.org/abs/2602.02473", "authors": ["Yinhuai Wang", "Qihan Zhao", "Yuen Fui Lau", "Runyi Yu", "Hok Wai Tsui", "Qifeng Chen", "Jingbo Wang", "Jiangmiao Pang", "Ping Tan"], "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos", "comment": null, "summary": "Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.", "AI": {"tldr": "HumanX\u6846\u67b6\u901a\u8fc7\u4ece\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u5373\u53ef\u8ba9\u4eff\u4eba\u673a\u5668\u4eba\u5b66\u4e60\u901a\u7528\u4ea4\u4e92\u6280\u80fd\uff0c\u5728\u591a\u4e2a\u8fd0\u52a8\u9886\u57df\u5b9e\u73b0\u96f6\u6837\u672c\u7269\u7406\u90e8\u7f72\u3002", "motivation": "\u5f53\u524d\u4eff\u4eba\u673a\u5668\u4eba\u4ea4\u4e92\u4efb\u52a1\u9762\u4e34\u4e24\u5927\u74f6\u9888\uff1a1\uff09\u7f3a\u4e4f\u771f\u5b9e\u7684\u4ea4\u4e92\u6570\u636e\uff1b2\uff09\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u51fd\u6570\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u5c31\u80fd\u4ece\u89c6\u9891\u5b66\u4e60\u901a\u7528\u4ea4\u4e92\u6280\u80fd\u7684\u65b9\u6cd5\u3002", "method": "HumanX\u5305\u542b\u4e24\u4e2a\u534f\u540c\u8bbe\u8ba1\u7684\u7ec4\u4ef6\uff1aXGen\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4ece\u89c6\u9891\u5408\u6210\u591a\u6837\u5316\u3001\u7269\u7406\u5408\u7406\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\u5e76\u652f\u6301\u53ef\u6269\u5c55\u6570\u636e\u589e\u5f3a\uff1bXMimic\u7edf\u4e00\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5b66\u4e60\u901a\u7528\u4ea4\u4e92\u6280\u80fd\u3002\u6846\u67b6\u4ece\u5355\u89c6\u9891\u6f14\u793a\u5b66\u4e60\uff0c\u65e0\u9700\u5916\u90e8\u611f\u77e5\u3002", "result": "\u5728\u7bee\u7403\u3001\u8db3\u7403\u3001\u7fbd\u6bdb\u7403\u3001\u8d27\u7269\u62fe\u53d6\u548c\u53cd\u5e94\u6027\u683c\u6597\u4e94\u4e2a\u9886\u57df\u6210\u529f\u5b66\u4e6010\u79cd\u6280\u80fd\uff0c\u96f6\u6837\u672c\u8fc1\u79fb\u5230Unitree G1\u7269\u7406\u4eff\u4eba\u673a\u5668\u4eba\u3002\u5305\u62ec\u590d\u6742\u52a8\u4f5c\u5982\u5047\u52a8\u4f5c\u8f6c\u8eab\u540e\u4ef0\u8df3\u6295\uff0c\u4ee5\u53ca\u6301\u7eed10\u4e2a\u5faa\u73af\u7684\u4eba\u673a\u4f20\u7403\u5e8f\u5217\u3002\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5b9e\u73b08\u500d\u4ee5\u4e0a\u7684\u6cdb\u5316\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "HumanX\u5c55\u793a\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u4efb\u52a1\u65e0\u5173\u7684\u9014\u5f84\uff0c\u7528\u4e8e\u5b66\u4e60\u591a\u529f\u80fd\u3001\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u6280\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eff\u4eba\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.02481", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02481", "abs": "https://arxiv.org/abs/2602.02481", "authors": ["Brent Yi", "Hongsuk Choi", "Himanshu Gaurav Singh", "Xiaoyu Huang", "Takara E. Truong", "Carmelo Sferrazza", "Yi Ma", "Rocky Duan", "Pieter Abbeel", "Guanya Shi", "Karen Liu", "Angjoo Kanazawa"], "title": "Flow Policy Gradients for Robot Control", "comment": "Project webpage: https://hongsukchoi.github.io/fpo-control", "summary": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.", "AI": {"tldr": "\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u66ff\u4ee3\u4f20\u7edf\u57fa\u4e8e\u4f3c\u7136\u7684\u7b56\u7565\u68af\u5ea6\uff0c\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u4f18\u7684\u8bad\u7ec3\u548c\u5fae\u8c03\u6027\u80fd", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4f3c\u7136\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u53ef\u5fae\u7684\u52a8\u4f5c\u4f3c\u7136\u8ba1\u7b97\uff0c\u8fd9\u9650\u5236\u4e86\u7b56\u7565\u8f93\u51fa\u53ea\u80fd\u4f7f\u7528\u7b80\u5355\u5206\u5e03\uff08\u5982\u9ad8\u65af\u5206\u5e03\uff09\uff0c\u65e0\u6cd5\u5229\u7528\u66f4\u590d\u6742\u7684\u8868\u8fbe\u6027\u7b56\u7565", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u6d41\u5339\u914d\u7b56\u7565\u68af\u5ea6\u76ee\u6807\u51fd\u6570\uff0c\u7ed5\u8fc7\u4f3c\u7136\u8ba1\u7b97\uff0c\u4f7f\u6d41\u8868\u793a\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u6709\u6548\uff1b\u8be5\u65b9\u6cd5\u652f\u6301\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u5e76\u5728\u8db3\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u3001\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u8ddf\u8e2a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fdb\u884c\u9a8c\u8bc1", "result": "\u65b9\u6cd5\u5728\u8db3\u5f0f\u8fd0\u52a8\u3001\u4eba\u5f62\u8fd0\u52a8\u8ddf\u8e2a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u5e76\u5728\u4e24\u4e2a\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u7a33\u5065\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\uff1b\u6d41\u8868\u793a\u6709\u52a9\u4e8e\u63a2\u7d22\uff0c\u5fae\u8c03\u9c81\u68d2\u6027\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u6d41\u5339\u914d\u7b56\u7565\u68af\u5ea6\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bad\u7ec3\u548c\u5fae\u8c03\u8868\u8fbe\u6027\u66f4\u5f3a\u7684\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u57fa\u4e8e\u4f3c\u7136\u65b9\u6cd5\u7684\u5206\u5e03\u9650\u5236\uff0c\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2602.00475", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00475", "abs": "https://arxiv.org/abs/2602.00475", "authors": ["Michael Psenka", "Michael Rabbat", "Aditi Krishnapriyan", "Yann LeCun", "Amir Bar"], "title": "Parallel Stochastic Gradient-Based Planning for World Models", "comment": "23 pages, 7 figures", "summary": "World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables (\"virtual states\") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.", "AI": {"tldr": "GRASP\u662f\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u4e16\u754c\u6a21\u578b\u7684\u5e76\u884c\u5316\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u865a\u62df\u72b6\u6001\u4f18\u5316\u548c\u968f\u673a\u6027\u5f15\u5165\u89e3\u51b3\u89c6\u89c9\u8f93\u5165\u7684\u957f\u65f6\u57df\u63a7\u5236\u4efb\u52a1", "motivation": "\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u4ece\u539f\u59cb\u89c6\u89c9\u8f93\u5165\u6a21\u62df\u73af\u5883\u52a8\u6001\uff0c\u4f46\u7528\u4e8e\u89c4\u5212\u65f6\u9762\u4e34\u641c\u7d22\u7a7a\u95f4\u5de8\u5927\u4e14\u975e\u7ed3\u6784\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89c4\u5212\u65b9\u6cd5", "method": "\u63d0\u51faGRASP\u89c4\u5212\u5668\uff1a1)\u5c06\u72b6\u6001\u89c6\u4e3a\u4f18\u5316\u53d8\u91cf\uff08\u865a\u62df\u72b6\u6001\uff09\u5e76\u65bd\u52a0\u8f6f\u52a8\u6001\u7ea6\u675f\uff1b2)\u5f15\u5165\u72b6\u6001\u968f\u673a\u6027\u4fc3\u8fdb\u63a2\u7d22\uff1b3)\u4fee\u6539\u68af\u5ea6\u7ed3\u6784\u4ee5\u964d\u4f4e\u9ad8\u7ef4\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u7684\u68af\u5ea6\u654f\u611f\u6027\uff0c\u4ec5\u9700\u52a8\u4f5c\u8f93\u5165\u68af\u5ea6", "result": "\u5728\u57fa\u4e8e\u89c6\u9891\u7684\u4e16\u754c\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cGRASP\u5728\u957f\u65f6\u57df\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4ea4\u53c9\u71b5\u65b9\u6cd5\uff08CEM\uff09\u548c\u666e\u901a\u68af\u5ea6\u4f18\u5316\uff08GD\uff09\uff0c\u6210\u529f\u7387\u548c\u6536\u655b\u65f6\u95f4\u5747\u6709\u63d0\u5347", "conclusion": "GRASP\u4f5c\u4e3a\u4e00\u79cd\u968f\u673a\u5316\u7684\u975e\u51dd\u805a\u6216\u914d\u70b9\u6700\u4f18\u63a7\u5236\u5668\uff0c\u5229\u7528\u53ef\u5fae\u5206\u4e16\u754c\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u89c4\u5212\uff0c\u4e3a\u89c6\u89c9\u8f93\u5165\u7684\u957f\u65f6\u57df\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01156", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01156", "abs": "https://arxiv.org/abs/2602.01156", "authors": ["Shunpeng Yang", "Ben Liu", "Hua Chen"], "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning", "comment": "Submitted to ICLR 2026", "summary": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.", "AI": {"tldr": "PolicyFlow\uff1a\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\uff08CNF\uff09\u7684\u65b0\u578b\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u91cd\u8981\u6027\u6bd4\u7387\u548c\u5e03\u6717\u6b63\u5219\u5316\u5668\uff0c\u89e3\u51b3\u4e86PPO\u5728\u8868\u8fbe\u6027\u7b56\u7565\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u548c\u7a33\u5b9a\u6027\u95ee\u9898", "motivation": "\u6807\u51c6PPO\u4f9d\u8d56\u4e8e\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u8fd9\u5728\u9ad8\u65af\u7b56\u7565\u4e2d\u6613\u4e8e\u8ba1\u7b97\uff0c\u4f46\u5bf9\u4e8e\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\uff08CNF\uff09\u7b56\u7565\uff0c\u6cbf\u6574\u4e2a\u6d41\u8f68\u8ff9\u8bc4\u4f30\u4f3c\u7136\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6570\u503c\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u8868\u8fbe\u6027CNF\u7b56\u7565\u4e0ePPO\u5f0f\u76ee\u6807\u7ed3\u5408\uff0c\u540c\u65f6\u907f\u514d\u6602\u8d35\u7684\u4f3c\u7136\u8bc4\u4f30\u3002", "method": "1. \u63d0\u51faPolicyFlow\u7b97\u6cd5\uff0c\u901a\u8fc7\u6cbf\u7b80\u5355\u63d2\u503c\u8def\u5f84\u7684\u6d41\u573a\u53d8\u5316\u8fd1\u4f3c\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u907f\u514d\u6cbf\u5b8c\u6574\u6d41\u8def\u5f84\u7684\u4f3c\u7136\u8bc4\u4f30\uff1b2. \u5f15\u5165\u5e03\u6717\u6b63\u5219\u5316\u5668\uff0c\u8fd9\u662f\u4e00\u79cd\u53d7\u5e03\u6717\u8fd0\u52a8\u542f\u53d1\u7684\u9690\u5f0f\u7b56\u7565\u71b5\u6b63\u5219\u5316\u5668\uff0c\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\u5e76\u9f13\u52b1\u591a\u6837\u5316\u884c\u4e3a\uff1b3. \u5c06\u8868\u8fbe\u6027CNF\u7b56\u7565\u4e0ePPO\u5f0f\u76ee\u6807\u96c6\u6210\uff0c\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728MultiGoal\u3001PointMaze\u3001IsaacLab\u548cMuJoCo Playground\u7b49\u591a\u79cd\u73af\u5883\u7684\u591a\u6837\u5316\u4efb\u52a1\u4e0a\uff0cPolicyFlow\u76f8\u6bd4\u4f7f\u7528\u9ad8\u65af\u7b56\u7565\u7684PPO\u4ee5\u53ca\u57fa\u4e8e\u6d41\u7684\u57fa\u7ebf\u65b9\u6cd5\uff08FPO\u548cDPPO\uff09\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002\u7279\u522b\u5728MultiGoal\u4efb\u52a1\u4e0a\uff0cPolicyFlow\u5c55\u73b0\u51fa\u6355\u83b7\u66f4\u4e30\u5bcc\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u7684\u80fd\u529b\u3002", "conclusion": "PolicyFlow\u6210\u529f\u89e3\u51b3\u4e86\u5c06PPO\u6269\u5c55\u5230\u8868\u8fbe\u6027CNF\u7b56\u7565\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8fd1\u4f3c\u91cd\u8981\u6027\u6bd4\u7387\u548c\u5e03\u6717\u6b63\u5219\u5316\u5668\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u8868\u73b0\u7684\u5e73\u8861\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u66f4\u590d\u6742\u7b56\u7565\u6a21\u578b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
