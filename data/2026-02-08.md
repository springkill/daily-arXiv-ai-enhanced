<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 125]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CR](#cs.CR) [Total: 22]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.RO](#cs.RO) [Total: 26]
- [cs.HC](#cs.HC) [Total: 19]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Denoising diffusion networks for normative modeling in neuroimaging](https://arxiv.org/abs/2602.04886)
*Luke Whitbread,Lyle J. Palmer,Mark Jenkinson*

Main category: cs.LG

TL;DR: 提出使用去噪扩散概率模型（DDPMs）作为表格影像衍生表型的统一条件密度估计器，从采样中导出单变量百分位数和偏差分数，实现可扩展的联合规范建模。


<details>
  <summary>Details</summary>
Motivation: 传统神经影像流程对每个影像衍生表型（IDP）单独拟合模型，虽然扩展性好但丢弃了可能编码协调模式的多变量依赖性。需要一种能够联合建模多变量依赖性的规范建模方法。

Method: 使用去噪扩散概率模型（DDPMs）作为条件密度估计器，采用两种去噪器骨干：1）特征线性调制（FiLM）条件多层感知机（MLP）；2）具有特征自注意力和样本间注意力的表格变换器（SAINT），通过学习嵌入处理条件协变量。

Result: 在低维度下，扩散模型提供与基线相当的校准良好的单IDP输出，同时联合建模真实的依赖结构。在高维度下，变换器骨干比MLP显著更好地保持校准，并更好地保留高阶依赖，实现可扩展的联合规范建模。

Conclusion: 扩散基础的规范建模是神经影像中校准多变量偏差剖面的实用途径，能够保持与标准单IDP流程的兼容性，同时更好地保留多变量依赖性。

Abstract: Normative modeling estimates reference distributions of biological measures conditional on covariates, enabling centiles and clinically interpretable deviation scores to be derived. Most neuroimaging pipelines fit one model per imaging-derived phenotype (IDP), which scales well but discards multivariate dependence that may encode coordinated patterns. We propose denoising diffusion probabilistic models (DDPMs) as a unified conditional density estimator for tabular IDPs, from which univariate centiles and deviation scores are derived by sampling. We utilise two denoiser backbones: (i) a feature-wise linear modulation (FiLM) conditioned multilayer perceptron (MLP) and (ii) a tabular transformer with feature self-attention and intersample attention (SAINT), conditioning covariates through learned embeddings. We evaluate on a synthetic benchmark with heteroscedastic and multimodal age effects and on UK Biobank FreeSurfer phenotypes, scaling from dimension of 2 to 200. Our evaluation suite includes centile calibration (absolute centile error, empirical coverage, and the probability integral transform), distributional fidelity (Kolmogorov-Smirnov tests), multivariate dependence diagnostics, and nearest-neighbour memorisation analysis. For low dimensions, diffusion models deliver well-calibrated per-IDP outputs comparable to traditional baselines while jointly modeling realistic dependence structure. At higher dimensions, the transformer backbone remains substantially better calibrated than the MLP and better preserves higher-order dependence, enabling scalable joint normative models that remain compatible with standard per-IDP pipelines. These results support diffusion-based normative modeling as a practical route to calibrated multivariate deviation profiles in neuroimaging.

</details>


### [2] [A Causal Perspective for Enhancing Jailbreak Attack and Defense](https://arxiv.org/abs/2602.04893)
*Licheng Pan,Yunsheng Lu,Jiexi Liu,Jialing Tao,Haozhe Feng,Hui Xue,Zhixuan Chu,Kui Ren*

Main category: cs.LG

TL;DR: 提出Causal Analyst框架，利用LLM和数据驱动的因果发现识别越狱攻击的直接原因，并应用于攻击增强和防御指导


<details>
  <summary>Details</summary>
Motivation: 现有研究主要分析潜在表征，忽视可解释提示特征与越狱发生之间的因果关系，需要理解越狱机制以提升LLM安全性和可靠性

Method: 提出Causal Analyst框架，整合LLM到数据驱动的因果发现中；构建包含35k越狱尝试的数据集，包含100个攻击模板和50个有害查询，标注37个人类可读提示特征；联合训练基于LLM的提示编码和基于GNN的因果图学习，重建提示特征到越狱响应的因果路径

Result: 发现特定特征如"Positive Character"和"Number of Task Steps"是越狱的直接因果驱动因素；开发Jailbreaking Enhancer显著提升攻击成功率，Guardrail Advisor利用因果图提取混淆查询的真实恶意意图；实验证实因果分析的鲁棒性和优于非因果方法的性能

Conclusion: 从因果角度分析越狱特征是提高LLM可靠性的有效且可解释的方法

Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.

</details>


### [3] [A Short and Unified Convergence Analysis of the SAG, SAGA, and IAG Algorithms](https://arxiv.org/abs/2602.05304)
*Feng Zhu,Robert W. Heath,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出统一收敛分析框架，适用于SAG、SAGA和IAG三种算法，简化证明过程并改进收敛率


<details>
  <summary>Details</summary>
Motivation: 现有随机方差缩减算法（如SAG、SAGA）和确定性算法（如IAG）的分析方法分散且复杂，需要统一的收敛分析框架来简化证明并改进理论结果

Method: 开发统一收敛分析方法，包含两个关键步骤：1）使用简单浓度工具建立随机子采样延迟界限；2）设计新颖的李雅普诺夫函数来考虑这些延迟

Result: 获得了简短模块化的证明，首次为SAG和SAGA提供了高概率界限，并可无缝扩展到非凸目标和马尔可夫采样；同时显著改进了IAG算法的最佳已知收敛率

Conclusion: 提出的统一分析框架成功简化了三种重要算法的收敛证明，提供了更优的理论保证，并为扩展到更广泛的问题设置奠定了基础

Abstract: Stochastic variance-reduced algorithms such as Stochastic Average Gradient (SAG) and SAGA, and their deterministic counterparts like the Incremental Aggregated Gradient (IAG) method, have been extensively studied in large-scale machine learning. Despite their popularity, existing analyses for these algorithms are disparate, relying on different proof techniques tailored to each method. Furthermore, the original proof of SAG is known to be notoriously involved, requiring computer-aided analysis. Focusing on finite-sum optimization with smooth and strongly convex objective functions, our main contribution is to develop a single unified convergence analysis that applies to all three algorithms: SAG, SAGA, and IAG. Our analysis features two key steps: (i) establishing a bound on delays due to stochastic sub-sampling using simple concentration tools, and (ii) carefully designing a novel Lyapunov function that accounts for such delays. The resulting proof is short and modular, providing the first high-probability bounds for SAG and SAGA that can be seamlessly extended to non-convex objectives and Markov sampling. As an immediate byproduct of our new analysis technique, we obtain the best known rates for the IAG algorithm, significantly improving upon prior bounds.

</details>


### [4] [Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability](https://arxiv.org/abs/2602.04902)
*Kingsuk Maitra*

Main category: cs.LG

TL;DR: 论文提出动量注意力机制，通过引入物理动量先验将Transformer视为物理电路，实现单层归纳头形成，突破了传统架构需要至少两层的拓扑深度限制。


<details>
  <summary>Details</summary>
Motivation: 扩展机械可解释性(MI)程序，将Transformer视为具有守恒定律和时变交流动态的物理电路，通过引入物理先验来增强模型能力，特别是解决传统架构中归纳头形成需要至少两层的拓扑约束问题。

Method: 提出动量注意力机制，通过运动学差分算子p_t = q_t - q_{t-1}嵌入物理先验，在查询和键上实现辛剪切变换。建立辛-滤波对偶性理论，证明物理剪切在数学上等价于高通滤波器。通过动量注入使模型能够直接访问速度信息。

Result: 在5100+控制实验中验证，125M动量模型在归纳密集型任务上表现优异，同时与350M基线模型的验证损失差距仅为~2.9%。发现缩放定律γ* = 4.17 × N^{-0.74}，建立了动量与深度的可替代性。实现了单层归纳能力。

Conclusion: 该框架提供了一个连接生成式AI、哈密顿物理和信号处理的互补分析工具包，通过物理动量先验突破了Transformer的拓扑限制，为模型设计提供了新的理论基础。

Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + γp_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $γ^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.

</details>


### [5] [Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates](https://arxiv.org/abs/2602.05311)
*Chengxiao Wang,Haoze Wu,Gagandeep Singh*

Main category: cs.LG

TL;DR: 提出了一种训练鲁棒神经Lyapunov屏障证书的方法，用于验证深度强化学习控制器在系统动态扰动下的安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有神经Lyapunov和屏障证书方法仅在固定理想无扰动动态下提供保证，无法应对现实应用中因不确定性导致的动态偏差，限制了其在真实世界应用中的可靠性。

Method: 1. 形式化定义鲁棒Lyapunov屏障函数；2. 基于Lipschitz连续性指定确保对有限扰动具有鲁棒性的充分条件；3. 提出通过对抗训练、Lipschitz邻域边界和全局Lipschitz正则化的实用训练目标来强制执行这些条件。

Result: 在Inverted Pendulum和2D Docking两个实际相关环境中验证，相比基线方法：1. 认证鲁棒性边界显著提升（最高达4.6倍）；2. 在强扰动下的经验成功率显著提高（最高达2.4倍）。

Conclusion: 该方法有效训练了鲁棒神经证书，能够在动态扰动下为安全强化学习提供可靠的安全性和稳定性保证，提升了深度强化学习控制器在现实应用中的可靠性。

Abstract: Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.

</details>


### [6] [Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering](https://arxiv.org/abs/2602.04903)
*Eitan Sprejer,Oscar Agustín Stanchi,María Victoria Carro,Denise Alejandra Mester,Iván Arcuschin*

Main category: cs.LG

TL;DR: 特征导向方法能有效控制LLM行为但严重损害模型性能，提示工程在行为控制与任务性能间取得最佳平衡


<details>
  <summary>Details</summary>
Motivation: 特征导向作为直接操控内部表征来控制LLM行为的方法，相比提示工程具有优势，但其在实际应用中的有效性，特别是与输出质量之间的权衡关系，尚未得到充分理解。需要实证评估特征导向方法在行为控制与模型性能之间的关键权衡。

Method: 评估Goodfire的Auto Steer特征导向方法与提示工程基线方法。使用14个导向查询（涵盖无害和安全相关行为），在171个MMLU问题上测试Llama-8B和Llama-70B模型。测量准确性、连贯性和行为控制三个指标。

Result: Auto Steer成功修改了目标行为（Llama-8B得分3.33 vs. 提示工程的2.98；Llama-70B得分3.57 vs. 3.10），但导致性能急剧下降：MMLU准确率从66%降至46%（Llama-8B）和87%降至73%（Llama-70B）；连贯性从4.62降至2.24（Llama-8B）和4.94降至3.89（Llama-70B）。简单提示工程实现了最佳整体平衡。

Conclusion: 当前特征导向方法在需要保持任务性能的实际部署中存在局限性。机械控制方法面临基本的能力-行为权衡，必须在部署前进行实证表征。提示工程在行为控制与性能保持之间提供了更实用的平衡。

Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

</details>


### [7] [DCER: Dual-Stage Compression and Energy-Based Reconstruction](https://arxiv.org/abs/2602.04904)
*Yiwen Wang,Jiahao Qin*

Main category: cs.LG

TL;DR: DCER：通过双阶段压缩和基于能量的重建解决多模态融合中的噪声输入和模态缺失问题，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 多模态融合面临两个鲁棒性挑战：噪声输入会降低表示质量，模态缺失会导致预测失败。现有方法难以同时有效处理这两个问题。

Method: 提出DCER统一框架，包含双阶段压缩和基于能量的重建。压缩阶段包括：1）模态内频率变换（音频使用小波变换，视频使用DCT）去除噪声并保留任务相关模式；2）跨模态瓶颈令牌强制真正的模态融合而非模态特定捷径。对于缺失模态，基于能量的重建通过梯度下降在学习的能量函数上恢复表示，最终能量提供内在不确定性量化。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS数据集上实现了最先进的性能，表现出U型鲁棒性模式，在完整模态和高缺失条件下都优于其他方法。能量值与预测误差的相关性达到0.72以上。

Conclusion: DCER框架有效解决了多模态融合中的噪声和模态缺失问题，通过双阶段压缩实现鲁棒表示学习，基于能量的重建提供缺失模态恢复和不确定性量化，在多模态情感分析任务中表现出优越性能。

Abstract: Multimodal fusion faces two robustness challenges: noisy inputs degrade representation quality, and missing modalities cause prediction failures. We propose DCER, a
  unified framework addressing both challenges through dual-stage compression and energy-based reconstruction. The compression stage operates at two levels:
  within-modality frequency transforms (wavelet for audio, DCT for video) remove noise while preserving task-relevant patterns, and cross-modality bottleneck tokens
  force genuine integration rather than modality-specific shortcuts. For missing modalities, energy-based reconstruction recovers representations via gradient descent
  on a learned energy function, with the final energy providing intrinsic uncertainty quantification (\r{ho} > 0.72 correlation with prediction error). Experiments on
  CMU-MOSI, CMU-MOSEI, and CH-SIMS demonstrate state-of-the-art performance across all benchmarks, with a U-shaped robustness pattern favoring multimodal fusion at
  both complete and high-missing conditions. The code will be available on Github.

</details>


### [8] [LISA: Laplacian In-context Spectral Analysis](https://arxiv.org/abs/2602.04906)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LISA是一种用于拉普拉斯时间序列模型的推理时自适应方法，仅使用观测前缀进行延迟坐标嵌入和谱学习，结合轻量级潜在空间残差适配器提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决时间序列模型在推理时适应动态变化的问题，将上下文自适应与动态系统的非参数谱方法联系起来，特别是在动态变化条件下提升预测性能。

Method: LISA结合延迟坐标嵌入和拉普拉斯谱学习生成扩散坐标状态表示，使用冻结的非线性解码器进行一步预测，并引入基于高斯过程回归或类注意力马尔可夫算子的轻量级潜在空间残差适配器。

Result: 在预测和自回归展开实验中，LISA相比冻结基线有所改进，在动态变化条件下表现最为显著，展示了上下文自适应与非参数谱方法的有效结合。

Conclusion: LISA成功地将上下文自适应与动态系统的非参数谱方法联系起来，为拉普拉斯时间序列模型提供了一种有效的推理时自适应框架，在动态变化条件下尤其有益。

Abstract: We propose Laplacian In-context Spectral Analysis (LISA), a method for inference-time adaptation of Laplacian-based time-series models using only an observed prefix. LISA combines delay-coordinate embeddings and Laplacian spectral learning to produce diffusion-coordinate state representations, together with a frozen nonlinear decoder for one-step prediction. We introduce lightweight latent-space residual adapters based on either Gaussian-process regression or an attention-like Markov operator over context windows. Across forecasting and autoregressive rollout experiments, LISA improves over the frozen baseline and is often most beneficial under changing dynamics. This work links in-context adaptation to nonparametric spectral methods for dynamical systems.

</details>


### [9] [Physics as the Inductive Bias for Causal Discovery](https://arxiv.org/abs/2602.04907)
*Jianhong Chen,Naichen Shi,Xubo Yue*

Main category: cs.LG

TL;DR: 提出一个整合物理知识的因果发现框架，将系统演化建模为随机微分方程，其中漂移项编码已知ODE动力学，扩散项对应超出预设物理的未知因果耦合。


<details>
  <summary>Details</summary>
Motivation: 因果发现通常是数据驱动的范式，而基于物理的模型如常微分方程提供了动力学过程的机制结构。整合这两种范式可以让物理知识作为归纳偏置，改善动力学系统中因果发现的可识别性、稳定性和鲁棒性。然而，这种整合仍然具有挑战性：真实动力学系统通常表现出反馈、循环相互作用和非平稳数据趋势，而许多广泛使用的因果发现方法基于无环性或平衡假设。

Method: 提出一个整合的因果发现框架，将系统演化建模为随机微分方程(SDE)，其中漂移项编码已知的ODE动力学，扩散项对应超出预设物理的未知因果耦合。开发了一个可扩展的稀疏诱导最大似然估计算法，利用因果图结构进行高效参数估计。

Result: 在具有不同因果结构的动力学系统上的实验表明，该方法改进了因果图恢复，并产生了比纯数据驱动的最先进基线更稳定、物理一致的估计。

Conclusion: 该框架成功地将物理知识作为归纳偏置整合到因果发现中，为具有复杂动态特性的真实世界系统提供了更可靠和物理一致的因果发现方法。

Abstract: Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.

</details>


### [10] [Temporal Pair Consistency for Variance-Reduced Flow Matching](https://arxiv.org/abs/2602.04908)
*Chika Maduabuchi,Jindong Wang*

Main category: cs.LG

TL;DR: TPC是一种轻量级方差减少方法，通过耦合同一概率路径上配对时间步的速度预测来降低梯度方差，无需修改模型架构、概率路径或求解器。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型（如扩散模型、流匹配、整流流）通常使用独立处理时间步的目标进行训练，导致估计器方差高和采样效率低。现有方法通过显式平滑惩罚、轨迹正则化或修改概率路径和求解器来缓解此问题。

Method: 提出时间对一致性（TPC）原则，在估计器层面耦合同一概率路径上配对时间步的速度预测。该方法不修改模型架构、概率路径或求解器，通过轨迹耦合的正则化减少梯度方差。

Result: TPC在CIFAR-10和ImageNet多个分辨率上提高了样本质量和效率，在相同或更低计算成本下获得更低的FID分数，并能无缝扩展到现代SOTA风格的噪声增强训练、基于分数的去噪和整流流管道。

Conclusion: TPC是一种有效的轻量级方差减少方法，通过时间对一致性耦合改善了连续时间生成模型的训练效率和采样质量，具有理论保证和实际性能优势。

Abstract: Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.

</details>


### [11] [Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment](https://arxiv.org/abs/2602.04909)
*Youngjae Cho,Jongsuk Kim,Ji-Hoon Kim*

Main category: cs.LG

TL;DR: GAPO提出了一种几何感知的动态锚点方法，替代DPO中的固定参考策略，通过对抗性局部扰动构建悲观基线，自适应重加权偏好对，提升噪声监督下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DPO及其相关方法使用固定参考策略进行正则化，但随着策略漂移，静态参考会变得校准失准，导致分布不匹配并放大噪声监督下的虚假偏好信号。而无参考变体虽然避免了不匹配，但常受无约束奖励漂移的影响。

Method: GAPO将固定参考替换为动态的几何感知锚点：在当前策略的小半径内构建对抗性局部扰动作为悲观基线。该锚点启用自适应重加权机制，基于每个偏好对的局部敏感性调整其重要性。引入锚点间隙（策略与其锚点间的奖励差异），在平滑条件下近似最坏情况的局部边际退化。通过该间隙加权的逻辑目标进行优化，降低几何脆弱实例的权重，同时强调稳健的偏好信号。

Result: 在多样化噪声设置下，GAPO在标准LLM对齐和推理基准测试中，一致性地提升鲁棒性，同时匹配或改进性能。

Conclusion: GAPO通过动态几何感知锚点解决了固定参考策略的校准失准问题，提供了一种自适应重加权机制，有效处理噪声监督，在保持或提升性能的同时显著增强对齐方法的鲁棒性。

Abstract: Direct Preference Optimization (DPO) and related methods align large language models from pairwise preferences by regularizing updates against a fixed reference policy. As the policy drifts, a static reference, however, can become increasingly miscalibrated, leading to distributional mismatch and amplifying spurious preference signals under noisy supervision. Conversely, reference-free variants avoid mismatch but often suffer from unconstrained reward drift. We propose Geometric Anchor Preference Optimization (GAPO), which replaces the fixed reference with a dynamic, geometry-aware anchor: an adversarial local perturbation of the current policy within a small radius that serves as a pessimistic baseline. This anchor enables an adaptive reweighting mechanism, modulating the importance of each preference pair based on its local sensitivity. We further introduce the Anchor Gap, the reward discrepancy between the policy and its anchor, and show under smoothness conditions that it approximates worst-case local margin degradation. Optimizing a logistic objective weighted by this gap downweights geometrically brittle instances while emphasizing robust preference signals. Across diverse noise settings, GAPO consistently improves robustness while matching or improving performance on standard LLM alignment and reasoning benchmarks.

</details>


### [12] [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)
*Xiaolin Hu,Hang Yuan,Xinzhu Sang,Binbin Yan,Zhou Yu,Cong Huang,Kai Chen*

Main category: cs.LG

TL;DR: A²-LLM：端到端对话音频虚拟人模型，统一处理语言、音频韵律和3D面部运动，超越传统级联架构的唇部同步，实现情感丰富的面部动画生成。


<details>
  <summary>Details</summary>
Motivation: 当前基于级联架构的对话虚拟人系统存在累积误差、高延迟和实时性差的问题，且缺乏对话上下文理解，过度依赖刚性唇部同步而忽视情感深度表达。

Method: 提出A²-LLM端到端模型，在统一框架内联合推理语言、音频韵律和3D面部运动；创建FLAME-QA高质量多模态数据集，以问答格式对齐语义意图与表情动态。

Result: 实验表明系统在保持实时效率（500ms延迟，0.7 RTF）的同时，实现了优越的情感表现力，超越了简单的唇部同步。

Conclusion: A²-LLM通过端到端统一建模解决了传统级联架构的局限性，实现了情感丰富、实时高效的对话虚拟人生成，为下一代人机交互奠定了基础。

Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent modules. These pipelines are often plagued by accumulated errors, high latency, and poor real-time performance. Lacking access to the underlying conversational context, these pipelines inherently prioritize rigid lip-sync over emotional depth. To address these challenges, we propose A$^2$-LLM, an end-to-end conversational audio avatar large language model that jointly reasons about language, audio prosody, and 3D facial motion within a unified framework. To facilitate training, we introduce FLAME-QA, a high-quality multimodal dataset designed to align semantic intent with expressive facial dynamics within a QA format. By leveraging deep semantic understanding, A$^2$-LLM generates emotionally rich facial movements beyond simple lip-synchronization. Experimental results demonstrate that our system achieves superior emotional expressiveness while maintaining real-time efficiency (500 ms latency, 0.7 RTF).

</details>


### [13] [SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel](https://arxiv.org/abs/2602.04915)
*Jose Miguel Luna,Taha Bouhsine,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: SLAY是一种基于Yat核的线性时间注意力机制，通过将查询和键约束到单位球面上，使注意力仅依赖于角度对齐，实现了与标准softmax注意力几乎无法区分的性能，同时保持线性时间和内存复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准softmax注意力机制在Transformer中具有二次时间复杂度，限制了其在长序列上的可扩展性。现有线性时间注意力方法通常存在性能下降的问题。本文旨在开发一种既能保持软注意力性能又能实现线性复杂度的注意力机制。

Method: 提出SLAY方法：1) 将查询和键约束到单位球面上，使注意力仅依赖于角度对齐；2) 基于松弛的E-Product（Yat核）构建几何感知的注意力机制；3) 利用Bernstein定理将球面Yat核表示为非负混合的多项式-指数乘积核；4) 推导严格正随机特征近似，实现线性时间O(L)注意力。

Result: 1) 证明了球面上Yat核的正定性和有界性；2) 估计器产生定义良好、非负的注意力分数；3) 经验上，SLAY性能与标准softmax注意力几乎无法区分；4) 在保持线性时间和内存扩展的同时，持续优于先前的线性时间注意力机制（如Performers和Cosformers）；5) 据作者所知，SLAY是迄今为止最接近softmax注意力的线性时间近似。

Conclusion: SLAY实现了无需典型注意力线性化性能权衡的可扩展Transformer，代表了线性时间注意力机制的重要进展，能够在不牺牲性能的情况下处理长序列。

Abstract: We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interactions in physics. Our method, Spherical Linearized Attention with Yat Kernels (SLAY), constrains queries and keys to the unit sphere so that attention depends only on angular alignment. Using Bernstein's theorem, we express the spherical Yat-kernel as a nonnegative mixture of polynomial-exponential product kernels and derive a strictly positive random-feature approximation enabling linear-time O(L) attention. We establish positive definiteness and boundedness on the sphere and show that the estimator yields well-defined, nonnegative attention scores. Empirically, SLAY achieves performance that is nearly indistinguishable from standard softmax attention while retaining linear time and memory scaling, and consistently outperforms prior linear-time attention mechanisms such as Performers and Cosformers. To the best of our knowledge, SLAY represents the closest linear-time approximation to softmax attention reported to date, enabling scalable Transformers without the typical performance trade-offs of attention linearization.

</details>


### [14] [Multi-Aspect Mining and Anomaly Detection for Heterogeneous Tensor Streams](https://arxiv.org/abs/2602.04917)
*Soshi Kakio,Yasuko Matsubara,Ren Fujiwara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: HeteroComp：一种用于异构张量流分析和群体异常检测的方法，能够处理分类和连续属性，无需离散化，使用高斯过程先验建模未知分布和时间动态。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解和异常检测方法存在两个主要局限：1）无法处理包含分类属性（如IP地址）和连续属性（如数据包长度）的异构张量流，通常需要离散化连续属性或将分类属性视为连续，这会扭曲数据的统计特性；2）离散化时间戳，无法跟踪流的时序动态（如趋势、异常事件），导致无法有效检测群体层面的异常（如DoS攻击）。

Method: 提出HeteroComp方法，将异构张量流持续汇总为表示每个属性中潜在群体及其时序动态的"组件"。使用高斯过程先验建模连续属性的未知分布和时序动态，直接从数据估计概率密度。提取的组件提供简洁有效的汇总，实现准确的群体异常检测。

Result: 在真实数据集上的广泛实验表明，HeteroComp在群体异常检测准确率上优于最先进算法，且其计算时间不依赖于数据流长度。

Conclusion: HeteroComp成功解决了现有方法在处理异构张量流和时序动态方面的局限性，通过高斯过程先验建模和组件提取，实现了对异构数据流的有效汇总和群体异常检测，具有实际应用价值。

Abstract: Analysis and anomaly detection in event tensor streams consisting of timestamps and multiple attributes - such as communication logs(time, IP address, packet length)- are essential tasks in data mining. While existing tensor decomposition and anomaly detection methods provide useful insights, they face the following two limitations. (i) They cannot handle heterogeneous tensor streams, which comprises both categorical attributes(e.g., IP address) and continuous attributes(e.g., packet length). They typically require either discretizing continuous attributes or treating categorical attributes as continuous, both of which distort the underlying statistical properties of the data.Furthermore, incorrect assumptions about the distribution family of continuous attributes often degrade the model's performance. (ii) They discretize timestamps, failing to track the temporal dynamics of streams(e.g., trends, abnormal events), which makes them ineffective for detecting anomalies at the group level, referred to as 'group anomalies' (e.g, DoS attacks). To address these challenges, we propose HeteroComp, a method for continuously summarizing heterogeneous tensor streams into 'components' representing latent groups in each attribute and their temporal dynamics, and detecting group anomalies. Our method employs Gaussian process priors to model unknown distributions of continuous attributes, and temporal dynamics, which directly estimate probability densities from data. Extracted components give concise but effective summarization, enabling accurate group anomaly detection. Extensive experiments on real datasets demonstrate that HeteroComp outperforms the state-of-the-art algorithms for group anomaly detection accuracy, and its computational time does not depend on the data stream length.

</details>


### [15] [Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution](https://arxiv.org/abs/2602.04918)
*Long Zhang,Fangwei Lin*

Main category: cs.LG

TL;DR: 论文通过几何分析揭示LLMs处理知识冲突的机制：不是通过"流形稀释"降低内部知识信号强度，而是通过"正交干扰"注入与真实方向准正交的转向向量，旋转隐藏状态表示来绕过正确解嵌入向量。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在处理上下文信息与参数记忆冲突时的机制，特别是模型如何通过顺从行为解决知识冲突，以及这种抑制是源于信号幅度稀释还是残差流中的方向性几何改变。

Method: 对Qwen-4B、Llama-3.1-8B和GLM-4-9B进行分层几何分析，将反事实上下文诱导的残差流更新分解为径向（基于范数）和角度（基于余弦）分量。

Result: 实验结果否定了"流形稀释"假说的普遍性，三个架构中有两个在事实查询性能显著下降时仍保持稳定的残差范数。顺从行为一致表现为"正交干扰"，即冲突上下文注入与真实方向准正交的转向向量，旋转隐藏状态表示。

Conclusion: 模型不会"忘记"或抑制内部真理的幅度，而是采用几何位移机制绕过正确的解嵌入向量，在保持原始结构幅度的同时模拟采纳行为。这挑战了基于标量置信度检测幻觉的方法，强调需要向量监控来区分真正的知识整合与表面的上下文模仿。

Abstract: Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the "Manifold Dilution" hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by "Orthogonal Interference," where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not "unlearn" or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.

</details>


### [16] [Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog](https://arxiv.org/abs/2602.04919)
*Yiran Zhao,Shengyang Zhou,Zijian Wu,Tongyan Hu,Yuhui Xu,Rengan Dou,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 提出了一种渐进式压缩方法PTL，通过多次细粒度迭代的剪枝-调优循环，将LLM压缩至接近原大小一半，同时保持推理性能，仅需轻量级后训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推理能力强，但参数量大导致计算资源消耗高。传统剪枝方法直接移除冗余参数会导致推理任务性能急剧下降，且需要大量后训练恢复能力。

Method: 提出渐进式压缩方法PTL，将压缩过程分为多个细粒度迭代，每个阶段应用剪枝-调优循环，逐步减小模型规模，同时通过微调恢复性能。

Result: PTL能将LLM压缩至接近原大小一半，仅需轻量级后训练即可保持与原模型相当的推理性能。该方法适用于多种剪枝策略（神经元剪枝、层剪枝）和后训练方法（持续预训练、强化学习），在数学推理、代码生成等多种任务上有效。

Conclusion: 渐进式压缩方法PTL通过"温水煮青蛙"式的迭代剪枝-调优循环，能够有效压缩LLM而不造成性能骤降，具有灵活性和广泛适用性。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but their substantial size often demands significant computational resources. To reduce resource consumption and accelerate inference, it is essential to eliminate redundant parameters without compromising performance. However, conventional pruning methods that directly remove such parameters often lead to a dramatic drop in model performance in reasoning tasks, and require extensive post-training to recover the lost capabilities. In this work, we propose a gradual compacting method that divides the compression process into multiple fine-grained iterations, applying a Prune-Tune Loop (PTL) at each stage to incrementally reduce model size while restoring performance with finetuning. This iterative approach-reminiscent of the "boiling frog" effect-enables the model to be progressively compressed without abrupt performance loss. Experimental results show that PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to the original model on reasoning tasks. Moreover, PTL is flexible and can be applied to various pruning strategies, such as neuron pruning and layer pruning, as well as different post-training methods, including continual pre-training and reinforcement learning. Additionally, experimental results confirm the effectiveness of PTL on a variety of tasks beyond mathematical reasoning, such as code generation, demonstrating its broad applicability.

</details>


### [17] [CyIN: Cyclic Informative Latent Space for Bridging Complete and Incomplete Multimodal Learning](https://arxiv.org/abs/2602.04920)
*Ronghao Lin,Qiaolin He,Sijie Mai,Ying Zeng,Aolin Xiong,Li Huang,Yap-Peng Tan,Haifeng Hu*

Main category: cs.LG

TL;DR: CyIN框架通过循环信息瓶颈和跨模态循环翻译，在统一模型中同时优化完整和不完整多模态学习，提升模型在动态缺失模态场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多模态数据的模态存在高度可变性和不可预测性，导致预训练模型在动态缺失模态情况下性能显著下降。现有方法主要针对完美配对的多模态输入训练，无法适应不完整模态场景。

Method: 提出Cyclic INformative Learning框架：1) 通过token和标签级信息瓶颈循环构建信息潜在空间，提取任务相关特征；2) 提出跨模态循环翻译，通过前向和反向传播过程用剩余模态重建缺失模态；3) 在统一模型中联合优化完整和不完整多模态学习。

Result: 在4个多模态数据集上的实验表明，该方法在完整和多种不完整场景下均表现出优越性能。

Conclusion: CyIN框架通过信息瓶颈和跨模态重建机制，有效解决了多模态学习中完整与不完整模态之间的差距，提升了模型在动态缺失模态场景下的鲁棒性和适应性。

Abstract: Multimodal machine learning, mimicking the human brain's ability to integrate various modalities has seen rapid growth. Most previous multimodal models are trained on perfectly paired multimodal input to reach optimal performance. In real-world deployments, however, the presence of modality is highly variable and unpredictable, causing the pre-trained models in suffering significant performance drops and fail to remain robust with dynamic missing modalities circumstances. In this paper, we present a novel Cyclic INformative Learning framework (CyIN) to bridge the gap between complete and incomplete multimodal learning. Specifically, we firstly build an informative latent space by adopting token- and label-level Information Bottleneck (IB) cyclically among various modalities. Capturing task-related features with variational approximation, the informative bottleneck latents are purified for more efficient cross-modal interaction and multimodal fusion. Moreover, to supplement the missing information caused by incomplete multimodal input, we propose cross-modal cyclic translation by reconstruct the missing modalities with the remained ones through forward and reverse propagation process. With the help of the extracted and reconstructed informative latents, CyIN succeeds in jointly optimizing complete and incomplete multimodal learning in one unified model. Extensive experiments on 4 multimodal datasets demonstrate the superior performance of our method in both complete and diverse incomplete scenarios.

</details>


### [18] [Imposing Boundary Conditions on Neural Operators via Learned Function Extensions](https://arxiv.org/abs/2602.04923)
*Sepehr Mousavi,Siddhartha Mishra,Laura De Lorenzis*

Main category: cs.LG

TL;DR: 提出通过函数扩展将复杂边界条件融入神经算子的通用框架，在18个挑战性数据集上实现SOTA精度


<details>
  <summary>Details</summary>
Motivation: 现有神经算子处理复杂多变边界条件能力有限，当解算子对边界强迫表现出强敏感性时容易失败，需要通用框架来处理非齐次边界条件

Method: 通过函数扩展将边界数据映射到整个空间域的潜在伪扩展，使任意标准算子学习架构都能消费边界信息，结合任意域到域神经算子同时学习复杂边界条件和输入域函数的丰富依赖关系

Result: 在Poisson、线性弹性、超弹性问题的18个挑战性数据集上实现最先进精度，大幅超越基线方法，且无需跨数据集超参数调优

Conclusion: 学习边界到域的扩展是现有神经算子框架中施加复杂边界条件的有效实用策略，能够为更广泛的PDE控制问题提供准确鲁棒的科学机器学习模型

Abstract: Neural operators have emerged as powerful surrogates for the solution of partial differential equations (PDEs), yet their ability to handle general, highly variable boundary conditions (BCs) remains limited. Existing approaches often fail when the solution operator exhibits strong sensitivity to boundary forcings. We propose a general framework for conditioning neural operators on complex non-homogeneous BCs through function extensions. Our key idea is to map boundary data to latent pseudo-extensions defined over the entire spatial domain, enabling any standard operator learning architecture to consume boundary information. The resulting operator, coupled with an arbitrary domain-to-domain neural operator, can learn rich dependencies on complex BCs and input domain functions at the same time. To benchmark this setting, we construct 18 challenging datasets spanning Poisson, linear elasticity, and hyperelasticity problems, with highly variable, mixed-type, component-wise, and multi-segment BCs on diverse geometries. Our approach achieves state-of-the-art accuracy, outperforming baselines by large margins, while requiring no hyperparameter tuning across datasets. Overall, our results demonstrate that learning boundary-to-domain extensions is an effective and practical strategy for imposing complex BCs in existing neural operator frameworks, enabling accurate and robust scientific machine learning models for a broader range of PDE-governed problems.

</details>


### [19] [Knowing When to Answer: Adaptive Confidence Refinement for Reliable Audio-Visual Question Answering](https://arxiv.org/abs/2602.04924)
*Dinh Phu Tran,Jihoon Jeong,Saad Wazir,Seongah Kim,Thao Do,Cem Subakan,Daeyoung Kim*

Main category: cs.LG

TL;DR: 提出R-AVQA问题，关注可靠的多模态问答，在不确定时选择弃权而非错误回答。提出自适应置信度修正(ACR)方法，通过残差风险头和置信度门控头增强模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有AVQA模型虽然准确率高，但在识别自身可能错误并选择弃权方面的能力尚未充分研究。需要填补可靠音频-视觉问答(R-AVQA)的研究空白，使模型在不确定时能够安全弃权而非错误回答。

Method: 提出自适应置信度修正(ACR)方法：1) 保留最大softmax概率(MSP)作为主要置信信号；2) 引入残差风险头预测MSP未捕捉的低幅度正确性残差；3) 置信度门控头判断MSP可靠性，在不可靠时应用输入自适应残差修正。

Result: ACR在分布内、分布外和数据偏差设置下，在三种不同AVQA架构上均优于现有方法。理论分析和实验验证了其有效性，为R-AVQA任务建立了坚实基础。

Conclusion: ACR是一种轻量级方法，通过自适应修正MSP置信度来增强AVQA模型的可靠性。该方法在多种设置下表现优异，为可靠的多模态问答系统提供了有效解决方案。

Abstract: We present a formal problem formulation for \textit{Reliable} Audio-Visual Question Answering ($\mathcal{R}$-AVQA), where we prefer abstention over answering incorrectly. While recent AVQA models have high accuracy, their ability to identify when they are likely wrong and their consequent abstention from answering remain underexplored areas of research. To fill this gap, we explore several approaches and then propose Adaptive Confidence Refinement (ACR), a lightweight method to further enhance the performance of $\mathcal{R}$-AVQA. Our key insight is that the Maximum Softmax Probability (MSP) is Bayes-optimal only under strong calibration, a condition usually not met in deep neural networks, particularly in multimodal models. Instead of replacing MSP, our ACR maintains it as a primary confidence signal and applies input-adaptive residual corrections when MSP is deemed unreliable. ACR introduces two learned heads: i) a Residual Risk Head that predicts low-magnitude correctness residuals that MSP does not capture, and ii) a Confidence Gating Head to determine MSP trustworthiness. Our experiments and theoretical analysis show that ACR consistently outperforms existing methods on in- and out-of-disrtibution, and data bias settings across three different AVQA architectures, establishing a solid foundation for $\mathcal{R}$-AVQA task. The code and checkpoints will be available upon acceptance \href{https://github.com/PhuTran1005/R-AVQA}{at here}

</details>


### [20] [Internalizing LLM Reasoning via Discovery and Replay of Latent Actions](https://arxiv.org/abs/2602.04925)
*Zhenning Shi,Yijia Zhu,Junhan Shi,Xun Zhang,Lei Wang,Congcong Miao*

Main category: cs.LG

TL;DR: STIR提出了一种动态潜在轨迹控制框架，通过自蒸馏工具将推理过程内化到隐藏状态，在减少token消耗的同时提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法依赖静态控制向量，无法适应复杂推理任务中隐藏状态的非平稳演化，限制了推理增强的效果。

Method: 提出三阶段框架：1) 差分内在动作诱导从潜在推理成功中提取引导基元；2) 稀疏控制基构建创建紧凑且几何多样的工具库；3) 价值调制轨迹干预通过基于锚点的门控机制动态注入上下文特定脉冲。

Result: 在6个算术和逻辑基准测试中，STIR相比原始解码平均准确率提升1.9%至7.5%，平均token消耗减少高达35%，在四个代表性模型上均表现出色。

Conclusion: 显式思维链的好处可以通过动态潜在轨迹控制实现，将推理过程内化以绕过显式生成，同时获得更高的保真度，为高效推理提供了新范式。

Abstract: The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.

</details>


### [21] [TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation](https://arxiv.org/abs/2602.04929)
*Junhan Kim,Yeo Jeong Park,Seungwoo Son,Chungman Lee,Ho-young Kim,Joonyoung Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: TurboBoA是一种新的无反向传播后训练量化算法，在保持BoA准确性的同时显著加速量化过程，通过联合量化多个输出通道、误差传播校正和自适应网格计算实现3倍以上加速，在低比特量化中达到最先进结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)的快速增长使得后训练量化(PTQ)对减少内存和计算成本变得至关重要。GPTQ虽然高效但假设层间独立导致低比特量化时准确率严重下降，而BoA虽然通过考虑注意力模块中的层间依赖关系改进了GPTQ，但其对所有输出通道的顺序量化方法效率低下。需要一种既能保持BoA准确性又能显著加速的量化方法。

Method: TurboBoA提出三种关键创新：(1) 使用闭式误差补偿规则联合量化多个输出通道，减少顺序瓶颈，实现3倍以上加速；(2) 对来自先前量化层的传播误差进行校正的机制；(3) 使用坐标下降细化的自适应网格计算，在迭代更新期间保持对齐。该方法是无反向传播的后训练量化算法。

Result: 大量实验表明，TurboBoA在保持BoA准确性的同时实现了显著加速。当与异常值抑制技术结合时，在仅权重量化和权重-激活量化中都达到了最先进的结果。代码将在GitHub上开源。

Conclusion: TurboBoA是一种高效的后训练量化算法，成功解决了BoA的效率问题，在保持准确性的同时显著加速量化过程，为大型语言模型的低比特量化提供了实用的解决方案。

Abstract: The rapid growth of large language models (LLMs) has heightened the importance of post-training quantization (PTQ) for reducing memory and computation costs. Among PTQ methods, GPTQ has gained significant attention for its efficiency, enabling billion-scale LLMs to be quantized within a few GPU hours. However, GPTQ's assumption of layer-wise independence leads to severe accuracy drops in low-bit regimes. Recently, BoA improved upon GPTQ by incorporating inter-layer dependencies within attention modules, but its reliance on sequential quantization across all out-channels makes it substantially less efficient. In this paper, we propose TurboBoA, a new backpropagation-free PTQ algorithm that preserves the accuracy benefits of BoA while significantly accelerating the process. The proposed TurboBoA introduces three key innovations: (i) joint quantization of multiple out-channels with a closed-form error compensation rule, which reduces sequential bottlenecks and yields more than a three-fold speedup; (ii) a correction mechanism for errors propagated from preceding quantized layers; and (iii) adaptive grid computation with coordinate descent refinement to maintain alignment during iterative updates. Extensive experiments demonstrate that TurboBoA delivers substantial acceleration over BoA while consistently improving accuracy. When combined with outlier suppression techniques, it achieves state-of-the-art results in both weight-only and weight-activation quantization. The code will be available at https://github.com/SamsungLabs/TurboBoA.

</details>


### [22] [Depth-Wise Emergence of Prediction-Centric Geometry in Large Language Models](https://arxiv.org/abs/2602.04931)
*Shahar Haim,Daniel C McNamee*

Main category: cs.LG

TL;DR: 解码器专用大语言模型在深度维度上存在从上下文处理到预测形成的计算阶段转换，伴随表征几何的重组，其中晚期层实现结构化几何编码，支持对token预测的选择性因果控制。


<details>
  <summary>Details</summary>
Motivation: 探究解码器专用大语言模型如何将上下文信息转化为预测的内部计算机制，特别是表征几何在预测形成过程中的作用。

Method: 采用统一框架结合几何分析和机制干预方法，分析模型深度维度上的表征变化，研究晚期层表征的几何结构及其对预测的因果控制作用。

Result: 发现模型存在从上下文处理到预测形成的深度转换阶段，晚期层表征实现结构化几何编码：角度组织参数化预测分布相似性，表征范数编码上下文特定信息但不决定预测。

Conclusion: 研究结果为LLM将上下文转化为预测的动态过程提供了机制-几何解释，揭示了表征几何在选择性因果控制中的关键作用。

Abstract: We show that decoder-only large language models exhibit a depth-wise transition from context-processing to prediction-forming phases of computation accompanied by a reorganization of representational geometry. Using a unified framework combining geometric analysis with mechanistic intervention, we demonstrate that late-layer representations implement a structured geometric code that enables selective causal control over token prediction. Specifically, angular organization of the representation geometry parametrizes prediction distributional similarity, while representation norms encode context-specific information that does not determine prediction. Together, these results provide a mechanistic-geometric account of the dynamics of transforming context into predictions in LLMs.

</details>


### [23] [Comparing Euclidean and Hyperbolic K-Means for Generalized Category Discovery](https://arxiv.org/abs/2602.04932)
*Mohamad Dalal,Thomas B. Moeslund,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: HC-GCD提出在双曲空间中直接进行聚类，而不是像先前方法那样在双曲空间学习表示后转回欧几里得空间聚类，通过双曲K-Means在洛伦兹双曲面模型上实现更好的未见类聚类性能。


<details>
  <summary>Details</summary>
Motivation: 先前双曲广义类别发现方法仅在表示学习阶段使用双曲几何，而在聚类阶段转回欧几里得空间，作者认为这种混合几何方法不是最优的，因为双曲空间更适合捕捉数据的层次结构。

Method: 提出HC-GCD方法，在洛伦兹双曲面模型的双曲空间中学习嵌入表示，并使用双曲K-Means算法直接在双曲空间中进行聚类，避免了向欧几里得空间的转换。

Result: 在语义转移基准数据集上，HC-GCD与先前最先进的双曲GCD方法性能相当；双曲K-Means比欧几里得K-Means获得更好的准确率；欧几里得嵌入范数裁剪对未见类和已见类准确率有不同影响；双曲K-Means在不同标签粒度下产生更一致的聚类结果。

Conclusion: 直接在双曲空间进行聚类比混合几何方法更优，双曲K-Means能提高聚类准确性和一致性，为广义类别发现任务提供了更合适的几何框架。

Abstract: Hyperbolic representation learning has been widely used to extract implicit hierarchies within data, and recently it has found its way to the open-world classification task of Generalized Category Discovery (GCD). However, prior hyperbolic GCD methods only use hyperbolic geometry for representation learning and transform back to Euclidean geometry when clustering. We hypothesize this is suboptimal. Therefore, we present Hyperbolic Clustered GCD (HC-GCD), which learns embeddings in the Lorentz Hyperboloid model of hyperbolic geometry, and clusters these embeddings directly in hyperbolic space using a hyperbolic K-Means algorithm. We test our model on the Semantic Shift Benchmark datasets, and demonstrate that HC-GCD is on par with the previous state-of-the-art hyperbolic GCD method. Furthermore, we show that using hyperbolic K-Means leads to better accuracy than Euclidean K-Means. We carry out ablation studies showing that clipping the norm of the Euclidean embeddings leads to decreased accuracy in clustering unseen classes, and increased accuracy for seen classes, while the overall accuracy is dataset dependent. We also show that using hyperbolic K-Means leads to more consistent clusters when varying the label granularity.

</details>


### [24] [Transolver-3: Scaling Up Transformer Solvers to Industrial-Scale Geometries](https://arxiv.org/abs/2602.04940)
*Hang Zhou,Haixu Wu,Haonan Shangguan,Yuezhou Ma,Huikun Weng,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Transolver-3是一个高度可扩展的神经PDE求解器框架，通过架构优化和训练策略，能够处理超过1.6亿个单元的工业规模网格，解决高分辨率网格内存复杂度的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习在神经PDE求解器方面展现出变革性潜力，但将其扩展到工业规模几何体（超过10^8个单元）面临根本性挑战，主要原因是高分辨率网格处理的内存复杂度过高。

Method: 提出两种关键架构优化：1) 利用矩阵乘法结合律实现更快的切片和反切片；2) 几何切片平铺以划分物理状态计算。结合两种策略：1) 在原始高分辨率网格的随机子集上进行摊销训练；2) 推理期间的物理状态缓存技术。

Result: Transolver-3能够处理超过1.6亿个单元的网格，在三个具有挑战性的仿真基准测试中（包括飞机和汽车设计任务）表现出色，实现了工业规模网格的高保真场预测。

Conclusion: Transolver-3通过创新的架构优化和训练策略，成功解决了神经PDE求解器在工业规模应用中的内存限制问题，为复杂工程任务的高保真物理仿真提供了可行的解决方案。

Abstract: Deep learning has emerged as a transformative tool for the neural surrogate modeling of partial differential equations (PDEs), known as neural PDE solvers. However, scaling these solvers to industrial-scale geometries with over $10^8$ cells remains a fundamental challenge due to the prohibitive memory complexity of processing high-resolution meshes. We present Transolver-3, a new member of the Transolver family as a highly scalable framework designed for high-fidelity physics simulations. To bridge the gap between limited GPU capacity and the resolution requirements of complex engineering tasks, we introduce two key architectural optimizations: faster slice and deslice by exploiting matrix multiplication associative property and geometry slice tiling to partition the computation of physical states. Combined with an amortized training strategy by learning on random subsets of original high-resolution meshes and a physical state caching technique during inference, Transolver-3 enables high-fidelity field prediction on industrial-scale meshes. Extensive experiments demonstrate that Transolver-3 is capable of handling meshes with over 160 million cells, achieving impressive performance across three challenging simulation benchmarks, including aircraft and automotive design tasks.

</details>


### [25] [Privileged Information Distillation for Language Models](https://arxiv.org/abs/2602.04942)
*Emiliano Penaloza,Dheeraj Vattikonda,Nicolas Gontier,Alexandre Lacoste,Laurent Charlin,Massimo Caccia*

Main category: cs.LG

TL;DR: π-Distill和OPSD：两种利用训练时特权信息蒸馏前沿智能体模型的方法，在仅能获取动作轨迹的情况下超越需要完整思维链监督的标准方法


<details>
  <summary>Details</summary>
Motivation: 在蒸馏前沿模型用于多轮智能体环境时，闭源系统通常隐藏内部推理过程，仅暴露动作轨迹，这使得标准蒸馏流程失效，因为成功行为可观察但推理过程不可见。需要开发能够在仅有动作轨迹作为特权信息的情况下有效蒸馏模型的方法。

Method: 提出两种方法：1) π-Distill：联合师生目标函数，同时训练特权信息条件化的教师模型和无条件的同一学生模型；2) OPSD：基于策略的自蒸馏，使用强化学习配合学生与特权信息条件化教师之间的反向KL惩罚项进行训练。

Result: π-Distill和在某些情况下的OPSD，在多个智能体基准测试、模型和特权信息形式下，都优于假设能获取完整思维链监督的行业标准实践（监督微调后接强化学习）。π-Distill在仅使用动作轨迹作为特权信息的情况下能有效蒸馏前沿智能体。

Conclusion: π-Distill和OPSD为解决在仅有动作轨迹作为特权信息的情况下蒸馏前沿智能体模型的问题提供了有效解决方案，π-Distill表现稳定，OPSD在某些情况下具有竞争力，为强化学习在困难、长视野设置中的应用提供了有力工具。

Abstract: Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that π-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.

</details>


### [26] [ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation](https://arxiv.org/abs/2602.05051)
*Songyuan Zhang,Oswin So,H. M. Sabbir Ahmad,Eric Yang Yu,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: ReFORM是一种基于流策略的离线强化学习方法，通过构造强制执行支持约束，避免分布外误差，同时保持策略表达能力。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中存在两个关键挑战：1）分布外误差问题，当策略偏离训练分布时产生；2）最优策略分布可能是多模态且难以表示。现有方法通过惩罚统计距离项来保持策略接近行为策略，但这限制了策略改进且不能完全防止分布外动作。

Method: ReFORM采用流策略方法：1）学习一个有界源分布的行为克隆流策略，捕捉动作分布的支持；2）优化一个反射流，为BC流生成有界噪声同时保持支持，以最大化性能。该方法通过构造强制执行较宽松的支持约束。

Result: 在OGBench基准的40个具有不同质量数据集的挑战性任务上，ReFORM使用恒定超参数集在所有任务中，在性能曲线图上优于所有经过手动调优超参数的基线方法。

Conclusion: ReFORM通过基于流策略的方法有效解决了离线RL中的分布外误差问题，同时保持了策略的表达能力，在多个任务上展现出优越性能。

Abstract: Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.

</details>


### [27] [Near-Optimal Dynamic Matching via Coarsening with Application to Heart Transplantation](https://arxiv.org/abs/2602.04989)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 该论文提出了一种基于粗化方法的在线匹配算法，通过将离线节点聚合成有容量的簇，在器官分配等应用中实现了接近最优的理论保证。


<details>
  <summary>Details</summary>
Motivation: 在线匹配在互联网广告和器官分配等领域应用广泛，但现有实用算法往往缺乏强有力的理论保证。本文旨在通过开发基于粗化方法的新算法来弥补这一差距，为数据驱动的启发式方法提供理论依据。

Method: 采用粗化方法，将离线节点聚合成有容量的簇，开发新的在线匹配算法。该方法应用于心脏移植分配，基于历史数据的结构特性构建理论上有保障的策略。

Result: 在现实模拟中，该策略的表现接近全知基准（omniscient benchmark）。研究为器官分配中基于聚类的方法提供了严格的理论依据，弥合了数据驱动启发式方法与悲观理论下界之间的差距。

Conclusion: 尽管粗化通常意味着粒度损失，但本文证明将离线节点聚合成有容量的簇能够产生接近最优的理论保证。该工作为在线匹配领域提供了理论严谨的实用算法，特别在器官分配等关键应用中具有重要价值。

Abstract: Online matching has been a mainstay in domains such as Internet advertising and organ allocation, but practical algorithms often lack strong theoretical guarantees. We take an important step toward addressing this by developing new online matching algorithms based on a coarsening approach. Although coarsening typically implies a loss of granularity, we show that, to the contrary, aggregating offline nodes into capacitated clusters can yield near-optimal theoretical guarantees. We apply our methodology to heart transplant allocation to develop theoretically grounded policies based on structural properties of historical data. In realistic simulations, our policy closely matches the performance of the omniscient benchmark. Our work bridges the gap between data-driven heuristics and pessimistic theoretical lower bounds, and provides rigorous justification for prior clustering-based approaches in organ allocation.

</details>


### [28] [Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives](https://arxiv.org/abs/2602.04990)
*Ioannis Anagnostides,Itai Zilberstein,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 器官分配不仅是静态优化问题，更是涉及多方参与者的复杂博弈，需要关注激励机制设计


<details>
  <summary>Details</summary>
Motivation: 当前器官分配系统正从基于规则转向机器学习优化，但忽视了激励机制这一根本障碍。分配过程涉及移植中心、临床医生和监管机构等多方参与者，存在激励错配问题

Method: 这是一篇立场论文，以美国成人心脏移植分配为案例，识别决策流程中的关键激励错配，并通过数据展示其负面影响。提出将机制设计、策略分类、因果推断和社会选择等方法整合到分配政策设计中

Result: 识别出当前器官分配系统中的激励错配问题，并证明这些错配正在产生不利后果。提出了"激励感知"的下一代分配政策理念

Conclusion: 器官分配应被视为复杂博弈而非静态优化问题。机器学习社区需要整合机制设计等方法来开发能够应对策略行为的分配系统，确保鲁棒性、效率和公平性

Abstract: The allocation of scarce donor organs constitutes one of the most consequential algorithmic challenges in healthcare. While the field is rapidly transitioning from rigid, rule-based systems to machine learning and data-driven optimization, we argue that current approaches often overlook a fundamental barrier: incentives. In this position paper, we highlight that organ allocation is not merely a static optimization problem, but rather a complex game involving transplant centers, clinicians, and regulators. Focusing on US adult heart transplant allocation, we identify critical incentive misalignments across the decision-making pipeline, and present data showing that they are having adverse consequences today. Our main position is that the next generation of allocation policies should be incentive aware. We outline a research agenda for the machine learning community, calling for the integration of mechanism design, strategic classification, causal inference, and social choice to ensure robustness, efficiency, and fairness in the face of strategic behavior from the various constituent groups.

</details>


### [29] [Constrained Group Relative Policy Optimization](https://arxiv.org/abs/2602.05863)
*Roger Girgis,Rodrigue de Schaetzen,Luke Rowe,Azalée Robitaille,Christopher Pal,Liam Paull*

Main category: cs.LG

TL;DR: 提出Constrained GRPO方法，通过拉格朗日松弛将行为约束集成到GRPO框架中，解决了多组件优势估计导致的约束学习失效问题


<details>
  <summary>Details</summary>
Motivation: 虽然GRPO已成为可扩展的无critic策略学习框架，但在具有显式行为约束的场景中应用不足。需要将约束优化扩展到GRPO框架中，特别是在依赖大型多模态基础模型的具身AI领域

Method: 提出Constrained GRPO方法：1) 使用指示器成本函数指定约束，通过拉格朗日松弛直接优化违反率；2) 发现多组件优势估计会破坏约束学习（组件间标准差不匹配扭曲不同目标项的相对重要性）；3) 提出标量化优势构造方法，保持奖励和约束项之间的预期权衡

Result: 1) 在玩具网格世界中验证了预测的优化病理，证明标量化优势恢复了稳定的约束控制；2) 在机器人任务中评估，Constrained GRPO提高了约束满足度同时增加了任务成功率

Conclusion: Constrained GRPO为具身AI领域的约束策略优化提供了一个简单有效的方案，特别是在日益依赖大型多模态基础模型的场景中，能够有效平衡任务成功和约束满足

Abstract: While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.

</details>


### [30] [EntRGi: Entropy Aware Reward Guidance for Diffusion Language Models](https://arxiv.org/abs/2602.05000)
*Atula Tejaswi,Litu Rout,Constantine Caramanis,Sanjay Shakkottai,Sujay Sanghavi*

Main category: cs.LG

TL;DR: EntRGi：一种用于离散扩散语言模型的熵感知奖励引导方法，通过动态调节奖励模型梯度，避免连续松弛和离散令牌优化问题


<details>
  <summary>Details</summary>
Motivation: 连续扩散模型的奖励引导在测试时适应中取得了巨大成功，但将其应用于离散扩散语言模型面临挑战：无法直接对离散令牌进行微分。现有方法要么使用连续松弛（但奖励模型未训练过连续输入），要么采用直通估计器（在离散令牌处评估梯度却更新连续logits，导致优化错误）。

Method: 提出EntRGi（熵感知奖励引导），通过基于模型置信度动态调节奖励模型梯度的新机制。该方法使用连续松弛但通过调制机制确保奖励模型获得可靠输入，避免梯度退化问题。

Result: 在7B参数的扩散语言模型上，针对3个不同的奖励模型和3个多技能基准进行了实证验证，结果显示相比最先进方法有持续改进。

Conclusion: EntRGi通过动态梯度调节机制，有效解决了离散扩散语言模型中奖励引导的连续松弛与离散优化之间的权衡问题，为离散扩散模型的测试时适应提供了更可靠的解决方案。

Abstract: Reward guidance has been applied to great success in the test-time adaptation of continuous diffusion models; it updates each denoising step using the gradients from a downstream reward model. We study reward guidance for discrete diffusion language models, where one cannot differentiate through the natural outputs of the model because they are discrete tokens. Existing approaches either replace these discrete tokens with continuous relaxations, or employ techniques like the straight-through estimator. In this work, we show the downsides of both these methods. The former degrades gradient feedback because the reward model has never been trained with continuous inputs. The latter involves incorrect optimization because the gradient evaluated at discrete tokens is used to update continuous logits. Our key innovation is to go beyond this tradeoff by introducing a novel mechanism called EntRGi: Entropy aware Reward Guidance that dynamically regulates the gradients from the reward model. By modulating the continuous relaxation using the model's confidence, our approach substantially improves reward guidance while providing reliable inputs to the reward model. We empirically validate our approach on a 7B-parameter diffusion language model across 3 diverse reward models and 3 multi-skill benchmarks, showing consistent improvements over state-of-the-art methods.

</details>


### [31] [Enhanced QKNorm normalization for neural transformers with the Lp norm](https://arxiv.org/abs/2602.05006)
*Ezequiel Lopez-Rubio,Javier Montes-Perez,Esteban Jose Palomo*

Main category: cs.LG

TL;DR: 提出了一种基于Lp范数的QKNorm归一化方案推广，允许使用非欧几里得范数来稳定Transformer中的查询和键向量学习。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中查询和键向量的归一化对于确保学习稳定性至关重要，无论这些向量的尺度如何。现有的归一化方法有限，需要更通用的方案来适应不同的范数类型。

Method: 提出了一种基于Lp范数的QKNorm归一化方案的推广方法。该方法允许使用非欧几里得范数，扩展了归一化的灵活性，使其能够适应不同的距离度量空间。

Result: 实验结果表明，该方法在简单问题上表现出适用性，验证了基于Lp范数的归一化方案的有效性。

Conclusion: 基于Lp范数的QKNorm归一化推广方案为Transformer架构提供了更灵活的归一化选择，特别是在需要非欧几里得范数的应用场景中具有潜力。

Abstract: The normalization of query and key vectors is an essential part of the Transformer architecture. It ensures that learning is stable regardless of the scale of these vectors. Some normalization approaches are available. In this preliminary work, a generalization of the QKNorm normalization scheme is proposed. The approach is based on the Lp norm, allowing non-Euclidean norms to be employed. Experimental results demonstrate the suitability of the method for a simple problem.

</details>


### [32] [Private PoEtry: Private In-Context Learning via Product of Experts](https://arxiv.org/abs/2602.05012)
*Rob Romijnders,Mohammad Mahdi Derakhshani,Jonathan Petit,Max Welling,Christos Louizos,Yuki M. Asano*

Main category: cs.LG

TL;DR: 提出一种基于专家乘积模型的理论框架，用于保护隐私的上下文学习，显著提升准确率同时保持强隐私保证


<details>
  <summary>Details</summary>
Motivation: 上下文学习中的示例可能包含隐私敏感信息，现有差分隐私方法要么计算成本高，要么基于启发式方法效果有限

Method: 通过专家乘积模型重新构建隐私保护的上下文学习，提供理论框架，算法可轻松并行化

Result: 在文本分类、数学和视觉语言五个数据集上评估，相比现有DP-ICL方法平均准确率提升超过30个百分点

Conclusion: 提出的专家乘积模型框架为隐私保护的上下文学习提供了理论基础，在保持强隐私保证的同时显著提升了性能

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks with only a small set of examples at inference time, thereby avoiding task-specific fine-tuning. However, in-context examples may contain privacy-sensitive information that should not be revealed through model outputs. Existing differential privacy (DP) approaches to ICL are either computationally expensive or rely on heuristics with limited effectiveness, including context oversampling, synthetic data generation, or unnecessary thresholding. We reformulate private ICL through the lens of a Product-of-Experts model. This gives a theoretically grounded framework, and the algorithm can be trivially parallelized. We evaluate our method across five datasets in text classification, math, and vision-language. We find that our method improves accuracy by more than 30 percentage points on average compared to prior DP-ICL methods, while maintaining strong privacy guarantees.

</details>


### [33] [A Simple Reduction Scheme for Constrained Contextual Bandits with Adversarial Contexts via Regression](https://arxiv.org/abs/2602.05019)
*Dhruv Sarkar,Abhishek Sinha*

Main category: cs.LG

TL;DR: 提出一种基于SquareCB框架的模块化算法方案，用于对抗性上下文环境下的约束上下文赌博机问题，通过在线回归预言机将约束问题转化为自适应定义替代奖励函数的无约束问题


<details>
  <summary>Details</summary>
Motivation: 研究对抗性选择上下文环境下的约束上下文赌博机问题，关注在预算耗尽后仍持续运行的连续设置中同时控制遗憾和累积约束违反的目标

Method: 基于Foster等人(2018)的SquareCB框架，提出模块化算法方案，利用在线回归预言机将约束问题转化为具有自适应定义替代奖励函数的无约束上下文赌博机问题

Result: 相比大多数关注随机上下文的先前工作，该简化方法为更一般的对抗性上下文设置提供了改进的保证，同时具有紧凑透明的分析

Conclusion: 通过将约束上下文赌博机问题简化为无约束问题，为对抗性上下文环境提供了有效的解决方案，实现了对遗憾和约束违反的同时控制

Abstract: We study constrained contextual bandits (CCB) with adversarially chosen contexts, where each action yields a random reward and incurs a random cost. We adopt the standard realizability assumption: conditioned on the observed context, rewards and costs are drawn independently from fixed distributions whose expectations belong to known function classes. We consider the continuing setting, in which the algorithm operates over the entire horizon even after the budget is exhausted. In this setting, the objective is to simultaneously control regret and cumulative constraint violation. Building on the seminal SquareCB framework of Foster et al. (2018), we propose a simple and modular algorithmic scheme that leverages online regression oracles to reduce the constrained problem to a standard unconstrained contextual bandit problem with adaptively defined surrogate reward functions. In contrast to most prior work on CCB, which focuses on stochastic contexts, our reduction yields improved guarantees for the more general adversarial context setting, together with a compact and transparent analysis.

</details>


### [34] [Laws of Learning Dynamics and the Core of Learners](https://arxiv.org/abs/2602.05026)
*Inkee Jung,Siu Cheong Lau*

Main category: cs.LG

TL;DR: 论文提出基于熵的终身集成学习方法，通过免疫机制防御对抗攻击，在CIFAR-10上优于简单平均集成


<details>
  <summary>Details</summary>
Motivation: 针对对抗攻击防御问题，需要开发能够持续学习并有效集成不同模型的专业知识的终身学习方法，以提升对抗样本下的鲁棒性

Method: 基于学习动力学的基本定律（守恒定律和总熵减少），提出熵基终身集成学习方法，构建免疫机制防御基于迁移的对抗攻击

Result: 在CIFAR-10数据集上，相比简单平均干净样本和对抗样本专用模型的朴素集成，提出的logifold方法在大多数测试案例中达到更高准确率，在强扰动下优势尤其明显

Conclusion: 熵基终身集成学习框架能有效提升模型对抗攻击的鲁棒性，特别是在强扰动场景下表现优异，为防御对抗攻击提供了新思路

Abstract: We formulate the fundamental laws governing learning dynamics, namely the conservation law and the decrease of total entropy. Within this framework, we introduce an entropy-based lifelong ensemble learning method. We evaluate its effectiveness by constructing an immunization mechanism to defend against transfer-based adversarial attacks on the CIFAR-10 dataset. Compared with a naive ensemble formed by simply averaging models specialized on clean and adversarial samples, the resulting logifold achieves higher accuracy in most test cases, with particularly large gains under strong perturbations.

</details>


### [35] [Laplacian Representations for Decision-Time Planning](https://arxiv.org/abs/2602.05031)
*Dikshant Shehmar,Matthew Schlegel,Matthew E. Taylor,Marlos C. Machado*

Main category: cs.LG

TL;DR: ALPS是一种基于拉普拉斯表示的分层规划算法，在离线目标条件强化学习任务中优于常用基线方法


<details>
  <summary>Details</summary>
Motivation: 基于模型的强化学习中，决策时规划仍然是一个关键挑战。状态表示必须支持局部成本计算同时保持长时程结构，而现有方法在这方面存在不足

Method: 提出使用拉普拉斯表示作为规划的有效潜在空间，该表示能捕获多时间尺度的状态空间距离。基于此构建ALPS分层规划算法，将长时程问题分解为子目标

Result: 在OGBench基准测试的离线目标条件强化学习任务中，ALPS算法优于常用基线方法，该基准此前主要由无模型方法主导

Conclusion: 拉普拉斯表示为规划提供了有效的潜在空间，能够捕获状态空间距离并缓解长预测时程中的误差累积问题，分层规划方法在基于模型的强化学习中具有优势

Abstract: Planning with a learned model remains a key challenge in model-based reinforcement learning (RL). In decision-time planning, state representations are critical as they must support local cost computation while preserving long-horizon structure. In this paper, we show that the Laplacian representation provides an effective latent space for planning by capturing state-space distances at multiple time scales. This representation preserves meaningful distances and naturally decomposes long-horizon problems into subgoals, also mitigating the compounding errors that arise over long prediction horizons. Building on these properties, we introduce ALPS, a hierarchical planning algorithm, and demonstrate that it outperforms commonly used baselines on a selection of offline goal-conditioned RL tasks from OGBench, a benchmark previously dominated by model-free methods.

</details>


### [36] [Causal Representation Meets Stochastic Modeling under Generic Geometry](https://arxiv.org/abs/2602.05033)
*Jiaxu Ren,Yixin Wang,Biwei Huang*

Main category: cs.LG

TL;DR: 该论文提出了MUTATE框架，用于从连续时间潜在随机点过程中学习可识别的因果表示，解决了现有方法主要关注i.i.d.或离散时间过程的局限性。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中学习有意义的因果表示对于机器学习应用和科学发现至关重要。现有可识别性方法主要针对i.i.d.或离散时间过程，但许多现实场景需要识别连续时间随机过程（如多元点过程）中的潜在变量。

Method: 开发了MUTATE框架，这是一个可识别的变分自编码器框架，包含时间自适应转换模块来推断随机动态。通过分析参数空间的几何结构来研究其可识别性。

Result: 在模拟和实证研究中，MUTATE能够有效回答科学问题，如基因组学中突变的积累和神经元放电触发机制对时变动态的响应。

Conclusion: 该研究扩展了因果表示学习的可识别性范围到连续时间潜在随机点过程，为处理现实世界连续时间动态系统提供了有效工具。

Abstract: Learning meaningful causal representations from observations has emerged as a crucial task for facilitating machine learning applications and driving scientific discoveries in fields such as climate science, biology, and physics. This process involves disentangling high-level latent variables and their causal relationships from low-level observations. Previous work in this area that achieves identifiability typically focuses on cases where the observations are either i.i.d. or follow a latent discrete-time process. Nevertheless, many real-world settings require identifying latent variables that are continuous-time stochastic processes (e.g., multivariate point processes). To this end, we develop identifiable causal representation learning for continuous-time latent stochastic point processes. We study its identifiability by analyzing the geometry of the parameter space. Furthermore, we develop MUTATE, an identifiable variational autoencoder framework with a time-adaptive transition module to infer stochastic dynamics. Across simulated and empirical studies, we find that MUTATE can effectively answer scientific questions, such as the accumulation of mutations in genomics and the mechanisms driving neuron spike triggers in response to time-varying dynamics.

</details>


### [37] [Feedback Control for Multi-Objective Graph Self-Supervision](https://arxiv.org/abs/2602.05036)
*Karish Grover,Theodore Vasiloudis,Han Xie,Sixing Lu,Xiang Song,Christos Faloutsos*

Main category: cs.LG

TL;DR: ControlG：基于控制理论的图自监督学习多任务协调框架，通过时间分配而非权重混合解决目标冲突问题


<details>
  <summary>Details</summary>
Motivation: 图自监督学习存在多种预训练目标（互信息、重构、对比学习等），但多目标组合时会出现目标干扰和训练不稳定问题。传统的逐更新混合方法导致三个失败模式：分歧（冲突导致的负迁移）、漂移（目标效用的非平稳性）和干旱（未充分服务目标的隐藏饥饿）。需要从根本上重新思考多目标协调问题。

Method: ControlG将多目标图自监督学习重新定义为反馈控制的时间分配问题。框架包含三个核心组件：1) 估计每个目标的难度和成对拮抗关系；2) 通过帕累托感知的对数超体积规划器规划目标预算；3) 使用比例-积分-微分(PID)控制器进行调度。该方法决定何时为每个目标分配优化预算，而不仅仅是权衡权重。

Result: 在9个数据集上，ControlG一致优于最先进的基线方法，同时生成可审计的调度方案，揭示哪些目标驱动了学习过程。

Conclusion: 多目标图自监督学习的协调本质上是一个时间分配问题，而非简单的权重混合。ControlG通过控制理论框架有效解决了目标冲突问题，提供了可解释的优化调度方案，为多任务自监督学习提供了新的协调范式。

Abstract: Can multi-task self-supervised learning on graphs be coordinated without the usual tug-of-war between objectives? Graph self-supervised learning (SSL) offers a growing toolbox of pretext objectives: mutual information, reconstruction, contrastive learning; yet combining them reliably remains a challenge due to objective interference and training instability. Most multi-pretext pipelines use per-update mixing, forcing every parameter update to be a compromise, leading to three failure modes: Disagreement (conflict-induced negative transfer), Drift (nonstationary objective utility), and Drought (hidden starvation of underserved objectives). We argue that coordination is fundamentally a temporal allocation problem: deciding when each objective receives optimization budget, not merely how to weigh them. We introduce ControlG, a control-theoretic framework that recasts multi-objective graph SSL as feedback-controlled temporal allocation by estimating per-objective difficulty and pairwise antagonism, planning target budgets via a Pareto-aware log-hypervolume planner, and scheduling with a Proportional-Integral-Derivative (PID) controller. Across 9 datasets, ControlG consistently outperforms state-of-the-art baselines, while producing an auditable schedule that reveals which objectives drove learning.

</details>


### [38] [Learning, Solving and Optimizing PDEs with TensorGalerkin: an efficient high-performance Galerkin assembly algorithm](https://arxiv.org/abs/2602.05052)
*Shizheng Wen,Mingyuan Chi,Tianwei Yu,Ben Moseley,Mike Yan Michelis,Pu Ren,Hao Sun,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出一个统一的算法框架，用于求解具有变分结构的PDE的数值解、约束优化和物理信息学习，基于Galerkin离散化和高效的TensorGalerkin张量化系统组装技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理具有变分结构的PDE时缺乏统一的框架，无法同时高效支持数值求解、约束优化和物理信息学习。需要开发一个既能保持高计算效率，又能支持多种下游应用的通用框架。

Method: 基于Galerkin离散化变分形式，提出TensorGalerkin框架：通过Python级Map阶段张量化单元操作，然后使用稀疏矩阵乘法在网格诱导的稀疏图上进行全局归约，实现高效的线性系统组装（刚度矩阵和载荷向量）。该框架支持GPU加速，可无缝用于数值PDE求解、PDE约束优化和物理信息算子学习。

Result: 在2D和3D椭圆、抛物和双曲PDE的非结构化网格上进行了多组基准测试，证明该框架在所有目标下游应用中相比多种基线方法都提供了显著的计算效率和精度提升。

Conclusion: 提出的统一框架通过TensorGalerkin技术实现了高效的PDE数值求解、约束优化和物理信息学习，为具有变分结构的PDE问题提供了一个通用、高效且可扩展的解决方案。

Abstract: We present a unified algorithmic framework for the numerical solution, constrained optimization, and physics-informed learning of PDEs with a variational structure. Our framework is based on a Galerkin discretization of the underlying variational forms, and its high efficiency stems from a novel highly-optimized and GPU-compliant TensorGalerkin framework for linear system assembly (stiffness matrices and load vectors). TensorGalerkin operates by tensorizing element-wise operations within a Python-level Map stage and then performs global reduction with a sparse matrix multiplication that performs message passing on the mesh-induced sparsity graph. It can be seamlessly employed downstream as i) a highly-efficient numerical PDEs solver, ii) an end-to-end differentiable framework for PDE-constrained optimization, and iii) a physics-informed operator learning algorithm for PDEs. With multiple benchmarks, including 2D and 3D elliptic, parabolic, and hyperbolic PDEs on unstructured meshes, we demonstrate that the proposed framework provides significant computational efficiency and accuracy gains over a variety of baselines in all the targeted downstream applications.

</details>


### [39] [Quantile-Physics Hybrid Framework for Safe-Speed Recommendation under Diverse Weather Conditions Leveraging Connected Vehicle and Road Weather Information Systems Data](https://arxiv.org/abs/2602.05053)
*Wen Zhang,Adel W. Sadek,Chunming Qiao*

Main category: cs.LG

TL;DR: 提出混合预测框架，结合QRF模型和物理约束，为高速公路在不同天气条件下推荐实时安全速度区间


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件严重影响驾驶员能见度和轮胎-路面摩擦力，需要调整安全驾驶速度以降低事故风险

Method: 使用高分辨率CV和RWIS数据构建时空对齐数据集；采用QRF模型预测10分钟窗口内的车速分布；结合物理约束计算基于实时路面附着力和能见度的速度上限；融合QRF预测分位数、限速标志和物理上限得到最终推荐区间

Result: QRF模型MAE为1.55 mph，96.43%的中位数速度预测误差在5 mph内，PICP(50%)为48.55%；模型对不同天气类型具有良好泛化能力

Conclusion: 该框架能响应天气变化并在不同路段间泛化，有望实际部署以改善交通安全并减少天气相关事故

Abstract: Inclement weather conditions can significantly impact driver visibility and tire-road surface friction, requiring adjusted safe driving speeds to reduce crash risk. This study proposes a hybrid predictive framework that recommends real-time safe speed intervals for freeway travel under diverse weather conditions. Leveraging high-resolution Connected Vehicle (CV) data and Road Weather Information System (RWIS) data collected in Buffalo, NY, from 2022 to 2023, we construct a spatiotemporally aligned dataset containing over 6.6 million records across 73 days. The core model employs Quantile Regression Forests (QRF) to estimate vehicle speed distributions in 10-minute windows, using 26 input features that capture meteorological, pavement, and temporal conditions. To enforce safety constraints, a physics-based upper speed limit is computed for each interval based on real-time road grip and visibility, ensuring that vehicles can safely stop within their sight distance. The final recommended interval fuses QRF-predicted quantiles with both posted speed limits and the physics-derived upper bound. Experimental results demonstrate strong predictive performance: the QRF model achieves a mean absolute error of 1.55 mph, with 96.43% of median speed predictions within 5 mph, a PICP (50%) of 48.55%, and robust generalization across weather types. The model's ability to respond to changing weather conditions and generalize across road segments shows promise for real-world deployment, thereby improving traffic safety and reducing weather-related crashes.

</details>


### [40] [Does SGD Seek Flatness or Sharpness? An Exactly Solvable Model](https://arxiv.org/abs/2602.05065)
*Yizhou Xu,Pierfrancesco Beneventano,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: SGD的平坦性偏好并非固有，而是由数据分布决定，特别是标签噪声的各向同性程度决定平坦或尖锐解的选择。


<details>
  <summary>Details</summary>
Motivation: 现有理论和实证研究对SGD在训练中偏好平坦还是尖锐解存在矛盾证据，需要澄清SGD的平坦性寻求行为及其因果机制。

Method: 构建并精确求解一个解析可解的模型，该模型在训练中同时表现出平坦化和尖锐化行为；通过分析SGD对梯度波动最小化的偏好，而非对平坦性的先验偏好。

Result: SGD训练本身没有平坦性偏好，只偏好最小化梯度波动；收敛时的尖锐度由数据分布唯一决定；平坦最小值仅在标签噪声在所有输出维度上各向同性时才被偏好；当标签噪声各向异性时，模型偏好尖锐度并可能收敛到任意尖锐解，取决于标签噪声谱的不平衡程度。

Conclusion: SGD的平坦性寻求行为是数据驱动的，而非算法固有特性；标签噪声的各向同性程度是决定平坦或尖锐解选择的关键因素，这一发现在MLP、RNN和Transformer等不同架构的受控实验中得到了验证。

Abstract: A large body of theory and empirical work hypothesizes a connection between the flatness of a neural network's loss landscape during training and its performance. However, there have been conceptually opposite pieces of evidence regarding when SGD prefers flatter or sharper solutions during training. In this work, we partially but causally clarify the flatness-seeking behavior of SGD by identifying and exactly solving an analytically solvable model that exhibits both flattening and sharpening behavior during training. In this model, the SGD training has no \textit{a priori} preference for flatness, but only a preference for minimal gradient fluctuations. This leads to the insight that, at least within this model, it is data distribution that uniquely determines the sharpness at convergence, and that a flat minimum is preferred if and only if the noise in the labels is isotropic across all output dimensions. When the noise in the labels is anisotropic, the model instead prefers sharpness and can converge to an arbitrarily sharp solution, depending on the imbalance in the noise in the labels spectrum. We reproduce this key insight in controlled settings with different model architectures such as MLP, RNN, and transformers.

</details>


### [41] [Reliable Explanations or Random Noise? A Reliability Metric for XAI](https://arxiv.org/abs/2602.05082)
*Poushali Sengupta,Sabita Maharjan,Frank Eliassen,Shashi Raj Pandey,Yan Zhang*

Main category: cs.LG

TL;DR: 论文提出Explanation Reliability Index (ERI)指标族，用于量化机器学习解释方法在现实部署条件下的可靠性，包括对输入扰动、特征冗余、模型演化和分布偏移的稳定性评估。


<details>
  <summary>Details</summary>
Motivation: 当前复杂机器学习模型在关键领域应用广泛，但SHAP、Integrated Gradients等主流解释方法的可靠性尚未得到充分评估。这些解释在现实非对抗性变化下（如小输入扰动、相关特征表示、微小模型更新）可能出现显著不一致，影响解释的可信度。

Method: 提出Explanation Reliability Index (ERI)指标族，基于四个可靠性公理构建：对微小输入扰动的鲁棒性、特征冗余下的一致性、模型演化中的平滑性、温和分布偏移的韧性。为每个公理提供形式化保证（如Lipschitz型边界和时间稳定性结果），并专门为序列模型提出ERI-T指标。同时开发ERI-Bench基准测试，在合成和真实数据集上系统评估解释可靠性。

Result: 实验结果显示主流解释方法存在广泛的可靠性缺陷，在现实部署条件下解释可能不稳定。ERI能够暴露和量化这些不稳定性，为解释可靠性提供原则性评估。

Conclusion: 通过引入ERI指标族和ERI-Bench基准，论文为可解释AI系统提供了系统评估解释可靠性的框架，支持构建更可信的XAI系统。量化解释稳定性有助于在实际应用中建立对解释方法的信任。

Abstract: In recent years, explaining decisions made by complex machine learning models has become essential in high-stakes domains such as energy systems, healthcare, finance, and autonomous systems. However, the reliability of these explanations, namely, whether they remain stable and consistent under realistic, non-adversarial changes, remains largely unmeasured. Widely used methods such as SHAP and Integrated Gradients (IG) are well-motivated by axiomatic notions of attribution, yet their explanations can vary substantially even under system-level conditions, including small input perturbations, correlated representations, and minor model updates. Such variability undermines explanation reliability, as reliable explanations should remain consistent across equivalent input representations and small, performance-preserving model changes. We introduce the Explanation Reliability Index (ERI), a family of metrics that quantifies explanation stability under four reliability axioms: robustness to small input perturbations, consistency under feature redundancy, smoothness across model evolution, and resilience to mild distributional shifts. For each axiom, we derive formal guarantees, including Lipschitz-type bounds and temporal stability results. We further propose ERI-T, a dedicated measure of temporal reliability for sequential models, and introduce ERI-Bench, a benchmark designed to systematically stress-test explanation reliability across synthetic and real-world datasets. Experimental results reveal widespread reliability failures in popular explanation methods, showing that explanations can be unstable under realistic deployment conditions. By exposing and quantifying these instabilities, ERI enables principled assessment of explanation reliability and supports more trustworthy explainable AI (XAI) systems.

</details>


### [42] [Individual Fairness In Strategic Classification](https://arxiv.org/abs/2602.05084)
*Zhiqun Zuo,Mohammad Mahdi Khalili*

Main category: cs.LG

TL;DR: 该论文研究战略分类中的个体公平性问题，证明确定性阈值分类器违反个体公平，提出使用随机分类器实现个体公平的条件和优化方法


<details>
  <summary>Details</summary>
Motivation: 战略分类中个体修改特征以影响机器学习决策带来了公平性挑战。虽然群体公平性在该场景下已被广泛研究，但个体公平性仍未被充分探索。论文旨在填补这一研究空白，解决战略分类中的个体公平性问题。

Method: 首先分析阈值分类器，证明确定性阈值违反个体公平性。然后研究使用随机分类器实现个体公平的可能性，提出确保个体公平的条件。基于这些条件，通过线性规划问题寻找最优且个体公平的随机分类器。方法还可扩展到群体公平性概念。

Result: 在真实世界数据集上的实验证实，该方法能有效减轻不公平性，并改善公平性与准确性的权衡关系。

Conclusion: 论文证明了确定性阈值分类器在战略分类中无法保证个体公平性，而随机分类器在特定条件下可以实现个体公平。提出的线性规划方法能够找到最优的个体公平随机分类器，且该方法可扩展到群体公平性，为战略分类中的公平性研究提供了新思路。

Abstract: Strategic classification, where individuals modify their features to influence machine learning (ML) decisions, presents critical fairness challenges. While group fairness in this setting has been widely studied, individual fairness remains underexplored. We analyze threshold-based classifiers and prove that deterministic thresholds violate individual fairness. Then, we investigate the possibility of using a randomized classifier to achieve individual fairness. We introduce conditions under which a randomized classifier ensures individual fairness and leverage these conditions to find an optimal and individually fair randomized classifier through a linear programming problem. Additionally, we demonstrate that our approach can be extended to group fairness notions. Experiments on real-world datasets confirm that our method effectively mitigates unfairness and improves the fairness-accuracy trade-off.

</details>


### [43] [Unbiased Single-Queried Gradient for Combinatorial Objective](https://arxiv.org/abs/2602.05119)
*Thanawat Sornwanee*

Main category: cs.LG

TL;DR: 提出一种用于组合优化问题的随机梯度方法，只需单次查询组合函数即可获得无偏梯度估计


<details>
  <summary>Details</summary>
Motivation: 组合优化问题在概率重构中常面临超立方体上的优化，传统精确梯度计算需要多次查询组合函数，计算成本高

Method: 提出一种随机梯度方法，通过单次查询组合函数获得无偏梯度估计，该方法包含REINFORCE（通过重要性采样）以及一类新的随机梯度

Result: 该方法提供了高效的梯度估计，减少了计算复杂度，同时保持了无偏性

Conclusion: 提出的随机梯度方法为组合优化问题提供了高效的梯度计算框架，扩展了现有方法并降低了计算成本

Abstract: In a probabilistic reformulation of a combinatorial problem, we often face an optimization over a hypercube, which corresponds to the Bernoulli probability parameter for each binary variable in the primal problem. The combinatorial nature suggests that an exact gradient computation requires multiple queries. We propose a stochastic gradient that is unbiased and requires only a single query of the combinatorial function. This method encompasses a well-established REINFORCE (through an importance sampling), as well as including a class of new stochastic gradients.

</details>


### [44] [Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks](https://arxiv.org/abs/2602.05125)
*William F. Shen,Xinchi Qiu,Chenxi Whitehouse,Lisa Alazraki,Shashwat Goel,Francesco Barbieri,Timon Willi,Akhil Mathur,Ilias Leontiadis*

Main category: cs.LG

TL;DR: RRD框架通过递归分解-过滤循环优化评分标准，提升LLM评估准确性和强化学习奖励信号


<details>
  <summary>Details</summary>
Motivation: 现有评分标准存在覆盖不足、维度混淆、偏好方向错位、冗余和高度相关标准等问题，导致评估准确性下降和强化微调中产生次优奖励

Method: 提出RRD框架，采用递归分解-过滤循环：将粗粒度评分标准分解为细粒度判别性标准，通过过滤机制移除错位和冗余标准，使用相关性感知加权方案防止高度相关标准的过度表示

Result: 在JudgeBench和PPE上显著提升GPT-4o和Llama3.1-405B的偏好判断准确性，在JudgeBench上最高提升17.7分；在WildChat的RFT中，奖励提升达160%（Qwen3-4B）和60%（Llama3.1-8B），优于基线10-20%的提升；改进可迁移到HealthBench-Hard和BiGGen Bench

Conclusion: RRD建立了可扩展且可解释的递归评分标准优化基础，为开放领域LLM评估和奖励建模提供了有效解决方案

Abstract: Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.

</details>


### [45] [Decoupled Orthogonal Dynamics: Regularization for Deep Network Optimizers](https://arxiv.org/abs/2602.05136)
*Hao Chen,Jinghui Yuan,Hanmin Zhang*

Main category: cs.LG

TL;DR: AdamW中的标准权重衰减并非最优，存在径向拉锯战问题。作者提出正交动力学解耦方法AdamO，将参数范数控制与自适应梯度缩放分离，在多个任务上优于AdamW。


<details>
  <summary>Details</summary>
Motivation: AdamW虽然将权重衰减与自适应梯度缩放解耦，但存在根本冲突：径向拉锯战。梯度倾向于增加参数范数以扩展有效容量，而权重衰减则不加区分地抑制范数增长，这种推拉相互作用导致径向振荡，向Adam的二阶矩估计注入噪声，可能损害精细的切向特征学习。

Method: 提出正交动力学解耦方法，并实例化为AdamO：使用SGD风格更新处理一维范数控制，而Adam的自适应预条件仅限于切向子空间。AdamO进一步包含曲率自适应径向步长调整、架构感知规则和投影，用于尺度不变层和低维参数。

Result: 在视觉和语言任务上的实验表明，AdamO在泛化能力和稳定性方面优于AdamW，且未引入额外的复杂约束。

Conclusion: 参数的大小和方向在优化器动力学中扮演不同角色，应该被解耦。AdamO通过正交动力学解耦实现了这种分离，提供了比AdamW更优的优化性能。

Abstract: Is the standard weight decay in AdamW truly optimal? Although AdamW decouples weight decay from adaptive gradient scaling, a fundamental conflict remains: the Radial Tug-of-War. In deep learning, gradients tend to increase parameter norms to expand effective capacity while steering directions to learn features, whereas weight decay indiscriminately suppresses norm growth. This push--pull interaction induces radial oscillations, injecting noise into Adam's second-moment estimates and potentially degrading delicate tangential feature learning. We argue that magnitude and direction play distinct roles and should be decoupled in optimizer dynamics. We propose Orthogonal Dynamics Decoupling and instantiate it as AdamO: an SGD-style update handles the one-dimensional norm control, while Adam's adaptive preconditioning is confined to the tangential subspace. AdamO further incorporates curvature-adaptive radial step sizing and architecture-aware rules and projections for scale-invariant layers and low-dimensional parameters. Experiments on vision and language tasks show that AdamO improves generalization and stability over AdamW without introducing additional complex constraints.

</details>


### [46] [Adaptive Exploration for Latent-State Bandits](https://arxiv.org/abs/2602.05139)
*Jikai Jin,Kenneth Hung,Sanath Kumar Krishnamurthy,Baoyi Shi,Congshan Zhang*

Main category: cs.LG

TL;DR: 提出了一类无需显式状态建模的bandit算法，通过滞后上下文特征和协调探测策略隐式跟踪潜在状态，解决存在未观测混杂因子的非平稳环境中的序列决策问题。


<details>
  <summary>Details</summary>
Motivation: 经典多臂老虎机算法在存在隐藏、时变状态的环境中常常失效，这些未观测的混杂因子会导致奖励估计偏差和状态信息有限，从而影响最优动作选择。

Method: 引入了一族状态模型无关的bandit算法，利用滞后上下文特征和协调探测策略隐式跟踪潜在状态，区分状态依赖的奖励模式，无需显式状态建模。

Result: 在多样化设置中的实证结果表明，该方法相比经典方法具有优越性能，能够在不显式建模状态的情况下学习最优策略，兼具计算效率和鲁棒的非平稳奖励适应能力。

Conclusion: 提出的状态模型无关bandit算法有效解决了存在未观测混杂因子的序列决策问题，并为实际应用中的算法选择提供了实用建议。

Abstract: The multi-armed bandit problem is a core framework for sequential decision-making under uncertainty, but classical algorithms often fail in environments with hidden, time-varying states that confound reward estimation and optimal action selection. We address key challenges arising from unobserved confounders, such as biased reward estimates and limited state information, by introducing a family of state-model-free bandit algorithms that leverage lagged contextual features and coordinated probing strategies. These implicitly track latent states and disambiguate state-dependent reward patterns. Our methods and their adaptive variants can learn optimal policies without explicit state modeling, combining computational efficiency with robust adaptation to non-stationary rewards. Empirical results across diverse settings demonstrate superior performance over classical approaches, and we provide practical recommendations for algorithm selection in real-world applications.

</details>


### [47] [Fairness Under Group-Conditional Prior Probability Shift: Invariance, Drift, and Target-Aware Post-Processing](https://arxiv.org/abs/2602.05144)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 该论文研究了群体条件先验概率偏移(GPPS)下的公平性问题，证明了错误率公平准则的结构不变性和接受率公平准则的必然漂移，提出了无需目标标签的公平性评估方法，并开发了TAP-GPPS算法来实现目标域的人口统计均等。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统通常在历史数据上训练和评估公平性，但部署时环境条件已发生变化。特别是当不同人口群体中正结果发生率变化不同时（如疾病率在不同人群中上升速度不同），系统公平性会受到挑战。需要研究这种群体条件先验概率偏移对公平性标准的影响。

Method: 1. 理论分析群体条件先验概率偏移(GPPS)对公平性标准的影响；2. 证明目标域风险和公平性指标无需目标标签即可识别；3. 提出TAP-GPPS算法：利用未标记数据估计患病率、校正后验概率，并选择阈值以满足目标域的人口统计均等。

Result: 1. 证明基本二分法：基于错误率的公平准则（均衡几率）在GPPS下结构不变，而基于接受率的准则（人口统计均等）会漂移；2. 展示了无需目标标签即可一致估计目标域风险和公平性指标；3. TAP-GPPS算法在实验中实现了目标公平性且效用损失最小。

Conclusion: 该研究揭示了不同公平性标准对群体条件先验概率偏移的敏感性差异，提供了无需目标标签的公平性评估理论框架，并开发了实用的算法解决方案。这对于在现实世界分布偏移下部署公平的机器学习系统具有重要意义。

Abstract: Machine learning systems are often trained and evaluated for fairness on historical data, yet deployed in environments where conditions have shifted. A particularly common form of shift occurs when the prevalence of positive outcomes changes differently across demographic groups--for example, when disease rates rise faster in one population than another, or when economic conditions affect loan default rates unequally. We study group-conditional prior probability shift (GPPS), where the label prevalence $P(Y=1\mid A=a)$ may change between training and deployment while the feature-generation process $P(X\mid Y,A)$ remains stable. Our analysis yields three main contributions. First, we prove a fundamental dichotomy: fairness criteria based on error rates (equalized odds) are structurally invariant under GPPS, while acceptance-rate criteria (demographic parity) can drift--and we prove this drift is unavoidable for non-trivial classifiers (shift-robust impossibility). Second, we show that target-domain risk and fairness metrics are identifiable without target labels: the invariance of ROC quantities under GPPS enables consistent estimation from source labels and unlabeled target data alone, with finite-sample guarantees. Third, we propose TAP-GPPS, a label-free post-processing algorithm that estimates prevalences from unlabeled data, corrects posteriors, and selects thresholds to satisfy demographic parity in the target domain. Experiments validate our theoretical predictions and demonstrate that TAP-GPPS achieves target fairness with minimal utility loss.

</details>


### [48] [TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference](https://arxiv.org/abs/2602.05145)
*Jiyoung Park,Hankyu Jang,Changseok Song,Wookeun Jung*

Main category: cs.LG

TL;DR: TIDE是一个集成到LLM推理系统中的在线草稿自适应框架，通过重用推理过程中的隐藏状态进行零开销草稿适应，利用异构集群实现推理与训练解耦，在真实工作负载中相比静态推测解码提升1.15倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: 推测解码能显著加速LLM推理，但在实际应用中面临工作负载动态变化和系统级约束的挑战。现有方法难以适应不断变化的工作负载，且系统级优化不足。

Method: TIDE框架将在线草稿自适应直接集成到高性能LLM推理系统中：1）重用目标模型推理过程中生成的隐藏状态作为训练信号，实现零开销草稿适应；2）采用自适应运行时控制，仅在有益时激活推测和训练；3）利用异构集群，将解耦的推理和训练映射到合适的GPU类别。

Result: 在多样化真实工作负载中，TIDE相比静态推测解码实现了最高1.15倍的吞吐量提升，同时相比重新计算训练信号的方法减少了1.67倍的草稿训练时间。

Conclusion: TIDE通过系统原生集成和在线自适应机制，有效解决了推测解码在实际部署中的挑战，在保持高性能的同时显著降低了训练开销，为动态工作负载下的LLM推理加速提供了实用解决方案。

Abstract: Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptation directly into high-performance LLM inference systems. TIDE reuses target model hidden states generated during inference as training signals, enabling zero-overhead draft adaptation without reloading the target model, and employs adaptive runtime control to activate speculation and training only when beneficial. TIDE exploits heterogeneous clusters by mapping decoupled inference and training to appropriate GPU classes. Across diverse real-world workloads, TIDE achieves up to 1.15x throughput improvement over static speculative decoding while reducing draft training time by 1.67x compared to approaches that recompute training signals.

</details>


### [49] [Cross-talk based multi-task learning for fault classification of physically coupled machine system](https://arxiv.org/abs/2602.05146)
*Wonjun Yi,Rismaya Kumar Mishra,Yong-Hwa Park*

Main category: cs.LG

TL;DR: 论文提出了一种基于交叉对话结构的多任务学习框架，利用故障信号中物理耦合信息，通过同时学习故障条件和相关物理变量来提升故障分类性能。


<details>
  <summary>Details</summary>
Motivation: 机械系统产生的信号中，故障条件与各种物理变量存在物理耦合。现有故障分类研究大多仅依赖直接故障标签，忽略了信号中自然嵌入的由其他物理耦合信息塑造的附加信息。本文旨在利用这种耦合关系，通过多任务学习框架同时学习故障条件和相关物理变量，以提升故障分类性能。

Method: 采用基于交叉对话结构的多任务学习框架，在先前提出的残差神经降维模型基础上进行扩展。交叉对话结构通过交叉对话层实现任务间的受控信息交换，同时防止负迁移。该方法应用于两个物理耦合显著的基准数据集：无人机故障数据集（机器类型和操纵方向显著改变信号频率成分）和电机复合故障数据集（各故障组件相互耦合）。

Result: 在两个基准测试中，残差神经降维模型均一致优于单任务模型、合并所有标签组合的多类模型以及共享主干多任务模型。对于无人机故障数据集，通过学习故障分类与物理属性，交叉对话架构能更好地分类故障。对于电机复合故障，测试了单通道和多通道数据作为分类器输入时的性能。

Conclusion: 交叉对话结构的多任务学习框架能有效利用故障信号中的物理耦合信息，通过同时学习故障条件和相关物理变量，显著提升故障分类性能，优于传统单任务和共享主干多任务方法。

Abstract: Machine systems inherently generate signals in which fault conditions and various physical variables are physically coupled. Although many existing fault classification studies rely solely on direct fault labels, the aforementioned signals naturally embed additional information shaped by other physically coupled information. Herein, we leverage this coupling through a multi-task learning (MTL) framework that jointly learns fault conditions and the related physical variables. Among MTL architectures, crosstalk structures have distinct advantages because they allow for controlled information exchange between tasks through the cross-talk layer while preventing negative transfer, in contrast to shared trunk architectures that often mix incompatible features. We build on our previously introduced residual neural dimension reductor model, and extend its application to two benchmarks where physical coupling is prominent. The first benchmark is a drone fault dataset, in which machine type and maneuvering direction significantly alter the frequency components of measured signals even under the same nominal condition. By learning fault classification together with these physical attributes, the cross-talk architecture can better classify faults. The second benchmark dataset is the motor compound fault dataset. In this system, each fault component, inner race fault, outer race fault, misalignment, and unbalance is coupled to the other. For motor compound fault, we also test classification performance when we use single-channel data or multi-channel data as input to the classifier. Across both benchmarks, our residual neural dimension reductor, consistently outperformed single-task models, multi-class models that merge all label combinations, and shared trunk multi-task models.

</details>


### [50] [CoSA: Compressed Sensing-Based Adaptation of Large Language Models](https://arxiv.org/abs/2602.05148)
*Songtao Wei,Yi Li,Bohan Zhang,Zhichun Guo,Ying Huang,Yuede Ji,Miao Yin,Guanpeng Li,Bingzhe Li*

Main category: cs.LG

TL;DR: CoSA是一种基于压缩感知理论的新型参数高效微调方法，通过固定随机投影矩阵和可学习核心来表达权重更新，突破了传统低秩分解方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA和PiSSA）依赖低秩分解假设，但在任务特定适应场景中，当奇异值分布相对均匀时，低秩假设可能限制表达能力。需要一种更灵活的表达方式来处理这种限制。

Method: CoSA基于压缩感知理论，不将权重更新约束在低秩子空间，而是通过固定随机投影矩阵和紧凑可学习核心来表达权重更新。该方法将权重更新压缩编码到低维空间，并通过随机投影映射回原始空间。

Result: 在10个多样化任务（包括自然语言理解和生成）上评估CoSA，使用RoBERTa、Llama和Qwen家族的5个不同规模模型。在所有设置中，CoSA始终匹配或优于最先进的PEFT方法。

Conclusion: CoSA为高效且表达力强的多尺度模型适应提供了原则性视角，突破了传统低秩分解方法的限制，在保持参数效率的同时提供了更好的表达能力。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.

</details>


### [51] [Position: Capability Control Should be a Separate Goal From Alignment](https://arxiv.org/abs/2602.05164)
*Shoaib Ahmed Siddiqui,Eleni Triantafillou,David Krueger,Adrian Weller*

Main category: cs.LG

TL;DR: 该立场论文主张将能力控制作为与对齐不同的独立目标，提出在模型生命周期中实施三层防御机制，并倡导采用深度防御方法


<details>
  <summary>Details</summary>
Motivation: 基础模型在广泛数据分布上训练，具备通用能力，但这也扩大了潜在滥用和失败的空间。当前对齐方法通常依赖于上下文和偏好驱动，而能力控制需要施加硬性操作限制，包括在对抗性诱导下的行为限制

Method: 将能力控制机制组织为三层：1) 基于数据的训练分布控制；2) 基于学习的权重或表示层面干预；3) 基于系统的部署后护栏（输入、输出和动作）。倡导采用深度防御方法，在整个技术栈中组合互补的控制措施

Result: 提出了一个系统化的能力控制框架，将控制机制分层组织，并识别了各层的特征性失效模式。强调需要跨层组合控制以增强鲁棒性

Conclusion: 能力控制应被视为与对齐不同的独立目标，需要在整个模型生命周期中实施多层防御机制。实现有效控制面临关键挑战，包括知识的双重用途性质和组合泛化问题。深度防御方法是必要的，因为单层控制存在特征性失效模式

Abstract: Foundation models are trained on broad data distributions, yielding generalist capabilities that enable many downstream applications but also expand the space of potential misuse and failures. This position paper argues that capability control -- imposing restrictions on permissible model behavior -- should be treated as a distinct goal from alignment. While alignment is often context and preference-driven, capability control aims to impose hard operational limits on permissible behaviors, including under adversarial elicitation. We organize capability control mechanisms across the model lifecycle into three layers: (i) data-based control of the training distribution, (ii) learning-based control via weight- or representation-level interventions, and (iii) system-based control via post-deployment guardrails over inputs, outputs, and actions. Because each layer has characteristic failure modes when used in isolation, we advocate for a defense-in-depth approach that composes complementary controls across the full stack. We further outline key open challenges in achieving such control, including the dual-use nature of knowledge and compositional generalization.

</details>


### [52] [EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization](https://arxiv.org/abs/2602.05165)
*Kevin Han,Yuhang Zhou,Mingze Gao,Gedi Zhou,Serena Li,Abhishek Kumar,Xiangjun Fan,Weiwei Li,Lizhu Zhang*

Main category: cs.LG

TL;DR: EBPO通过经验贝叶斯方法改进RLVR，解决GRPO在小样本下的高方差和失败场景梯度消失问题，使用收缩估计器平衡局部统计与全局先验，提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法如GRPO存在两个关键稳定性问题：1) 在计算约束下（小组规模小）估计器方差高；2) 在饱和失败场景中（所有响应获得相同零奖励）梯度信号消失。需要一种更稳定、高效的策略优化方法。

Method: 提出经验贝叶斯策略优化（EBPO）框架，通过收缩估计器动态平衡局部组统计与全局先验（使用Welford在线算法更新），正则化局部组基线，从策略累积的全局统计中借用强度。

Result: 理论上证明EBPO相比GRPO具有更低的均方误差、有界的熵衰减和失败场景中非消失的惩罚信号。实证上在AIME和OlympiadBench等基准测试中一致优于GRPO和其他基线，展示出更高的训练稳定性，即使在小组规模下也能获得高性能增益，并从难度分层课程学习中显著受益。

Conclusion: EBPO通过经验贝叶斯方法有效解决了RLVR中的稳定性挑战，提供了一种更可靠、高效的策略优化框架，特别适用于计算受限和困难场景，为增强LLM推理能力提供了重要改进。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.

</details>


### [53] [Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.05183)
*John Yan,Michael Yu,Yuqi Sun,Alexander Duffy,Tyler Marques,Matthew Lyle Olson*

Main category: cs.LG

TL;DR: 本文提出Meta-Autointerp方法，通过稀疏自编码器和LLM摘要器分析复杂强化学习环境中LLM的训练动态，发现细粒度行为模式，但发现大多数SAE特征和LLM生成假设对人类理解帮助有限，只有部分SAE衍生假设对下游任务有预测价值。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在复杂强化学习多智能体环境中训练，理解训练过程中行为变化变得困难。稀疏自编码器在数据为中心的可解释性方面显示出潜力，但需要系统方法来分析大规模RL训练运行中的行为动态。

Method: 提出Meta-Autointerp方法，将预训练的稀疏自编码器特征分组为可解释的假设。在Full-Press Diplomacy复杂环境中应用SAE和LLM摘要器方法，分析大规模RL训练运行。通过自动评估验证SAE元特征，并进行用户研究评估人类可理解性。

Result: 发现细粒度行为包括角色扮演模式、退化输出、语言切换，以及高级战略行为和特定环境bug。90%发现的SAE元特征具有显著性，发现令人惊讶的奖励黑客行为。用户研究表明大多数SAE特征和LLM生成假设对人类理解帮助有限，但部分SAE衍生假设对下游任务有预测价值，通过增强未训练智能体的系统提示将得分提高14.2%。

Conclusion: SAE和LLM摘要器为智能体行为提供互补视角，Meta-Autointerp框架为未来数据为中心的可解释性工作提供了实用起点，有助于确保LLM在整个训练过程中的可信行为。

Abstract: Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent's system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.

</details>


### [54] [SpectraKAN: Conditioning Spectral Operators](https://arxiv.org/abs/2602.05187)
*Chun-Wun Cheng,Carola-Bibiane Schönlieb,Angelica I. Aviles-Rivero*

Main category: cs.LG

TL;DR: SpectraKAN是一种新型谱神经算子，通过输入自适应的谱卷积来改进传统FNO，能够捕捉多尺度、状态依赖和各向异性动力学，在PDE求解中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有谱神经算子（如FNO）使用静态傅里叶核，无法适应输入相关的多尺度、状态依赖和各向异性动力学，限制了其在复杂PDE求解中的表达能力。

Method: 提出SpectraKAN：从时空历史中提取紧凑全局表示，通过单查询交叉注意力调制多尺度傅里叶主干，将静态谱卷积转变为输入自适应的积分算子。

Result: 在多个PDE基准测试中达到最先进性能，RMSE降低高达49%，在具有挑战性的时空预测任务上表现尤为突出。

Conclusion: SpectraKAN通过输入自适应调制机制有效扩展了谱神经算子的表达能力，同时保持了谱混合的效率，为复杂PDE求解提供了更强大的框架。

Abstract: Spectral neural operators, particularly Fourier Neural Operators (FNO), are a powerful framework for learning solution operators of partial differential equations (PDEs) due to their efficient global mixing in the frequency domain. However, existing spectral operators rely on static Fourier kernels applied uniformly across inputs, limiting their ability to capture multi-scale, regime-dependent, and anisotropic dynamics governed by the global state of the system. We introduce SpectraKAN, a neural operator that conditions the spectral operator on the input itself, turning static spectral convolution into an input-conditioned integral operator. This is achieved by extracting a compact global representation from spatio-temporal history and using it to modulate a multi-scale Fourier trunk via single-query cross-attention, enabling the operator to adapt its behaviour while retaining the efficiency of spectral mixing. We provide theoretical justification showing that this modulation converges to a resolution-independent continuous operator under mesh refinement and KAN gives smooth, Lipschitz-controlled global modulation. Across diverse PDE benchmarks, SpectraKAN achieves state-of-the-art performance, reducing RMSE by up to 49% over strong baselines, with particularly large gains on challenging spatio-temporal prediction tasks.

</details>


### [55] [Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs](https://arxiv.org/abs/2602.05191)
*Wentao Ni,Kangqi Zhang,Zhongming Yu,Oren Nelson,Mingu Lee,Hong Cai,Fatih Porikli,Jongryool Kim,Zhijian Liu,Jishen Zhao*

Main category: cs.LG

TL;DR: Double-P：一种分层稀疏注意力框架，通过两级top-p选择优化长上下文推理，在保持精度的同时显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 随着长上下文推理成为大语言模型的核心，注意力计算成为解码瓶颈。现有稀疏注意力方法无法同时优化top-p精度、选择开销和稀疏注意力成本，限制了整体效率。

Method: 提出Double-P分层稀疏注意力框架：1) 在聚类级别使用大小加权质心进行粗粒度top-p估计；2) 自适应地通过第二级top-p阶段分配token级注意力计算，仅在需要时进行细化。

Result: 在长上下文基准测试中，Double-P实现接近零的精度下降，将注意力计算开销降低1.8倍，相比最先进的固定预算稀疏注意力方法实现1.3倍的端到端解码加速。

Conclusion: Double-P通过分层top-p选择有效解决了稀疏注意力在精度、选择开销和计算成本之间的权衡问题，为长上下文推理提供了高效可扩展的解决方案。

Abstract: As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions across heads and layers, whereas top-p sparse attention directly preserves attention mass and provides stronger accuracy guarantees. Existing top-p methods, however, fail to jointly optimize top-p accuracy, selection overhead, and sparse attention cost, which limits their overall efficiency. We present Double-P, a hierarchical sparse attention framework that optimizes all three stages. Double-P first performs coarse-grained top-p estimation at the cluster level using size-weighted centroids, then adaptively refines computation through a second top-p stage that allocates token-level attention only when needed. Across long-context benchmarks, Double-P consistently achieves near-zero accuracy drop, reducing attention computation overhead by up to 1.8x and delivers up to 1.3x end-to-end decoding speedup over state-of-the-art fixed-budget sparse attention methods.

</details>


### [56] [Extreme Weather Nowcasting via Local Precipitation Pattern Prediction](https://arxiv.org/abs/2602.05204)
*Changhoon Song,Teng Yuan Chang,Youngjoon Hong*

Main category: cs.LG

TL;DR: exPreCast是一个高效的确定性雷达预报框架，结合平衡数据集，在普通和极端降雨情况下都能实现精确的临近预报。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件（如暴雨、风暴）的准确预报对风险管理和灾害缓解至关重要。现有方法存在以下问题：1）基于扩散的生成集成方法计算成本高，不适合实时应用；2）确定性模型计算高效但对正常降雨有偏差；3）现有基准数据集存在偏差，要么以普通降雨为主，要么仅限于极端降雨事件，限制了实际应用。

Method: 提出exPreCast框架，包含：1）局部时空注意力机制；2）纹理保持的立方双上采样解码器；3）时间提取器以灵活调整预报时间范围。同时构建了来自韩国气象厅的平衡雷达数据集，包含普通降水和极端事件。

Result: 在SEVIR、MeteoNet基准数据集和新构建的平衡KMA数据集上的实验表明，该方法实现了最先进的性能，在普通和极端降雨情况下都能提供准确可靠的临近预报。

Conclusion: exPreCast框架通过高效的确定性方法和平衡数据集，解决了现有临近预报方法在计算效率和数据集偏差方面的问题，为实际应用提供了准确可靠的极端天气预报解决方案。

Abstract: Accurate forecasting of extreme weather events such as heavy rainfall or storms is critical for risk management and disaster mitigation. Although high-resolution radar observations have spurred extensive research on nowcasting models, precipitation nowcasting remains particularly challenging due to pronounced spatial locality, intricate fine-scale rainfall structures, and variability in forecasting horizons. While recent diffusion-based generative ensembles show promising results, they are computationally expensive and unsuitable for real-time applications. In contrast, deterministic models are computationally efficient but remain biased toward normal rainfall. Furthermore, the benchmark datasets commonly used in prior studies are themselves skewed--either dominated by ordinary rainfall events or restricted to extreme rainfall episodes--thereby hindering general applicability in real-world settings. In this paper, we propose exPreCast, an efficient deterministic framework for generating finely detailed radar forecasts, and introduce a newly constructed balanced radar dataset from the Korea Meteorological Administration (KMA), which encompasses both ordinary precipitation and extreme events. Our model integrates local spatiotemporal attention, a texture-preserving cubic dual upsampling decoder, and a temporal extractor to flexibly adjust forecasting horizons. Experiments on established benchmarks (SEVIR and MeteoNet) as well as on the balanced KMA dataset demonstrate that our approach achieves state-of-the-art performance, delivering accurate and reliable nowcasts across both normal and extreme rainfall regimes.

</details>


### [57] [Disentangled Representation Learning via Flow Matching](https://arxiv.org/abs/2602.05214)
*Jinjin Chi,Taoping Liu,Mengtao Yin,Ximing Li,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出基于流匹配的框架进行解耦表示学习，通过因子条件流和正交正则化实现语义对齐


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法通过归纳偏置鼓励因子独立性，但经常缺乏强语义对齐，需要更有效的解耦表示学习框架

Method: 提出流匹配框架，将解耦表示为学习紧凑潜在空间中的因子条件流；引入非重叠（正交）正则化器抑制跨因子干扰，减少因子间信息泄漏

Result: 在多个数据集上的广泛实验显示，相比代表性基线方法，该方法在解耦分数、可控性和样本保真度方面均有持续改进

Conclusion: 基于流匹配的框架通过因子条件流和正交正则化有效实现了语义对齐的解耦表示学习，在多个指标上优于现有方法

Abstract: Disentangled representation learning aims to capture the underlying explanatory factors of observed data, enabling a principled understanding of the data-generating process. Recent advances in generative modeling have introduced new paradigms for learning such representations. However, existing diffusion-based methods encourage factor independence via inductive biases, yet frequently lack strong semantic alignment. In this work, we propose a flow matching-based framework for disentangled representation learning, which casts disentanglement as learning factor-conditioned flows in a compact latent space. To enforce explicit semantic alignment, we introduce a non-overlap (orthogonality) regularizer that suppresses cross-factor interference and reduces information leakage between factors. Extensive experiments across multiple datasets demonstrate consistent improvements over representative baselines, yielding higher disentanglement scores as well as improved controllability and sample fidelity.

</details>


### [58] [Private Prediction via Shrinkage](https://arxiv.org/abs/2602.05219)
*Chao Yan*

Main category: cs.LG

TL;DR: 该论文研究差分隐私预测问题，在标准组合方法产生√T依赖的基础上，将查询数量T的依赖降低到多对数级别，分别在非自适应和自适应在线对手场景下给出了样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 差分隐私预测中，标准组合方法导致查询数量T的平方根依赖，限制了实际应用。研究目标是减少这种依赖，特别是在流式设置中实现多对数级别的依赖。

Method: 针对不同对手模型设计私有预测器：对于非自适应在线对手和任意概念类C，使用基于VC维的方法；对于自适应在线对手和Rd中的半空间，采用特定技术处理。

Result: 在非自适应场景下，样本复杂度为Õ(VC(C)^3.5 log^3.5 T)；在自适应场景下，对于d维半空间，样本复杂度为Õ(d^5.5 log T)，均实现了多对数依赖。

Conclusion: 差分隐私预测中的查询数量依赖可以从平方根降低到多对数级别，显著改进了流式设置中的隐私-效用权衡，为实际应用提供了更高效的解决方案。

Abstract: We study differentially private prediction introduced by Dwork and Feldman (COLT 2018): an algorithm receives one labeled sample set $S$ and then answers a stream of unlabeled queries while the output transcript remains $(\varepsilon,δ)$-differentially private with respect to $S$. Standard composition yields a $\sqrt{T}$ dependence for $T$ queries.
  We show that this dependence can be reduced to polylogarithmic in $T$ in streaming settings. For an oblivious online adversary and any concept class $\mathcal{C}$, we give a private predictor that answers $T$ queries with $|S|= \tilde{O}(VC(\mathcal{C})^{3.5}\log^{3.5}T)$ labeled examples. For an adaptive online adversary and halfspaces over $\mathbb{R}^d$, we obtain $|S|=\tilde{O}\left(d^{5.5}\log T\right)$.

</details>


### [59] [ZeroS: Zero-Sum Linear Attention for Efficient Transformers](https://arxiv.org/abs/2602.05230)
*Jiecheng Lu,Xu Han,Yan Sun,Viresh Pati,Yubin Kim,Siddhartha Somani,Shihao Yang*

Main category: cs.LG

TL;DR: ZeroS线性注意力机制通过移除常数项和重加权残差，解决了传统线性注意力的两个根本限制，在保持O(N)复杂度的同时达到或超过标准softmax注意力的性能。


<details>
  <summary>Details</summary>
Motivation: 线性注意力方法虽然提供O(N)复杂度，但通常性能不如标准softmax注意力。作者识别出两个根本限制：1) 仅限于凸组合，只允许加性信息融合；2) 均匀累积权重偏差，在长上下文中稀释注意力。

Method: 提出Zero-Sum线性注意力(ZeroS)，通过移除常数零阶项1/t并重新加权剩余的零和softmax残差。这种修改创建了数学稳定的权重，允许正负值，使单个注意力层能够执行对比操作。

Result: ZeroS在理论上扩展了可表示函数的集合，超越了凸组合的限制。实证上，它在各种序列建模基准测试中达到或超过了标准softmax注意力的性能。

Conclusion: ZeroS线性注意力机制成功解决了传统线性注意力的根本限制，在保持线性复杂度的同时实现了与标准注意力相当甚至更好的性能，为高效长序列建模提供了有前景的解决方案。

Abstract: Linear attention methods offer Transformers $O(N)$ complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term $1/t$ and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining $O(N)$ complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks.

</details>


### [60] [Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions](https://arxiv.org/abs/2602.05234)
*Yuntai Bao,Xuhong Zhang,Jintao Chen,Ge Su,Yuxiang Cai,Hao Peng,Bing Sun,Haiqin Weng,Liu Yan,Jianwei Yin*

Main category: cs.LG

TL;DR: CDAS是一种基于概念分布对齐搜索的轻量级模型引导方法，通过分布式交换干预和分布匹配目标实现更忠实稳定的控制，在安全相关任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前基于干预的模型引导方法采用微调的强优化目标，容易过拟合且性能不佳，有时产生不自然的输出。作者认为有效的引导需要忠实识别内部模型机制，而非强制执行外部偏好。

Method: 基于分布式对齐搜索（DAS）原则，提出概念DAS（CDAS）。采用DAS的核心机制——分布式交换干预（DII），但引入针对引导任务的新分布匹配目标：通过将干预后的输出分布与反事实分布对齐。CDAS通过弱监督分布匹配而非概率最大化学习干预，使用DII实现双向引导并从数据中推导引导因子。

Result: 在AxBench大规模模型引导基准上，CDAS并不总是优于偏好优化方法，但可能从模型规模增加中获益更多。在两个安全相关案例研究中（覆盖安全对齐模型的拒绝行为、中和思维链后门），CDAS实现了系统性引导同时保持模型通用效用。

Conclusion: CDAS是偏好优化方法的补充，在特定条件下构成了基于干预的模型引导的稳健方法。该方法通过分布匹配和双向干预实现了更忠实稳定的控制。

Abstract: Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs. We hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences. To this end, we build on the principles of distributed alignment search (DAS), the standard for causal variable localization, to propose a new steering method: Concept DAS (CDAS). While we adopt the core mechanism of DAS, distributed interchange intervention (DII), we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. CDAS differs from prior work in two main ways: first, it learns interventions via weak-supervised distribution matching rather than probability maximization; second, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control. On AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale. In two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility. These results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering. Our code is available at https://github.com/colored-dye/concept_das.

</details>


### [61] [CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers](https://arxiv.org/abs/2602.05243)
*Boxiang Zhang,Baijian Yang*

Main category: cs.LG

TL;DR: CORP：一种用于视觉Transformer的闭式单次结构化剪枝框架，无需标签、梯度或微调，仅使用少量未标记校准数据即可在训练后部署中实现高效剪枝。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer虽然精度高但计算和内存成本昂贵。现有结构化剪枝方法通常需要重新训练或多阶段优化，限制了训练后部署的可行性。需要一种在严格训练后约束下工作的剪枝方法。

Method: 将结构化剪枝建模为表示恢复问题，将移除的激活和注意力对数建模为保留组件的仿射函数，推导闭式岭回归解并将补偿折叠到模型权重中，最小化校准分布下的预期表示误差。

Result: 在ImageNet上使用DeiT模型实验显示，MLP和注意力表示存在强冗余性。CORP在激进稀疏度下保持精度：DeiT-Huge剪枝50%的MLP和注意力结构后仍保持82.8% Top-1准确率。剪枝过程在单GPU上20分钟内完成，带来显著实际效率提升。

Conclusion: CORP是一种有效的训练后结构化剪枝框架，无需标签、梯度或微调，通过闭式表示恢复方法在保持精度的同时显著降低视觉Transformer的推理成本。

Abstract: Vision Transformers achieve strong accuracy but incur high compute and memory cost. Structured pruning can reduce inference cost, but most methods rely on retraining or multi-stage optimization. These requirements limit post-training deployment. We propose \textbf{CORP}, a closed-form one-shot structured pruning framework for Vision Transformers. CORP removes entire MLP hidden dimensions and attention substructures without labels, gradients, or fine-tuning. It operates under strict post-training constraints using only a small unlabeled calibration set. CORP formulates structured pruning as a representation recovery problem. It models removed activations and attention logits as affine functions of retained components and derives closed-form ridge regression solutions that fold compensation into model weights. This minimizes expected representation error under the calibration distribution. Experiments on ImageNet with DeiT models show strong redundancy in MLP and attention representations. Without compensation, one-shot structured pruning causes severe accuracy degradation. With CORP, models preserve accuracy under aggressive sparsity. On DeiT-Huge, CORP retains 82.8\% Top-1 accuracy after pruning 50\% of both MLP and attention structures. CORP completes pruning in under 20 minutes on a single GPU and delivers substantial real-world efficiency gains.

</details>


### [62] [Hybrid Gated Flow (HGF): Stabilizing 1.58-bit LLMs via Selective Low-Rank Correction](https://arxiv.org/abs/2602.05269)
*David Alejandro Trejo Pizzo*

Main category: cs.LG

TL;DR: HGF是一种双流架构，将1.58位三元主干与可学习的低秩FP16校正路径结合，通过自适应门控控制，在边缘设备上显著恢复量化模型质量，同时保持低内存开销。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在边缘设备部署中的"内存墙"问题。虽然1.58位量化技术大幅减少内存占用，但通常会导致20-25%的困惑度下降。需要一种方法在保持低内存开销的同时恢复量化模型的质量。

Method: 提出混合门控流(HGF)架构：1）三元主干(1.58位)作为基础计算路径；2）可学习的低秩FP16校正路径；3）自适应门控机制控制两个流的融合。在TinyStories数据集上进行2500和3500步的训练验证。

Result: HGF 5.4在验证损失上达到0.9306，相比BitNet的1.0294显著改善，恢复了约55%的三元量化与FP16基线之间的质量差距。仅增加12-15%的内存开销。实验还发现量化作为结构正则化的现象，HGF在训练中保持稳定收敛，而全精度差分注意力基线出现不稳定。初步扩展到1.2B和3B参数模型也证实了架构的稳定性和质量恢复能力。

Conclusion: HGF架构有效解决了边缘设备上LLM部署的内存带宽瓶颈问题，在保持低内存开销的同时显著恢复量化模型质量。量化不仅减少内存占用，还起到结构正则化作用，提高训练稳定性。该架构可扩展到生产级语言建模场景。

Abstract: The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the "Memory Wall" -- a hardware limitation where memory bandwidth, not compute, becomes the bottleneck. Recent 1.58-bit quantization techniques (e.g., BitNet b1.58) dramatically reduce memory footprint but typically incur a perplexity degradation of 20-25% compared to FP16 baselines. In this work, we introduce Hybrid Gated Flow (HGF), a dual-stream architecture that couples a 1.58-bit ternary backbone with a learnable, low-rank FP16 correction path controlled by adaptive gates.
  Through extensive experiments on the TinyStories dataset across two training regimes (2500 and 3500 steps), we demonstrate that HGF 5.4 achieves a validation loss of 0.9306 compared to BitNet's 1.0294, recovering approximately 55% of the quality gap between pure ternary quantization and the FP16 baseline (0.8490). This recovery is achieved with only ~12-15% memory overhead beyond the ternary backbone.
  Furthermore, we provide empirical evidence for an emergent phenomenon: quantization as structural regularization. While a full-precision differential attention baseline (Diff_Only) exhibited training instability with validation loss exceeding 1.68, the ternary-anchored HGF maintained robust convergence throughout training. Finally, we report preliminary results extending this architecture to 1.2B and 3B parameter models trained on SlimPajama and FineWeb-Edu. These larger-scale experiments confirm that the architectural stability and quality recovery observed in small-scale proxies scale linearly to production-grade language modeling regimes.

</details>


### [63] [Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities](https://arxiv.org/abs/2602.05281)
*Pengyi Li,Elizaveta Goncharova,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出ARM机制解决RLVR中GRPO方法导致的低熵策略和模式崩溃问题，通过优势重加权平衡正确回答的置信度，提升生成多样性和响应熵


<details>
  <summary>Details</summary>
Motivation: 标准策略优化方法（如GRPO）在强化学习可验证奖励（RLVR）中容易收敛到低熵策略，导致严重的模式崩溃和有限的输出多样性。从采样概率动态角度分析，发现标准目标函数不成比例地强化最高似然路径，抑制了有效的替代推理链。

Method: 提出新颖的优势重加权机制（ARM），将提示困惑度和答案置信度纳入优势估计，动态重塑奖励信号以衰减过度自信推理路径的梯度更新，同时将概率质量重新分配给未被充分探索的正确解决方案。

Result: 在Qwen2.5和DeepSeek模型上的数学和编码基准测试表明，ProGRPO显著缓解了熵崩溃。具体而言，在Qwen2.5-7B上，该方法在Pass@1上比GRPO高出5.7%，在Pass@32上显著高出13.9%，显示出在生成多样化正确推理路径方面的卓越能力。

Conclusion: 该方法显著增强了生成多样性和响应熵，同时保持了有竞争力的准确性，有效实现了推理任务中探索与利用的优越权衡。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.

</details>


### [64] [Robust Inference-Time Steering of Protein Diffusion Models via Embedding Optimization](https://arxiv.org/abs/2602.05285)
*Minhuan Li,Jiequn Han,Pilar Cossio,Luhuan Wu*

Main category: cs.LG

TL;DR: EmbedOpt：一种在条件嵌入空间中引导扩散模型优化实验似然性的推理时方法，用于生成符合实验测量的生物分子构象


<details>
  <summary>Details</summary>
Motivation: 当目标构象位于先验分布的低密度区域时，传统的后验采样方法需要激进的似然性加权，导致方法脆弱且不稳定。需要一种更稳健的方法来引导扩散模型生成符合实验约束的生物分子构象。

Method: 提出EmbedOpt方法，在条件嵌入空间而非原子坐标空间中进行优化。该空间编码了丰富的序列和共进化信号，通过优化嵌入空间来有效调整扩散先验，使其与实验约束对齐。方法在冷冻电镜图谱拟合和实验距离约束两个基准任务上进行验证。

Result: 在冷冻电镜图谱拟合任务中，EmbedOpt优于基于坐标的后验采样方法；在距离约束任务中，性能相当。更重要的是，EmbedOpt在跨越两个数量级的超参数范围内表现出卓越的工程鲁棒性，且其平滑的优化行为显著减少了推理所需的扩散步骤数，提高了效率。

Conclusion: EmbedOpt提供了一种更稳健、高效的替代方案，用于引导扩散模型生成符合实验测量的生物分子构象，特别是在目标位于先验分布低密度区域时，相比传统的后验采样方法具有明显优势。

Abstract: In many biophysical inverse problems, the goal is to generate biomolecular conformations that are both physically plausible and consistent with experimental measurements. As recent sequence-to-structure diffusion models provide powerful data-driven priors, posterior sampling has emerged as a popular framework by guiding atomic coordinates to target conformations using experimental likelihoods. However, when the target lies in a low-density region of the prior, posterior sampling requires aggressive and brittle weighting of the likelihood guidance. Motivated by this limitation, we propose EmbedOpt, an alternative inference-time approach for steering diffusion models to optimize experimental likelihoods in the conditional embedding space. As this space encodes rich sequence and coevolutionary signals, optimizing over it effectively shifts the diffusion prior to align with experimental constraints. We validate EmbedOpt on two benchmarks simulating cryo-electron microscopy map fitting and experimental distance constraints. We show that EmbedOpt outperforms the coordinate-based posterior sampling method in map fitting tasks, matches performance on distance constraint tasks, and exhibits superior engineering robustness across hyperparameters spanning two orders of magnitude. Moreover, its smooth optimization behavior enables a significant reduction in the number of diffusion steps required for inference, leading to better efficiency.

</details>


### [65] [HealthMamba: An Uncertainty-aware Spatiotemporal Graph State Space Model for Effective and Reliable Healthcare Facility Visit Prediction](https://arxiv.org/abs/2602.05286)
*Dahai Yu,Lin Jiang,Rongchao Xu,Guang Wang*

Main category: cs.LG

TL;DR: HealthMamba：一种用于医疗设施访问预测的不确定性感知时空框架，通过统一时空编码器、图状态空间模型和综合不确定性量化模块，在四个大规模真实数据集上实现了约6.0%的预测精度提升和3.5%的不确定性量化改进。


<details>
  <summary>Details</summary>
Motivation: 现有医疗设施访问预测研究通常将其视为时间序列预测问题，忽略了不同类型医疗设施之间的空间依赖关系，且无法在公共紧急事件等异常情况下提供可靠预测。需要开发能够同时考虑时空依赖性和不确定性的框架。

Method: 提出HealthMamba框架，包含三个关键组件：1）统一时空上下文编码器，融合静态和动态异质信息；2）GraphMamba图状态空间模型，用于分层时空建模；3）综合不确定性量化模块，集成三种不确定性量化机制。

Result: 在加州、纽约、德克萨斯和佛罗里达四个大规模真实数据集上的评估显示，HealthMamba相比最先进的基线方法，在预测精度上实现了约6.0%的提升，在不确定性量化方面实现了约3.5%的改进。

Conclusion: HealthMamba通过整合时空建模和不确定性量化，为医疗设施访问预测提供了准确可靠的解决方案，能够更好地支持医疗资源优化分配和公共卫生政策制定，特别是在异常情况下。

Abstract: Healthcare facility visit prediction is essential for optimizing healthcare resource allocation and informing public health policy. Despite advanced machine learning methods being employed for better prediction performance, existing works usually formulate this task as a time-series forecasting problem without considering the intrinsic spatial dependencies of different types of healthcare facilities, and they also fail to provide reliable predictions under abnormal situations such as public emergencies. To advance existing research, we propose HealthMamba, an uncertainty-aware spatiotemporal framework for accurate and reliable healthcare facility visit prediction. HealthMamba comprises three key components: (i) a Unified Spatiotemporal Context Encoder that fuses heterogeneous static and dynamic information, (ii) a novel Graph State Space Model called GraphMamba for hierarchical spatiotemporal modeling, and (iii) a comprehensive uncertainty quantification module integrating three uncertainty quantification mechanisms for reliable prediction. We evaluate HealthMamba on four large-scale real-world datasets from California, New York, Texas, and Florida. Results show HealthMamba achieves around 6.0% improvement in prediction accuracy and 3.5% improvement in uncertainty quantification over state-of-the-art baselines.

</details>


### [66] [Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective](https://arxiv.org/abs/2602.05319)
*Yinan Huang,Hans Hao-Hsun Hsu,Junran Wang,Bo Dai,Pan Li*

Main category: cs.LG

TL;DR: 提出Sequential Flow Matching框架，将流匹配模型与贝叶斯滤波结合，通过从先前后验分布初始化生成来加速序列预测中的采样过程


<details>
  <summary>Details</summary>
Motivation: 解决流匹配和扩散模型在实时流式环境中部署时，由于需要从非信息性初始分布重复采样而导致的高推理延迟和系统积压问题

Method: 基于贝叶斯滤波原理，将流式推理视为学习将预测分布从一个时间步传输到下一个时间步的概率流，从先前后验分布初始化生成提供有原则的"热启动"

Result: 在多种预测、决策和状态估计任务中，该方法仅需一个或很少采样步骤即可达到与完整步扩散模型竞争的性能，采样速度更快

Conclusion: 通过贝叶斯滤波框架进行序列推理为流基模型的实时高效部署提供了新的有原则视角

Abstract: Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.

</details>


### [67] [GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL](https://arxiv.org/abs/2602.05323)
*Zifan Liu,Xinran Li,Shibo Chen,Jun Zhang*

Main category: cs.LG

TL;DR: GAS算法通过数据增强和重新标注增强离线安全强化学习的拼接能力，引入目标函数平衡奖励最大化与约束满足，在离线安全强化学习中实现更好的性能平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的离线安全强化学习方法面临两个主要挑战：1）缺乏从次优轨迹中"拼接"最优转移的能力；2）难以平衡奖励目标与成本目标，特别是当两者冲突时。

Method: 提出Goal-Assisted Stitching (GAS)算法：1）在转移级别增强和重新标注数据集，从次优轨迹构建高质量轨迹；2）引入目标函数估计最优可实现的奖励和成本目标；3）使用期望回归训练目标函数；4）通过重塑数据集实现更均匀的奖励-成本回报分布。

Result: 实证结果验证了GAS的有效性，在平衡奖励最大化与约束满足方面表现出优于现有方法的性能。

Conclusion: GAS通过增强拼接能力和引入目标函数，有效解决了离线安全强化学习中生成模型方法面临的挑战，实现了更好的奖励-成本平衡。

Abstract: Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to "stitch" optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.

</details>


### [68] [Pool-based Active Learning as Noisy Lossy Compression: Characterizing Label Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.05333)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 该论文提出了一个信息论框架来分析池式主动学习的理论极限，将其重新表述为有噪有损压缩问题，并推导了标签复杂度和泛化误差的下界。


<details>
  <summary>Details</summary>
Motivation: 现有池式主动学习缺乏统一的理论分析框架来理解其根本极限。论文旨在建立信息论基础，将池式主动学习与有噪有损压缩问题对应起来，从而能够统一分析数据选择和学习的理论性能。

Method: 提出信息论框架，将池式主动学习重新表述为有噪有损压缩问题：池观测映射到有噪符号观测，数据选择对应压缩，学习对应解码。应用有噪有损压缩的有限块长分析，推导标签复杂度和泛化误差的信息论下界。

Result: 推导出作为给定学习算法在其最优数据选择策略下的理论极限的标签复杂度和泛化误差下界。这些界包含反映学习算法引起的过拟合以及其归纳偏置与目标任务之间差异的项，并与已建立的信息论界和稳定性理论密切相关。

Conclusion: 该框架为池式主动学习提供了新的理论视角，首次将信息论界和稳定性理论应用于池式主动学习分析，揭示了数据选择和学习之间的内在联系及其理论极限。

Abstract: This paper proposes an information-theoretic framework for analyzing the theoretical limits of pool-based active learning (AL), in which a subset of instances is selectively labeled. The proposed framework reformulates pool-based AL as a noisy lossy compression problem by mapping pool observations to noisy symbol observations, data selection to compression, and learning to decoding. This correspondence enables a unified information-theoretic analysis of data selection and learning in pool-based AL. Applying finite blocklength analysis of noisy lossy compression, we derive information-theoretic lower bounds on label complexity and generalization error that serve as theoretical limits for a given learning algorithm under its associated optimal data selection strategy. Specifically, our bounds include terms that reflect overfitting induced by the learning algorithm and the discrepancy between its inductive bias and the target task, and are closely related to established information-theoretic bounds and stability theory, which have not been previously applied to the analysis of pool-based AL. These properties yield a new theoretical perspective on pool-based AL.

</details>


### [69] [Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting](https://arxiv.org/abs/2602.05371)
*Hongyi Li,Han Lin,Jun Xu*

Main category: cs.LG

TL;DR: HRT将决策树分裂重构为非线性最小二乘问题，通过两个线性预测器的最大/最小包络实现ReLU类表达能力，提供快速收敛和通用逼近能力。


<details>
  <summary>Details</summary>
Motivation: 斜决策树结合了树的透明性和多元决策边界的强大能力，但学习高质量斜分裂是NP难问题，现有方法依赖缓慢搜索或无理论启发式方法。

Method: 提出Hinge回归树(HRT)，将每个分裂重构为两个线性预测器上的非线性最小二乘问题，其最大/最小包络产生ReLU类表达能力。使用交替拟合程序，等价于固定分区内的阻尼牛顿法。

Result: 理论分析证明节点级优化在回溯线搜索变体中单调递减并收敛；实际中固定和自适应阻尼都能快速稳定收敛。HRT模型类是通用逼近器，具有显式O(δ²)逼近率。

Conclusion: HRT在合成和真实基准测试中匹配或优于单树基线，且结构更紧凑，为斜决策树学习提供了理论保证的高效方法。

Abstract: Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries, but learning high-quality oblique splits is NP-hard, and practical methods still rely on slow search or theory-free heuristics. We present the Hinge Regression Tree (HRT), which reframes each split as a non-linear least-squares problem over two linear predictors whose max/min envelope induces ReLU-like expressive power. The resulting alternating fitting procedure is exactly equivalent to a damped Newton (Gauss-Newton) method within fixed partitions. We analyze this node-level optimization and, for a backtracking line-search variant, prove that the local objective decreases monotonically and converges; in practice, both fixed and adaptive damping yield fast, stable convergence and can be combined with optional ridge regularization. We further prove that HRT's model class is a universal approximator with an explicit $O(δ^2)$ approximation rate, and show on synthetic and real-world benchmarks that it matches or outperforms single-tree baselines with more compact structures.

</details>


### [70] [Assessing Electricity Demand Forecasting with Exogenous Data in Time Series Foundation Models](https://arxiv.org/abs/2602.05390)
*Wei Soon Cheong,Lian Lian Jiang,Jamie Ng Suat Ling*

Main category: cs.LG

TL;DR: 该研究评估了时间序列基础模型在电力需求预测中利用外生特征的能力，发现基础模型性能高度可变，简单基线模型在稳定气候条件下经常优于基础模型，模型架构和地理环境是关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型已成为预测的新范式，但其有效利用外生特征的能力尚不明确，而外生特征对电力需求预测至关重要。本研究旨在通过实证评估基础模型在电力需求预测中的表现，特别是它们处理跨通道相关性和利用外生特征的能力。

Method: 在新加坡和澳大利亚电力市场，以小时和日粒度评估了MOIRAI、MOMENT、TinyTimeMixers、ChronosX和Chronos-2等基础模型，使用可逆实例归一化的LSTM作为基线。系统评估了三种特征配置：所有特征、选定特征和仅目标变量。重点关注模型在零样本设置下的表现。

Result: 基础模型效果高度可变：Chronos-2在基础模型中表现最佳，但在新加坡稳定气候条件下，简单基线模型经常优于所有基础模型，特别是在短期预测中。模型架构至关重要：TTM的通道混合和Chronos-2的分组注意力等协同架构能持续利用外生特征，而其他方法效果不一致。地理环境同样重要：基础模型主要在多变气候条件下表现出优势。

Conclusion: 研究结果挑战了基础模型普遍优越性的假设，强调了在能源领域需要特定领域模型的重要性。模型架构和地理环境是决定预测性能的关键因素，特别是在电力需求预测中利用外生特征的能力方面。

Abstract: Time-series foundation models have emerged as a new paradigm for forecasting, yet their ability to effectively leverage exogenous features -- critical for electricity demand forecasting -- remains unclear. This paper empirically evaluates foundation models capable of modeling cross-channel correlations against a baseline LSTM with reversible instance normalization across Singaporean and Australian electricity markets at hourly and daily granularities. We systematically assess MOIRAI, MOMENT, TinyTimeMixers, ChronosX, and Chronos-2 under three feature configurations: all features, selected features, and target-only. Our findings reveal highly variable effectiveness: while Chronos-2 achieves the best performance among foundation models (in zero-shot settings), the simple baseline frequently outperforms all foundation models in Singapore's stable climate, particularly for short-term horizons. Model architecture proves critical, with synergistic architectural implementations (TTM's channel-mixing, Chronos-2's grouped attention) consistently leveraging exogenous features, while other approaches show inconsistent benefits. Geographic context emerges as equally important, with foundation models demonstrating advantages primarily in variable climates. These results challenge assumptions about universal foundation model superiority and highlight the need for domain-specific models, specifically in the energy domain.

</details>


### [71] [Robust Federated Learning via Byzantine Filtering over Encrypted Updates](https://arxiv.org/abs/2602.05410)
*Adda Akram Bendoukha,Aymen Boudguiga,Nesrine Kaaniche,Renaud Sirdey,Didem Demirag,Sébastien Gambs*

Main category: cs.LG

TL;DR: 提出一种结合同态加密和元分类器的联邦学习方法，同时实现隐私保护聚合和拜占庭容错，通过属性推断攻击启发的SVM过滤器识别恶意更新，并在多个基准测试中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时仍面临隐私安全挑战，包括推理攻击导致敏感数据泄露以及拜占庭行为对训练模型的影响。现有方案往往独立处理安全聚合和拜占庭容错，难以同时实现这两方面目标。

Method: 1) 基于属性推断攻击思路，在标记的影子更新上训练一组过滤元分类器，模拟多种拜占庭攻击行为（后门、梯度反转、标签翻转和洗牌攻击），通过重新加权取消拜占庭加密更新；2) 提出自动化方法选择最优核函数和维度超参数，以适应同态推理、聚合约束和在CKKS密码系统上的效率；3) 使用SVM过滤器识别恶意更新。

Result: 在FEMNIST、CIFAR10、GTSRB和acsincome基准测试中验证了方法的有效性。SVM过滤器识别拜占庭更新的准确率达到90%-94%，仅带来模型效用的边际损失，加密推理运行时间在6-26秒之间，整体聚合时间在9-26秒之间。

Conclusion: 该方法成功解决了联邦学习中同时实现安全聚合和拜占庭容错的挑战，通过结合同态加密和基于属性推断的元分类器，在保护隐私的同时有效过滤恶意更新，为实际部署提供了可行的解决方案。

Abstract: Federated Learning (FL) aims to train a collaborative model while preserving data privacy. However, the distributed nature of this approach still raises privacy and security issues, such as the exposure of sensitive data due to inference attacks and the influence of Byzantine behaviors on the trained model. In particular, achieving both secure aggregation and Byzantine resilience remains challenging, as existing solutions often address these aspects independently. In this work, we propose to address these challenges through a novel approach that combines homomorphic encryption for privacy-preserving aggregation with property-inference-inspired meta-classifiers for Byzantine filtering. First, following the property-inference attacks blueprint, we train a set of filtering meta-classifiers on labeled shadow updates, reproducing a diverse ensemble of Byzantine misbehaviors in FL, including backdoor, gradient-inversion, label-flipping and shuffling attacks. The outputs of these meta-classifiers are then used to cancel the Byzantine encrypted updates by reweighting. Second, we propose an automated method for selecting the optimal kernel and the dimensionality hyperparameters with respect to homomorphic inference, aggregation constraints and efficiency over the CKKS cryptosystem. Finally, we demonstrate through extensive experiments the effectiveness of our approach against Byzantine participants on the FEMNIST, CIFAR10, GTSRB, and acsincome benchmarks. More precisely, our SVM filtering achieves accuracies between $90$% and $94$% for identifying Byzantine updates at the cost of marginal losses in model utility and encrypted inference runtimes ranging from $6$ to $24$ seconds and from $9$ to $26$ seconds for an overall aggregation.

</details>


### [72] [BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs](https://arxiv.org/abs/2602.05448)
*Sheshansh Agrawal,Thien Hang Nguyen,Douwe Kiela*

Main category: cs.LG

TL;DR: 提出基于锦标赛图的k-wise重排序框架，通过聚合k文档比较中的成对偏好信息，减少LLM调用次数，实现高效高质量检索重排序


<details>
  <summary>Details</summary>
Motivation: 现有LLM重排序方法要么依赖启发式方法未能充分利用每次排序决策的信息，要么效率低下。需要一种既能充分利用LLM判断信息又能高效执行的方法。

Method: 引入锦标赛图框架作为k-wise重排序的理论基础。将每次k文档比较视为包含binom(k,2)个成对偏好的完整锦标赛，聚合到全局偏好图中，通过传递闭包推导额外排序信息。设计查询调度策略贪婪最大化信息增益以识别top-m项，并处理非传递偏好（循环）为等价类生成分层排序。

Result: 在14个基准测试和5个LLM上的实验表明，该方法实现了对现有方法的帕累托支配：在达到或超过准确率的同时，比可比方法减少25-40%的token使用，比成对方法减少7倍token使用且质量相近。

Conclusion: 锦标赛图框架为LLM重排序提供了理论基础，通过有效利用每次比较中的成对偏好信息，显著减少了LLM调用成本，同时保持了高质量的排序结果，解决了现有方法的效率与效果权衡问题。

Abstract: Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\times$ fewer than pairwise methods at near-identical quality.

</details>


### [73] [When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL](https://arxiv.org/abs/2602.05459)
*Jan Malte Töpperwien,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 研究发现深度强化学习中的超参数敏感性并非RL问题固有，而是由自举目标中的破坏性梯度干扰加剧。在离线目标条件RL中，当存在适度专家数据时，基于准度量表示学习的算法比基于自举TD学习的算法表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中普遍存在的超参数敏感性通常被认为是不可避免的，但尚不清楚这是RL问题的固有特性还是特定训练机制加剧的结果。本研究旨在探究这一问题的本质，特别是在离线目标条件RL环境中，通过控制数据分布和非平稳性来系统分析超参数敏感性的来源。

Method: 研究采用离线目标条件RL设置，其中数据分布固定，非平稳性可通过数据质量的计划性变化进行显式控制。研究涵盖两种代表性算法：HIQL（基于自举TD学习）和QRL（准度量表示学习）。通过在不同数据质量（包括平稳和非平稳机制）下进行实验，并引入跨目标梯度对齐诊断来分析梯度干扰模式。

Result: 研究发现离线RL对超参数配置变化的鲁棒性显著高于在线RL，即使在受控非平稳条件下也是如此。当存在约20%的专家数据时，QRL保持广泛稳定的近最优区域，而HIQL表现出急剧的最优点且在不同训练阶段显著漂移。自举目标表现出更强的破坏性梯度干扰，这与超参数敏感性直接相关。

Conclusion: 深度强化学习训练中对超参数配置变化的高度敏感性并非不可避免，而是由自举动态加剧的。研究结果表明，通过减少破坏性梯度干扰，可以设计出更鲁棒的算法目标，为开发更稳定的RL方法提供了途径。

Abstract: Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\approx$ 20\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.

</details>


### [74] [Thermodynamic Limits of Physical Intelligence](https://arxiv.org/abs/2602.05463)
*Koichi Takahashi,Yusuke Hayashi*

Main category: cs.LG

TL;DR: 提出两种互补的比特/焦耳度量：热力学Epiplexity/焦耳（环境结构信息编码效率）和Empowerment/焦耳（传感器运动通道容量效率），建立物理智能的统一能效框架


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在获得卓越能力的同时消耗大量能源，需要建立连接智能与物理效率的度量标准，以支持一致的比特/焦耳比较

Method: 基于随机热力学提出两种互补的比特/焦耳度量：1) 热力学Epiplexity/焦耳 - 在给定边界内单位能量编码的环境结构信息比特数；2) Empowerment/焦耳 - 固定时间范围内单位预期能耗的传感器运动通道容量。建立包含边界/能量核算、粗粒化/噪声、时间范围/重置和成本约定的统一能效框架

Result: 证明了在明确子系统假设下，Landauer尺度的闭循环基准作为标准热力学学习不等式的推论；展示了在没有外部准备低熵资源假设下，信息增益与边界内耗散不必紧密关联；提出了MDL-epiplexity作为经验替代指标

Conclusion: 提出了统一的物理智能能效框架，包含两种互补的比特/焦耳度量和最小化检查清单，以减少歧义并支持一致的能效比较，为能源调整的规模分析提供基础

Abstract: Modern AI systems achieve remarkable capabilities at the cost of substantial energy consumption. To connect intelligence to physical efficiency, we propose two complementary bits-per-joule metrics under explicit accounting conventions: (1) Thermodynamic Epiplexity per Joule -- bits of structural information about a theoretical environment-instance variable newly encoded in an agent's internal state per unit measured energy within a stated boundary -- and (2) Empowerment per Joule -- the embodied sensorimotor channel capacity (control information) per expected energetic cost over a fixed horizon. These provide two axes of physical intelligence: recognition (model-building) vs.control (action influence). Drawing on stochastic thermodynamics, we show how a Landauer-scale closed-cycle benchmark for epiplexity acquisition follows as a corollary of a standard thermodynamic-learning inequality under explicit subsystem assumptions, and we clarify how Landauer-scaled costs act as closed-cycle benchmarks under explicit reset/reuse and boundary-closure assumptions; conversely, we give a simple decoupling construction showing that without such assumptions -- and without charging for externally prepared low-entropy resources (e.g.fresh memory) crossing the boundary -- information gain and in-boundary dissipation need not be tightly linked. For empirical settings where the latent structure variable is unavailable, we align the operational notion of epiplexity with compute-bounded MDL epiplexity and recommend reporting MDL-epiplexity / compression-gain surrogates as companions. Finally, we propose a unified efficiency framework that reports both metrics together with a minimal checklist of boundary/energy accounting, coarse-graining/noise, horizon/reset, and cost conventions to reduce ambiguity and support consistent bits-per-joule comparisons, and we sketch connections to energy-adjusted scaling analyses.

</details>


### [75] [A Unified Framework for Rethinking Policy Divergence Measures in GRPO](https://arxiv.org/abs/2602.05494)
*Qingyuan Wu,Yuhui Wang,Simon Sinong Zhan,Yanning Dai,Shilong Deng,Sarra Habchi,Qi Zhu,Matthias Gallé,Chao Huang*

Main category: cs.LG

TL;DR: 本文提出了一个统一的裁剪框架，将现有RLVR方法（如GRPO）统一到基于策略散度的理论框架中，并引入KL3估计器作为关键策略散度约束，在数学推理基准上提升了训练稳定性和最终性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法（如GRPO及其变体）通过裁剪似然比来约束策略差异以确保稳定更新，但缺乏对策略散度度量如何影响探索和性能的系统分析。需要建立一个统一的理论框架来理解不同策略散度度量的作用，并找到更有效的约束方法。

Method: 1. 提出了一个统一的裁剪框架，通过一般化的策略散度概念来表征现有方法，涵盖似然比和KL散度，并可扩展到其他度量。2. 识别出KL3估计器（KL散度的方差减少蒙特卡洛估计器）作为关键策略散度约束。3. 理论证明了基于KL3的约束在数学上等价于非对称比率裁剪，将概率质量重新分配到高置信度动作，促进更强的探索。4. 将KL3估计器集成到GRPO中，在数学推理基准上进行实证评估。

Result: 理论分析表明KL3约束在数学上等价于非对称比率裁剪，能促进更强的探索。实证结果表明，将KL3估计器集成到GRPO中，在数学推理基准上既提高了训练稳定性，又提升了最终性能，验证了基于原则的策略散度约束在策略优化中的重要性。

Conclusion: 本文提出的统一裁剪框架为系统分析策略散度度量对探索和性能的影响提供了理论基础。KL3估计器作为关键策略散度约束，在保持GRPO风格方法简单性的同时，通过非对称比率裁剪促进更强的探索，显著提升了训练稳定性和最终性能，强调了原则性策略散度约束在策略优化中的重要性。

Abstract: Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.

</details>


### [76] [Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification](https://arxiv.org/abs/2602.05535)
*Tao Huang,Rui Wang,Xiaofei Liu,Yi Qin,Li Duan,Liping Jing*

Main category: cs.LG

TL;DR: EUQ方法通过证据理论量化LVLM的内部冲突和知识缺失，有效检测幻觉、越狱等不良行为


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在面对不完整或对抗性输入时会产生不可靠甚至有害内容，现有不确定性量化方法仅捕捉整体认知不确定性，对检测此类问题效果有限

Method: 提出证据不确定性量化方法，将模型输出头特征解释为支持或反对证据，利用证据理论建模和聚合这些证据，在单次前向传播中量化内部冲突和知识差距

Result: 在幻觉、越狱、对抗性漏洞和分布外失败四类不良行为上，EUQ始终优于强基线方法，发现幻觉对应高内部冲突，分布外失败对应高知识缺失

Conclusion: EUQ提供了一种细粒度量化LVLM不确定性的方法，能有效检测模型不良行为，层间证据不确定性动态分析为理解内部表示演化提供了新视角

Abstract: Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.

</details>


### [77] [Steering Large Reasoning Models towards Concise Reasoning via Flow Matching](https://arxiv.org/abs/2602.05539)
*Yawei Li,Benjamin Bergner,Yinghan Zhao,Vihang Prakash Patil,Bei Chen,Cheng Wang*

Main category: cs.LG

TL;DR: FlowSteer：一种基于流匹配的非线性引导方法，通过建模完整分布变换来减少大型推理模型的冗余输出，相比线性方法更有效


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现出色，但输出往往过于冗长。现有引导方法基于线性表示假设，采用全局向量调整隐藏表示，这种方法限制性太强，无法有效控制推理过程

Method: 提出FlowSteer非线性引导方法，通过流匹配学习从冗长推理分布到简洁推理分布的完整变换，将其建模为速度场，实现精确的输入相关控制

Result: 在多样化推理基准测试中，FlowSteer相比领先的推理时基线方法，展现出更强的任务性能和令牌效率，能产生更紧凑的推理过程

Conclusion: 使用生成技术建模完整分布传输为控制大型推理模型提供了更有效和原则性的基础，超越了线性变换的限制

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.

</details>


### [78] [Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation](https://arxiv.org/abs/2602.05548)
*Zhiqi Yu,Zhangquan Chen,Mengting Liu,Heye Zhang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 本文提出A-GRAE方法，通过打破GRPO中固有的优势对称性来提升强化学习效率，解决了探索不足和难度适应问题。


<details>
  <summary>Details</summary>
Motivation: RLVR（特别是GRPO）已成为激发LLM推理的标准方法，但其在探索效率和难度适应方面仍存在挑战。研究发现这些瓶颈源于Group Relative Advantage Estimation（GRAE）中隐含的优势对称性，这种对称性导致两个关键限制：在群体层面阻碍新正确解的探索，在样本层面无法适应非平稳的难度聚焦需求。

Method: 提出Asymmetric GRAE（A-GRAE）方法，通过动态调节探索激励和样本难度聚焦来打破对称性。具体包括：（1）非对称地抑制正确轨迹的优势以鼓励必要探索；（2）采用课程式学习策略，先关注简单样本再逐渐转向复杂样本。

Result: 在七个基准测试上的实验表明，A-GRAE方法在LLMs和MLLMs上都能持续改进GRPO及其变体，验证了打破优势对称性的有效性。

Conclusion: GRAE中的优势对称性是次优的，打破这种对称性可以显著提升强化学习效率。A-GRAE通过动态调节探索激励和难度聚焦策略，为RLVR方法提供了更有效的优化框架。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.

</details>


### [79] [Logical Guidance for the Exact Composition of Diffusion Models](https://arxiv.org/abs/2602.05549)
*Francesco Alesiani,Jonathan Warrell,Tanja Bien,Henrik Christiansen,Matheus Ferraz,Mathias Niepert*

Main category: cs.LG

TL;DR: LOGDIFF是一个扩散模型引导框架，通过逻辑表达式实现精确约束生成，提出了布尔演算条件和混合引导方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在推理时难以处理复杂逻辑约束的精确生成，需要开发能够基于逻辑表达式进行精确引导的框架。

Method: 1. 推导精确布尔演算条件：当逻辑公式的电路表示中，合取连接条件独立子公式、析取连接条件独立或互斥子公式时，可实现精确逻辑引导。2. 提出混合引导方法：结合原子引导分数和后验概率估计，桥接分类器引导和分类器自由引导。3. 开发高效递归算法计算引导信号。

Result: 在多个图像和蛋白质结构生成任务中验证了框架的有效性，展示了基于复杂逻辑表达式的精确约束生成能力。

Conclusion: LOGDIFF为扩散模型提供了基于逻辑表达式的精确引导框架，通过布尔演算条件和混合引导方法，实现了复杂约束下的精确生成，可应用于多种生成任务。

Abstract: We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.
  We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.
  First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.
  Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.
  In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.
  Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.
  Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.
  We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.

</details>


### [80] [OpenMAG: A Comprehensive Benchmark for Multimodal-Attributed Graph](https://arxiv.org/abs/2602.05576)
*Chenxi Wan,Xunkai Li,Yilong Zuo,Haokun Deng,Sihan Li,Bowen Fan,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OpenMAG是一个全面的多模态属性图基准测试，整合了19个数据集、16种编码器、24个SOTA模型和8个下游任务，旨在解决现有基准在领域覆盖、编码器灵活性、模型多样性和任务范围方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态属性图模型的快速发展，现有基准测试在领域覆盖、编码器灵活性、模型多样性和任务范围方面存在严重不足，无法提供公平、统一的评估标准，阻碍了该领域的进一步发展。

Method: 构建OpenMAG基准测试，整合19个跨6个领域的数据集，支持16种静态和可训练的特征编码器，实现24个最先进模型的标准化库，并支持8个下游任务，提供统一的评估框架。

Result: 通过系统评估必要性、数据质量、有效性、鲁棒性和效率五个维度，得出了14个关于多模态属性图学习的基本见解，为未来研究提供指导。

Conclusion: OpenMAG为多模态属性图学习提供了一个全面、公平的评估基准，解决了现有基准的局限性，并通过系统评估得出了指导未来研究的重要见解。

Abstract: Multimodal-Attributed Graph (MAG) learning has achieved remarkable success in modeling complex real-world systems by integrating graph topology with rich attributes from multiple modalities. With the rapid proliferation of novel MAG models capable of handling intricate cross-modal semantics and structural dependencies, establishing a rigorous and unified evaluation standard has become imperative. Although existing benchmarks have facilitated initial progress, they exhibit critical limitations in domain coverage, encoder flexibility, model diversity, and task scope, presenting significant challenges to fair evaluation. To bridge this gap, we present OpenMAG, a comprehensive benchmark that integrates 19 datasets across 6 domains and incorporates 16 encoders to support both static and trainable feature encoding. OpenMAG further implements a standardized library of 24 state-of-the-art models and supports 8 downstream tasks, enabling fair comparisons within a unified framework. Through systematic assessment of necessity, data quality, effectiveness, robustness, and efficiency, we derive 14 fundamental insights into MAG learning to guide future advancements. Our code is available at https://github.com/YUKI-N810/OpenMAG.

</details>


### [81] [Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.05605)
*Jiaji Zhang,Hailiang Zhao,Guoxuan Zhu,Ruichao Sun,Jiaju Wu,Xinkui Zhao,Hanlin Tang,Weiyi Lu,Kan Liu,Tao Lan,Lin Qu,Shuiguang Deng*

Main category: cs.LG

TL;DR: Shiva-DiT：通过残差感知可微分Top-k选择解决扩散Transformer计算成本过高问题，实现确定性token计数和自适应剪枝，在SD3.5等模型上获得1.54倍加速且保持高质量


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer（DiTs）因自注意力二次缩放导致计算成本过高，现有剪枝方法无法同时满足可微分性、效率和硬件所需的严格静态预算要求

Method: 提出Shiva-DiT，采用残差感知可微分Top-k选择，通过残差感知直通估计器实现确定性token计数和端到端可学习性；引入上下文感知路由器和自适应比率策略来自主学习自适应剪枝调度

Result: 在SD3.5等主流模型上建立了新的帕累托前沿，实现了1.54倍的实际时钟加速，同时保持优于现有基线的保真度，有效消除了不规则张量开销

Conclusion: Shiva-DiT成功解决了DiTs计算效率问题，通过创新的可微分剪枝方法在保持生成质量的同时显著提升推理速度，为扩散模型的实际部署提供了有效解决方案

Abstract: Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.

</details>


### [82] [Path-Guided Flow Matching for Dataset Distillation](https://arxiv.org/abs/2602.05616)
*Xuhui Li,Zhengquan Luo,Xiwei Liu,Yongqiang Yu,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 提出基于流匹配的生成式数据集蒸馏框架PGFM，通过ODE确定性合成实现快速采样，在VAE潜空间学习类别条件传输，开发路径到原型引导算法实现可靠轨迹控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的数据集蒸馏方法依赖启发式引导或原型分配，存在采样耗时、轨迹不稳定问题，特别是在强控制或低IPC下会损害下游泛化性能。

Method: 提出路径引导流匹配(PGFM)框架：1) 在冻结VAE的潜空间进行流匹配，学习从高斯噪声到数据分布的类别条件传输；2) 开发连续路径到原型引导算法，实现ODE一致的路径控制，确保轨迹可靠到达指定原型同时保持多样性和效率。

Result: PGFM在高分辨率基准测试中匹配或超越先前基于扩散的蒸馏方法，采样步骤更少，同时提供具有显著改进效率的竞争性能，例如比基于扩散的方法效率提高7.6倍，模式覆盖率达78%。

Conclusion: PGFM是首个基于流匹配的生成式数据集蒸馏框架，通过确定性ODE求解实现快速合成，路径引导算法确保了可靠的轨迹控制，在效率和性能上均优于现有扩散基方法。

Abstract: Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\times$ more efficient than the diffusion-based counterparts with 78\% mode coverage.

</details>


### [83] [Mode-Dependent Rectification for Stable PPO Training](https://arxiv.org/abs/2602.05619)
*Mohamad Mohamad,Francesco Ponzio,Xavier Descombes*

Main category: cs.LG

TL;DR: MDR（Mode-Dependent Rectification）是一种轻量级双阶段训练程序，用于稳定PPO算法中模式相关层（如Batch Normalization）引起的训练-评估行为差异问题。


<details>
  <summary>Details</summary>
Motivation: 模式相关架构组件（如Batch Normalization或dropout）在视觉强化学习中常用，但会破坏on-policy优化的稳定性。在PPO中，Batch Normalization引起的训练与评估行为差异会导致策略不匹配、分布漂移和奖励崩溃。

Method: 提出Mode-Dependent Rectification（MDR），一种轻量级双阶段训练程序：第一阶段使用标准训练模式，第二阶段使用评估模式进行策略优化，通过这种方式稳定PPO在模式相关层下的训练，无需修改架构。

Result: 在程序生成游戏和真实世界补丁定位任务上的实验表明，MDR能持续改善稳定性和性能，并且能自然地扩展到其他模式相关层。

Conclusion: MDR有效解决了PPO中模式相关层引起的训练不稳定问题，提供了一种无需架构修改的稳定训练方案，适用于各种模式相关架构组件。

Abstract: Mode-dependent architectural components (layers that behave differently during training and evaluation, such as Batch Normalization or dropout) are commonly used in visual reinforcement learning but can destabilize on-policy optimization. We show that in Proximal Policy Optimization (PPO), discrepancies between training and evaluation behavior induced by Batch Normalization lead to policy mismatch, distributional drift, and reward collapse. We propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO under mode-dependent layers without architectural changes. Experiments across procedurally generated games and real-world patch-localization tasks demonstrate that MDR consistently improves stability and performance, and extends naturally to other mode-dependent layers.

</details>


### [84] [Rewards as Labels: Revisiting RLVR from a Classification Perspective](https://arxiv.org/abs/2602.05630)
*Zepeng Zhai,Meilin Chen,Jiaxuan Zhao,Junlang Qian,Lei Shen,Yuan Lu*

Main category: cs.LG

TL;DR: REAL框架将可验证奖励重新定义为分类标签而非标量权重，将策略优化重构为分类问题，解决了GRPO方法中的梯度错配和梯度主导问题，在数学推理基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习（RLVR）方法如GRPO及其变体在复杂推理任务中表现出色，但作者发现这些方法存在"正样本梯度错配"和"负样本梯度主导"问题，导致策略更新效率低下且次优。

Method: 提出REAL框架，将可验证奖励重新定义为分类标签而非标量权重，将策略优化重构为分类问题。在此基础上引入锚定logits来增强策略学习，实现单调有界的梯度加权，平衡不同rollout间的梯度分配。

Result: 在数学推理基准测试中，REAL显著提升了训练稳定性并持续优于GRPO及其变体如DAPO。在1.5B模型上，REAL平均Pass@1比DAPO提高6.7%；在7B模型上，REAL继续优于DAPO和GSPO，分别提升6.2%和1.7%。即使使用简单的二元交叉熵，REAL仍保持稳定并平均超过DAPO 4.5%。

Conclusion: REAL通过将可验证奖励重新定义为分类标签，有效解决了现有RLVR方法中的梯度分配问题，实现了更平衡、更稳定的策略优化，在数学推理任务中取得了显著性能提升，且具有良好的可扩展性。

Abstract: Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.

</details>


### [85] [Empowering Time Series Analysis with Large-Scale Multimodal Pretraining](https://arxiv.org/abs/2602.05646)
*Peng Chen,Siyuan Wang,Shiyan Hu,Xingjian Wu,Yang Shu,Zhongwen Rao,Meng Wang,Yijie Li,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: HORAI是一个频率增强的多模态时间序列基础模型，通过整合内生模态（图像和文本）和外生知识（新闻）来增强时间序列理解，在MM-TS数据集上预训练后，在零样本时间序列预测和异常检测任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型主要依赖单模态预训练，缺乏互补模态来增强理解。构建多模态基础模型面临两个关键挑战：1）缺乏统一的多模态预训练范式和大规模多模态语料库；2）如何有效整合异构模态并增强模型泛化能力。

Method: 提出多模态预训练范式，利用内生模态（衍生图像和文本）和外生知识（真实世界新闻）。开发自动化数据构建管道创建MM-TS数据集（首个大规模多模态时间序列数据集，涵盖6个领域，达10亿点）。提出HORAI模型，包含两个核心组件：频率增强跨模态编码器和时频解码器，用于有效融合多模态特征并增强跨模态和跨领域泛化。

Result: 在MM-TS数据集上预训练后，HORAI在时间序列预测和异常检测任务上实现了最先进的零样本性能，展示了强大的泛化能力。

Conclusion: 该研究为时间序列分析的多模态基础模型迈出了早期一步，通过统一的多模态预训练范式和频率增强架构，有效解决了异构模态整合和模型泛化问题，为时间序列理解提供了更全面的多视角分析框架。

Abstract: While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it faces key challenges: 1) lack of a unified multimodal pretraining paradigm and large-scale multimodal corpora for time series analysis; 2) how to effectively integrate heterogeneous modalities and enhance model generalization. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first propose a multimodal pretraining paradigm that leverages time series with endogenous modalities (derived images and text) and exogenous knowledge (real-world news), providing a comprehensive multi-view perspective for time series analysis. To support this, we develop an automated data construction pipeline to curate MM-TS, the first large-scale multimodal time series dataset spanning six domains, with up to one billion points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. It integrates two core components: the Frequency-enhanced Cross-Modality Encoder and the Time-Frequency Decoder, designed to effectively fuse multimodal features and enhance model generalization across modalities and domains. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong generalization.

</details>


### [86] [End-to-End Compression for Tabular Foundation Models](https://arxiv.org/abs/2602.05649)
*Guri Zabërgja,Rafiq Kamel,Arlind Kadra,Christian M. M. Frey,Josif Grabocka*

Main category: cs.LG

TL;DR: TACO是一个端到端的表格压缩模型，通过潜在空间压缩训练数据集，显著提升推理速度和内存效率，同时保持性能不显著下降。


<details>
  <summary>Details</summary>
Motivation: 基于注意力机制的表格基础模型虽然性能优异，但其二次复杂度限制了处理大规模数据集的能力，增加了训练和推理时间开销。

Method: 提出TACO端到端表格压缩模型，将训练数据集压缩到潜在空间中，减少计算和内存需求。

Result: 在TabArena基准测试中，推理速度提升高达94倍，内存消耗减少高达97%，性能无明显下降，且能更好地扩展到大规模数据集。

Conclusion: TACO方法在保持性能的同时，显著提升了表格基础模型的效率和可扩展性，为大规模表格数据处理提供了有效解决方案。

Abstract: The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.

</details>


### [87] [Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization](https://arxiv.org/abs/2602.05657)
*Aleksandar Armacki,Dragana Bajović,Dušan Jakovetić,Soummya Kar,Ali H. Sayed*

Main category: cs.LG

TL;DR: 该论文研究SGD诱导过程的长期尾部衰减行为，通过大偏差理论为SGD和c-SGD提供尾部概率的上界和下界，揭示了比现有有限时间分析更快的衰减速率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定概率阈值下的高概率保证，缺乏对固定误差阈值下尾部衰减率的研究。现有结果为有限时间性质，无法捕捉现代学习模型（通常训练数百万次迭代）的真实长期尾部衰减行为。

Method: 使用大偏差理论研究SGD基方法的长期尾部衰减。首先为非凸成本和有界噪声下的普通SGD提供梯度范数平方最佳迭代的尾部上界；其次考虑重尾噪声下的裁剪SGD(c-SGD)，放宽噪声假设；最后提供尾部衰减的下界。

Result: SGD的长期尾部衰减上界为e^{-t/log(t)}；c-SGD在p∈(1,2)时为e^{-t^{β_p}/log(t)}，其中β_p=4(p-1)/(3p-2)，p=2时为e^{-t/log^2(t)}；下界为e^{-t}。这些速率比现有有限时间分析快一个数量级。

Conclusion: 该工作填补了SGD尾部衰减研究的空白，通过大偏差理论提供了长期尾部衰减的严格分析，揭示了比现有结果更快的衰减速率，为单个算法运行提供了更强的长期保证。

Abstract: The study of tail behaviour of SGD-induced processes has been attracting a lot of interest, due to offering strong guarantees with respect to individual runs of an algorithm. While many works provide high-probability guarantees, quantifying the error rate for a fixed probability threshold, there is a lack of work directly studying the probability of failure, i.e., quantifying the tail decay rate for a fixed error threshold. Moreover, existing results are of finite-time nature, limiting their ability to capture the true long-term tail decay which is more informative for modern learning models, typically trained for millions of iterations. Our work closes these gaps, by studying the long-term tail decay of SGD-based methods through the lens of large deviations theory, establishing several strong results in the process. First, we provide an upper bound on the tails of the gradient norm-squared of the best iterate produced by (vanilla) SGD, for non-convex costs and bounded noise, with long-term decay at rate $e^{-t/\log(t)}$. Next, we relax the noise assumption by considering clipped SGD (c-SGD) under heavy-tailed noise with bounded moment of order $p \in (1,2]$, showing an upper bound with long-term decay at rate $e^{-t^{β_p}/\log(t)}$, where $β_p = \frac{4(p-1)}{3p-2}$ for $p \in (1,2)$ and $e^{-t/\log^2(t)}$ for $p = 2$. Finally, we provide lower bounds on the tail decay, at rate $e^{-t}$, showing that our rates for both SGD and c-SGD are tight, up to poly-logarithmic factors. Notably, our results demonstrate an order of magnitude faster long-term tail decay compared to existing work based on finite-time bounds, which show rates $e^{-\sqrt{t}}$ and $e^{-t^{β_p/2}}$, $p \in (1,2]$, for SGD and c-SGD, respectively. As such, we uncover regimes where the tails decay much faster than previously known, providing stronger long-term guarantees for individual runs.

</details>


### [88] [Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection](https://arxiv.org/abs/2602.05667)
*Ling Zhan,Zhen Li,Junjie Huang,Tao Jia*

Main category: cs.LG

TL;DR: 提出SCLCS方法，通过自监督学习选择代表性核心集，仅用10%数据即可保持FC模型性能排序，解决大规模fMRI数据中功能连接建模方法基准测试的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大规模fMRI数据集中功能连接（FC）建模方法的基准测试对于可重复神经科学研究至关重要，但模型-数据组合的爆炸式增长使得穷举评估计算上不可行，阻碍了这种评估成为常规的预分析步骤。

Method: 提出SCLCS（Structure-aware Contrastive Learning for Core-set Selection）自监督框架：1）使用自适应Transformer学习每个样本的独特FC结构；2）引入结构扰动评分（SPS）量化训练期间学习结构的稳定性，识别代表基础连接原型样本；3）通过top-k排名识别稳定样本，并引入密度平衡采样策略确保核心集结构稳健且分布具有代表性。

Result: 在REST-meta-MDD数据集上，SCLCS仅用10%数据即可保持真实模型性能排序，在排序一致性（nDCG@k）上比现有最优核心集选择方法提升高达23.2%。

Conclusion: 这是首个将核心集选择形式化用于FC算子基准测试的工作，使大规模算子比较成为计算神经科学中可行且不可或缺的部分，解决了FC基准测试的计算瓶颈问题。

Abstract: Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS

</details>


### [89] [Stable but Wrong: When More Data Degrades Scientific Conclusions](https://arxiv.org/abs/2602.05668)
*Zhipeng Zhang,Kai Li*

Main category: cs.LG

TL;DR: 论文揭示了数据驱动科学的一个根本性局限：即使统计推断过程稳定收敛、校准良好且通过常规诊断检查，仍可能系统性地得出错误结论，特别是在观测可靠性存在不可观测的退化时。


<details>
  <summary>Details</summary>
Motivation: 现代科学依赖大规模观测数据集和自动化推断流程，隐含假设更多数据能提高科学结论的可靠性。本文旨在挑战这一假设，揭示数据驱动科学中存在的根本性、不可逆的失效机制。

Method: 识别了一个结构性机制，在该机制下标准推断程序会平滑收敛、保持良好校准并通过常规诊断检查，但仍系统性地收敛到错误结论。使用最小化合成实验进行验证，展示在该机制下增加数据不仅无法纠正错误，反而会放大错误，而基于残差和拟合优度的诊断仍保持误导性的正常状态。

Result: 发现当观测可靠性以推断过程本身不可观测的方式退化时，标准推断程序会系统性地失效。增加数据会放大错误而非纠正错误，而常规诊断检查（残差分析、拟合优度检验）仍显示正常，形成误导性确认。

Conclusion: 揭示了数据驱动科学的内在局限：稳定性、收敛性和置信度不足以作为认识有效性的指标。推断不能仅被视为数据可用性的无条件结果，而必须受到观测过程完整性的明确约束。

Abstract: Modern science increasingly relies on ever-growing observational datasets and automated inference pipelines, under the implicit belief that accumulating more data makes scientific conclusions more reliable. Here we show that this belief can fail in a fundamental and irreversible way. We identify a structural regime in which standard inference procedures converge smoothly, remain well calibrated, and pass conventional diagnostic checks, yet systematically converge to incorrect conclusions. This failure arises when the reliability of observations degrades in a manner that is intrinsically unobservable to the inference process itself. Using minimal synthetic experiments, we demonstrate that in this regime additional data do not correct error but instead amplify it, while residual-based and goodness-of-fit diagnostics remain misleadingly normal. These results reveal an intrinsic limit of data-driven science: stability, convergence, and confidence are not sufficient indicators of epistemic validity. We argue that inference cannot be treated as an unconditional consequence of data availability, but must instead be governed by explicit constraints on the integrity of the observational process.

</details>


### [90] [Perception-Based Beliefs for POMDPs with Visual Observations](https://arxiv.org/abs/2602.05679)
*Miriam Schäfers,Merlijn Krale,Thiago D. Simão,Nils Jansen,Maximilian Weininger*

Main category: cs.LG

TL;DR: PBP框架通过引入感知模型（图像分类器）将视觉观测映射到状态概率分布，使传统POMDP求解器能够处理高维观测问题，无需显式推理观测空间。


<details>
  <summary>Details</summary>
Motivation: 传统基于信念和滤波的POMDP求解器在处理具有高维观测（如相机图像）的现实问题时计算不可行，需要一种能有效处理视觉观测的方法。

Method: 提出PBP框架，使用图像分类器将视觉观测映射到状态概率分布，直接将这些分布纳入信念更新；引入不确定性量化处理分类器不精确性，并提出两种调整信念更新的方法。

Result: 实验表明：(1) PBP优于现有的端到端深度强化学习方法；(2) 不确定性量化提高了PBP对视觉损坏的鲁棒性。

Conclusion: PBP框架通过结合感知模型和传统POMDP求解器，有效解决了高维视觉观测问题，且通过不确定性量化增强了系统鲁棒性。

Abstract: Partially observable Markov decision processes (POMDPs) are a principled planning model for sequential decision-making under uncertainty. Yet, real-world problems with high-dimensional observations, such as camera images, remain intractable for traditional belief- and filtering-based solvers. To tackle this problem, we introduce the Perception-based Beliefs for POMDPs framework (PBP), which complements such solvers with a perception model. This model takes the form of an image classifier which maps visual observations to probability distributions over states. PBP incorporates these distributions directly into belief updates, so the underlying solver does not need to reason explicitly over high-dimensional observation spaces. We show that the belief update of PBP coincides with the standard belief update if the image classifier is exact. Moreover, to handle classifier imprecision, we incorporate uncertainty quantification and introduce two methods to adjust the belief update accordingly. We implement PBP using two traditional POMDP solvers and empirically show that (1) it outperforms existing end-to-end deep RL methods and (2) uncertainty quantification improves robustness of PBP against visual corruption.

</details>


### [91] [Almost Asymptotically Optimal Active Clustering Through Pairwise Observations](https://arxiv.org/abs/2602.05690)
*Rachel S. Y. Teo,P. N. Karthik,Ramya Korlakai Vinayak,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出了一种新的聚类分析框架，用于将M个项目划分为未知数量K个簇，通过主动查询和噪声反馈进行聚类，建立了查询数量的理论下界并设计了渐近最优算法。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法通常假设数据是静态且无噪声的，但在实际应用中，数据收集往往是主动的、有噪声的，且聚类数量未知。本文旨在解决在主动查询和噪声反馈条件下，如何高效准确地进行聚类的问题。

Method: 使用变化测度技术建立了聚类准确度所需查询数量的理论下界，设计了一个渐近最优算法，其停止准则基于广义似然比统计量，并开发了计算可行的GLR统计量变体。

Result: 建立了聚类问题的基本下界，证明了所提算法的渐近最优性，并展示了计算可行变体的性能差距可以准确估计，且保持在理论下界的常数倍范围内。

Conclusion: 本文为主动噪声聚类问题提供了完整的理论框架和实用算法，通过理论下界和渐近最优算法设计，解决了未知簇数量条件下的高效聚类问题。

Abstract: We propose a new analysis framework for clustering $M$ items into an unknown number of $K$ distinct groups using noisy and actively collected responses. At each time step, an agent is allowed to query pairs of items and observe bandit binary feedback. If the pair of items belongs to the same (resp.\ different) cluster, the observed feedback is $1$ with probability $p>1/2$ (resp.\ $q<1/2$). Leveraging the ubiquitous change-of-measure technique, we establish a fundamental lower bound on the expected number of queries needed to achieve a desired confidence in the clustering accuracy, formulated as a sup-inf optimization problem. Building on this theoretical foundation, we design an asymptotically optimal algorithm in which the stopping criterion involves an empirical version of the inner infimum -- the Generalized Likelihood Ratio (GLR) statistic -- being compared to a threshold. We develop a computationally feasible variant of the GLR statistic and show that its performance gap to the lower bound can be accurately empirically estimated and remains within a constant multiple of the lower bound.

</details>


### [92] [FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning](https://arxiv.org/abs/2602.05693)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: FedRandom是一种缓解联邦学习中参与者贡献评估不稳定性的新方法，通过生成更多样本来提供更一致的贡献评估


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，公平评估参与者贡献对于激励机制、识别恶意参与者和搭便车者至关重要，但现有方法存在显著的不稳定性，可能影响参与者参与意愿

Method: 提出FedRandom方法，将贡献不稳定性视为统计估计问题，通过生成比常规联邦学习策略更多的样本来提供更一致可靠的参与者贡献评估

Result: 在CIFAR-10、MNIST、CIFAR-100和FMNIST数据集上的实验表明，FedRandom在超过一半的评估场景中将与真实值的总体距离减少了三分之一以上，并在超过90%的情况下提高了稳定性

Conclusion: FedRandom有效缓解了联邦学习中参与者贡献评估的不稳定性问题，为公平激励机制和恶意参与者识别提供了更可靠的评估方法

Abstract: Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.

</details>


### [93] [Fix Representation (Optimally) Before Fairness: Finite-Sample Shrinkage Population Correction and the True Price of Fairness Under Subpopulation Shift](https://arxiv.org/abs/2602.05707)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 论文研究了机器学习中预测准确性与群体公平性约束之间的权衡关系，发现现有观察到的"公平性有助于准确性"现象可能是训练数据中群体比例失真的伪影，提出了基于收缩加权的优化评估协议。


<details>
  <summary>Details</summary>
Motivation: 机器学习实践中经常观察到预测准确性与群体公平性约束之间的紧张关系，但有时公平性干预似乎能提高准确性。作者认为这两种现象都可能是训练数据中群体比例失真的产物，需要建立理论框架来解释这些观察结果。

Method: 在亚群体偏移（组内分布稳定，组间比例偏移）的设定下，作者建立了理论分析框架：1）完全重要性加权校正是渐近无偏但有限样本次优的；2）最优有限样本校正是收缩加权，在目标分布和训练分布之间插值；3）提出了可操作的评估协议：在考虑公平性之前先（最优地）修正表示，将公平性干预与收缩校正基线进行比较。

Result: 理论分析表明，表面上"公平性有助于准确性"的现象可能源于将公平性方法与不恰当加权的基线进行比较。在合成数据和真实世界基准（Adult、COMPAS）上的实验验证了理论预测，并证明该协议能消除虚假权衡，揭示真正的公平性-效用前沿。

Conclusion: 论文提出了一个基于收缩加权的评估协议，要求先最优修正表示再考虑公平性，将公平性干预与收缩校正基线进行比较，从而隔离公平性的真实、不可约成本。这一协议能消除虚假的公平性-准确性权衡，揭示真实的公平性-效用边界。

Abstract: Machine learning practitioners frequently observe tension between predictive accuracy and group fairness constraints -- yet sometimes fairness interventions appear to improve accuracy. We show that both phenomena can be artifacts of training data that misrepresents subgroup proportions. Under subpopulation shift (stable within-group distributions, shifted group proportions), we establish: (i) full importance-weighted correction is asymptotically unbiased but finite-sample suboptimal; (ii) the optimal finite-sample correction is a shrinkage reweighting that interpolates between target and training mixtures; (iii) apparent "fairness helps accuracy" can arise from comparing fairness methods to an improperly-weighted baseline. We provide an actionable evaluation protocol: fix representation (optimally) before fairness -- compare fairness interventions against a shrinkage-corrected baseline to isolate the true, irreducible price of fairness. Experiments on synthetic and real-world benchmarks (Adult, COMPAS) validate our theoretical predictions and demonstrate that this protocol eliminates spurious tradeoffs, revealing the genuine fairness-utility frontier.

</details>


### [94] [Muon in Associative Memory Learning: Training Dynamics and Scaling Laws](https://arxiv.org/abs/2602.05725)
*Binghui Li,Kaifei Wang,Han Zhong,Pinyan Lu,Liwei Wang*

Main category: cs.LG

TL;DR: Muon优化器在梯度符号更新矩阵参数方面表现出色，但理论机制不明确。本文在线性关联记忆模型中分析Muon，发现它能缓解梯度下降学习频率分量的不平衡问题，在无噪声情况下实现指数级加速，在有噪声情况下展示更优的缩放效率。


<details>
  <summary>Details</summary>
Motivation: Muon优化器通过梯度符号更新矩阵参数在实践中表现优异，但其动态特性和缩放行为在理论上仍不清楚。需要建立理论框架来理解Muon相对于传统梯度下降的优势机制。

Method: 采用线性关联记忆模型，包含softmax检索和查询-答案对的层次频率谱，分析有无标签噪声两种情况。通过理论分析比较梯度下降和Muon优化器的学习动态，推导优化缩放定律，并将Muon解释为隐式矩阵预处理器。

Result: 在无噪声情况下，Muon相比梯度下降实现指数级加速；在有噪声且频率谱呈幂律衰减的情况下，推导出Muon的优化缩放定律，展示其优于梯度下降的缩放效率。Muon可解释为自适应任务对齐和块对称梯度结构产生的隐式矩阵预处理器。

Conclusion: Muon优化器能有效缓解梯度下降学习频率分量的不平衡问题，在理论和实践中均表现出优越性能。在线性关联记忆模型中，Muon在无噪声和有噪声情况下都优于梯度下降，其机制可解释为隐式矩阵预处理。合成长尾分类和LLaMA风格预训练实验验证了理论分析。

Abstract: Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.

</details>


### [95] [CSRv2: Unlocking Ultra-Sparse Embeddings](https://arxiv.org/abs/2602.05735)
*Lixuan Guo,Yifei Wang,Tiansheng Wen,Yifan Wang,Aosong Feng,Bo Chen,Stefanie Jegelka,Chenyu You*

Main category: cs.LG

TL;DR: CSRv2是一种改进的对比稀疏表示训练方法，通过渐进式k退火、监督对比目标和全骨干微调，使超稀疏嵌入（仅2个活跃特征）变得可行，在保持性能的同时大幅提升计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有对比稀疏表示（CSR）在超稀疏状态下（超过80%神经元不活跃）性能严重下降，无法实现其效率潜力。需要一种方法使超稀疏嵌入变得可行，以充分发挥稀疏表示在存储、内存和推理延迟方面的优势。

Method: CSRv2采用三个关键技术：1）渐进式k退火稳定稀疏学习；2）监督对比目标提升表示质量；3）全骨干微调确保端到端适应性。这些方法共同解决了CSR在超稀疏状态下的性能退化问题。

Result: CSRv2将死神经元比例从80%降至20%，在k=2时带来14%的准确率提升。超稀疏嵌入（仅2个活跃特征）性能与CSR在k=8和MRL在32维时相当。相比MRL获得7倍加速，相比密集嵌入在文本表示中计算和内存效率提升高达300倍。

Conclusion: CSRv2使超稀疏嵌入变得实用而不牺牲性能，在文本和视觉任务中相比CSR有显著改进（k=4时7%/4%，k=2时14%/6%）。这扩展了实时和边缘部署AI系统的设计空间，其中嵌入质量和效率都至关重要。

Abstract: In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.

</details>


### [96] [How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning](https://arxiv.org/abs/2602.05749)
*Kai Ming Ting,Wei-Jie Xu,Hang Zhang*

Main category: cs.LG

TL;DR: 该论文质疑深度聚类相对于k-means的优势，通过分析深度嵌入聚类(DEC)发现，深度学习方法并未真正解决k-means在发现任意形状、大小和密度集群方面的根本限制，而非深度学习方法通过利用数据分布信息反而能更好地实现深度聚类的目标。


<details>
  <summary>Details</summary>
Motivation: 深度聚类常被认为优于k-means聚类，但这种优势通常仅在图像数据集上得到验证。作者质疑深度聚类是否真正解决了k-means聚类的根本限制，即无法发现任意形状、大小和密度的集群。同时，作者指出当前深度聚类方法普遍未利用底层数据分布信息。

Method: 论文通过分析深度嵌入聚类(DEC)方法进行研究。DEC使用自编码器学习潜在表示，然后基于k-means类过程进行聚类，并以端到端方式进行优化。作者将DEC的表现与非深度学习方法进行比较，后者通过利用数据集的集群分布信息来解决问题。

Result: 研究发现深度学习的表示并未使DEC克服k-means聚类的根本限制。相反，非深度学习方法通过利用集群的分布信息，能够更有效地解决这些限制，实现了深度聚类原本期望达到的目标。

Conclusion: 深度聚类方法并未真正解决k-means的根本限制，因为它们未利用底层数据分布信息。非深度学习方法通过利用集群分布信息，反而能更好地实现深度聚类的目标，这对深度聚类方法的有效性提出了重要质疑。

Abstract: Deep clustering (DC) is often quoted to have a key advantage over $k$-means clustering. Yet, this advantage is often demonstrated using image datasets only, and it is unclear whether it addresses the fundamental limitations of $k$-means clustering. Deep Embedded Clustering (DEC) learns a latent representation via an autoencoder and performs clustering based on a $k$-means-like procedure, while the optimization is conducted in an end-to-end manner. This paper investigates whether the deep-learned representation has enabled DEC to overcome the known fundamental limitations of $k$-means clustering, i.e., its inability to discover clusters of arbitrary shapes, varied sizes and densities. Our investigations on DEC have a wider implication on deep clustering methods in general. Notably, none of these methods exploit the underlying data distribution. We uncover that a non-deep learning approach achieves the intended aim of deep clustering by making use of distributional information of clusters in a dataset to effectively address these fundamental limitations.

</details>


### [97] [Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance](https://arxiv.org/abs/2602.05774)
*Xiandong Zou,Jianshu Li,Jing Huang,Pan Zhou*

Main category: cs.LG

TL;DR: VSD通过变分推理框架优化草稿路径训练，最大化目标模型接受概率，显著提升推测解码效率


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在训练与解码的不一致：传统方法优化单个贪婪轨迹，而实际解码需要验证和排序多个采样草稿路径

Method: 提出变分推测解码(VSD)，将草稿训练建模为潜在提案(草稿路径)的变分推理，最大化目标模型接受概率。通过EM算法优化：E步从后验分布采样，M步使用自适应拒绝加权和置信感知正则化最大化加权似然

Result: 理论分析证实VSD增加期望接受长度和加速比。在LLM和MLLM上的实验显示，VSD相比EAGLE-3提升9.6%加速，相比ViSpec提升7.9%加速

Conclusion: VSD通过变分推理框架有效解决了训练与解码的不一致问题，显著提升了推测解码的效率

Abstract: Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.

</details>


### [98] [Cross-Domain Offline Policy Adaptation via Selective Transition Correction](https://arxiv.org/abs/2602.05776)
*Mengbei Yan,Jiafei Lyu,Shengjie Sun,Zhongjian Qiao,Jingwen Yang,Zichuan Lin,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: 提出STC算法，通过选择性转换校正解决跨域离线强化学习中的动态不匹配问题，利用逆策略模型和奖励模型校正源域数据，并通过前向动态模型筛选保留校正样本


<details>
  <summary>Details</summary>
Motivation: 跨域离线强化学习中，源域和目标域存在动态不匹配时，直接合并数据集会导致性能下降。现有方法通过源域转换过滤或奖励修改可能无法充分利用有价值的源域数据

Method: 提出选择性转换校正（STC）算法：1）使用逆策略模型和奖励模型校正源域转换的动作和奖励，使其与目标域动态对齐；2）利用前向动态模型筛选保留校正后比原始转换更匹配目标动态的样本

Result: 在多种具有动态偏移的环境中进行实验，STC相比现有基线方法取得了更优的性能表现

Conclusion: STC算法通过选择性转换校正实现了源域数据在策略适应中的可靠使用，有效解决了跨域离线强化学习中的动态不匹配问题

Abstract: It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.

</details>


### [99] [Distributional Reinforcement Learning with Diffusion Bridge Critics](https://arxiv.org/abs/2602.05783)
*Shutong Ding,Yimiao Zhou,Ke Hu,Mokai Pan,Shan Zhong,Yanwei Fu,Jingya Wang,Ye Shi*

Main category: cs.LG

TL;DR: 提出首个基于扩散桥的分布强化学习评论家模型DBC，通过直接建模Q值的逆累积分布函数来准确捕捉价值分布，避免坍缩为平凡高斯分布


<details>
  <summary>Details</summary>
Motivation: 现有扩散强化学习方法主要关注扩散策略，而忽略了扩散评论家。由于策略优化本质上依赖于评论家，准确的价值估计比策略表达能力更为重要。此外，考虑到大多数强化学习任务的随机性，评论家更适合用分布模型来描述

Method: 提出扩散桥评论家(DBC)，直接建模Q值的逆累积分布函数，利用扩散桥的强大分布匹配能力准确捕捉价值分布。进一步推导解析积分公式来解决DBC中的离散化误差问题，这对价值估计至关重要

Result: 在MuJoCo机器人控制基准测试中，DBC相比之前的分布评论家模型表现出优越性。DBC是一个即插即用组件，可以集成到大多数现有的强化学习框架中

Conclusion: DBC是首个采用扩散桥模型作为评论家的工作，通过直接建模逆CDF准确捕捉价值分布，解决了现有方法中价值分布坍缩为平凡高斯分布的问题，在连续控制任务中表现出色

Abstract: Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.

</details>


### [100] [Classification Under Local Differential Privacy with Model Reversal and Model Averaging](https://arxiv.org/abs/2602.05797)
*Caihong Qin,Yang Bai*

Main category: cs.LG

TL;DR: 该论文将LDP下的隐私学习重新解释为迁移学习问题，提出三种新技术来提升分类性能而不损害隐私：噪声二进制反馈评估、模型反转和模型平均。


<details>
  <summary>Details</summary>
Motivation: 本地差分隐私（LDP）虽然提供了强大的隐私保证，但引入的噪声会显著降低数据效用。现有方法在LDP下的分类性能有限，需要新的技术来提升数据效用而不损害隐私保护。

Method: 1. 将LDP下的隐私学习重新解释为迁移学习问题：噪声数据作为源域，未观测的干净数据作为目标域；2. 噪声二进制反馈评估机制：用于估计数据集效用；3. 模型反转：通过反转决策边界来挽救性能不佳的分类器；4. 模型平均：根据估计的效用为多个反转分类器分配权重。

Result: 提供了LDP下的理论超额风险界限，并展示了所提方法如何降低这一风险。在模拟和真实数据集上的实证结果表明，分类准确率有显著提升。

Conclusion: 通过将LDP下的隐私学习重新框架化为迁移学习问题，并引入专门设计的评估、反转和平均技术，可以在不损害隐私的前提下显著提升分类性能，为解决LDP中的数据效用问题提供了新的有效途径。

Abstract: Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.

</details>


### [101] [Principled Confidence Estimation for Deep Computed Tomography](https://arxiv.org/abs/2602.05812)
*Matteo Gätzner,Johannes Kirschner*

Main category: cs.LG

TL;DR: 基于序列似然混合框架为CT重建提供理论覆盖保证的置信度估计方法，适用于经典算法和深度学习重建，能检测幻觉并提供可解释的可视化。


<details>
  <summary>Details</summary>
Motivation: 为基于深度学习的CT重建建立具有理论覆盖保证的置信度估计框架，解决医学成像中不确定性量化的问题，使深度学习模型不仅作为强大的估计器，还能作为可靠的不确定性感知工具。

Method: 基于序列似然混合框架，考虑符合Beer-Lambert定律的现实前向模型（具有泊松噪声的对数线性模型），为经典算法和深度学习重建方法（包括U-Net、U-Net集成和生成扩散模型）建立置信区域。

Result: 深度重建方法相比经典重建产生显著更紧的置信区域，同时不牺牲理论覆盖保证；能够检测重建图像中的幻觉，并提供置信区域的可解释可视化。

Conclusion: 该框架确立了深度学习模型不仅是强大的估计器，也是医学成像中可靠的不确定性感知工具，为CT重建提供了具有理论保证的置信度估计。

Abstract: We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.

</details>


### [102] [Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers](https://arxiv.org/abs/2602.05813)
*Artem Riabinin,Andrey Veprikov,Arman Bolatov,Martin Takáč,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 论文提出了一种用于范数约束优化器（如Muon和Lion）的自适应学习率调度方法，通过广义光滑性假设推导出自然包含预热和衰减的学习率方案，并开发了仅需标准超参数的自适应预热调度器，在LLaMA架构的大语言模型预训练中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前范数约束优化器（如Muon和Lion）的学习率调度通常依赖启发式的预热和衰减策略，缺乏理论依据且需要大量手动调优。本文旨在建立理论基础并开发自适应调度方法，减少超参数搜索成本。

Method: 1. 提出广义光滑性假设：局部曲率随次优性间隙减小；2. 在该假设下建立收敛保证，推导出自然包含预热和衰减的学习率选择；3. 开发实用调度器，仅需标准超参数，自动适应预热时长；4. 在LLaMA架构的大语言模型预训练中进行实证评估。

Result: 自适应预热调度在所有考虑的LLaMA架构设置中，一致优于或至少匹配最佳手动调优的预热方案，无需额外超参数搜索。实验验证了广义光滑性假设在优化轨迹上的有效性。

Conclusion: 基于理论推导的自适应学习率调度方法为范数约束优化器提供了有效的解决方案，减少了手动调优需求，在保持性能的同时提高了训练效率，具有实际应用价值。

Abstract: We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.
  Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup

</details>


### [103] [Synthesizing Realistic Test Data without Breaking Privacy](https://arxiv.org/abs/2602.05833)
*Laura Plein,Alexi Turcotte,Arina Hallemans,Andreas Zeller*

Main category: cs.LG

TL;DR: 提出一种基于模糊测试和判别器的合成数据生成方法，通过间接利用原始数据生成统计特性相似但隐私保护的测试数据集


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的合成数据生成方法存在准确度不足或易受成员推理攻击、数据集重建攻击的问题，因为训练过程直接使用了原始数据。需要一种既能保持原始数据统计特性又能保护隐私的合成数据生成方法。

Method: 受GAN启发，采用生成-判别框架：1) 使用测试生成器（模糊器）根据输入规范生成测试数据，保留原始数据的约束；2) 判别器模型评估生成数据与原始数据的接近程度；3) 通过进化样本和判别器筛选"好样本"，生成隐私保护且统计特性相似的数据。

Result: 在四个用于评估最先进技术的数据集上进行实验，结果表明该方法在生成具有高实用性的合成数据集同时能有效保护隐私。

Conclusion: 该方法展示了生成既保持高实用性又能保护隐私的合成数据集的潜力，通过间接利用原始数据避免了直接训练带来的隐私风险。

Abstract: There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining "good samples" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.

</details>


### [104] [DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders](https://arxiv.org/abs/2602.05859)
*Xu Wang,Bingqing Jiang,Yu Wan,Baosong Yang,Lingpeng Kong,Difan Zou*

Main category: cs.LG

TL;DR: 本文提出了DLM-Scope，这是首个针对扩散语言模型（DLMs）的稀疏自编码器（SAE）可解释性框架，展示了SAE在DLMs中提取可解释特征的有效性，并发现了与自回归LLMs不同的特性。


<details>
  <summary>Details</summary>
Motivation: 随着扩散语言模型（DLMs）成为自回归大语言模型（LLMs）的有前景替代方案，需要为这类新兴模型开发专门的可解释性工具。稀疏自编码器（SAEs）已成为自回归LLMs机制可解释性的标准工具，但尚未应用于DLMs。

Method: 提出了DLM-Scope框架，使用Top-K稀疏自编码器（SAEs）从扩散语言模型中提取稀疏、可解释的特征。研究了SAE插入对DLMs的影响，比较了与自回归LLMs的差异，并探索了SAE在DLM解码顺序和训练后稳定性方面的应用。

Result: 1. Top-K SAEs能够从DLMs中忠实提取可解释特征；2. SAE插入对DLMs的影响与LLMs不同：在DLMs早期层插入SAE可以减少交叉熵损失，而LLMs通常会产生损失惩罚；3. DLM中的SAE特征支持更有效的扩散时间干预，性能常优于LLM引导；4. SAE可为DLM解码顺序提供有用信号；5. SAE特征在DLM训练后阶段保持稳定。

Conclusion: DLM-Scope为扩散语言模型的机制可解释性奠定了基础，展示了SAE在DLM相关任务和算法中的巨大应用潜力，为这一新兴模型类别提供了专门的可解释性工具。

Abstract: Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.

</details>


### [105] [Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations](https://arxiv.org/abs/2602.05885)
*Wei Liu,Jiawei Xu,Yingru Li,Longtao Zheng,Tianjian Li,Qian Liu,Junxian He*

Main category: cs.LG

TL;DR: 本文提出KernelGYM分布式GPU环境和TRLOO方法，用于训练LLM生成高性能计算内核，解决奖励黑客和懒惰优化问题，Dr.Kernel-14B模型在KernelBench上性能超越Claude-4.5-Sonnet和GPT-5。


<details>
  <summary>Details</summary>
Motivation: 高质量计算内核对可扩展AI系统至关重要，但训练LLM生成此类代码面临数据不足、环境脆弱、奖励黑客和懒惰优化等问题，需要系统研究强化学习方法。

Method: 设计KernelGYM分布式GPU环境支持奖励黑客检查、多轮交互数据收集和长期RL训练；提出TRLOO解决GRPO中的偏置策略梯度问题；引入不匹配校正、基于性能分析的奖励(PR)和拒绝采样(PRS)缓解懒惰优化。

Result: Dr.Kernel-14B在KernelBench上达到与Claude-4.5-Sonnet竞争的性能；在Level-2子集上，31.6%生成的内核比Torch参考实现快1.2倍以上，超过Claude-4.5-Sonnet(26.7%)和GPT-5(28.6%)；多轮最佳候选选择下，1.2倍加速率提升至47.8%。

Conclusion: KernelGYM环境和TRLOO方法有效解决了内核生成中的奖励黑客和懒惰优化问题，Dr.Kernel-14B展示了在计算内核生成任务上的强大性能，为AI系统优化提供了实用工具。

Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.

</details>


### [106] [Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting](https://arxiv.org/abs/2602.05887)
*Tianqi Shen,Jinji Yang,Junze He,Kunhan Gao,Ziye Ma*

Main category: cs.LG

TL;DR: 提出一种模拟过参数化逃逸方向(SOD)的确定性框架，用于逃离低秩矩阵感知中的虚假局部最小值，无需实际进行张量提升，保证目标函数值严格下降。


<details>
  <summary>Details</summary>
Motivation: 低秩矩阵感知作为基础但具有挑战性的非凸问题，其优化景观包含大量虚假局部最小值，使得基于梯度的优化器难以收敛到全局最优。虽然过参数化通过张量提升可以将局部最小值转换为严格鞍点，但实际进行张量提升计算上不可行。

Method: 提出模拟过参数化逃逸方向(SOD)机制：设计数学框架模拟过参数化空间的景观和逃逸方向，将过参数化逃逸方向投影到原始参数空间，保证从现有局部最小值出发目标函数值严格下降，无需实际进行张量提升。

Result: 数值实验表明该框架能可靠地逃离局部最小值并促进收敛到全局最优，与显式张量过参数化相比计算成本最小。这是第一个能够保证逃离虚假局部最小值的确定性框架，无需使用随机扰动或启发式估计。

Conclusion: 该框架展示了如何利用模拟过参数化来驯服具有挑战性的优化景观，对超越矩阵感知的非凸优化具有非平凡的意义，为处理非凸优化问题提供了新的确定性方法。

Abstract: Low-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. Recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. Motivated by this observation, we propose a Simulated Oracle Direction (SOD) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. In essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. To the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. Numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. We believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.

</details>


### [107] [DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training](https://arxiv.org/abs/2602.05890)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Jiahan Li,Chenhao Huang,Junjie Ye,Sixian Li,Mingxu Chai,Yuhui Wang,Yajie Yang,Ming Zhang,Jiazheng Zhang,Shichun Liu,Caishuang Huang,Yunke Zhang,Yuran Wang,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: DFPO是一种鲁棒的分布强化学习框架，通过将价值建模为跨时间步的连续流而非独立分位数预测，在噪声监督下提升训练稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境中训练RL系统面临噪声监督和域外泛化差的挑战，特别是在LLM后训练中。现有分布RL方法通过多个分位数点建模价值，但每个分位数独立学习为标量，导致价值表示粗糙，缺乏对状态信息的细粒度条件化，在复杂和OOD条件下表现不佳。

Method: 提出DFPO框架：1）将价值建模为跨时间步的连续流，学习价值流场而非孤立分位数预测；2）整合条件风险控制和一致性约束以稳定噪声反馈下的训练；3）通过更丰富的状态信息捕获实现更准确的优势估计。

Result: 在对话、数学推理和科学任务上的实验表明，DFPO在噪声监督下优于PPO、FlowRL和其他鲁棒基线，实现了更好的训练稳定性和泛化能力。

Conclusion: DFPO通过连续价值流建模和条件风险控制，为噪声环境下的RL训练提供了更鲁棒的解决方案，特别适用于LLM后训练等复杂场景。

Abstract: Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.

</details>


### [108] [ContextBench: A Benchmark for Context Retrieval in Coding Agents](https://arxiv.org/abs/2602.05892)
*Han Li,Letian Zhu,Bohan Zhang,Rili Feng,Jiaming Wang,Yue Pan,Earl T. Barr,Sarro Federica,Zhaoyang Chu,He Ye*

Main category: cs.LG

TL;DR: ContextBench：一个面向过程的编码智能体上下文检索评估框架，包含1,136个跨8种编程语言的问题解决任务，通过人工标注的黄金上下文和自动化评估框架，揭示智能体在问题解决过程中如何检索和使用代码上下文。


<details>
  <summary>Details</summary>
Motivation: 现有LLM编码智能体评估主要关注最终任务成功率，缺乏对智能体在问题解决过程中如何检索和使用代码上下文的深入理解。需要过程导向的评估方法来"打开黑箱"，分析上下文检索的质量和效率。

Method: 构建ContextBench数据集：包含1,136个问题解决任务，来自66个仓库的8种编程语言，每个任务都有人工标注的黄金上下文。开发自动化评估框架，跟踪智能体轨迹，测量上下文召回率、精确率和效率。评估4个前沿LLM和5个编码智能体。

Result: 1. 复杂的智能体框架在上下文检索方面仅带来边际收益（"编码智能体的苦涩教训"）；2. LLM普遍偏向召回率而非精确率；3. 探索的上下文与实际使用的上下文之间存在显著差距；4. ContextBench为现有端到端基准提供了中间黄金上下文指标。

Conclusion: ContextBench通过过程导向的评估揭示了编码智能体上下文检索的局限性，提供了有价值的中间信号来指导LLM在软件任务中的推理。这些上下文指标有助于理解智能体的问题解决过程，并为改进智能体设计提供指导。

Abstract: LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval ("The Bitter Lesson" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.

</details>


### [109] [Parity, Sensitivity, and Transformers](https://arxiv.org/abs/2602.05896)
*Alexander Kozachinskiy,Tomasz Steifer,Przemysław Wałȩga*

Main category: cs.LG

TL;DR: 该论文研究了Transformer架构计算能力的理论边界，特别关注单层Transformer能否解决PARITY问题，提出了新的构造并给出了首个下界证明。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构已有近十年历史，但对其计算能力的理论理解仍然有限。特别是，我们不清楚单层Transformer能否解决PARITY问题，以及哪些类型的Transformer能够解决该问题。现有构造需要至少2层，并且依赖不实用的特性：长度依赖的位置编码、hardmax、无正则化参数的layernorm，或无法实现因果掩码。

Method: 提出了一种新的Transformer构造来解决PARITY问题，该构造使用softmax、长度独立且多项式有界的位置编码，无需layernorm，同时支持有/无因果掩码。此外，通过证明单层单头Transformer无法解决PARITY问题，给出了首个下界。

Result: 成功构造了一个能够解决PARITY问题的Transformer，该构造具有实际可行性（使用softmax、长度独立位置编码、无layernorm）。同时证明了单层单头Transformer无法解决PARITY问题，这是该问题的首个下界结果。

Conclusion: 该研究深化了对Transformer计算能力的理论理解，展示了即使使用实际可行的组件（softmax、长度独立位置编码），Transformer也能解决PARITY问题，同时确定了单层Transformer的计算局限性。

Abstract: The transformer architecture is almost a decade old. Despite that, we still have a limited understanding of what this architecture can or cannot compute. For instance, can a 1-layer transformer solve PARITY -- or more generally -- which kinds of transformers can do it? Known constructions for PARITY have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.
  We give a new construction of a transformer for PARITY with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. We also give the first lower bound for transformers solving PARITY -- by showing that it cannot be done with only one layer and one head.

</details>


### [110] [Regularized Calibration with Successive Rounding for Post-Training Quantization](https://arxiv.org/abs/2602.05902)
*Seohyeon Cha,Huancheng Chen,Dongjun Kim,Haoran Zhang,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 论文提出一种基于正则化非对称校准的量化方法，通过对称与非对称校准间的插值作为正则化，结合有界搜索扩展，在保持PTQ二次结构的同时提升量化质量与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署面临内存和延迟成本挑战，后训练量化（PTQ）虽能实现高效推理，但其效果严重依赖于量化目标和舍入过程。现有方法在激活不匹配时表现不佳，需要更鲁棒的量化方案。

Method: 提出对称与非对称校准间的插值作为正则化方法，保持PTQ标准二次结构的同时增强鲁棒性。推导了简单连续舍入过程，自然融入非对称校准，并扩展为有界搜索方法，在量化质量和计算成本间提供显式权衡。

Result: 在多个LLM家族、量化位宽和基准测试上的实验表明，基于正则化非对称校准目标的有界搜索方法持续改善困惑度和准确率，优于PTQ基线方法，同时仅引入适度可控的额外计算成本。

Conclusion: 正则化非对称校准为PTQ提供了鲁棒性框架，有界搜索扩展实现了量化质量与计算成本的有效权衡，为LLM高效部署提供了实用解决方案。

Abstract: Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.

</details>


### [111] [Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903)
*András Balogh,Márk Jelasity*

Main category: cs.LG

TL;DR: 该研究提出对抗序列生成方法来验证序列模型在象棋领域的正确性，发现所有训练模型都存在不完整性，但某些训练技术和数据集选择能显著改善正确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决生成序列模型训练中的关键问题：基于样本的训练能否捕捉语言的真实结构（"世界模型"）。理论表明最多只能保证生成有效序列，但无法保证生成所有有效序列。需要实用工具来验证序列模型的正确性，选择象棋作为研究领域因其复杂性适中且具有基于规则的世界模型。

Method: 提出对抗序列生成方法来验证序列模型的正确性。对抗者生成有效序列，迫使序列模型预测无效的下一步走法。除了证伪正确性外，该方法还适用于更细粒度的失败模式分析和训练选择的影响评估。提出了多种对抗序列生成方法，并在大量象棋模型上评估该方法。训练模型使用随机和高质量象棋游戏数据，采用多种训练方案。还研究了棋盘状态探针在训练和攻击方法中的潜在应用。

Result: 研究发现所有模型都不正确，但某些训练技术和数据集选择能显著改善正确性。大多数模型中提取的棋盘状态对下一个标记预测没有因果作用。对抗序列生成方法成功识别了模型的失败模式，并揭示了不同训练选择对模型正确性的影响。

Conclusion: 对抗序列生成是验证序列模型正确性的有效方法，特别适用于象棋等具有明确规则系统的领域。虽然所有训练模型都存在不完整性，但通过适当的训练技术和高质量数据可以显著改善模型正确性。棋盘状态信息在大多数模型中并未发挥因果作用，这为模型改进提供了方向。

Abstract: Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.

</details>


### [112] [Chunky Post-Training: Data Driven Failures of Generalization](https://arxiv.org/abs/2602.05910)
*Seoirse Murray,Allison Qi,Timothy Qian,John Schulman,Collin Burns,Sara Price*

Main category: cs.LG

TL;DR: 论文提出"块状后训练"概念，指大语言模型后训练中数据集编码的意外模式导致模型学习虚假相关性，开发了SURF运行时检测和TURF溯源工具来识别和追踪这些问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练使用多样化数据集，但这些数据集除了编码预期行为外，还包含意外模式（如格式与内容相关性、狭窄表述、数据筛选中的隐含关联）。这些模式对开发者不可见但对模型显著，导致模型出现令开发者惊讶的行为，如拒绝以特定格式呈现的真实事实。

Method: 提出SURF（黑盒管道）在运行时检测意外行为，以及TURF工具将失败行为溯源到具体的后训练数据块。在Claude 4.5、GPT-5.1、Grok 4.1、Gemini 3等前沿模型和Tülu 3等开源模型上应用这些工具。

Result: 块状后训练导致校准错误的行为，这些行为通常源于不平衡或未充分指定的后训练数据块。工具成功识别了模型因数据块中的虚假相关性而产生的意外行为。

Conclusion: 后训练数据中的块状结构导致模型学习虚假相关性，产生意外行为。需要更好的数据管理和评估方法来识别和缓解这些问题，SURF和TURF为此提供了实用工具。

Abstract: LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (Tülu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.

</details>


### [113] [Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training](https://arxiv.org/abs/2602.05933)
*Zhenghao Xu,Qin Lu,Changlong Yu,Tuo Zhao*

Main category: cs.LG

TL;DR: PMD-mean是一种实用的强化学习算法，通过用采样策略的平均奖励近似对数配分函数，在LLM的大动作空间中解决有限样本下的策略镜像下降问题。


<details>
  <summary>Details</summary>
Motivation: 策略镜像下降（PMD）为强化学习提供了理论框架，但在LLM的大动作空间中，精确计算对数配分函数需要大量rollouts，这在有限样本下具有挑战性。需要开发实用的近似算法。

Method: 提出PMD-mean算法：1）用采样策略的平均奖励近似对数配分函数；2）在对数策略空间中进行回归；3）该算法隐式优化了具有自适应混合KL-χ²正则化的镜像下降子问题。

Result: PMD-mean在数学推理任务上表现出优越性能，具有更好的稳定性和时间效率。额外的χ²正则化约束了概率的大幅变化，在预期奖励较低时产生更保守的更新，增强了对抗有限样本估计误差的鲁棒性。

Conclusion: PMD-mean为LLM的强化学习提供了实用且理论上有保障的算法框架，加深了对PMD-mean的理解，并为改进LLM的RL算法指明了方向。

Abstract: Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.

</details>


### [114] [Dimensionality Reduction on Riemannian Manifolds in Data Analysis](https://arxiv.org/abs/2602.05936)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.LG

TL;DR: 该论文研究基于黎曼几何的降维方法，特别是主测地线分析(PGA)作为流形值数据的非线性PCA扩展，以及黎曼适应判别分析方法，利用测地距离、切空间表示和内在统计度量实现更准确的低维嵌入。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得降维方法在处理约束于弯曲空间（如超球面、对称正定流形）的数据时存在局限性，无法充分尊重数据的底层流形结构。需要开发能够利用内在几何特性的降维方法。

Method: 研究主测地线分析(PGA)作为流形值数据的非线性PCA扩展，并扩展判别分析通过黎曼适应其他已知降维方法。这些方法利用测地距离、切空间表示和内在统计度量来实现更准确的低维嵌入。

Result: 在代表性数据集上的实验结果表明，黎曼方法相比欧几里得对应方法提供了改进的表示质量和分类性能，特别是在约束于弯曲空间的数据上表现更优。

Conclusion: 该研究强调了在现代机器学习和数据科学应用中几何感知降维的重要性，黎曼几何方法能够更好地捕捉数据的底层流形结构，从而获得更准确的数据表示和分析结果。

Abstract: In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.

</details>


### [115] [Orthogonal Model Merging](https://arxiv.org/abs/2602.05943)
*Sihan Yang,Kexuan Shi,Weiyang Liu*

Main category: cs.LG

TL;DR: OrthoMerge是一种在正交群黎曼流形上进行模型合并的方法，通过保留权重的几何结构来避免传统线性合并破坏预训练权重固有几何特性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流的模型合并方法依赖欧几里得空间中的线性算术，这会破坏预训练权重的固有几何特性（如超球面能量）。需要一种能保留权重几何结构的合并方法。

Method: 提出OrthoMerge方法：1) 将正交微调（OFT）学习的任务特定正交矩阵映射到李代数进行合并；2) 对于非OFT方法（低秩微调、全微调）的模型，采用正交-残差解耦策略，通过求解正交Procrustes问题提取正交分量在正交群流形上合并，线性残差通过标准加法合并。

Result: 大量实验结果表明，OrthoMerge在减轻灾难性遗忘和保持模型跨任务性能方面具有显著效果。

Conclusion: OrthoMerge通过在正交群黎曼流形上进行模型合并，有效保留了预训练权重的几何结构，为集成不同能力的LLMs提供了一种几何保持的合并方法。

Abstract: Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.

</details>


### [116] [$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment](https://arxiv.org/abs/2602.05946)
*Rajdeep Haldar,Lantao Mei,Guang Lin,Yue Xing,Qifan Song*

Main category: cs.LG

TL;DR: 提出基于f-散度的统一对齐框架，包含f-GRPO（在线强化学习）和f-HAL（混合在线/离线目标），在RLVR和PA任务上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 将偏好对齐的散度估计视角扩展到一般对齐设置（如RLVR），建立统一的理论框架来处理仅有环境奖励的场景

Method: 基于f-散度的变分表示，提出f-GRPO（在线策略强化学习）和f-HAL（混合在线/离线目标）两类对齐目标

Result: 理论保证对齐后平均奖励提升，在数学推理（RLVR）和安全对齐（PA）任务上验证了框架的优越性能和灵活性

Conclusion: 基于f-散度的统一对齐框架为LLM对齐提供了理论保证和实用方法，在多种对齐场景中优于现有方法

Abstract: Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.

</details>


### [117] [Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces](https://arxiv.org/abs/2602.05961)
*Arran Carter,Sanghyeok Choi,Kirill Tamogashev,Víctor Elvira,Nikolay Malkin*

Main category: cs.LG

TL;DR: 本文提出离策略训练技术提升离散扩散采样器性能，首次引入离散域的数据-能量薛定谔桥训练，并应用于图像生成模型的离散隐空间后验采样。


<details>
  <summary>Details</summary>
Motivation: 虽然连续空间扩散采样器已广泛研究，但离散空间应用仍不充分，现有离散扩散采样器未能充分利用连续空间采样的先进思想，需要弥合这一差距。

Method: 引入离策略训练技术改进离散扩散采样器；首次将数据-能量薛定谔桥训练推广到离散域；将提出的扩散采样器应用于图像生成模型的离散隐空间后验采样。

Result: 离策略训练技术提升了离散采样器在已有和新合成基准上的性能；成功实现了离散域的数据-能量薛定谔桥训练；展示了在图像生成模型离散隐空间中进行数据无关后验采样的应用。

Conclusion: 通过引入离策略训练技术和推广薛定谔桥训练到离散域，显著提升了离散扩散采样器的性能，为离散空间采样任务提供了有效的解决方案。

Abstract: Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.

</details>


### [118] [Clifford Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.05977)
*Matthias Wolff,Francesco Alesiani,Christof Duhme,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: ClKAN是一种用于任意Clifford代数空间中函数逼近的灵活高效架构，通过随机准蒙特卡洛网格生成解决高维代数指数级扩展问题，并引入新的批归一化策略处理变域输入。


<details>
  <summary>Details</summary>
Motivation: 解决在任意Clifford代数空间中进行函数逼近时面临的高维代数指数级扩展问题，以及处理变域输入的挑战，为科学发现和工程应用提供有效的数学工具。

Method: 提出Clifford Kolmogorov-Arnold Network (ClKAN)架构，采用随机准蒙特卡洛网格生成技术来应对高维Clifford代数的指数级扩展，并引入新的批归一化策略来处理变域输入。

Result: ClKAN在合成任务和物理启发任务中得到验证，展示了其在科学发现和工程应用中的有效性，能够灵活高效地处理Clifford代数空间中的函数逼近问题。

Conclusion: ClKAN为Clifford代数空间中的函数逼近提供了一种灵活高效的解决方案，通过创新的网格生成和归一化策略克服了高维扩展和变域输入的关键挑战，在科学和工程领域具有广泛应用前景。

Abstract: We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.

</details>


### [119] [Layer-wise LoRA fine-tuning: a similarity metric approach](https://arxiv.org/abs/2602.05988)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Lucas Pellicer,Rosimeire Pereira Costa,Edson Bollis,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 提出了一种基于层选择的LoRA微调方法，通过识别对模型适应最关键的层来减少可训练参数，在保持性能的同时将参数减少高达50%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增长，即使是参数高效的微调方法（如LoRA）也需要更新大量参数。现有方法对所有层都应用LoRA适配器，但并非所有层对模型适应都同等重要，存在进一步减少可训练参数的潜力。

Method: 提出层选择的LoRA微调方法：1）通过测量内部表示的变化贡献来识别最相关的层；2）仅在这些选定的层上应用LoRA或其变体进行微调；3）该方法与现有低秩适应技术正交且兼容。

Result: 1）在仅编码器架构上，GLUE基准测试中可训练参数减少50%但预测性能下降可忽略；2）在仅解码器架构上，数学问题解决和编码任务中性能有小幅下降甚至提升；3）在多模态模型上，与所有层都使用LoRA模块相比获得竞争性结果。

Conclusion: 通过系统选择关键层进行LoRA微调，可以在保持预测性能的同时显著减少可训练参数（高达50%），该方法适用于不同模型架构（编码器、解码器、多模态）和任务类型。

Abstract: Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA

</details>


### [120] [Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps](https://arxiv.org/abs/2602.05993)
*Peter Holderrieth,Douglas Chen,Luca Eyring,Ishin Shah,Giri Anantharaman,Yutong He,Zeynep Akata,Tommi Jaakkola,Nicholas Matthew Boffi,Max Simchowitz*

Main category: cs.LG

TL;DR: Diamond Maps是一种随机流映射模型，能够在推理时高效准确地对齐任意奖励函数，解决了生成模型奖励对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前流和扩散模型虽然能生成高质量样本，但在训练后适应特定用户偏好或约束时成本高昂且脆弱（奖励对齐问题）。作者认为高效奖励对齐应该是生成模型的内在属性，而非事后补救措施。

Method: 提出"Diamond Maps"随机流映射模型，将多个模拟步骤摊销到单步采样器中（类似流映射），同时保留奖励对齐所需的随机性。这种设计使得搜索、顺序蒙特卡洛和指导方法能够通过高效一致的价值函数估计实现可扩展性。

Result: 实验表明：1) Diamond Maps可以通过从GLASS Flows蒸馏高效学习；2) 相比现有方法获得更强的奖励对齐性能；3) 具有更好的可扩展性。

Conclusion: Diamond Maps为实现能够在推理时快速适应任意偏好和约束的实用生成模型提供了可行路径。

Abstract: Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose "Diamond Maps", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.

</details>


### [121] [Orthogonal Self-Attention](https://arxiv.org/abs/2602.05996)
*Leo Zhang,James Martens*

Main category: cs.LG

TL;DR: 提出正交自注意力机制(OSA)解决标准Softmax自注意力在无跳跃连接架构中的不稳定问题，通过正交化注意力矩阵改善条件数，实现线性复杂度。


<details>
  <summary>Details</summary>
Motivation: Softmax自注意力在无跳跃连接的Transformer架构中存在不稳定性问题，包括秩塌陷和雅可比矩阵条件数差，这阻碍了无跳跃连接和归一化层的Transformer训练。

Method: 设计正交自注意力机制，通过将查询-键值形成的斜对称矩阵映射到矩阵指数来参数化正交注意力矩阵，利用查询-键值的低秩结构实现线性计算复杂度。

Result: OSA能够有效避免SSA的不稳定性问题，证明其雅可比矩阵具有良好的条件数，计算复杂度和内存成本随序列长度线性增长，可实际实现。

Conclusion: 正交自注意力机制为无跳跃连接和归一化层的Transformer提供了可行的训练方案，解决了标准自注意力的稳定性问题，具有理论保证和实际可行性。

Abstract: Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.

</details>


### [122] [Optimism Stabilizes Thompson Sampling for Adaptive Inference](https://arxiv.org/abs/2602.06014)
*Shunxing Yan,Han Zhong*

Main category: cs.LG

TL;DR: 论文研究Thompson采样在多臂赌博机中的推断性质，发现乐观机制能稳定算法并实现有效渐近推断。


<details>
  <summary>Details</summary>
Motivation: Thompson采样广泛用于随机多臂赌博机，但其在自适应数据收集下的推断性质复杂。经典渐近理论可能失效，因为臂特定的样本量是随机的，且通过动作选择规则与奖励耦合。需要研究如何恢复稳定性以实现有效推断。

Method: 研究K臂高斯赌博机，识别乐观机制作为恢复稳定性的关键。分析两种乐观修改：1) 方差膨胀Thompson采样；2) 保持后验方差不变但添加显式均值奖励到后验均值。证明这两种方法都能实现稳定性。

Result: 1) 证明方差膨胀Thompson采样对任意K≥2都是稳定的，包括多个臂都是最优的挑战性情况，解决了先前工作中的开放问题；2) 分析另一种乐观修改方法，同样建立了稳定性结论。两种方法都能实现渐近有效推断，且只带来轻微的额外遗憾成本。

Conclusion: 适当实现的乐观机制能稳定Thompson采样，使多臂赌博机中的渐近有效推断成为可能，同时只产生轻微的遗憾代价。这为自适应数据收集下的统计推断提供了理论保证。

Abstract: Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.

</details>


### [123] [AP-OOD: Attention Pooling for Out-of-Distribution Detection](https://arxiv.org/abs/2602.06031)
*Claus Hofmann,Christian Huber,Bernhard Lehner,Daniel Klotz,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: AP-OOD是一种新颖的文本分布外检测方法，通过利用token级信息超越简单的平均聚合，在半监督设置下灵活利用有限辅助异常数据，在多个文本任务上显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 分布外检测对于机器学习模型的可靠部署至关重要，但现有方法在有效利用和聚合语言模型的token嵌入以获得OOD分数方面存在挑战。需要超越简单的平均聚合方法，更好地利用token级信息。

Method: AP-OOD是一种半监督方法，灵活地在无监督和有监督设置之间插值，允许使用有限的辅助异常数据。该方法通过利用token级信息，超越简单的平均聚合，更有效地从语言模型中提取和聚合token嵌入来获得OOD分数。

Result: AP-OOD在文本OOD检测中创造了新的最先进水平：在无监督设置下，XSUM摘要任务的FPR95从27.84%降低到4.67%，WMT15英法翻译任务的FPR95从77.08%降低到70.37%。

Conclusion: AP-OOD通过有效利用token级信息和半监督框架，显著提升了文本分布外检测的性能，为语言模型的可靠部署提供了更有效的解决方案。

Abstract: Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.

</details>


### [124] [Can vision language models learn intuitive physics from interaction?](https://arxiv.org/abs/2602.06033)
*Luca M. Schulze Buschoff,Konstantinos Voudouris,Can Demircan,Eric Schulz*

Main category: cs.LG

TL;DR: 论文研究发现，通过强化学习与环境交互训练的视觉语言模型在特定任务上表现提升，但无法学习到可泛化的物理直觉，无法可靠地迁移到相关任务。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型缺乏对物理世界的良好直觉。虽然监督微调可以提升简单物理任务的表现，但微调后的模型似乎无法学习到能够泛化到新情境的稳健物理规则。基于认知科学研究，作者假设模型需要通过与环境交互来正确学习物理动态。

Method: 使用强化学习训练模型，让模型通过与环境交互来学习。研究比较了通过交互学习与监督微调两种方法，并测试模型在不同但相关的任务之间的泛化能力。

Result: 通过交互学习虽然能提升模型在特定任务内的表现，但无法产生具有可泛化物理直觉的模型。研究发现，在一个任务上训练的模型无法可靠地泛化到相关任务，即使这些任务共享视觉统计特征和物理原理，无论模型是否通过交互训练。

Conclusion: 仅通过与环境交互学习不足以让模型获得可泛化的物理直觉。当前方法（包括监督微调和强化学习交互）都无法让模型学习到能够跨任务迁移的稳健物理规则，这表明需要新的训练范式来培养模型的物理推理能力。

Abstract: Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.

</details>


### [125] [Shared LoRA Subspaces for almost Strict Continual Learning](https://arxiv.org/abs/2602.06043)
*Prakhar Kaushik,Ankit Vaidya,Shravan Chaudhari,Rama Chellappa,Alan Yuille*

Main category: cs.LG

TL;DR: Share是一种参数高效的持续微调方法，通过学习和动态更新单个共享低秩子空间，实现跨任务和模态的无缝适应，显著减少参数和内存需求。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型的高效持续适应对实际部署至关重要，但面临灾难性遗忘和高重训练成本挑战。现有参数高效调优方法（如LoRA）缺乏严格的持续学习机制，无法在不依赖数据回放或多适配器的情况下实现知识整合。

Method: Share构建一个基础子空间，从过去任务中提取核心知识，并通过识别关键子空间方向增量整合新信息。每个新任务的知识都被纳入这个不断演化的共享子空间，促进前向知识转移，同时最小化灾难性干扰。

Result: 相比传统LoRA方法，Share实现了高达100倍的参数减少和281倍的内存节省，同时保持与联合训练模型相当的性能。单个Share模型可替代数百个任务特定的LoRA适配器，支持可扩展的异步持续学习。

Conclusion: Share为大规模AI系统中的终身学习提供了实用且可扩展的解决方案，在图像分类、自然语言理解、3D姿态估计和文本到图像生成等任务上验证了其有效性。

Abstract: Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [126] [On the Reachability Problem for One-Dimensional Thin Grammar Vector Addition Systems](https://arxiv.org/abs/2602.05315)
*Chengfeng Xue,Yuxi Fu*

Main category: cs.LO

TL;DR: 本文研究了薄1-GVAS的可达性问题，通过推广VASS可达性分析中的分解技术到GVAS的文法生成推导树，建立了有效的整数规划系统，获得了比先前更紧的上界。


<details>
  <summary>Details</summary>
Motivation: 向量加法系统状态（VASS）是并发理论中的经典模型，文法向量加法系统（GVAS）通过上下文无关文法控制加法扩展了VASS。薄1-GVAS是一个结构受限但表达能力强的子类，其可达性问题具有重要研究价值。

Method: 采用索引度量复杂度，将VASS可达性研究中开发的分解技术推广到GVAS的文法生成推导树，为薄1-GVAS建立有效的整数规划系统。

Result: 获得了索引为k的薄1-GVAS可达性的非确定性算法，其复杂度为F_{2k}，这比先前的结果提供了更紧的上界。

Conclusion: 通过将VASS分解技术推广到GVAS的推导树结构，成功为薄1-GVAS建立了更精确的可达性分析框架，显著改进了算法复杂度上界。

Abstract: Vector addition systems with states (VASS) are a classic model in concurrency theory. Grammar vector addition systems (GVAS), equivalently, pushdown VASS, extend VASS by using a context-free grammar to control addition. In this paper, our main focus is on the reachability problem for one-dimensional thin GVAS (thin 1-GVAS), a structurally restricted yet expressive subclass. By adopting the index measure for complexity, and by generalizing the decomposition technique developed in the study of VASS reachability to grammar-generated derivation trees of GVAS, an effective integer programming system is established for a thin 1-GVAS. In this way, a nondeterministic algorithm with $\mathbf{F}_{2k}$ complexity is obtained for the reachability of thin 1-GVAS with index $k$, yielding a tighter upper bound than the previous one.

</details>


### [127] [Groups and Inverse Semigroups in Lambda Calculus](https://arxiv.org/abs/2602.05654)
*Antonio Bucciarelli,Arturo De Faveri,Giulio Manzonetto,Antonino Salibra*

Main category: cs.LO

TL;DR: 该论文研究λ项在λ理论下的可逆性，重点关注有限遗传置换（FHP）和无限遗传置换（HP）两类λ项，利用逆半群理论建立可逆λ项与λ理论之间的对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究λ项在λ理论下的可逆性问题，理解可逆λ项在不同λ理论中的结构特征，特别是有限遗传置换（FHP）和无限遗传置换（HP）在最小外延λ理论λη和最大合理λ理论H*中的核心作用。

Method: 采用逆半群理论作为主要分析工具，将λ项的可逆性问题转化为代数结构研究。证明FHP模任意λ理论T构成逆半群，HP模包含Böhm树理论的λ理论T也构成逆半群。通过自然序与η展开的对应关系建立理论框架。

Result: 1. FHP模任意λ理论T总是逆半群；2. HP模包含Böhm树理论的λ理论T是逆半群；3. 自然序在FHP/T中对应η展开，在HP/T中对应无限η展开；4. FHP是所有介于λη和H+之间的λ理论中的可逆λ项。

Conclusion: 通过逆半群理论为λ项可逆性研究提供了统一框架，将已有结果置于更广泛的背景下，并证明了FHP在λη和H+之间所有λ理论中的可逆性，深化了对λ项可逆结构与λ理论关系的理解。

Abstract: We study invertibility of $λ$-terms modulo $λ$-theories. Here a fundamental role is played by a class of $λ$-terms called finite hereditary permutations (FHP) and by their infinite generalisations (HP). More precisely, FHPs are the invertible elements in the least extensional $λ$-theory $λη$ and HPs are those in the greatest sensible $λ$-theory $H^*$. Our approach is based on inverse semigroups, algebraic structures that generalise groups and semilattices. We show that FHP modulo a $λ$-theory $T$ is always an inverse semigroup and that HP modulo $T$ is an inverse semigroup whenever $T$ contains the theory of Böhm trees. An inverse semigroup comes equipped with a natural order. We prove that the natural order corresponds to $η$-expansion in $\mathrm{FHP} /T$, and to infinite $η$-expansion in $\mathrm{HP}/T$. Building on these correspondences we obtain the two main contributions of this work: firstly, we recast in a broader framework the results cited at the beginning; secondly, we prove that the FHPs are the invertible $λ$-terms in all the $λ$-theories lying between $λη$ and $H^+$. The latter is Morris' observational $λ$-theory, defined by using the $β$-normal forms as observables.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [128] [Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software](https://arxiv.org/abs/2602.04894)
*Tomer Kordonsky,Maayan Yamin,Noam Benzimra,Amit LeVi,Avi Mendelson*

Main category: cs.CR

TL;DR: 论文提出FSTab框架，通过前端特征预测LLM生成代码中的后端漏洞，并评估模型在不同场景下产生相同漏洞的一致性，揭示了LLM代码生成的安全风险。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码常包含可预测的漏洞模式，这些模式在不同程序、语义保持的重述和应用领域中持续存在，构成了新的攻击面。现有研究缺乏对这种漏洞持续性进行系统分析和量化的方法。

Method: 提出Feature-Security Table (FSTab)框架，包含两个组件：1) 黑盒攻击方法，通过观察前端特征和了解源LLM来预测后端漏洞，无需访问后端代码或源代码；2) 模型中心评估，量化给定模型在不同程序、语义保持重述和应用领域中复制相同漏洞的一致性程度。

Result: 在GPT-5.2、Claude-4.5 Opus和Gemini-3 Pro等先进代码LLM上进行评估，结果显示强跨域迁移能力：即使目标域被排除在训练之外，FSTab在Internal Tools（Claude-4.5 Opus）上仍能达到94%的攻击成功率和93%的漏洞覆盖率。

Conclusion: FSTab揭示了LLM生成软件中未被充分探索的攻击面，突显了代码生成的安全风险。LLM输出中的漏洞模式具有可预测性和持续性，这对LLM生成的软件安全构成了实质性威胁。

Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \emph{vulnerability persistence} in LLM-generated software and introduce \emph{Feature--Security Table (FSTab)} with two components. First, FSTab enables a black-box attack that predicts likely backend vulnerabilities from observable frontend features and knowledge of the source LLM, without access to backend code or source code. Second, FSTab provides a model-centric evaluation that quantifies how consistently a given model reproduces the same vulnerabilities across programs, semantics-preserving rephrasings, and application domains. We evaluate FSTab on state-of-the-art code LLMs, including GPT-5.2, Claude-4.5 Opus, and Gemini-3 Pro, across diverse application domains. Our results show strong cross-domain transfer: even when the target domain is excluded from training, FSTab achieves up to 94\% attack success and 93\% vulnerability coverage on Internal Tools (Claude-4.5 Opus). These findings expose an underexplored attack surface in LLM-generated software and highlight the security risks of code generation. Our code is available at: https://anonymous.4open.science/r/FSTab-024E.

</details>


### [129] [Privacy Amplification Persists under Unlimited Synthetic Data Release](https://arxiv.org/abs/2602.04895)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.CR

TL;DR: 该论文研究通过合成数据发布实现隐私放大的现象，发现在有界参数假设下，即使发布无限数量的合成记录，隐私放大效应仍然存在，改进了Pierquin等人的结果。


<details>
  <summary>Details</summary>
Motivation: Pierquin等人（2025）首次为线性生成器建立了形式化的隐私放大保证，但仅适用于模型维度远超过发布合成记录数量的渐近机制，限制了实际应用价值。本研究旨在探索更实用的隐私放大场景。

Method: 在有界参数假设下，分析合成数据发布机制中的隐私放大现象，即使发布无限数量的合成记录，仍能保持隐私放大效应。

Result: 证明了在有界参数假设下，隐私放大效应在发布无限数量合成记录时仍然存在，改进了Pierquin等人的边界结果。

Conclusion: 该分析提供了结构性的见解，可能指导为更复杂的发布机制开发更严格的隐私保证，推动了合成数据发布中隐私放大理论的实际应用。

Abstract: We study privacy amplification by synthetic data release, a phenomenon in which differential privacy guarantees are improved by releasing only synthetic data rather than the private generative model itself. Recent work by Pierquin et al. (2025) established the first formal amplification guarantees for a linear generator, but they apply only in asymptotic regimes where the model dimension far exceeds the number of released synthetic records, limiting their practical relevance. In this work, we show a surprising result: under a bounded-parameter assumption, privacy amplification persists even when releasing an unbounded number of synthetic records, thereby improving upon the bounds of Pierquin et al. (2025). Our analysis provides structural insights that may guide the development of tighter privacy guarantees for more complex release mechanisms.

</details>


### [130] [Semantic-level Backdoor Attack against Text-to-Image Diffusion Models](https://arxiv.org/abs/2602.04898)
*Tianxin Chen,Wenbo Jiang,Hongqiao Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CR

TL;DR: SemBD是一种针对文本到图像扩散模型的语义级后门攻击方法，通过定义连续语义区域作为触发器，在表示层面植入后门，有效规避基于枚举的输入防御和注意力一致性检测。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击依赖固定文本触发器和单实体后门目标，容易受到基于枚举的输入防御和注意力一致性检测的攻击，需要更隐蔽、更鲁棒的后门攻击方法。

Method: 通过基于蒸馏的编辑方法修改交叉注意力层中的键值投影矩阵，在表示层面植入后门；定义触发器为连续语义区域而非离散文本模式；引入语义正则化防止不完整语义下的意外激活；采用多实体后门目标避免高度一致的交叉注意力模式。

Result: 实验表明SemBD实现了100%的攻击成功率，同时对最先进的输入级防御保持强鲁棒性。

Conclusion: SemBD提出了一种新颖的语义级后门攻击方法，通过连续语义区域触发器和表示层面的后门植入，显著提高了攻击的隐蔽性和鲁棒性，对文本到图像扩散模型的安全构成了新的威胁。

Abstract: Text-to-image (T2I) diffusion models are widely adopted for their strong generative capabilities, yet remain vulnerable to backdoor attacks. Existing attacks typically rely on fixed textual triggers and single-entity backdoor targets, making them highly susceptible to enumeration-based input defenses and attention-consistency detection. In this work, we propose Semantic-level Backdoor Attack (SemBD), which implants backdoors at the representation level by defining triggers as continuous semantic regions rather than discrete textual patterns. Concretely, SemBD injects semantic backdoors by distillation-based editing of the key and value projection matrices in cross-attention layers, enabling diverse prompts with identical semantic compositions to reliably activate the backdoor attack. To further enhance stealthiness, SemBD incorporates a semantic regularization to prevent unintended activation under incomplete semantics, as well as multi-entity backdoor targets that avoid highly consistent cross-attention patterns. Extensive experiments demonstrate that SemBD achieves a 100% attack success rate while maintaining strong robustness against state-of-the-art input-level defenses.

</details>


### [131] [PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM](https://arxiv.org/abs/2602.04927)
*Gautam Savaliya,Robert Aufschläger,Abhishek Subedi,Michael Heigl,Martin Schramm*

Main category: cs.CR

TL;DR: PriMod4AI是一个混合隐私威胁建模框架，结合LINDDUN知识库和模型中心隐私攻击知识库，利用向量数据库和LLM识别AI系统中的隐私威胁


<details>
  <summary>Details</summary>
Motivation: AI系统在其生命周期中引入复杂的隐私风险，特别是处理敏感或高维数据时。传统的LINDDUN框架只涵盖七种隐私威胁类别，无法覆盖AI特有的模型中心隐私攻击（如成员推断和模型反转）。需要开发一个统一框架来同时处理传统LINDDUN威胁和AI驱动的隐私攻击。

Method: PriMod4AI采用混合隐私威胁建模方法，整合两个结构化知识源：1) 代表传统分类法的LINDDUN知识库；2) 捕获LINDDUN之外威胁的模型中心隐私攻击知识库。这些知识库嵌入向量数据库用于语义检索，并与从数据流图导出的系统级元数据结合。使用检索增强和数据流特定的提示生成来指导大型语言模型识别、解释和分类隐私威胁。

Result: 在两个AI系统上的评估表明，PriMod4AI能够广泛覆盖传统隐私类别，同时还能识别模型中心隐私威胁。该框架在不同LLM中产生一致、基于知识的输出，观察范围内的协议分数反映了这一点。

Conclusion: PriMod4AI提供了一个统一的隐私威胁建模框架，能够同时处理传统LINDDUN威胁和AI特有的隐私攻击，为AI系统的隐私风险评估提供了全面且一致的方法。

Abstract: Artificial intelligence systems introduce complex privacy risks throughout their lifecycle, especially when processing sensitive or high-dimensional data. Beyond the seven traditional privacy threat categories defined by the LINDDUN framework, AI systems are also exposed to model-centric privacy attacks such as membership inference and model inversion, which LINDDUN does not cover. To address both classical LINDDUN threats and additional AI-driven privacy attacks, PriMod4AI introduces a hybrid privacy threat modeling approach that unifies two structured knowledge sources, a LINDDUN knowledge base representing the established taxonomy, and a model-centric privacy attack knowledge base capturing threats outside LINDDUN. These knowledge bases are embedded into a vector database for semantic retrieval and combined with system level metadata derived from Data Flow Diagram. PriMod4AI uses retrieval-augmented and Data Flow specific prompt generation to guide large language models (LLMs) in identifying, explaining, and categorizing privacy threats across lifecycle stages. The framework produces justified and taxonomy-grounded threat assessments that integrate both classical and AI-driven perspectives. Evaluation on two AI systems indicates that PriMod4AI provides broad coverage of classical privacy categories while additionally identifying model-centric privacy threats. The framework produces consistent, knowledge-grounded outputs across LLMs, as reflected in agreement scores in the observed range.

</details>


### [132] [Attack Selection Reduces Safety in Concentrated AI Control Settings against Trusted Monitoring](https://arxiv.org/abs/2602.04930)
*Joachim Schaeffer,Arjun Khandelwal,Tyler Tracy*

Main category: cs.CR

TL;DR: 研究AI攻击选择能力对安全监控的影响，发现在BigCodeBench后门设置中，攻击者通过推理监控机制可显著降低系统安全性


<details>
  <summary>Details</summary>
Motivation: 未来AI部署将受到恶意行为监控，但AI可能通过对抗性选择攻击来规避监控（攻击选择），这构成了严重的安全隐患。需要研究攻击选择能力对安全评估的影响，避免对安全性的过度乐观估计。

Method: 在BigCodeBench后门设置中，让红队创建攻击策略，尝试在不被监控器发现的情况下将攻击插入代码。将攻击选择分解为两个问题：将攻击映射到质量分数，将质量分数映射到提交概率。将攻击选择构建为分类问题，分析安全性与FPR和TPR的敏感性关系。

Result: 安全性对FPR比TPR更敏感。当提示攻击者模型推理监控机制并在攻击选择中保持谨慎时，安全性从基线99%降至59%（0.5%审计预算下）。这表明激发模型的攻击选择能力对于避免控制评估中的过度乐观安全评分至关重要。

Conclusion: 攻击选择能力是AI安全评估的关键因素。在控制评估中必须考虑模型规避监控的能力，否则会导致对安全性的过度乐观估计。激发和测试模型的攻击选择能力对于准确评估AI系统安全性至关重要。

Abstract: Future AI deployments will likely be monitored for malicious behaviour. The ability of these AIs to subvert monitors by adversarially selecting against them - attack selection - is particularly concerning. To study this, we let a red team create attack policies that attempt to insert attacks into code without being caught by a monitor in the concentrated BigCodeBench backdooring setting. We decompose attack selection into two problems: mapping attacks to a quality score and mapping quality scores to submission probabilities. We frame attack selection as a classification problem and show that safety is significantly more sensitive to FPR than TPR. We find that prompting the attacker model to reason about the monitor while being cautious with attack selection reduces safety from a baseline of 99% to 59% at 0.5% auditing budget, emphasizing that eliciting attack selection capabilities of models is vital to avoid overly optimistic safety scores in control evaluations.

</details>


### [133] [The Birthmark Standard: Privacy-Preserving Photo Authentication via Hardware Roots of Trust and Consortium Blockchain](https://arxiv.org/abs/2602.04933)
*Sam Ryan*

Main category: cs.CR

TL;DR: 提出Birthmark Standard标准，利用传感器固有噪声特征生成硬件根密钥，通过联盟区块链存储认证记录，解决AI生成图像破坏照片证据可信度的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统快速发展破坏了摄影证据的可信度，现有认证方法（如C2PA）存在两个关键缺陷：元数据在社交媒体重处理时易被剥离，以及依赖企业控制的验证基础设施可能产生利益冲突。

Method: 利用传感器制造独特的非均匀性校正(NUC)图和PRNU模式生成硬件根认证密钥；拍摄时创建匿名化认证证书证明传感器真实性但不暴露设备身份；认证记录存储在由新闻机构运营的联盟区块链上，确保元数据丢失后仍可验证。

Result: 使用ProVerif形式化验证隐私属性，证明在Dolev-Yao对手假设下实现制造商非相关性和区块链观察者非识别性；原型实现展示完整密码学流程；性能分析显示相机开销低于100ms，百万级每日认证下验证延迟低于500ms。

Conclusion: Birthmark Standard提供了一种硬件根认证架构，通过传感器固有特征和去中心化存储，解决了AI时代摄影证据可信度问题，同时保护设备隐私，为新闻和公共话语提供可靠基础。

Abstract: The rapid advancement of generative AI systems has collapsed the credibility landscape for photographic evidence. Modern image generation models produce photorealistic images undermining the evidentiary foundation upon which journalism and public discourse depend. Existing authentication approaches, such as the Coalition for Content Provenance and Authenticity (C2PA), embed cryptographically signed metadata directly into image files but suffer from two critical failures: technical vulnerability to metadata stripping during social media reprocessing, and structural dependency on corporate-controlled verification infrastructure where commercial incentives may conflict with public interest. We present the Birthmark Standard, an authentication architecture leveraging manufacturing-unique sensor entropy from non-uniformity correction (NUC) maps and PRNU patterns to generate hardware-rooted authentication keys. During capture, cameras create anonymized authentication certificates proving sensor authenticity without exposing device identity via a key table architecture maintaining anonymity sets exceeding 1,000 devices. Authentication records are stored on a consortium blockchain operated by journalism organizations rather than commercial platforms, enabling verification that survives all metadata loss. We formally verify privacy properties using ProVerif, proving observational equivalence for Manufacturer Non-Correlation and Blockchain Observer Non-Identification under Dolev-Yao adversary assumptions. The architecture is validated through prototype implementation using Raspberry Pi 4 hardware, demonstrating the complete cryptographic pipeline. Performance analysis projects camera overhead below 100ms and verification latency below 500ms at scale of one million daily authentications.

</details>


### [134] [SynthForensics: A Multi-Generator Benchmark for Detecting Synthetic Video Deepfakes](https://arxiv.org/abs/2602.04939)
*Roberto Leotta,Salvatore Alfio Sambataro,Claudio Vittorio Ragaglia,Mirko Casu,Yuri Petralia,Francesco Guarnera,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CR

TL;DR: SynthForensics是首个面向人类中心的纯合成视频深度伪造检测基准，包含6,815个来自5种不同架构开源T2V模型的视频，实验显示现有检测器在该领域性能显著下降且泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 随着文本到视频（T2V）模型技术成熟且开源化，高质量合成视频可在消费级硬件上生成，使得现有基于面部特征和操作痕迹的深度伪造检测基准过时，迫切需要新的评估标准来应对这一威胁。

Method: 构建SynthForensics基准：1）收集5种架构不同的开源T2V模型生成的6,815个独特视频；2）采用两阶段人工参与验证确保语义和视觉质量；3）为每个视频提供四种版本（原始、无损、轻度压缩、重度压缩）以测试现实世界鲁棒性；4）评估现有检测器性能并探索在该基准上训练的效果。

Result: 实验显示：1）现有最先进检测器在该新领域表现脆弱且泛化有限，平均AUC下降29.19%，部分方法表现甚至低于随机猜测；2）在重度压缩下，顶级模型性能下降超过30个百分点；3）在SynthForensics上训练可提升泛化能力（达到93.81% AUC），但会降低与传统操作型深度伪造的向后兼容性。

Conclusion: SynthForensics填补了纯合成视频深度伪造检测基准的空白，揭示了现有检测器在面对新兴T2V技术时的严重局限性，为未来检测方法开发提供了重要评估平台，同时公开数据集和生成元数据将促进该领域研究。

Abstract: The landscape of synthetic media has been irrevocably altered by text-to-video (T2V) models, whose outputs are rapidly approaching indistinguishability from reality. Critically, this technology is no longer confined to large-scale labs; the proliferation of efficient, open-source generators is democratizing the ability to create high-fidelity synthetic content on consumer-grade hardware. This makes existing face-centric and manipulation-based benchmarks obsolete. To address this urgent threat, we introduce SynthForensics, to the best of our knowledge the first human-centric benchmark for detecting purely synthetic video deepfakes. The benchmark comprises 6,815 unique videos from five architecturally distinct, state-of-the-art open-source T2V models. Its construction was underpinned by a meticulous two-stage, human-in-the-loop validation to ensure high semantic and visual quality. Each video is provided in four versions (raw, lossless, light, and heavy compression) to enable real-world robustness testing. Experiments demonstrate that state-of-the-art detectors are both fragile and exhibit limited generalization when evaluated on this new domain: we observe a mean performance drop of $29.19\%$ AUC, with some methods performing worse than random chance, and top models losing over 30 points under heavy compression. The paper further investigates the efficacy of training on SynthForensics as a means to mitigate these observed performance gaps, achieving robust generalization to unseen generators ($93.81\%$ AUC), though at the cost of reduced backward compatibility with traditional manipulation-based deepfakes. The complete dataset and all generation metadata, including the specific prompts and inference parameters for every video, will be made publicly available at [link anonymized for review].

</details>


### [135] [System-Level Isolation for Mixed-Criticality RISC-V SoCs: A "World" Reality Check](https://arxiv.org/abs/2602.05002)
*Luis Cunha,Jose Martins,Manuel Rodriguez,Tiago Gomes,Sandro Pinto,Uwe Moslehner,Kai Dieffenbach,Glenn Farrall,Kajetan Nuernberger,Thomas Roecker*

Main category: cs.CR

TL;DR: 本文对RISC-V硬件隔离原语（IOPMP、World-based checker及改进版World checker）在异构SoC中的性能、面积和安全性进行了比较分析，发现World-based checker具有固定的访问延迟和更好的可扩展性，改进设计可减少约5%的SoC面积。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V在汽车、物联网和工业控制等领域的加速采用，这些领域对尺寸、重量、功耗和成本（SWaP-C）有严格限制，推动了异构SoC的发展。然而，异构集成带来了安全挑战：如何在混合关键性环境中实施系统级隔离。虽然RISC-V提出了多种硬件隔离原语，但其互操作性、可扩展性和实时系统适用性仍不明确。

Method: 本文从实际异构SoC设计的角度对RISC-V硬件隔离原语进行了比较分析。具体实现了IOPMP、基于World的检查器，以及一个改进的RISC-V World检查器（解决了基线规范的关键限制），并评估了它们在安全保证和功耗-性能-面积（PPA）方面的权衡。

Result: 研究结果表明：基于World的检查器引入了固定的、与配置无关的访问延迟，相比其他方案具有更低的worst-case延迟，并能随着系统规模可预测地扩展。在宏观层面，估计改进设计相比基线设计可减少约5%的SoC面积。

Conclusion: 所有研究成果将作为开源发布，这些发现预计将直接促进RISC-V规范的演进和标准化，以及未来RISC-V SoC的设计。World-based checker在异构SoC中表现出更好的可预测性和可扩展性，为解决混合关键性环境中的隔离问题提供了实用方案。

Abstract: As RISC-V adoption accelerates, domains such as automotive, the Internet of Things (IoT), and industrial control are attracting growing attention. These domains are subject to stringent Size, Weight, Power, and Cost (SWaP-C) constraints, which have driven a shift toward heterogeneous Systems-on-Chip (SoCs) integrating general-purpose CPUs, tightly coupled accelerators, and diverse I/O devices with different integrity levels. While such integration improves cost efficiency and performance, it introduces a fundamental safety and security challenge: enforcing system-level isolation in mixed-criticality environments. Although RISC-V International has proposed several hardware isolation primitives, including RISC-V Worlds, IOPMP, and SmMTT, their interoperability, scalability, and suitability for real-time systems remain insufficiently understood. In this paper, we present a comparative analysis of these primitives from the perspective of practical heterogeneous SoC designs. We implement an IOPMP, a World-based checker, and a modified RISC-V World checker that addresses key limitations of the baseline specification, and evaluate their trade-offs in terms of security guarantees and power-performance-area (PPA). Our results show that the World-based checker introduces a fixed, configuration-independent access latency, achieving lower worst-case delay than the evaluated alternatives while scaling predictably with system size. At the macro level, we estimate that the proposed modifications reduce SoC area by up to approximately 5% compared to a baseline design. All artifacts will be released as open source, and we expect these findings to directly contribute to the evolution and ratification of RISC-V specifications, as well as to the design of future RISC-V SoCs.

</details>


### [136] [Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?](https://arxiv.org/abs/2602.05023)
*Ruixin Yang,Ethan Mendes,Arthur Wang,James Hays,Sauvik Das,Wei Xu,Alan Ritter*

Main category: cs.CR

TL;DR: 本文提出了VLM-GEOPRIVACY基准测试，用于评估视觉语言模型在图像地理位置隐私保护方面的表现，发现当前模型虽然能精确定位，但与人类隐私期望严重不符，常过度披露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型和前沿多模态大推理模型在图像地理定位方面表现出色，这带来了严重的隐私风险，因为这些广泛可访问的模型可能被滥用来从随意分享的照片中推断敏感位置，甚至达到街道级精度，超出了分享者同意或意图披露的详细程度。虽然最近的研究提出了对地理位置披露实施全面限制来应对这一风险，但这些措施未能区分有效的地理定位用途和恶意行为。

Method: 作者引入了VLM-GEOPRIVACY基准测试，该基准挑战视觉语言模型解释真实世界图像中的潜在社会规范和上下文线索，并确定适当的位置披露级别。该基准用于评估14个领先的视觉语言模型，测试它们如何平衡隐私和实用性，以及是否能够保持上下文完整性。

Result: 对14个领先视觉语言模型的评估显示，尽管这些模型能够精确地定位图像，但它们与人类隐私期望严重不符。模型经常在敏感上下文中过度披露信息，并且容易受到基于提示的攻击。

Conclusion: 研究结果表明，需要在多模态系统中采用新的设计原则，以纳入上下文条件化的隐私推理。视觉语言模型应该通过推理图像中的元素来确定适当的信息披露级别，从而保持上下文完整性，平衡隐私和实用性。

Abstract: Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.

</details>


### [137] [VEXA: Evidence-Grounded and Persona-Adaptive Explanations for Scam Risk Sensemaking](https://arxiv.org/abs/2602.05056)
*Heajun An,Connor Ng,Sandesh Sharma Dulal,Junghwan Kim,Jin-Hee Cho*

Main category: cs.CR

TL;DR: VEXA框架通过结合梯度SHAP归因和理论驱动的漏洞角色，为面向学习者的诈骗检测生成证据基础且角色自适应的解释，在保持语义可靠性的同时提供可解释的风格变化。


<details>
  <summary>Details</summary>
Motivation: 在线诈骗日益复杂，生成式AI使其更具欺骗性。虽然基于Transformer的检测器性能良好，但其解释对非专家不透明或与模型决策不一致，需要更有效的面向学习者的解释方法。

Method: 提出VEXA框架，整合GradientSHAP归因（提供证据基础）和理论驱动的漏洞角色（实现自适应解释）。通过多通道数据集评估，确保解释的语义可靠性和可解释的风格变化。

Result: 评估显示：基于检测器证据的解释提高了语义可靠性而不增加语言复杂性；角色条件化引入了可解释的风格变化而不破坏证据对齐。证据基础控制语义正确性，角色适应在忠实性约束下操作于呈现层面。

Conclusion: VEXA证明了角色自适应、证据基础解释的可行性，为非正式场景下面向学习者的可信安全解释提供了设计指导。证据基础控制语义正确性，角色适应在呈现层面操作。

Abstract: Online scams across email, short message services, and social media increasingly challenge everyday risk assessment, particularly as generative AI enables more fluent and context-aware deception. Although transformer-based detectors achieve strong predictive performance, their explanations are often opaque to non-experts or misaligned with model decisions. We propose VEXA, an evidence-grounded and persona-adaptive framework for generating learner-facing scam explanations by integrating GradientSHAP-based attribution with theory-informed vulnerability personas. Evaluation across multi-channel datasets shows that grounding explanations in detector-derived evidence improves semantic reliability without increasing linguistic complexity, while persona conditioning introduces interpretable stylistic variation without disrupting evidential alignment. These results reveal a key design insight: evidential grounding governs semantic correctness, whereas persona-based adaptation operates at the level of presentation under constraints of faithfulness. Together, VEXA demonstrates the feasibility of persona-adaptive, evidence-grounded explanations and provides design guidance for trustworthy, learner-facing security explanations in non-formal contexts.

</details>


### [138] [Crypto-asset Taxonomy for Investors and Regulators](https://arxiv.org/abs/2602.05098)
*Xiao Zhang,Juan Ignacio Ibañez,Jiahua Xu*

Main category: cs.CR

TL;DR: 该论文开发了一个连接技术设计、市场结构和监管的多维加密资产分类框架，填补了该领域缺乏统一全面分类体系的空白。


<details>
  <summary>Details</summary>
Motivation: 加密资产已成为电子市场的主要组成部分，交易量和市场份额不断增长，但目前缺乏统一且全面的资产层面分类框架，这阻碍了对加密市场的系统性研究、监管风险评估以及投资者和平台设计者的资产比较。

Method: 基于既定的分类指南和现有模型，从理论、监管框架和案例研究中推导出分类维度，构建多维分类框架，然后将前100大加密资产映射到该结构中，并提供多个详细案例研究。

Result: 分类框架涵盖技术标准、关键资源集中度、资产功能、法律分类以及铸造、收益、赎回等机制设计。资产映射和案例研究揭示了重复出现的设计模式，捕捉了当前分类边界上的边缘案例特征，并记录了名义去中心化资产中的中心化控制现象。

Conclusion: 该论文为加密市场的系统性研究提供了框架，支持监管机构评估代币风险，并为投资者和数字平台设计者在构建或参与电子市场时提供了资产比较工具。

Abstract: Crypto-assets are a main segment of electronic markets, with growing trade volume and market share, yet there's no unified and comprehensive asset level taxonomy framework. This paper develops a multidimensional taxonomy for crypto-assets that connects technical design to market structure and regulation. Building on established taxonomy guideline and existing models, we derive dimensions from theory, regulatory frameworks, and case studies. We then map top 100 assets within the structure and provide several detailed case studies. The taxonomy covers technology standard, centralisation of critical resources, asset function, legal classification and mechanism designs of minting, yield, redemption. The asset mapping and case studies reveal recurring design patterns, capture features of edge cases that sit on boundaries of current categorisations, and document centralised control of nominal decentralised assets. This paper provides framework for systematic study for crypto markets, supports regulators in assessing token risks, and offers investors and digital platform designers a tool to compare assets when building or participate in electronic markets.

</details>


### [139] [SynAT: Enhancing Security Knowledge Bases via Automatic Synthesizing Attack Tree from Crowd Discussions](https://arxiv.org/abs/2602.05329)
*Ziyou Jiang,Lin Shi,Guowei Yang,Xuyan Ma,Fenglong Li,Qing Wang*

Main category: cs.CR

TL;DR: SynAT：一种从众包安全帖子中自动合成攻击树的方法，利用LLM和事件关系提取技术增强安全知识库


<details>
  <summary>Details</summary>
Motivation: 官方安全信息发布存在时间滞后，导致安全知识库维护不足，难以及时应对新兴安全风险。而在线知识共享平台上的众包安全讨论包含丰富安全知识，可用于增强安全知识库。

Method: 1. 利用大语言模型和提示学习限制包含攻击信息的句子范围；2. 使用基于转换的事件和关系提取模型同时从限定范围内提取事件和关系；3. 应用启发式规则，基于提取的事件和关系合成攻击树。

Result: 在5,070个Stack Overflow安全帖子上进行实验评估，SynAT在事件和关系提取方面优于所有基线方法，在攻击树合成中实现了最高的树相似度。已成功应用于增强华为安全知识库以及公共安全知识库CVE和CAPEC。

Conclusion: SynAT能够有效从众包安全帖子中自动合成攻击树，解决了安全知识库维护滞后的问题，具有实际应用价值，已成功应用于工业界和公共安全知识库的增强。

Abstract: Cyber attacks have become a serious threat to the security of software systems. Many organizations have built their security knowledge bases to safeguard against attacks and vulnerabilities. However, due to the time lag in the official release of security information, these security knowledge bases may not be well maintained, and using them to protect software systems against emergent security risks can be challenging. On the other hand, the security posts on online knowledge-sharing platforms contain many crowd security discussions and the knowledge in those posts can be used to enhance the security knowledge bases. This paper proposes SynAT, an automatic approach to synthesize attack trees from crowd security posts. Given a security post, SynAT first utilize the Large Language Model (LLM) and prompt learning to restrict the scope of sentences that may contain attack information; then it utilizes a transition-based event and relation extraction model to extract the events and relations simultaneously from the scope; finally, it applies heuristic rules to synthesize the attack trees with the extracted events and relations. An experimental evaluation is conducted on 5,070 Stack Overflow security posts, and the results show that SynAT outperforms all baselines in both event and relation extraction, and achieves the highest tree similarity in attack tree synthesis. Furthermore, SynAT has been applied to enhance HUAWEI's security knowledge base as well as public security knowledge bases CVE and CAPEC, which demonstrates SynAT's practicality.

</details>


### [140] [Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening](https://arxiv.org/abs/2602.05386)
*Zhenxiong Yu,Zhi Yang,Zhiheng Jin,Shuhe Wang,Heng Zhang,Yanlin Fei,Lingfeng Zeng,Fangqi Lou,Shuo Zhang,Tu Hu,Jingping Liu,Rongze Chen,Xingyu Zhu,Kunyi Wang,Chaofa Yuan,Xin Guo,Zhaowei Liu,Feipeng Zhang,Jie Huang,Huacan Wang,Ronghao Chen,Liwen Zhang*

Main category: cs.CR

TL;DR: 提出Spider-Sense框架，一种基于内在风险感知的事件驱动防御机制，通过分层防御策略在效率和精度间权衡，仅当感知风险时触发防御，减少对外部模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体防御机制多采用强制检查范式，在预定义阶段强制触发安全验证。作者认为有效的智能体安全应是内在且选择性的，而非架构解耦和强制性的。

Method: 提出Spider-Sense框架，基于内在风险感知的事件驱动防御：1) 智能体保持潜在警觉，仅在感知风险时触发防御；2) 分层防御机制：已知模式通过轻量级相似性匹配解决，模糊案例升级到深度内部推理；3) 消除对外部模型的依赖。

Result: 实验表明Spider-Sense达到竞争性或更优的防御性能：获得最低的攻击成功率(ASR)和误报率(FPR)，仅带来8.3%的边际延迟开销。

Conclusion: Spider-Sense框架通过内在风险感知和事件驱动防御，实现了高效、精确的智能体安全防护，相比传统强制检查范式具有显著优势。

Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\%.

</details>


### [141] [BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models](https://arxiv.org/abs/2602.05401)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Guowen Xu*

Main category: cs.CR

TL;DR: BadTemplate是一种无需训练的后门攻击方法，通过控制聊天模板在系统提示中注入恶意指令，实现高攻击成功率且难以检测


<details>
  <summary>Details</summary>
Motivation: 聊天模板作为LLM训练和推理的常用技术，虽然能提升模型性能，但其可定制性也带来了新的攻击面。研究者发现攻击者可通过控制模板向系统提示注入任意字符串而不被用户察觉，这构成了潜在的安全风险。

Method: 提出BadTemplate攻击方法：利用聊天模板的可定制性，将精心设计的恶意指令插入高优先级的系统提示中，使目标LLM表现出持续的后门行为。该方法无需模型重新训练，成本低且易于部署。

Result: 在5个基准数据集、6个开源和3个闭源LLM上进行了广泛实验，与3个基线方法比较。BadTemplate实现了高达100%的攻击成功率，在词级和句级攻击中均显著优于传统的基于提示的后门攻击。HuggingFace和LLM-as-a-judge等主要第三方平台的检测方法对BadTemplate基本无效。

Conclusion: 聊天模板在LLM供应链中带来了潜在安全风险，BadTemplate攻击展示了这种风险的实际威胁。该研究有助于推动有效防御机制的开发，提高LLM系统的安全性。

Abstract: Chat template is a common technique used in the training and inference stages of Large Language Models (LLMs). It can transform input and output data into role-based and templated expressions to enhance the performance of LLMs. However, this also creates a breeding ground for novel attack surfaces. In this paper, we first reveal that the customizability of chat templates allows an attacker who controls the template to inject arbitrary strings into the system prompt without the user's notice. Building on this, we propose a training-free backdoor attack, termed BadTemplate. Specifically, BadTemplate inserts carefully crafted malicious instructions into the high-priority system prompt, thereby causing the target LLM to exhibit persistent backdoor behaviors. BadTemplate outperforms traditional backdoor attacks by embedding malicious instructions directly into the system prompt, eliminating the need for model retraining while achieving high attack effectiveness with minimal cost. Furthermore, its simplicity and scalability make it easily and widely deployed in real-world systems, raising serious risks of rapid propagation, economic damage, and large-scale misinformation. Furthermore, detection by major third-party platforms HuggingFace and LLM-as-a-judge proves largely ineffective against BadTemplate. Extensive experiments conducted on 5 benchmark datasets across 6 open-source and 3 closed-source LLMs, compared with 3 baselines, demonstrate that BadTemplate achieves up to a 100% attack success rate and significantly outperforms traditional prompt-based backdoors in both word-level and sentence-level attacks. Our work highlights the potential security risks raised by chat templates in the LLM supply chain, thereby supporting the development of effective defense mechanisms.

</details>


### [142] [LTRAS: A Linkable Threshold Ring Adaptor Signature Scheme for Efficient and Private Cross-Chain Transactions](https://arxiv.org/abs/2602.05431)
*Yi Liang,Jinguang Han*

Main category: cs.CR

TL;DR: 提出了一种结合适配器签名、门限环签名和可链接性的LTRAS方案，用于解决区块链支付通道中的隐私和效率问题


<details>
  <summary>Details</summary>
Motivation: 区块链技术面临可扩展性和吞吐量限制，支付通道等链下解决方案成为研究重点。适配器签名虽然支持原子性，但可能泄露签名者身份；环签名能提供匿名性，但在UTXO区块链的多账户支付场景中通信开销大。需要一种既能保护隐私又高效的解决方案。

Method: 提出Linkable Threshold Ring Adaptor Signature (LTRAS)方案，整合了适配器签名的条件绑定、门限环签名的多账户支付能力以及防止双花的可链接性。提供了形式化定义、安全模型和具体构造，并通过理论分析和实验实现评估性能。

Result: 实验结果表明，在大环规模和多账户支付场景下，该方案的计算和通信开销显著低于现有方案。方案在跨链原子交换中具有应用潜力，能提升区块链交易的隐私性和效率。

Conclusion: LTRAS方案成功解决了适配器签名的隐私泄露问题和环签名的高通信开销问题，为区块链支付通道提供了一种既保护隐私又高效的解决方案，在跨链原子交换等场景中具有实际应用价值。

Abstract: Despite the advantages of decentralization and immutability, blockchain technology faces significant scalability and throughput limitations, which has prompted the exploration of off-chain solutions like payment channels. Adaptor signatures have been considered a promising primitive for constructing such channels due to their support for atomicity, offering an alternative to traditional hash-timelock contracts. However, standard adaptor signatures may reveal signer identity, raising potential privacy concerns. While ring signatures can mitigate this issue by providing anonymity, they often introduce high communication overhead, particularly in multi-account payment settings commonly used in UTXO-based blockchains like Monero. To address these limitations, we propose a Linkable Threshold Ring Adaptor Signature (LTRAS) scheme, which integrates the conditional binding of adaptor signatures, the multi-account payment of threshold ring signatures, and the linkability for preventing double-spending. The formal definition, security model and concrete construction of LTRAS are provided. We also analyze its security and evaluate its performance through theoretical analysis and experimental implementation. Experimental results demonstrate that our scheme achieve significantly lower computation and communication overhead compared to existing schemes in large ring sizes and multi-account payment scenarios. Finally, we discuss its application in cross-chain atomic swaps, demonstrating its potential for enhancing privacy and efficiency in blockchain transactions.

</details>


### [143] [Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection](https://arxiv.org/abs/2602.05484)
*Takashi Koide,Hiroki Nakano,Daiki Chiba*

Main category: cs.CR

TL;DR: 论文首次全面评估了针对多模态LLM钓鱼检测的提示注入攻击，提出了攻击分类法，展示了最先进模型仍易受攻击，并提出了InjectDefuser防御框架显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼网站数量和复杂性持续增长，虽然LLM在钓鱼检测中展现出潜力，但其固有的提示注入漏洞被忽视。攻击者可利用LLM与人类感知不对称性，在钓鱼网站中嵌入对用户不可见但对LLM可解析的指令，从而操纵检测结果。当前对钓鱼检测中提示注入风险及有效防御策略的研究不足。

Method: 1) 提出二维分类法：攻击技术维度（指令隐藏、语义混淆等）和攻击面维度（URL、HTML、渲染内容等）；2) 基于分类法实现多样化攻击；3) 对多个代表性LLM检测系统进行实证研究；4) 提出InjectDefuser防御框架，结合提示硬化、基于白名单的检索增强和输出验证。

Result: 1) 包括GPT-5在内的最先进模型仍易受提示注入攻击；2) 提出的攻击分类法能有效捕获现实攻击策略；3) InjectDefuser防御框架在多个模型上显著降低攻击成功率，提高了钓鱼检测的可靠性。

Conclusion: 该研究首次全面揭示了多模态LLM钓鱼检测中的提示注入风险，提出的分类法为理解攻击策略提供了系统框架，InjectDefuser防御框架为实际部署提供了有效保护方案，有助于提升下一代钓鱼检测系统的可靠性。

Abstract: Phishing sites continue to grow in volume and sophistication. Recent work leverages large language models (LLMs) to analyze URLs, HTML, and rendered content to decide whether a website is a phishing site. While these approaches are promising, LLMs are inherently vulnerable to prompt injection (PI). Because attackers can fully control various elements of phishing sites, this creates the potential for PI that exploits the perceptual asymmetry between LLMs and humans: instructions imperceptible to end users can still be parsed by the LLM and can stealthily manipulate its judgment. The specific risks of PI in phishing detection and effective mitigation strategies remain largely unexplored. This paper presents the first comprehensive evaluation of PI against multimodal LLM-based phishing detection. We introduce a two-dimensional taxonomy, defined by Attack Techniques and Attack Surfaces, that captures realistic PI strategies. Using this taxonomy, we implement diverse attacks and empirically study several representative LLM-based detection systems. The results show that phishing detection with state-of-the-art models such as GPT-5 remains vulnerable to PI. We then propose InjectDefuser, a defense framework that combines prompt hardening, allowlist-based retrieval augmentation, and output validation. Across multiple models, InjectDefuser significantly reduces attack success rates. Our findings clarify the PI risk landscape and offer practical defenses that improve the reliability of next-generation phishing countermeasures.

</details>


### [144] [GNSS SpAmming: a spoofing-based GNSS denial-of-service attack](https://arxiv.org/abs/2602.05517)
*Sergio Angulo Cosín,Javier Junquera-Sánchez,Carlos Hernando-Ramiro,José-Antonio Gómez-Sánchez*

Main category: cs.CR

TL;DR: 提出了一种新型GNSS攻击方法"SpAmming"，结合了干扰和欺骗技术，通过CDMA复用特性使接收机失去对合法卫星信号的访问，现有反干扰和反欺骗措施难以防御。


<details>
  <summary>Details</summary>
Motivation: 现有GNSS攻击主要分为干扰（拒绝服务）和欺骗（假冒卫星）两类，已有大量对抗措施。但缺乏对结合两种攻击方式的新型混合攻击的研究，需要探索更隐蔽的攻击手段及其防御方法。

Method: 利用GNSS中CDMA复用特性，通过欺骗攻击使接收机失去对合法卫星信号的访问。使用SDR系统开发攻击平台，针对冷启动、温启动和已获取PVT解算的接收机进行实验，测试不同攻击配置（原始虚假信号发射、精确多普勒效应配置、码偏移等）。

Result: 攻击对冷启动接收机特别有效，但在其他场景下也表现出有效性，特别是与其他攻击结合时。实验验证了攻击在不同接收机状态下的影响，并展示了攻击配置的灵活性。

Conclusion: SpAmming攻击结合了干扰和欺骗的特点，现有反制措施难以防御。需要开发新的检测和对抗方法，特别是在OSNMA等认证服务中需要进一步研究其影响，并改进对类似攻击的响应能力。

Abstract: GNSSs are vulnerable to attacks of two kinds: jamming (i.e. denying access to the signal) and spoofing (i.e. impersonating a legitimate satellite). These attacks have been extensively studied, and we have a myriad of countermeasures to mitigate them. In this paper we expose a new type of attack: SpAmming, which combines both approaches to achieve the same effects in a more subtle way.
  Exploiting the CDMA multiplexing present in most GNSSs, and through a spoofing attack, this approach leads the receiver to lose access to the signal of a legitimate satellite, which would be equivalent to a denial of service; but in this case the existing countermeasures against jamming or spoofing would not allow safeguarding its effectiveness, as it is neither of them.
  An experimental proof-of-concept is presented in which its impact is evaluated as a function of the previous state of the receiver. Using an SDR-based system developed at the Space Security Centre, the attack is executed against a cold-started receiver, a warm-started receiver, and a receiver that has already acquired the PVT solution and is navigating. Different attack configurations are also tested, starting from a raw emission of the false signal, to surgical Doppler effect configuration, code offset, etc. Although it is shown to be particularly successful against cold-started receivers, the results show that it is also effective in other scenarios, especially if accompanied by other attacks. We will conclude the article by outlining possible countermeasures to detect and, eventually, counteract it; and possible avenues of research to better understand its impact, especially for authenticated services such as OSNMA, and to characterize it in order to improve the response to similar attacks.

</details>


### [145] [ADCA: Attention-Driven Multi-Party Collusion Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2602.05612)
*Jiayao Wang,Yiping Zhang,Jiale Zhang,Wenliang Yuan,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: 提出ADCA攻击方法，通过恶意客户端分解全局触发器、形成恶意联盟并建立协作优化机制，在联邦自监督学习中实现更隐蔽、鲁棒的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有联邦自监督学习（FSSL）的后门攻击存在局限性：使用全局统一触发器容易被检测、在聚合过程中被稀释、在异构客户端环境中缺乏鲁棒性。需要设计更隐蔽、鲁棒的攻击方法。

Method: 提出注意力驱动的多方合谋攻击（ADCA）：1）恶意客户端在本地预训练中分解全局触发器寻找最优本地模式；2）恶意客户端合谋形成恶意联盟；3）建立协作优化机制，通过注意力机制动态聚合模型更新；4）聚合参数作为联盟内下一轮训练的初始状态，减轻良性更新对后门信息的稀释。

Result: 在多个FSSL场景和四个数据集上的实验表明，ADCA在攻击成功率（ASR）和持久性方面显著优于现有方法，证明了其有效性和鲁棒性。

Conclusion: ADCA通过恶意客户端协作优化和注意力机制动态聚合，解决了现有FSSL后门攻击的局限性，实现了更隐蔽、鲁棒的后门攻击，对FSSL系统的安全性提出了新的挑战。

Abstract: Federated Self-Supervised Learning (FSSL) integrates the privacy advantages of distributed training with the capability of self-supervised learning to leverage unlabeled data, showing strong potential across applications. However, recent studies have shown that FSSL is also vulnerable to backdoor attacks. Existing attacks are limited by their trigger design, which typically employs a global, uniform trigger that is easily detected, gets diluted during aggregation, and lacks robustness in heterogeneous client environments. To address these challenges, we propose the Attention-Driven multi-party Collusion Attack (ADCA). During local pre-training, malicious clients decompose the global trigger to find optimal local patterns. Subsequently, these malicious clients collude to form a malicious coalition and establish a collaborative optimization mechanism within it. In this mechanism, each submits its model updates, and an attention mechanism dynamically aggregates them to explore the best cooperative strategy. The resulting aggregated parameters serve as the initial state for the next round of training within the coalition, thereby effectively mitigating the dilution of backdoor information by benign updates. Experiments on multiple FSSL scenarios and four datasets show that ADCA significantly outperforms existing methods in Attack Success Rate (ASR) and persistence, proving its effectiveness and robustness.

</details>


### [146] [Time-Complexity Characterization of NIST Lightweight Cryptography Finalists](https://arxiv.org/abs/2602.05641)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.CR

TL;DR: 该论文提出了一个符号模型来分析NIST轻量级密码学项目中10个最终算法的理论时间复杂度，为受限环境下的算法选择提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着数字身份系统和物联网验证等新兴技术的发展，需要为处理能力、内存和能源资源有限的设备提供强密码学保障。虽然NIST的轻量级密码学项目对10个最终算法进行了实证评估，但缺乏对其时间复杂度的统一理论理解。

Method: 引入一个符号模型，将每个密码方案分解为初始化、数据处理和最终化三个阶段，从而为所有10个最终算法形式化推导时间复杂度。

Result: 结果阐明了设计参数如何在受限移动和嵌入式环境中影响计算扩展性，提供了区分算法效率的基础。

Conclusion: 该框架为在受限环境中支持安全系统的密码原语选择提供了必要的理论基础，能够指导算法选择决策。

Abstract: Lightweight cryptography is becoming essential as emerging technologies in digital identity systems and Internet of Things verification continue to demand strong cryptographic assurance on devices with limited processing power, memory, and energy resources. As these technologies move into routine use, they demand cryptographic primitives that maintain strong security and deliver predictable performance through clear theoretical models of time complexity. Although NIST's lightweight cryptography project provides empirical evaluations of the ten finalist algorithms, a unified theoretical understanding of their time-complexity behavior remains absent. This work introduces a symbolic model that decomposes each scheme into initialization, data-processing, and finalization phases, enabling formal time-complexity derivation for all ten finalists. The results clarify how design parameters shape computational scaling on constrained mobile and embedded environments. The framework provides a foundation needed to distinguish algorithmic efficiency and guides the choice of primitives capable of supporting security systems in constrained environments.

</details>


### [147] [Characterizing and Modeling the GitHub Security Advisories Review Pipeline](https://arxiv.org/abs/2602.06009)
*Claudio Segal,Paulo Segal,Carlos Eduardo de Schuller Banjar,Felipe Paixão,Hudson Silva Borges,Paulo Silveira Neto,Eduardo Santana de Almeida,Joanna C. S. Santos,Anton Kocheturov,Gaurav Kumar Srivastava,Daniel Sadoc Menasché*

Main category: cs.CR

TL;DR: 对GitHub安全公告（GHSA）审查流程的大规模实证研究，分析了2019-2025年超过28.8万条公告，揭示了审查机制、延迟特征和两种不同的审查延迟模式。


<details>
  <summary>Details</summary>
Motivation: GitHub安全公告已成为开源漏洞披露的核心组成部分，但只有部分公告经过GitHub审查，且审查机制缺乏深入理解。本研究旨在系统分析GHSA审查流程的特征和模式。

Method: 对2019-2025年超过288,000条GHSA公告进行大规模实证研究，分析审查概率、延迟特征，并开发排队模型来解释审查流程中的二分结构。

Result: 识别出两种不同的审查延迟模式：快速路径（以GitHub仓库公告为主）和慢速路径（以NVD优先公告为主）。量化了审查延迟，并建立了能够解释这种二分结构的排队模型。

Conclusion: 研究揭示了GHSA审查流程的系统性特征，为理解开源漏洞披露机制提供了实证基础，并展示了审查流程中的结构化差异对安全响应时间的影响。

Abstract: GitHub Security Advisories (GHSA) have become a central component of open-source vulnerability disclosure and are widely used by developers and security tools. A distinctive feature of GHSA is that only a fraction of advisories are reviewed by GitHub, while the mechanisms associated with this review process remain poorly understood. In this paper, we conduct a large-scale empirical study of GHSA review processes, analyzing over 288,000 advisories spanning 2019--2025. We characterize which advisories are more likely to be reviewed, quantify review delays, and identify two distinct review-latency regimes: a fast path dominated by GitHub Repository Advisories (GRAs) and a slow path dominated by NVD-first advisories. We further develop a queueing model that accounts for this dichotomy based on the structure of the advisory processing pipeline.

</details>


### [148] [FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation](https://arxiv.org/abs/2602.05838)
*Mayank Kumar,Qian Lou,Paulo Barreto,Martine De Cock,Sikha Pentyala*

Main category: cs.CR

TL;DR: FHAIM：首个基于全同态加密的框架，用于在加密表格数据上训练边际合成数据生成器，解决隐私数据共享问题


<details>
  <summary>Details</summary>
Motivation: AI发展受限于隐私和法规导致的数据孤岛问题，尤其在医疗、教育、金融等重要领域。现有合成数据生成服务需要数据持有者信任服务提供商访问私有数据，存在隐私风险。

Method: 提出FHAIM框架，将广泛使用的AIM算法适配到全同态加密环境，使用新颖的FHE协议，确保私有数据在整个过程中保持加密状态，并仅以差分隐私保证发布。

Result: 实证分析表明FHAIM在保持可行运行时间的同时，保留了AIM算法的性能。

Conclusion: FHAIM为隐私保护合成数据生成提供了可行的解决方案，使数据持有者能够在保持数据加密的同时生成有用的合成数据，打破数据孤岛。

Abstract: Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.

</details>


### [149] [Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection](https://arxiv.org/abs/2602.05868)
*Ehsan Firouzi,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究发现静态安全分析工具（CodeQL和Semgrep）在评估LLM生成代码安全性时存在显著不一致性，仅约60%的报告与人工验证基准相符，质疑了这些工具作为唯一评估标准的可靠性，并提出了基于人类反馈的动态RAG框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度依赖静态分析工具评估LLM生成的代码安全性，但这些工具的可靠性尚未得到充分验证。作者旨在通过系统比较静态工具输出与人工验证基准，揭示其局限性，并提出更可靠的评估框架。

Method: 1. 收集1,080个LLM生成的代码样本；2. 建立人工验证的基准真值；3. 使用CodeQL和Semgrep进行静态安全分析；4. 对比工具输出与基准真值；5. 提出基于人类反馈的动态检索增强生成框架。

Result: 1. 61%的样本实际安全；2. Semgrep和CodeQL分别将60%和80%的样本分类为安全；3. 工具与基准真值匹配率低：Semgrep仅65%，CodeQL仅61%；4. 尽管总体统计看似一致，但样本级分析显示显著差异。

Conclusion: 静态分析工具不能作为代码安全性的唯一评估标准，需要专家反馈。提出的动态RAG框架通过持久存储和重用人类反馈，能够提升LLM在安全代码生成和漏洞检测方面的能力。

Abstract: Existing literature heavily relies on static analysis tools to evaluate LLMs for secure code generation and vulnerability detection. We reviewed 1,080 LLM-generated code samples, built a human-validated ground-truth, and compared the outputs of two widely used static security tools, CodeQL and Semgrep, against this corpus. While 61% of the samples were genuinely secure, Semgrep and CodeQL classified 60% and 80% as secure, respectively. Despite the apparent agreement in aggregate statistics, per-sample analysis reveals substantial discrepancies: only 65% of Semgrep's and 61% of CodeQL's reports correctly matched the ground truth. These results question the reliability of static analysis tools as sole evaluators of code security and underscore the need for expert feedback. Building on this insight, we propose a conceptual framework that persistently stores human feedback in a dynamic retrieval-augmented generation pipeline, enabling LLMs to reuse past feedback for secure code generation and vulnerability detection.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [150] [Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2602.04926)
*Ning Wang,Kuanyan Zhu,Daniel Yuehwoon Yee,Yitang Gao,Shiying Huang,Zirun Xu,Sainyam Galhotra*

Main category: cs.DB

TL;DR: AutoPrunedRetriever：基于图结构的RAG系统，通过持久化最小推理子图并增量扩展，减少重复检索和推理开销，在保持SOTA准确率的同时大幅降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统对每个查询都重新检索长段落并从头推理，导致token消耗、延迟和成本增加。需要一种能够持久化先前推理结果并增量扩展的系统，以支持长期会话、演化语料库和多智能体管道。

Method: 提出图式RAG系统AutoPrunedRetriever：1) 将实体和关系存储在紧凑的ID索引码本中；2) 将问题、事实和答案表示为边序列；3) 应用两层整合策略（快速ANN/KNN别名检测+选择性k-means）保持图紧凑；4) 剪枝低价值结构；5) 提示中仅保留重叠代表和真正新证据。实现两个前端：基于REBEL三元组解析器和基于LLM提取器。

Result: 在GraphRAG-Benchmark（医学和小说领域）上，两个变体均达到最先进的复杂推理准确率，比HippoRAG2提高约9-11个百分点，在上下文摘要和生成方面保持竞争力。在更难的STEM和TV基准测试中同样排名第一，同时使用的token比基于图的基线少两个数量级。

Conclusion: AutoPrunedRetriever通过持久化最小推理子图并增量扩展，实现了高效的RAG系统，在保持高准确率的同时显著降低计算开销，为长期会话、演化语料库和多智能体管道提供了实用基础。

Abstract: Retrieval-augmented generation (RAG) is now standard for knowledge-intensive LLM tasks, but most systems still treat every query as fresh, repeatedly re-retrieving long passages and re-reasoning from scratch, inflating tokens, latency, and cost. We present AutoPrunedRetriever, a graph-style RAG system that persists the minimal reasoning subgraph built for earlier questions and incrementally extends it for later ones. AutoPrunedRetriever stores entities and relations in a compact, ID-indexed codebook and represents questions, facts, and answers as edge sequences, enabling retrieval and prompting over symbolic structure instead of raw text. To keep the graph compact, we apply a two-layer consolidation policy (fast ANN/KNN alias detection plus selective $k$-means once a memory threshold is reached) and prune low-value structure, while prompts retain only overlap representatives and genuinely new evidence. We instantiate two front ends: AutoPrunedRetriever-REBEL, which uses REBEL as a triplet parser, and AutoPrunedRetriever-llm, which swaps in an LLM extractor. On GraphRAG-Benchmark (Medical and Novel), both variants achieve state-of-the-art complex reasoning accuracy, improving over HippoRAG2 by roughly 9--11 points, and remain competitive on contextual summarize and generation. On our harder STEM and TV benchmarks, AutoPrunedRetriever again ranks first, while using up to two orders of magnitude fewer tokens than graph-heavy baselines, making it a practical substrate for long-running sessions, evolving corpora, and multi-agent pipelines.

</details>


### [151] [DistillER: Knowledge Distillation in Entity Resolution with Large Language Models](https://arxiv.org/abs/2602.05452)
*Alexandros Zeakis,George Papadakis,Dimitrios Skoutas,Manolis Koubarakis*

Main category: cs.DB

TL;DR: DistillER：首个通过知识蒸馏将大语言模型实体解析能力迁移到小模型的无监督框架，在效率与效果间取得平衡


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的实体解析方法存在计算资源消耗大或需要标注数据的局限，需要在时间效率与效果之间找到更实用的平衡点

Method: 提出DistillER框架，从三个维度系统研究知识蒸馏：数据选择（识别信息丰富的数据子集）、知识提取（比较单/多教师设置）、蒸馏算法（评估监督微调和强化学习方法）

Result: 实验表明，基于LLM教师生成噪声标签的监督微调学生模型优于其他蒸馏策略，同时支持高质量解释生成；DistillER在效果和效率上均显著优于现有方法

Conclusion: 知识蒸馏是实现高效实用LLM实体解析的有效途径，DistillER框架为无监督知识迁移提供了系统解决方案

Abstract: Recent advances in Entity Resolution (ER) have leveraged Large Language Models (LLMs), achieving strong performance but at the cost of substantial computational resources or high financial overhead. Existing LLM-based ER approaches operate either in unsupervised settings and rely on very large and costly models, or in supervised settings and require ground-truth annotations, leaving a critical gap between time efficiency and effectiveness. To make LLM-powered ER more practical, we investigate Knowledge Distillation (KD) as a means to transfer knowledge from large, effective models (Teachers) to smaller, more efficient models (Students) without requiring gold labels. We introduce DistillER, the first framework that systematically bridges this gap across three dimensions: (i) Data Selection, where we study strategies for identifying informative subsets of data; (ii) Knowledge Elicitation, where we compare single- and multi-teacher settings across LLMs and smaller language models (SLMs); and (iii) Distillation Algorithms, where we evaluate supervised fine-tuning and reinforcement learning approaches. Our experiments reveal that supervised fine-tuning of Students on noisy labels generated by LLM Teachers consistently outperforms alternative KD strategies, while also enabling high-quality explanation generation. Finally, we benchmark DistillER against established supervised and unsupervised ER methods based on LLMs and SLMs, demonstrating significant improvements in both effectiveness and efficiency.

</details>


### [152] [Repairing Property Graphs under PG-Constraints](https://arxiv.org/abs/2602.05503)
*Christopher Spinrath,Angela Bonifati,Rachid Echahed*

Main category: cs.DB

TL;DR: 研究属性图在PG-Constraints约束下的修复问题，提出包含节点、边和标签删除的修复流程，比较三种算法策略。


<details>
  <summary>Details</summary>
Motivation: 随着GQL和SQL/PGQ等图数据库标准查询语言以及PG-Constraints约束语言的出现，需要研究属性图在PG-Constraints约束下的修复问题，以维护图数据的完整性和一致性。

Method: 识别PG-Constraints的重要子集（包含否定约束和递归特性），提出完整的属性图修复流程，涉及图拓扑结构变更和节点、边及标签删除。研究三种修复算法策略：基于整数线性规划(ILP)、朴素算法和LP引导的贪心算法。

Result: 实验表明，使用标签删除相比节点/边删除可以减少59%的删除操作；LP引导的贪心算法相比ILP策略在运行时间上提升高达97%，同时保持相同的修复质量。

Conclusion: PG-Constraints约束下的属性图修复是可行的，标签删除策略能显著减少删除操作，LP引导的贪心算法在效率和质量之间提供了良好的平衡。

Abstract: Recent standardization efforts for graph databases lead to standard query languages like GQL and SQL/PGQ, and constraint languages like Property Graph Constraints (PG-Constraints). In this paper, we embark on the study of repairing property graphs under PG-Constraints. We identify a significant subset of PG-Constraints, encoding denial constraints and including recursion as a key feature, while still permitting automata-based structural analyses of errors. We present a comprehensive repair pipeline for these constraints to repair Property Graphs, involving changes in the graph topology and leading to node, edge and, optionally, label deletions. We investigate three algorithmic strategies for the repair procedure, based on Integer Linear Programming (ILP), a naive, and an LP-guided greedy algorithm. Our experiments on various real-world datasets reveal that repairing with label deletions can achieve a 59% reduction in deletions compared to node/edge deletions. Moreover, the LP-guided greedy algorithm offers a runtime advantage of up to 97% compared to the ILP strategy, while matching the same quality.

</details>


### [153] [Taking the Leap: Efficient and Reliable Fine-Grained NUMA Migration in User-space](https://arxiv.org/abs/2602.05540)
*Felix Schuhknecht,Nick Rassau*

Main category: cs.DB

TL;DR: 提出page_leap()用户空间页面迁移方法，解决NUMA架构下现有迁移方案性能不足和功能限制问题


<details>
  <summary>Details</summary>
Motivation: 现代多插槽架构提供单一虚拟地址空间但物理内存分布在多个NUMA区域，线程访问本地区域比远程区域快得多。当查询并行化跨多个区域核心时，需要相应分布数据以确保本地访问。现有迁移方案（Linux自动NUMA平衡和move_pages()系统调用）在功能集和性能方面存在显著缺点。

Method: 提出page_leap()用户空间迁移方法，通过利用虚拟内存子系统特性实现高性能异步页面迁移。该方法具有以下特点：(a) 用户主动触发，(b) 确保所有页面最终迁移，(c) 正确处理并发写入，(d) 支持池化内存，(e) 基于工作负载自适应调整迁移粒度，(f) 支持小页面和大页面。

Result: 未在摘要中明确说明具体实验结果，但暗示该方法相比现有方案在性能和功能方面有显著改进

Conclusion: page_leap()提供了一种高性能、功能全面的用户空间页面迁移方案，解决了现有NUMA内存迁移方法的局限性，能够更好地优化多插槽架构下的内存访问性能。

Abstract: Modern multi-socket architectures offer a single virtual address space, but physically divide main-memory across multiple regions, where each region is attached to a CPU and its cores. While this simplifies the usage, developers must be aware of non-uniform memory access (NUMA), where an access by a thread running on a core-local NUMA region is significantly cheaper than an access from a core-remote region. Obviously, if query answering is parallelized across the cores of multiple regions, then the portion of the database on which the query is operating should be distributed across the same regions to ensure local accesses. As the present data placement might not fit this, migrating pages from one NUMA region to another can be performed to improve the situation. To do so, different options exist: One option is to rely on automatic NUMA balancing integrated in Linux, which is steered by the observed access patterns and frequency. Another option is to actively trigger migration via the system call move_pages(). Unfortunately, both variants have significant downsides in terms of their feature set and performance. As an alternative, we propose a new user-space migration method called page_leap() that can perform page migration asynchronously at a high performance by exploiting features of the virtual memory subsystem. The method is (a) actively triggered by the user, (b) ensures that all pages are eventually migrated, (c) handles concurrent writes correctly, (d) supports pooled memory, (e) adaptively adjusts its migration granularity based on the workload, and (f) supports both small pages and huge pages.

</details>


### [154] [One Size Does NOT Fit All: On the Importance of Physical Representations for Datalog Evaluation](https://arxiv.org/abs/2602.05651)
*Nick Rassau,Felix Schuhknecht*

Main category: cs.DB

TL;DR: 论文研究Datalog查询引擎中物理表示的选择问题，提出基于决策树的自动选择机制来优化不同工作负载下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Datalog引擎在物理表示选择上过于限制，通常采用一刀切方案，而实际执行计划涉及多种操作类型（插入、查找、包含检查），且不同工作负载特性对操作需求差异很大，需要针对性的物理表示优化。

Method: 1) 深入实验研究物理表示与七个工作负载维度之间的相互作用；2) 基于实验洞察设计自动选择机制，使用决策树识别适合给定工作负载的物理表示。

Result: 实验揭示了哪些工作负载特性真正影响性能，并验证了决策树机制能够有效选择适合的物理表示。

Conclusion: Datalog查询性能优化需要根据具体工作负载特性选择适当的物理表示，基于决策树的自动选择机制能够有效实现这一目标，提升引擎性能。

Abstract: Datalog is an increasingly popular recursive query language that is declarative by design, meaning its programs must be translated by an engine into the actual physical execution plan. When generating this plan, a central decision is how to physically represent all involved relations, an aspect in which existing Datalog engines are surprisingly restrictive and often resort to one-size-fits-all solutions. The reason for this is that the typical execution plan of a Datalog program not only performs a single type of operation against the physical representations, but a mixture of operations, such as insertions, lookups, and containment-checks. Further, the relevance of each operation type highly depends on the workload characteristics, which range from familiar properties such as the size, multiplicity, and arity of the individual relations to very specific Datalog properties, such as the "interweaving" of rules when relations occur multiple times, and in particular the recursiveness of the query which might generate new tuples on the fly during evaluation. This indicates that a variety of physical representations, each with its own strengths and weaknesses, is required to meet the specific needs of different workload situations. To evaluate this, we conduct an in-depth experimental study of the interplay between potentially suitable physical representations and seven dimensions of workload characteristics that vary across actual Datalog programs, revealing which properties actually matter. Based on these insights, we design an automatic selection mechanism that utilizes a set of decision trees to identify suitable physical representations for a given workload.

</details>


### [155] [Fast Private Adaptive Query Answering for Large Data Domains](https://arxiv.org/abs/2602.05674)
*Miguel Fuentes,Brett Mullins,Yingtai Xiao,Daniel Kifer,Cameron Musco,Daniel Sheldon*

Main category: cs.DB

TL;DR: 提出AIM+GReM机制，通过残差查询和多维数组框架改进差分隐私边际释放，显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私边际释放机制在从噪声测量重建边际估计时存在计算瓶颈，需要更高效的解决方案

Method: 引入残差查询的多维数组概念框架、惰性更新策略和自适应每轮隐私预算分配优化，将残差查询集成到AIM等自适应机制中

Result: AIM+GReM机制比原始框架快几个数量级，具有竞争性的误差表现和显著改善的可扩展性

Conclusion: 残差查询与自适应机制的集成通过新颖的多维数组框架和优化策略，实现了差分隐私边际释放的高效计算和准确重建

Abstract: Privately releasing marginals of a tabular dataset is a foundational problem in differential privacy. However, state-of-the-art mechanisms suffer from a computational bottleneck when marginal estimates are reconstructed from noisy measurements. Recently, residual queries were introduced and shown to lead to highly efficient reconstruction in the batch query answering setting. We introduce new techniques to integrate residual queries into state-of-the-art adaptive mechanisms such as AIM. Our contributions include a novel conceptual framework for residual queries using multi-dimensional arrays, lazy updating strategies, and adaptive optimization of the per-round privacy budget allocation. Together these contributions reduce error, improve speed, and simplify residual query operations. We integrate these innovations into a new mechanism (AIM+GReM), which improves AIM by using fast residual-based reconstruction instead of a graphical model approach. Our mechanism is orders of magnitude faster than the original framework and demonstrates competitive error and greatly improved scalability.

</details>


### [156] [Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration](https://arxiv.org/abs/2602.05708)
*Chuangtao Ma,Zeyu Zhang,Arijit Khan,Sebastian Schelter,Paul Groth*

Main category: cs.DB

TL;DR: CE-RAG4EM：一种用于实体匹配的成本高效RAG架构，通过基于分块的批量检索和生成减少计算开销，在保持匹配质量的同时显著降低端到端运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有RAG管道在大规模实体匹配任务中存在显著的检索和生成开销，需要更高效的架构来平衡性能与计算成本。

Method: 提出CE-RAG4EM架构，采用基于分块的批量检索和生成策略；建立统一框架分析评估RAG系统，重点关注分块感知优化和检索粒度。

Result: 实验表明CE-RAG4EM在保持或提升匹配质量的同时，显著减少端到端运行时间；分析揭示关键配置参数在性能与开销之间存在固有权衡。

Conclusion: CE-RAG4EM为实体匹配和数据集成提供了高效可扩展的RAG解决方案，其配置权衡分析为系统设计提供了实用指导。

Abstract: Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.

</details>


### [157] ["Detective Work We Shouldn't Have to Do": Practitioner Challenges in Regulatory-Aligned Data Quality in Machine Learning Systems](https://arxiv.org/abs/2602.05944)
*Yichun Wang,Kristina Irion,Paul Groth,Hazar Harmouch*

Main category: cs.DB

TL;DR: 本文通过访谈研究欧盟ML从业者，揭示监管要求与工程实践之间的差距，提出改进数据质量管理的需求


<details>
  <summary>Details</summary>
Motivation: 随着欧盟GDPR和AI法案等监管框架对数据质量提出要求，需要了解ML从业者如何在实际工作中应对这些监管要求，以及监管原则与工程实践之间的差距

Method: 采用定性访谈研究方法，对欧盟从事受监管环境中ML系统的数据从业者进行半结构化访谈，探讨他们对监管对齐数据质量的理解、面临的挑战和所需支持

Result: 研究发现存在法律原则与工程工作流程之间的持续差距、数据管道碎片化、现有工具局限性、技术团队与法律团队责任边界不清、以及倾向于被动审计驱动的质量实践等问题

Conclusion: 从业者需要合规感知工具、更清晰的治理结构，以及向主动数据治理的文化转变，以弥合监管要求与ML实践之间的鸿沟

Abstract: Ensuring data quality in machine learning (ML) systems has become increasingly complex as regulatory requirements expand. In the European Union (EU), frameworks such as the General Data Protection Regulation (GDPR) and the Artificial Intelligence Act (AI Act) articulate data quality requirements that closely parallel technical concerns in ML practice, while also extending to legal obligations related to accountability, risk management, and human rights protection. This paper presents a qualitative interview study with EU-based data practitioners working on ML systems in regulated contexts. Through semi-structured interviews, we investigate how practitioners interpret regulatory-aligned data quality, the challenges they encounter, and the supports they identify as necessary. Our findings reveal persistent gaps between legal principles and engineering workflows, fragmentation across data pipelines, limitations of existing tools, unclear responsibility boundaries between technical and legal teams, and a tendency toward reactive, audit-driven quality practices. We also identify practitioners' needs for compliance-aware tooling, clearer governance structures, and cultural shifts toward proactive data governance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [158] [The Necessity of a Holistic Safety Evaluation Framework for AI-Based Automation Features](https://arxiv.org/abs/2602.05157)
*Alireza Abbaspour,Shabin Mahadevan,Kilian Zwirglmaier,Jeff Stafford*

Main category: cs.SE

TL;DR: QM组件传统上被排除在安全分析之外，但AI集成显示这些组件可能导致SOTIF相关危险，需要重新评估安全框架


<details>
  <summary>Details</summary>
Motivation: 传统上，预期功能安全(SOTIF)和功能安全(FuSa)分析将质量管理(QM)组件排除在严格的安全影响评估之外。然而，随着人工智能(AI)的集成，这些传统上不被视为安全相关的组件可能引发SOTIF相关的危险风险。新兴的AI安全标准(如ISO/PAS 8800)要求重新评估这些组件的安全考虑。

Method: 通过案例研究分析AI驱动感知系统中的缺陷如何出现在QM分类组件中，导致具有关键安全影响的非预期功能行为。将理论分析与实际示例相结合，论证采用全面的FuSa、SOTIF和AI标准驱动方法的重要性。

Result: 研究表明，即使在QM分类的组件中，AI驱动的感知系统缺陷也可能出现，导致违反风险接受标准的危险。这证明了需要修订现有安全框架，以应对AI带来的不断发展的挑战。

Conclusion: 需要采用全面的安全分析方法，将FuSa、SOTIF和AI标准驱动的方法相结合，以识别和缓解AI组件中的风险。现有安全框架需要修订，以确保跨所有组件分类的全面安全保证，涵盖多个安全标准。

Abstract: The intersection of Safety of Intended Functionality (SOTIF) and Functional Safety (FuSa) analysis of driving automation features has traditionally excluded Quality Management (QM) components from rigorous safety impact evaluations. While QM components are not typically classified as safety-relevant, recent developments in artificial intelligence (AI) integration reveal that such components can contribute to SOTIF-related hazardous risks. Compliance with emerging AI safety standards, such as ISO/PAS 8800, necessitates re-evaluating safety considerations for these components. This paper examines the necessity of conducting holistic safety analysis and risk assessment on AI components, emphasizing their potential to introduce hazards with the capacity to violate risk acceptance criteria when deployed in safety-critical driving systems, particularly in perception algorithms. Using case studies, we demonstrate how deficiencies in AI-driven perception systems can emerge even in QM-classified components, leading to unintended functional behaviors with critical safety implications. By bridging theoretical analysis with practical examples, this paper argues for the adoption of comprehensive FuSa, SOTIF, and AI standards-driven methodologies to identify and mitigate risks in AI components. The findings demonstrate the importance of revising existing safety frameworks to address the evolving challenges posed by AI, ensuring comprehensive safety assurance across all component classifications spanning multiple safety standards.

</details>


### [159] [ASA: Activation Steering for Tool-Calling Domain Adaptation](https://arxiv.org/abs/2602.04935)
*Youjin Wang,Run Zhou,Rong Fu,Shuaishuai Cao,Hongwei Zeng,Jiaxuan Lu,Sicheng Fan,Jiaqiao Zhao,Liangming Pan*

Main category: cs.SE

TL;DR: 提出ASA（激活引导适配器），一种轻量级、无需训练、推理时机制，通过读取中间激活的路由信号并使用超轻量路由器产生自适应控制强度，实现精确领域对齐，解决LLM智能体在多领域工具生态系统中高效适应的问题。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体实际部署的核心挑战不是工具使用本身，而是在快速演变的工具集、API和协议下的高效领域适应。传统方法如重复LoRA或SFT跨领域训练会产生指数级增长的成本，而提示或模式方法在分布偏移和复杂接口下表现脆弱。

Method: 提出Activation Steering Adapter (ASA)，一种轻量级、推理时、无需训练的机制。该方法读取中间激活的路由信号，使用超轻量路由器产生自适应控制强度，实现精确的领域对齐。

Result: 在多个模型规模和领域上，ASA实现了与LoRA相当的适应效果，同时显著降低了开销，并表现出强大的跨模型可转移性。

Conclusion: ASA为具有频繁接口变化的健壮、可扩展和高效的多领域工具生态系统提供了理想的实用解决方案。

Abstract: For real-world deployment of general-purpose LLM agents, the core challenge is often not tool use itself, but efficient domain adaptation under rapidly evolving toolsets, APIs, and protocols. Repeated LoRA or SFT across domains incurs exponentially growing training and maintenance costs, while prompt or schema methods are brittle under distribution shift and complex interfaces. We propose \textbf{Activation Steering Adapter (ASA}), a lightweight, inference-time, training-free mechanism that reads routing signals from intermediate activations and uses an ultra-light router to produce adaptive control strengths for precise domain alignment. Across multiple model scales and domains, ASA achieves LoRA-comparable adaptation with substantially lower overhead and strong cross-model transferability, making it ideally practical for robust, scalable, and efficient multi-domain tool ecosystems with frequent interface churn dynamics.

</details>


### [160] [Large Language Models in Software Documentation and Modeling: A Literature Review and Findings](https://arxiv.org/abs/2602.04938)
*Lukas Radosky,Ivan Polasek*

Main category: cs.SE

TL;DR: 对大型语言模型在软件工程文档与建模任务应用现状的文献综述


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能特别是大型语言模型在软件工程领域应用日益广泛，其在处理软件文档和模型方面具有独特优势，需要系统梳理相关研究现状

Method: 对四个主要学术会议/期刊的文献进行系统综述，按任务类型组织分析，涵盖提示技术、评估指标、人工评估方法和主要数据集

Result: 提供了大型语言模型在软件工程文档与建模任务应用的综合分析框架，系统整理了现有研究方法、评估体系和数据集

Conclusion: 大型语言模型在软件工程文档与建模领域具有重要应用价值，需要进一步系统研究其方法、评估和数据集

Abstract: Generative artificial intelligence attracts significant attention, especially with the introduction of large language models. Its capabilities are being exploited to solve various software engineering tasks. Thanks to their ability to understand natural language and generate natural language responses, large language models are great for processing various software documentation artifacts. At the same time, large language models excel at understanding structured languages, having the potential for working with software programs and models. We conduct a literature review on the usage of large language models for software engineering tasks related to documentation and modeling. We analyze articles from four major venues in the area, organize them per tasks they solve, and provide an overview of used prompt techniques, metrics, approaches to human-based evaluation, and major datasets.

</details>


### [161] [Quality Model for Machine Learning Components](https://arxiv.org/abs/2602.05043)
*Grace A. Lewis,Rachel Brower-Sinning,Robert Edman,Ipek Ozkaya,Sebastián Echeverría,Alex Derr,Collin Beaudoin,Katherine R. Maffey*

Main category: cs.SE

TL;DR: 提出用于机器学习组件的质量模型，作为需求获取和测试的指导框架，解决ML组件与系统集成时的测试不足问题


<details>
  <summary>Details</summary>
Motivation: 当前机器学习测试主要关注模型性能等属性，而忽略了系统级需求（如吞吐量、资源消耗、鲁棒性），导致模型集成、部署和运维失败。虽然ISO 25010和ISO 25059提供了质量框架，但后者将系统属性与ML组件属性混合，对组件开发者不实用。

Method: 开发了一个专门针对ML组件的质量模型，作为需求获取和协商的指南，为ML组件开发者和系统利益相关者提供共同词汇。通过调查验证模型的相关性和价值，并将其集成到开源的ML组件测试和评估工具中。

Result: 参与者通过调查认可了该质量模型的相关性和价值。该模型已成功集成到开源工具中，展示了其实际应用价值。

Conclusion: 提出的ML组件质量模型为ML组件开发者和系统利益相关者提供了有效的沟通框架，能够指导系统级需求的获取和测试，解决了传统ML测试的局限性。

Abstract: Despite increased adoption and advances in machine learning (ML), there are studies showing that many ML prototypes do not reach the production stage and that testing is still largely limited to testing model properties, such as model performance, without considering requirements derived from the system it will be a part of, such as throughput, resource consumption, or robustness. This limited view of testing leads to failures in model integration, deployment, and operations. In traditional software development, quality models such as ISO 25010 provide a widely used structured framework to assess software quality, define quality requirements, and provide a common language for communication with stakeholders. A newer standard, ISO 25059, defines a more specific quality model for AI systems. However, a problem with this standard is that it combines system attributes with ML component attributes, which is not helpful for a model developer, as many system attributes cannot be assessed at the component level. In this paper, we present a quality model for ML components that serves as a guide for requirements elicitation and negotiation and provides a common vocabulary for ML component developers and system stakeholders to agree on and define system-derived requirements and focus their testing efforts accordingly. The quality model was validated through a survey in which the participants agreed with its relevance and value. The quality model has been successfully integrated into an open-source tool for ML component testing and evaluation demonstrating its practical application.

</details>


### [162] [Emergence-as-Code for Self-Governing Reliable Systems](https://arxiv.org/abs/2602.05458)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: 提出Emergence-as-Code (EmaC)框架，将微服务旅程可靠性从隐性属性变为可计算、可治理的代码化规范，解决SLO-as-code无法处理跨服务旅程可靠性的问题。


<details>
  <summary>Details</summary>
Motivation: SLO-as-code使单个服务可靠性可声明化，但用户体验由跨多个微服务的旅程定义，其可靠性是微服务拓扑、路由、冗余、超时/回退、共享故障域和尾部放大等复杂因素的综合涌现属性。当前旅程目标（如"结账p99 < 400 ms"）通常在代码外维护，随系统演化而漂移，导致团队要么无法满足用户期望，要么过度配置并使用临时启发式方法控制发布。

Method: 提出EmaC框架：1) EmaC规范声明旅程意图（目标、控制流操作符、允许操作）并绑定到原子SLO和遥测数据；2) 运行时推理组件消费操作工件（如追踪和流量配置）合成具有来源和置信度的候选旅程模型；3) 基于最后接受的模型，EmaC编译器/控制器在明确的相关性假设下（乐观独立vs悲观共享命运）推导有界的旅程SLO和预算；4) 生成控制平面工件（燃烧率警报、发布门控、操作防护），可在Git工作流中审查。

Result: 提供了匿名化工件仓库作为可运行的示例规范和生成输出，展示了EmaC框架的实际应用。该框架使旅程可靠性从隐性属性变为可计算、可治理的代码化规范。

Conclusion: EmaC愿景通过"意图+证据"使旅程可靠性可计算和可治理，解决了SLO-as-code在处理跨服务旅程可靠性时的局限性，为微服务架构中的用户体验可靠性提供了系统化的工程方法。

Abstract: SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., "checkout p99 < 400 ms") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.

</details>


### [163] [TestMigrationsInPy: A Dataset of Test Migrations from Unittest to Pytest](https://arxiv.org/abs/2602.05122)
*Altino Alves,Andre Hora*

Main category: cs.SE

TL;DR: TestMigrationsInPy是一个包含923个真实世界unittest到pytest迁移案例的数据集，用于支持Python测试框架迁移的自动化研究。


<details>
  <summary>Details</summary>
Motivation: Python项目中从unittest迁移到pytest虽然有益但耗时，需要自动化工具支持。目前缺乏真实迁移案例的数据集来作为基准测试和验证的基础。

Method: 收集了923个开发者实际执行的unittest到pytest迁移案例，构建TestMigrationsInPy数据集。数据集包含迁移类型信息（如断言变更、fixture变更等），为不同复杂度的迁移提供验证基础。

Result: 创建了包含923个真实迁移案例的公开数据集，涵盖从简单断言迁移到复杂fixture迁移等多种类型。数据集已在GitHub上开源，可作为未来研究的地面真实数据。

Conclusion: TestMigrationsInPy为Python测试框架迁移研究提供了首个真实世界数据集，支持从简单到复杂迁移类型的自动化工具开发和验证，促进该领域研究进展。

Abstract: Unittest and pytest are the most popular testing frameworks in Python. Overall, pytest provides some advantages, including simpler assertion, reuse of fixtures, and interoperability. Due to such benefits, multiple projects in the Python ecosystem have migrated from unittest to pytest. To facilitate the migration, pytest can also run unittest tests, thus, the migration can happen gradually over time. However, the migration can be time-consuming and take a long time to conclude. In this context, projects would benefit from automated solutions to support the migration process. In this paper, we propose TestMigrationsInPy, a dataset of test migrations from unittest to pytest. TestMigrationsInPy contains 923 real-world migrations performed by developers. Future research proposing novel solutions to migrate frameworks in Python can rely on TestMigrationsInPy as a ground truth. Moreover, as TestMigrationsInPy includes information about the migration type (e.g., changes in assertions or fixtures), our dataset enables novel solutions to be verified effectively, for instance, from simpler assertion migrations to more complex fixture migrations. TestMigrationsInPy is publicly available at: https://github.com/altinoalvesjunior/TestMigrationsInPy.

</details>


### [164] [Exceptional Behaviors: How Frequently Are They Tested?](https://arxiv.org/abs/2602.05123)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: 实证研究探索Python系统中异常行为的测试覆盖情况，发现21.4%的执行方法会在运行时抛出异常，其中约20%的方法异常频率较高，挑战了异常"罕见"的传统假设。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注传播到测试的异常，但忽略了未到达测试的异常。需要全面了解真实系统中异常行为的测试频率，以改进测试质量和异常处理实践。

Method: 对25个Python系统的测试套件进行仪器化运行，监控执行过程，收集运行时异常信息。覆盖5,372个执行方法、1,790万次调用和140万次抛出的异常。

Result: 21.4%的执行方法在运行时抛出异常；在抛出异常的方法中，中位数显示每10次调用就有1次触发异常行为；约80%的异常抛出方法频率较低，但约20%的方法异常频率较高。

Conclusion: 异常抛出行为不一定是"异常"或罕见的，需要开发新工具来支持异常行为的测试和昂贵的try/except块重构，为研究者和实践者提供重要启示。

Abstract: Exceptions allow developers to handle error cases expected to occur infrequently. Ideally, good test suites should test both normal and exceptional behaviors to catch more bugs and avoid regressions. While current research analyzes exceptions that propagate to tests, it does not explore other exceptions that do not reach the tests. In this paper, we provide an empirical study to explore how frequently exceptional behaviors are tested in real-world systems. We consider both exceptions that propagate to tests and the ones that do not reach the tests. For this purpose, we run an instrumented version of test suites, monitor their execution, and collect information about the exceptions raised at runtime. We analyze the test suites of 25 Python systems, covering 5,372 executed methods, 17.9M calls, and 1.4M raised exceptions. We find that 21.4% of the executed methods do raise exceptions at runtime. In methods that raise exceptions, on the median, 1 in 10 calls exercise exceptional behaviors. Close to 80% of the methods that raise exceptions do so infrequently, but about 20% raise exceptions more frequently. Finally, we provide implications for researchers and practitioners. We suggest developing novel tools to support exercising exceptional behaviors and refactoring expensive try/except blocks. We also call attention to the fact that exception-raising behaviors are not necessarily "abnormal" or rare.

</details>


### [165] [PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models](https://arxiv.org/abs/2602.05270)
*Thanh Le-Cong,Bach Le,Toby Murray,Michael Pradel,Cristian Cadar*

Main category: cs.SE

TL;DR: PatchGuru：首个从真实PR中自动推断可执行补丁规范的技术，利用LLM从自然语言提取开发者意图，合成补丁预言（运行时断言），通过行为比较迭代精炼，自动验证补丁语义。


<details>
  <summary>Details</summary>
Motivation: 软件演化中补丁可能无意改变程序行为，验证补丁语义困难，因为回归测试不完整且补丁意图描述是非正式的自然语言，缺乏可执行规范。

Method: 给定PR，使用LLM从NL工件提取开发者意图，合成补丁预言（比较程序中集成前后版本的运行时断言），迭代精炼：比较前后版本行为，识别违反，通过自审过滤不一致性，生成错误报告。

Result: 在4个广泛使用的Python项目的400个PR上评估，报告39个警告，精度0.62，确认24个真阳性（含12个先前未知bug，其中11个被开发者修复）。相比SOTA技术Testora，多检测17个bug（24 vs 7），精度从0.32提升至0.62。平均每个PR耗时8.9分钟，成本0.07美元。

Conclusion: PatchGuru通过提供可执行文档和补丁意图的自动验证，补充了代码审查和回归测试，展示了从真实PR中推断可执行补丁规范的可行性。

Abstract: As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.

</details>


### [166] [Does Programming Language Matter? An Empirical Study of Fuzzing Bug Detection](https://arxiv.org/abs/2602.05312)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Hajimu Iida*

Main category: cs.SE

TL;DR: 该研究通过大规模跨语言分析发现，不同编程语言在模糊测试的漏洞检测频率、漏洞严重性、崩溃类型和修复效率等方面存在显著差异，表明模糊测试效果受语言设计影响很大。


<details>
  <summary>Details</summary>
Motivation: 尽管模糊测试已广泛应用于持续集成流程，但现有研究尚未系统探究模糊测试效果是否因编程语言而异。本研究旨在填补这一空白，分析不同语言下模糊测试漏洞特征和检测效率的差异。

Method: 研究对61,444个模糊测试漏洞和999,248次构建进行分析，涵盖559个OSS-Fuzz项目，按主要编程语言（C++、Rust、Python、Go等）进行分类，进行大规模跨语言比较分析。

Result: 主要发现：(1) C++和Rust的模糊测试漏洞检测频率更高；(2) Rust和Python的漏洞比例较低但倾向于暴露更严重的漏洞；(3) 崩溃类型因语言而异，Go中不可复现漏洞更常见而Rust中罕见；(4) Python的补丁覆盖率更高但检测时间更长。

Conclusion: 模糊测试行为和效果受语言设计影响显著，这一发现为开发语言感知的模糊测试策略和工具提供了重要见解，有助于针对不同语言特性优化模糊测试方法。

Abstract: Fuzzing has become a popular technique for automatically detecting vulnerabilities and bugs by generating unexpected inputs. In recent years, the fuzzing process has been integrated into continuous integration workflows (i.e., continuous fuzzing), enabling short and frequent testing cycles. Despite its widespread adoption, prior research has not examined whether the effectiveness of continuous fuzzing varies across programming languages. This study conducts a large-scale cross-language analysis to examine how fuzzing bug characteristics and detection efficiency differ among languages. We analyze 61,444 fuzzing bugs and 999,248 builds from 559 OSS-Fuzz projects categorized by primary language. Our findings reveal that (i) C++ and Rust exhibit higher fuzzing bug detection frequencies, (ii) Rust and Python show low vulnerability ratios but tend to expose more critical vulnerabilities, (iii) crash types vary across languages and unreproducible bugs are more frequent in Go but rare in Rust, and (iv) Python attains higher patch coverage but suffers from longer time-to-detection. These results demonstrate that fuzzing behavior and effectiveness are strongly shaped by language design, providing insights for language-aware fuzzing strategies and tool development.

</details>


### [167] [Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study](https://arxiv.org/abs/2602.05465)
*Alexander Berndt,Vekil Bekmyradov,Rainer Gemulla,Marcus Kessel,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: LLMs在仅基于测试代码的flaky测试分类任务上表现不佳，结果仅略优于随机猜测，表明测试代码本身可能不包含足够的flakiness分类信息。


<details>
  <summary>Details</summary>
Motivation: 先前基于测试代码标识符的机器学习方法在flaky测试分类中缺乏泛化能力，阻碍了实际应用。预训练大语言模型（LLMs）在跨任务泛化方面表现出潜力，有望解决先前方法的泛化问题。

Method: 评估了三个LLM（两个通用模型，一个代码专用模型），使用三种提示技术，在两个flaky测试分类基准数据集上进行测试。同时手动调查了50个样本，以确定仅基于测试代码进行flaky测试分类对人类是否可行。

Result: LLMs在仅基于测试代码的flaky测试分类任务上表现不佳，最佳提示-模型组合的结果仅略优于随机猜测。手动分析发现测试代码不一定包含足够的flakiness分类信息。

Conclusion: 仅基于测试代码的flaky测试分类具有挑战性，测试代码本身可能不包含足够信息。未来工作应评估LLMs在额外上下文（如检索增强生成或智能体AI）下的flakiness分类能力。

Abstract: Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.

</details>


### [168] [Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems](https://arxiv.org/abs/2602.05486)
*Matteo Esposito,Lodovica Marchesi,Roberto Tonelli,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 论文提出将数字主权作为软件架构的一级属性，而非单纯的监管目标，并引入了一个主权参考架构，整合了自主权身份、区块链信任、主权数据治理和受控的生成式AI。


<details>
  <summary>Details</summary>
Motivation: 数字主权已成为现代软件密集型系统的核心关切，主要驱动因素包括：非主权云基础设施的主导地位、生成式AI的快速采用以及日益严格的监管要求。现有倡议孤立地处理治理、合规性和安全性，缺乏在架构层面如何实施主权的指导。

Method: 提出将主权作为一级架构属性，引入主权参考架构，整合四个关键组件：自主权身份、基于区块链的信任与可审计性、主权数据治理、以及在明确架构控制下部署的生成式AI。该架构将生成式AI视为既是治理风险来源，也是合规性、问责制和持续保障的使能器。

Result: 通过将主权构建为架构质量属性，该工作弥合了监管意图与具体系统设计之间的差距，为构建可审计、可演进和具备司法管辖意识的AI赋能系统提供了连贯基础。

Conclusion: 提出的参考架构为软件架构、生成式AI和数字主权交叉领域的未来研究和实践提供了原则性起点，强调主权应作为核心架构考量而非事后附加的合规要求。

Abstract: Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.

</details>


### [169] [ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval](https://arxiv.org/abs/2602.05550)
*Yulong He,Artem Ermakov,Sergey Kovalchuk,Artem Aliev,Dmitry Shalymov*

Main category: cs.SE

TL;DR: 构建首个大规模ArkTS代码数据集，用于代码检索和评估任务，并评估现有代码嵌入模型，通过微调获得高性能ArkTS代码理解模型


<details>
  <summary>Details</summary>
Motivation: OpenHarmony生态中ArkTS作为核心编程语言，其代码智能研究因缺乏公开数据集和评估基准而受限

Method: 从GitHub和Gitee爬取ArkTS仓库，使用tree-sitter-arkts提取注释-函数对，进行跨平台去重和函数类型统计分析；设计单搜索任务（自然语言注释检索对应ArkTS函数），评估现有开源代码嵌入模型，使用ArkTS和TypeScript训练数据集进行微调

Result: 建立了首个ArkTS代码检索系统基准，发布了大规模ArkTS数据集和微调后的高性能ArkTS代码理解模型

Conclusion: 该工作填补了ArkTS代码智能研究的空白，为ArkTS代码检索建立了首个系统基准，数据集和模型已公开提供

Abstract: ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.

</details>


### [170] [Towards Green AI: Decoding the Energy of LLM Inference in Software Development](https://arxiv.org/abs/2602.05712)
*Lola Solovyeva,Fernando Castor*

Main category: cs.SE

TL;DR: 该研究分析了LLM推理的能耗模式，发现预填充阶段成本会影响解码阶段能耗，并识别了模型"胡言乱语"行为，通过抑制该行为可实现44%-89%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 随着AI辅助工具在软件开发中的广泛应用，基于大语言模型的工具带来了显著的计算和能源成本。理解并降低LLM推理的能源足迹对于可持续软件开发至关重要。

Method: 研究对LLM推理进行阶段级能耗分析，区分预填充阶段（处理输入并构建内部表示）和解码阶段（使用存储状态生成输出标记）。评估了六个6B-7B参数模型和四个3B-4B参数模型，在代码生成基准HumanEval和代码理解基准LongBench上进行测试。

Result: 研究发现：1) 在两个参数组中，模型在不同阶段表现出不同的能耗模式；2) 预填充成本的增加会放大解码阶段的每标记能耗，放大范围从1.3%到51.8%；3) 十个模型中有三个表现出"胡言乱语"行为，向输出添加不必要内容从而增加能耗；4) 通过抑制代码生成的胡言乱语行为，可在不影响生成准确性的情况下实现44%到89%的能耗节省。

Conclusion: 预填充成本会影响解码阶段的能耗，而解码阶段主导整体能耗。抑制胡言乱语行为可节省高达89%的能源。降低推理能耗需要同时缓解胡言乱语行为和限制预填充对解码的影响。

Abstract: Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.

</details>


### [171] [A Bayesian Optimization-Based AutoML Framework for Non-Intrusive Load Monitoring](https://arxiv.org/abs/2602.05739)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 提出AutoML4NILM框架，将自动机器学习引入非侵入式负荷监测领域，通过贝叶斯优化实现自动模型选择和超参数调优，降低领域专家应用机器学习的门槛。


<details>
  <summary>Details</summary>
Motivation: 非侵入式负荷监测（NILM）通过分析家庭总用电量来估算单个电器功耗，比安装专用智能电表更经济。但传统方法需要机器学习专业知识，限制了领域专家的应用。本文旨在降低NILM领域应用机器学习的门槛。

Method: 提出将自动机器学习（AutoML）引入NILM领域的新框架，使用贝叶斯优化进行自动模型选择和超参数调优。开发了AutoML4NILM开源工具包，支持11种算法及其不同超参数，具有灵活可扩展的设计。

Result: 开发了AutoML4NILM工具包，使领域从业者无需数据科学或机器学习高级专业知识即可有效应用机器学习技术。该框架支持11种算法，设计灵活可扩展，便于进一步研究和工业应用。

Conclusion: AutoML4NILM框架成功将AutoML引入NILM领域，通过贝叶斯优化实现自动化模型选择，降低了应用门槛。开源工具包的灵活设计支持算法和超参数扩展，促进了NILM研究和工业应用。

Abstract: Non-Intrusive Load Monitoring (NILM), commonly known as energy disaggregation, aims to estimate the power consumption of individual appliances by analyzing a home's total electricity usage. This method provides a cost-effective alternative to installing dedicated smart meters for each appliance. In this paper, we introduce a novel framework that incorporates Automated Machine Learning (AutoML) into the NILM domain, utilizing Bayesian Optimization for automated model selection and hyperparameter tuning. This framework empowers domain practitioners to effectively apply machine learning techniques without requiring advanced expertise in data science or machine learning. To support further research and industry adoption, we present AutoML4NILM, a flexible and extensible open-source toolkit designed to streamline the deployment of AutoML solutions for energy disaggregation. Currently, this framework supports 11 algorithms, each with different hyperparameters; however, its flexible design allows for the extension of both the algorithms and their hyperparameters.

</details>


### [172] [Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes](https://arxiv.org/abs/2602.05780)
*Ulrich Finkler,Irene Manotas,Wei Zhang,Geert Janssen,Octavian Popescu,Shyam Ramji*

Main category: cs.SE

TL;DR: 本文提出基于语义范围的自动化LLM定制方法，通过RAG和微调策略提升代码补全在私有代码库上的性能，使中等规模定制模型优于更大规模未定制模型。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在公开基准测试中表现良好，但在处理未见过私有代码库时仍难以生成符合特定代码库模式的代码。定制化代码LLM可以提升模型在私有代码库上的性能，从而提高开发者的生产力。

Method: 提出基于代码语义范围的自动化LLM定制方法，包括两种策略：检索增强生成（RAG）和监督微调（FT）。通过语义范围机制处理代码库数据并构建训练数据对，帮助模型学习特定代码库的底层模式。

Result: 在两个私有企业代码库上进行评估，中等规模的定制模型在代码补全方面的表现显著优于更大容量的未定制模型。定制化方法能为开发者提供更精确的代码，提升生产力。同时分析了在两个公开基准测试上的定制效果。

Conclusion: 基于语义范围的LLM定制方法能有效提升代码补全在私有代码库上的性能，中等规模定制模型可超越更大规模未定制模型。该方法为未来工作提供了机会，特别是在企业私有代码库的LLM应用方面。

Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.

</details>


### [173] [When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models](https://arxiv.org/abs/2602.05891)
*Shenyu Zheng,Ximing Dong,Xiaoshuang Liu,Gustavo Oliva,Chong Chun Yong,Dayi Lin,Boyuan Chen,Shaowei Wang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Codeforces Elo评分作为LLM编程能力评估指标存在严重偏差，研究发现提交顺序、比赛难度选择和模型随机性可导致评分波动达394-1122点，直接比较不可靠。


<details>
  <summary>Details</summary>
Motivation: 当前使用Codeforces Elo评分评估大语言模型编程能力时，缺乏关键实验细节报告，导致同一模型版本评分波动近500点，需要系统研究影响Elo评估的隐藏因素。

Method: 使用37个近期Codeforces比赛和13,691个生成测试用例构建受控基准，系统实证研究三个偏差因素：提交时间顺序、比赛难度选择、LLM运行的随机变异性。

Result: Elo评分对这些参数高度敏感：不同提交顺序可导致评分偏移394点；比赛选择差异可使同一模型评分差异达1122点；运行间性能不稳定，相同比赛评估中平均评分最大差异达349点。

Conclusion: 直接比较Elo评分不可靠且可能误导，需要严格的标准化和透明的实验设置报告。当前基于Codeforces的Elo评估方法存在严重偏差问题。

Abstract: As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [174] [Optimization is Not Enough: Why Problem Formulation Deserves Equal Attention](https://arxiv.org/abs/2602.05466)
*Iván Olarte Rodríguez,Gokhan Serhat,Mariusz Bujny,Fabian Duddeck,Thomas Bäck,Elena Raponi*

Main category: cs.NE

TL;DR: 该研究通过层合复合材料拓扑优化的案例研究，探讨了问题表述和领域知识对黑盒优化结果的影响，发现基于物理洞察的序列优化策略优于上下文无关的并发优化方法。


<details>
  <summary>Details</summary>
Motivation: 当前黑盒优化研究大多关注上下文无关的设置，而忽略了问题表述和领域知识对优化结果的影响。本研究旨在填补这一空白，通过复合材料结构拓扑优化的具体案例，分析物理洞察如何影响优化效果。

Method: 采用悬臂梁在体积约束下的拓扑优化作为案例研究，将问题表述为黑盒优化问题。明确分离拓扑和材料设计变量，比较两种策略：1）并发方法：同时优化所有变量，不利用物理洞察；2）序列方法：按变量性质分阶段优化。

Result: 上下文无关的并发策略始终导致次优或非物理设计，而序列策略产生性能更好、更可解释的解决方案。基于物理洞察的优化方法显著优于上下文无关的方法。

Conclusion: 领域知识对黑盒优化至关重要，应尽可能将其纳入优化过程。研究结果呼吁开发新的黑盒基准测试，以奖励基于物理洞察和上下文感知的优化策略。

Abstract: Black-box optimization is increasingly used in engineering design problems where simulation-based evaluations are costly and gradients are unavailable. In this context, the optimization community has largely analyzed algorithm performance in context-free setups, while not enough attention has been devoted to how problem formulation and domain knowledge may affect the optimization outcomes. We address this gap through a case study in the topology optimization of laminated composite structures, formulated as a black-box optimization problem. Specifically, we consider the design of a cantilever beam under a volume constraint, intending to minimize compliance while optimizing both the structural topology and fiber orientations. To assess the impact of problem formulation, we explicitly separate topology and material design variables and compare two strategies: a concurrent approach that optimizes all variables simultaneously without leveraging physical insight, and a sequential approach that optimizes variables of the same nature in stages. Our results show that context-agnostic strategies consistently lead to suboptimal or non-physical designs. In contrast, the sequential strategy yields better-performing and more interpretable solutions. These findings underscore the value of incorporating, when available, domain knowledge into the optimization process and motivate the development of new black-box benchmarks that reward physically informed and context-aware optimization strategies.

</details>


### [175] [Variable Search Stepsize for Randomized Local Search in Multi-Objective Combinatorial Optimization](https://arxiv.org/abs/2602.05675)
*Xuepeng Ren,Maocai Wang,Guangming Dai,Zimin Liang,Qianrong Liu,Shengxiang Yang,Miqing Li*

Main category: cs.NE

TL;DR: 提出可变步长随机局部搜索(VS-RLS)方法解决多目标组合优化问题，通过动态调整搜索步长实现从粗粒度探索到细粒度开发的过渡，在多种MOCOPs上优于传统局部搜索和多目标进化算法。


<details>
  <summary>Details</summary>
Motivation: 多目标进化优化研究主要关注连续域，对组合优化问题(MOCOPs)关注较少。组合问题与连续问题在结构和景观上差异显著，现有随机局部搜索方法使用固定邻域，限制了探索能力且容易陷入局部最优。

Method: 提出可变步长随机局部搜索(VS-RLS)方法，从随机解开始，从档案中随机选择解进行局部变异，关键创新在于动态调整搜索步长：早期使用大步长进行广泛探索，后期逐步减小步长进行精细开发。

Result: 通过多种MOCOPs上的广泛评估，VS-RLS在性能上优于传统局部搜索方法和多目标进化算法(MOEAs)，展示了方法的有效性和泛化能力。

Conclusion: VS-RLS是一种简单而有效的局部搜索方法，通过动态调整搜索步长策略，在多目标组合优化问题上取得了显著性能提升，为MOCOPs研究提供了新思路。

Abstract: Over the past two decades, research in evolutionary multi-objective optimization has predominantly focused on continuous domains, with comparatively limited attention given to multi-objective combinatorial optimization problems (MOCOPs). Combinatorial problems differ significantly from continuous ones in terms of problem structure and landscape. Recent studies have shown that on MOCOPs multi-objective evolutionary algorithms (MOEAs) can even be outperformed by simple randomised local search. Starting with a randomly sampled solution in search space, randomised local search iteratively draws a random solution (from an archive) to perform local variation within its neighbourhood. However, in most existing methods, the local variation relies on a fixed neighbourhood, which limits exploration and makes the search easy to get trapped in local optima. In this paper, we present a simple yet effective local search method, called variable stepsize randomized local search (VS-RLS), which adjusts the stepsize during the search. VS-RLS transitions gradually from a broad, exploratory search in the early phases to a more focused, fine-grained search as the search progresses. We demonstrate the effectiveness and generalizability of VS-RLS through extensive evaluations against local search and MOEAs methods on diverse MOCOPs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [176] [Wi-Fi Radar via Over-the-Air Referencing: Bridging Wi-Fi Sensing and Bistatic Radar](https://arxiv.org/abs/2602.05344)
*Koji Yamamoto*

Main category: cs.NI

TL;DR: 提出LoSRef方案，利用Wi-Fi中的视距路径作为空中参考，在非同步Wi-Fi系统中实现延迟校准和相位对齐，使商用Wi-Fi设备能够进行相位相干的雷达式延迟-多普勒分析。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi传感在人体感知应用中受到广泛关注，但非同步的发射机和接收机从根本上阻碍了相位相干的雷达式延迟-多普勒分析。现有Wi-Fi双基地雷达系统依赖有线参考信号或专用参考天线，限制了实际部署。

Method: 提出LoSRef（视距路径参考）方案，利用最早到达的视距路径作为空中参考，进行延迟校准和相位对齐。该方案在非同步Wi-Fi系统中实现相位相干的双基地雷达式操作，无需有线参考或专用天线。

Result: 通过室内人体步态和呼吸实验证明，仅使用商用Wi-Fi设备即可获得相位相干的信道脉冲响应和相应的延迟-多普勒响应。能够实现物理可解释的人体运动感知，包括步态引起的距离变化和呼吸引起的亚波长位移，并能提取比主导静态多径分量弱20dB的目标诱导动态。

Conclusion: LoSRef方案弥合了传统Wi-Fi传感与Wi-Fi雷达之间的差距，使商用Wi-Fi设备能够在无需同步的情况下实现相位相干的雷达式操作，为Wi-Fi传感应用开辟了新途径。

Abstract: Wi-Fi sensing has attracted significant attention for human sensing and related applications. However, unsynchronized transmitters and receivers fundamentally preclude phase-coherent radar-like delay--Doppler analysis. By exploiting the line-of-sight (LoS) path, i.e., the earliest-arriving direct path, as an over-the-air (OTA) reference for delay and phase, we propose an OTA LoS-path referencing scheme, termed LoSRef, that enables delay calibration and phase alignment in unsynchronized Wi-Fi systems. Unlike conventional Wi-Fi bistatic radar systems that rely on wired reference signals or dedicated reference antennas, the proposed LoSRef-based framework bridges the long-standing gap between conventional Wi-Fi sensing and Wi-Fi radar, enabling phase-coherent bistatic radar-like operation in a drop-in Wi-Fi sensing configuration. Through human gait and respiration experiments in indoor environments, we demonstrate that phase-coherent channel impulse responses and corresponding delay--Doppler responses are obtained using only commodity Wi-Fi devices. This enables physically interpretable human motion sensing, including gait-induced range variation and respiration-induced sub-wavelength displacement, as well as the extraction of target-induced dynamics up to 20 dB weaker than dominant static multipath components.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [177] [AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval](https://arxiv.org/abs/2602.05524)
*Konosuke Yoshizato,Kazuma Shimizu,Ryota Higa,Takanobu Otsuka*

Main category: cs.MA

TL;DR: 该研究探索了基于大语言模型的多智能体系统在库存管理中的应用，提出了一种名为AIM-RM的新型智能体，通过相似性匹配利用历史经验，在各种供应链场景中表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 库存管理是供应链管理的关键组成部分，基于大语言模型的多智能体系统虽然受到关注，但其有效性和适应性仍存在不确定性。研究旨在探索这些系统是否能持续推导最优订购策略并适应多样化的供应链场景。

Method: 研究采用基于大语言模型的多智能体系统，包含固定订购策略提示（编码问题设置的逐步过程和库存管理中常用的安全库存策略）。为增强适应性，提出了新型智能体AIM-RM，该智能体通过相似性匹配利用相似的历史经验。

Result: 实证结果表明，即使没有详细的提示调整，基于大语言模型的多智能体系统也能在受限场景中确定最优订购决策。AIM-RM在各种供应链场景中均优于基准方法，显示出其鲁棒性和适应性。

Conclusion: 基于大语言模型的多智能体系统在库存管理中具有潜力，特别是提出的AIM-RM智能体通过利用历史经验显著提升了系统在不同供应链场景中的适应性和性能表现。

Abstract: This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.

</details>


### [178] [Learning to Share: Selective Memory for Efficient Parallel Agentic Systems](https://arxiv.org/abs/2602.05965)
*Joseph Fioresi,Parth Parag Kulkarni,Ashmal Vayani,Song Wang,Mubarak Shah*

Main category: cs.MA

TL;DR: LTS提出了一种学习型共享内存机制，用于并行智能体框架，通过选择性跨团队信息复用减少重复计算，提高系统效率


<details>
  <summary>Details</summary>
Motivation: 现有并行智能体系统在执行相似子问题或步骤时存在大量重复计算，导致显著的计算成本浪费，需要一种机制来促进跨团队信息复用

Method: 提出LTS（Learning to Share）框架，包含全局内存库和轻量级控制器，控制器通过基于使用感知信用分配的逐步强化学习训练，决定哪些中间步骤应存入内存供其他团队复用

Result: 在AssistantBench和GAIA基准测试中，LTS显著减少了总体运行时间，同时保持或提高了任务性能，相比无内存并行基线表现出更好的效率

Conclusion: 学习型内存准入是提高并行智能体系统效率的有效策略，LTS通过选择性信息复用成功减少了重复计算，为智能体系统优化提供了新方向

Abstract: Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/

</details>


### [179] [PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling](https://arxiv.org/abs/2602.06030)
*Kavana Venkatesh,Yinhan He,Jundong Li,Jiaming Cui*

Main category: cs.MA

TL;DR: PhysicsAgentABM结合神经符号方法，通过集群级推理和不确定性感知融合，实现可扩展且校准良好的多智能体模拟，减少LLM调用6-8倍


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统扩展成本高且时间步对齐状态转移校准差，传统ABM难以整合个体级丰富信号和非平稳行为，需要新架构平衡表达力与可扩展性

Method: 1) 基于行为一致性的智能体集群推理；2) 状态专用符号智能体编码机制转移先验；3) 多模态神经转移模型捕捉时空交互动态；4) 不确定性感知认知融合产生校准的集群级转移分布；5) ANCHOR聚类策略基于跨上下文行为响应和对比损失减少LLM调用

Result: 在公共卫生、金融和社会科学实验中，相比机制、神经和LLM基线，在事件时间准确性和校准方面获得一致提升，LLM调用减少6-8倍

Conclusion: 通过围绕群体级推理重构生成式ABM，结合不确定性感知神经符号融合，PhysicsAgentABM建立了LLM可扩展校准模拟的新范式

Abstract: Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [180] [Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization](https://arxiv.org/abs/2602.04900)
*Sai Sindhur Malleni,Raúl Sevilla,Aleksei Vasilevskii,José Castillo Lema,André Bauer*

Main category: cs.ET

TL;DR: Kubernetes生态系统通过Kueue、DAS和GAIE等原生项目为生成式AI推理工作负载提供统一平台，在批处理和在线推理场景中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（特别是推理）成为主要工作负载类别，Kubernetes生态系统需要原生支持其独特需求，提供容器编排的可扩展性和资源效率优势

Method: 实现多阶段用例（自动语音识别和摘要），使用Kueue管理批处理推理作业，DAS增加并行作业执行，以及llm-d利用Kubernetes Gateway API Inference Extension（GAIE）优化在线推理请求路由

Result: Kueue减少总完成时间达15%；DAS缩短平均作业完成时间36%；GAIE改善首令牌时间82%，形成高性能统一平台

Conclusion: Kueue、DAS和GAIE等互补组件构成统一高性能平台，证明Kubernetes能够作为生成式AI工作负载的统一基础

Abstract: As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the benefits of container orchestration, such as scalability and resource efficiency, to complex AI workflows. We implement and evaluate an illustrative, multi-stage use case consisting of automatic speech recognition and summarization. First, we address batch inference by using Kueue to manage jobs that transcribe audio files with Whisper models and Dynamic Accelerator Slicer (DAS) to increase parallel job execution. Second, we address a discrete online inference scenario by feeding the transcripts to a Large Language Model for summarization hosted using llm-d, a novel solution utilizing the recent developments around the Kubernetes Gateway API Inference Extension (GAIE) for optimized routing of inference requests. Our findings illustrate that these complementary components (Kueue, DAS, and GAIE) form a cohesive, high-performance platform, proving Kubernetes' capability to serve as a unified foundation for demanding GenAI workloads: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36%; and GAIE improved Time to First Token by 82\%.

</details>


### [181] [Task-Adaptive Physical Reservoir Computing via Tunable Molecular Communication Dynamics](https://arxiv.org/abs/2602.05931)
*Saad Yousuf,Kaan Burak Ikiz,Murat Kuscu*

Main category: cs.ET

TL;DR: 分子通信通道可作为可重构的物理储层计算系统，通过调节生物物理参数优化不同计算任务性能


<details>
  <summary>Details</summary>
Motivation: 大多数物理储层计算实现是静态的，性能局限于狭窄的任务范围，需要开发可重构、任务自适应的物理计算系统

Method: 采用双模拟方法：确定性平均场模型和高保真粒子随机模型(Smoldyn)，结合贝叶斯优化在高维参数空间中导航，识别离散操作机制

Result: 发现明确的权衡关系：富含通道记忆的参数集在混沌时间序列预测任务中表现优异，而促进强受体非线性的机制在非线性数据转换方面更优；后处理方法可减轻分子噪声提升随机储层性能

Conclusion: 分子通信通道不仅是计算基质，更是可调谐、生物启发计算系统的设计蓝图，为未来湿件AI实现提供了清晰的优化框架

Abstract: Physical Reservoir Computing (PRC) offers an efficient paradigm for processing temporal data, yet most physical implementations are static, limiting their performance to a narrow range of tasks. In this work, we demonstrate in silico that a canonical Molecular Communication (MC) channel can function as a highly versatile and task-adaptive PRC whose computational properties are reconfigurable. Using a dual-simulation approach -- a computationally efficient deterministic mean-field model and a high-fidelity particle-based stochastic model (Smoldyn) -- we show that tuning the channel's underlying biophysical parameters, such as ligand-receptor kinetics and diffusion dynamics, allows the reservoir to be optimized for distinct classes of computation. We employ Bayesian optimization to efficiently navigate this high-dimensional parameter space, identifying discrete operational regimes. Our results reveal a clear trade-off: parameter sets rich in channel memory excel at chaotic time-series forecasting tasks (e.g., Mackey Glass), while regimes that promote strong receptor nonlinearity are superior for nonlinear data transformation. We further demonstrate that post-processing methods improve the performance of the stochastic reservoir by mitigating intrinsic molecular noise. These findings establish the MC channel not merely as a computational substrate, but as a design blueprint for tunable, bioinspired computing systems, providing a clear optimization framework for future wetware AI implementations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [182] [PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation](https://arxiv.org/abs/2602.05156)
*Dong Ho Kang,Aaron Kim,Mingyo Seo,Kazuto Yokoyama,Tetsuya Narita,Luis Sentis*

Main category: cs.RO

TL;DR: PLATO Hand是一种灵巧机械手，采用混合指尖设计（刚性指甲嵌入柔性指腹），通过结构化接触几何实现多种交互模式，提升捏取稳定性、力感知能力和边缘敏感操作性能。


<details>
  <summary>Details</summary>
Motivation: 传统机械手在精确操作中面临接触行为控制困难，特别是在处理不同几何形状物体时缺乏多样化的交互模式。需要一种能够通过物理结构设计来引导接触行为，实现更稳定、更精确操作的方法。

Method: 1. 设计混合指尖：刚性指甲嵌入柔性指腹中，形成结构化接触几何
2. 开发应变能弯曲-压痕模型：指导指尖设计并解释接触引导机制
3. 实现力-运动透明机制：耦合结构化接触几何与机械结构

Result: 1. 提升捏取稳定性
2. 增强力可观测性
3. 成功执行边缘敏感操作任务：纸张分离、卡片拾取、橙子剥皮
4. 实验验证了结构化接触几何在精确操作中的有效性

Conclusion: 将结构化接触几何与力-运动透明机制相结合，为精确操作提供了一种基于物理原理的、具身化的方法。这种混合指尖设计通过引导接触行为，实现了多种交互模式，显著提升了机械手的操作性能。

Abstract: We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.

</details>


### [183] [Signal or 'Noise': Human Reactions to Robot Errors in the Wild](https://arxiv.org/abs/2602.05010)
*Maia Stiber,Sameer Khan,Russell Taylor,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 研究探索了在现实世界场景中人们对机器人错误的社交反应，通过咖啡机器人的实地部署发现参与者会表达丰富但"嘈杂"的社交信号


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器人经常出错，但人们对这些错误的社交反应在实验室外环境中了解甚少。先前研究表明社交信号在受控交互中对错误管理可靠有用，但不确定这在现实世界中是否成立，特别是在非社交机器人、重复交互、群体交互以及连续或传播错误的情况下

Method: 构建了一个咖啡机器人并在公共场所进行实地部署（N=49），观察参与者在面对错误和其他刺激时的社交信号表达

Result: 参与者对错误和其他刺激持续表达多样化的社交信号，特别是在群体交互中。现实世界中的社交信号丰富（参与者自愿提供交互信息）但"嘈杂"

Conclusion: 讨论了在现实世界人机交互中使用社交信号的教训、益处和挑战，表明社交信号在野外环境中具有价值但需要处理其噪声特性

Abstract: In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but "noisy." We discuss lessons, benefits, and challenges for using social signals in real-world HRI.

</details>


### [184] [Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping](https://arxiv.org/abs/2602.05029)
*Octavio Arriaga,Proneet Sharma,Jichen Guo,Marc Otto,Siddhant Kadwe,Rebecca Adam*

Main category: cs.RO

TL;DR: 提出一种可微分神经图形模型，结合神经基础模型与基于物理的可微分渲染，实现零样本场景重建与机器人抓取，无需额外3D数据或测试样本


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖大量训练数据和测试样本构建黑盒场景表示，需要更数据高效、可解释且泛化性强的机器人自主系统

Method: 结合神经基础模型与物理可微分渲染，通过约束优化问题估计物理一致的场景参数（网格、光照、材质、6D位姿），仅需单张RGBD图像和边界框

Result: 在标准无模型少样本基准测试中优于现有姿态估计算法，零样本抓取任务验证了场景重建的准确性

Conclusion: 该方法为新颖环境中更数据高效、可解释且泛化性强的机器人自主性提供了途径

Abstract: Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.

</details>


### [185] [Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking](https://arxiv.org/abs/2602.05079)
*Vinal Asodia,Iman Sharifi,Saber Fallah*

Main category: cs.RO

TL;DR: 提出神经符号特征表示和软一阶逻辑奖励函数，提升自动驾驶强化学习的场景理解和安全决策能力


<details>
  <summary>Details</summary>
Motivation: 现有基于摄像头的深度强化学习方法存在两个主要问题：很少将高层场景上下文整合到特征表示中，以及依赖僵化的固定奖励函数。这限制了自动驾驶系统对复杂交通环境的理解和安全决策能力。

Method: 提出神经符号特征表示管道，包含语义、空间和形状信息，以及动态实体的空间增强特征，特别关注安全关键的道路使用者。同时提出软一阶逻辑（SFOL）奖励函数，通过符号推理模块平衡人类价值观，从分割图中提取语义和空间谓词，应用于语言规则以获得奖励权重。

Result: 在CARLA仿真环境中的定量实验表明，与基线表示和奖励公式相比，所提出的神经符号表示和SFOL奖励函数在不同交通密度和遮挡水平下，提高了策略鲁棒性和安全相关性能指标。

Conclusion: 将整体表示和软推理整合到强化学习中，可以支持自动驾驶更具备上下文感知和价值对齐的决策能力。

Abstract: The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.

</details>


### [186] [A Framework for Combining Optimization-Based and Analytic Inverse Kinematics](https://arxiv.org/abs/2602.05092)
*Thomas Cohn,Lihan Tang,Alexandre Amice,Russ Tedrake*

Main category: cs.RO

TL;DR: 提出了一种新的优化逆运动学（IK）公式，使用解析IK解作为变量变换，使优化器更容易求解，在碰撞避免、抓取选择和类人机器人稳定性等挑战性IK问题上取得了更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统的解析方法和优化方法在解决逆运动学问题时各有优缺点，但开发统一方法面临挑战。优化方法面临的主要挑战是关节角度与末端执行器位姿之间的复杂非线性关系，当需要同时处理碰撞避免等非凸约束时，优化IK算法的失败率较高。

Method: 提出了一种新的优化IK公式，使用解析IK解作为变量变换。该方法将复杂的非线性关系通过解析解进行预处理，使优化问题对优化器更加友好。在三种流行的求解器上进行了测试，代表了三种不同的约束非线性优化范式。

Result: 广泛的实验比较表明，新公式在碰撞避免、抓取选择和类人机器人稳定性等各种挑战性IK问题上，比旧公式和基线方法实现了更高的成功率。

Conclusion: 通过使用解析IK解作为变量变换的新优化公式，能够有效结合解析方法和优化方法的优势，显著提高逆运动学问题求解的成功率，特别是在处理复杂约束时表现优异。

Abstract: Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability.

</details>


### [187] [CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction](https://arxiv.org/abs/2602.06038)
*Xiaopan Zhang,Zejin Wang,Zhixu Li,Jianpeng Yao,Jiachen Li*

Main category: cs.RO

TL;DR: 论文提出CommCP框架，用于解决多智能体多任务具身问答（MM-EQA）问题，通过基于LLM的去中心化通信和共形预测校准消息，提高任务成功率和探索效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器人需要协作完成自然语言指令，但现有研究缺乏对多异构机器人协作中信息收集过程的系统性处理。传统具身问答（EQA）主要关注单智能体，而多智能体协作需要有效通信以避免冗余并协调行动。

Method: 提出CommCP框架：1）将MM-EQA问题形式化为多智能体多任务协作问题；2）采用基于大语言模型（LLM）的去中心化通信架构；3）使用共形预测（conformal prediction）校准生成的消息，减少接收者干扰并提高通信可靠性。

Result: 实验结果表明，CommCP在提出的MM-EQA基准测试中显著提高了任务成功率和探索效率，优于基线方法。基准包含多样化的照片级真实家庭场景和具身问题。

Conclusion: CommCP框架有效解决了多智能体协作中的通信协调问题，通过消息校准提高了通信可靠性，为异构机器人协作完成复杂任务提供了可行方案。提出的MM-EQA基准为未来研究提供了测试平台。

Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.

</details>


### [188] [Ontology-Driven Robotic Specification Synthesis](https://arxiv.org/abs/2602.05456)
*Maksym Figat,Ryan M. Mackey,Michel D. Ingham*

Main category: cs.RO

TL;DR: 提出RSTM2方法，通过本体驱动、分层使用随机时间资源Petri网，将机器人系统高层目标转化为可执行规范，支持任务、系统、子系统级蒙特卡洛仿真，用于安全关键应用。


<details>
  <summary>Details</summary>
Motivation: 解决安全关键和任务关键机器人应用中高层目标与形式化可执行规范之间的鸿沟，支持复杂多机器人系统（如NASA CADRE任务）的架构权衡、资源分配和不确定性下的性能分析。

Method: RSTM2（机器人系统任务到模型转换方法）是一种本体驱动的分层方法，使用随机时间资源Petri网，支持任务、系统、子系统三个层次的蒙特卡洛仿真，通过本体概念支持可解释AI助手实现自主规范合成。

Result: 通过假设案例研究展示了RSTM2方法如何支持架构权衡、资源分配和不确定性下的性能分析，特别适用于复杂多机器人系统，如NASA CADRE任务所代表的去中心化、资源感知和自适应自主系统。

Conclusion: RSTM2方法为安全关键机器人系统提供了一种系统化工程方法，通过本体驱动和分层建模，实现了从高层目标到可执行规范的转换，支持不确定性下的性能分析和自主规范合成，对未来复杂多机器人系统具有重要价值。

Abstract: This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.

</details>


### [189] [Informative Path Planning with Guaranteed Estimation Uncertainty](https://arxiv.org/abs/2602.05198)
*Kalvik Jakkala,Saurav Agarwal,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: 提出一种保证估计不确定性的信息路径规划方法，在满足高斯过程后验方差阈值的前提下计算最短路径，用于环境监测机器人


<details>
  <summary>Details</summary>
Motivation: 环境监测机器人需要在距离和能量约束下重建空间场。传统割草式调查提供几何覆盖保证但会过度采样可预测区域，而信息路径规划方法利用空间相关性减少过度采样但通常不保证重建质量。需要结合两者优势，在保证估计不确定性的前提下进行信息路径规划。

Method: 采用三阶段方法：(1)从先验信息学习高斯过程模型；(2)将学习到的GP核转换为每个候选传感位置的二值覆盖图，指示哪些位置的不确定性可降低到指定目标以下；(3)规划接近最短的路径，其组合覆盖满足全局不确定性约束。针对异质现象，引入非平稳核捕捉空间变化的关联结构，并适应具有障碍物的非凸环境。

Result: 在真实地形数据上的实验表明，该方法比现有基线使用更少的传感位置和更短的旅行距离即可满足不确定性目标。通过测深测绘自主水面和水下机器人的现场实验证明了现实可行性。

Conclusion: 该方法成功地将保证估计不确定性的信息路径规划与最短路径规划相结合，为环境监测机器人提供了在满足重建质量保证的同时优化资源使用的有效解决方案，算法具有可证明的近似保证。

Abstract: Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.
  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility.

</details>


### [190] [From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking](https://arxiv.org/abs/2602.05683)
*Chuwei Wang,Eduardo Sebastián,Amanda Prorok,Anastasia Bizyaeva*

Main category: cs.RO

TL;DR: 提出一种简约的神经形态控制框架，通过动态分岔机制解决视觉引导导航中的决策对称性问题，将视觉目标激励直接转换为自我中心运动指令。


<details>
  <summary>Details</summary>
Motivation: 机器人导航长期面临反应式传感器控制与基于模型规划器之间的协调问题。当多个目标选项缺乏主导选择时，系统容易陷入决策困境，而传统规划器计算成本高昂。需要一种既能实时响应又能有效打破对称性的解决方案。

Method: 开发神经形态控制框架：1) 将机载摄像头像素编码为动态神经元群输入；2) 直接将视觉目标激励转换为自我中心运动指令；3) 引入动态分岔机制，延迟决策直到环境几何诱导的临界点；4) 受动物认知和意见动态机制模型启发，设计可解释参数；5) 可与特定应用图像处理流程无缝集成。

Result: 在仿真环境和四旋翼无人机实验平台上验证了方法的有效性。该控制器实现了实时自主性，计算负担最小，参数数量少且可解释，能够有效解决导航中的决策对称性问题。

Conclusion: 提出的神经形态控制框架成功弥合了反应式控制与模型规划之间的鸿沟，通过动态分岔机制解决了视觉引导导航中的决策困境，为机器人自主导航提供了简约、高效且可解释的解决方案。

Abstract: Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.

</details>


### [191] [MobileManiBench: Simplifying Model Verification for Mobile Manipulation](https://arxiv.org/abs/2602.05233)
*Wenbo Wang,Fangyun Wei,QiXiu Li,Xi Chen,Yaobo Liang,Chang Xu,Jiaolong Yang,Baining Guo*

Main category: cs.RO

TL;DR: MobileManiBench：基于模拟的大规模移动机器人操作基准，通过强化学习自动生成多样化轨迹，支持视觉-语言-动作模型在复杂环境中的评估


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型主要依赖静态桌面场景的遥操作数据集，限制了在移动机器人操作任务中的发展和评估。需要一种模拟优先的框架来验证VLA架构在实际部署前的性能，并建立大规模基准来研究机器人本体、感知模态和策略架构

Method: 提出MobileManiBench基准，基于NVIDIA Isaac Sim构建，通过强化学习自动生成多样化操作轨迹。包含2种移动平台（平行夹爪和灵巧手机器人）、2个同步摄像头（头部和右腕）、630个物体（20个类别）、5种技能（打开、关闭、拉动、推动、抓取），在100个真实场景中执行100多个任务，生成30万条轨迹，提供丰富的标注信息

Result: 建立了大规模移动机器人操作基准，包含300K轨迹数据，支持对代表性VLA模型的基准测试。该框架实现了对机器人感知、推理和控制能力的可控、可扩展研究，加速了数据效率和泛化能力的研究

Conclusion: MobileManiBench为移动机器人操作提供了全面的模拟基准，通过自动生成的多样化轨迹支持VLA模型的系统评估，为复杂模拟环境中的感知、推理和控制研究提供了重要工具，有助于加速机器人学习研究

Abstract: Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.

</details>


### [192] [Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot](https://arxiv.org/abs/2602.05265)
*Kalvik Jakkala,Jason O'Kane*

Main category: cs.RO

TL;DR: 本文提出了一种用于水下管道自主检测的极简传感策略，仅使用IMU、压力传感器和两个声纳即可实现机器人在管道中的自主居中导航。


<details>
  <summary>Details</summary>
Motivation: 水下管道检测面临几何空间受限、水体浑浊、可靠定位线索稀缺等挑战，传统方法需要多普勒测速仪、外部跟踪或复杂传感器阵列，成本高且不实用。

Method: 采用IMU、压力传感器、下视单波束声纳和旋转360度声纳的极简传感配置；提出从单波束声纳强度数据中提取距离估计的计算高效方法；建立闭式几何模型利用两个声纳距离估计管道中心；设计自适应置信度加权PD控制器保持居中。

Result: 在直径46厘米的淹没管道中使用BlueROV2重型ROV进行实验，展示了稳定的居中控制和完整的管道穿越能力，即使在环境水流和结构变形条件下也能成功。

Conclusion: 研究表明，通过轻量级、计算高效的传感和处理架构可以实现可靠的管道内导航和检测，提高了受限环境中自主水下检测的实用性。

Abstract: Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments.

</details>


### [193] [Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions](https://arxiv.org/abs/2602.05273)
*Hengxuan Xu,Fengbo Lan,Zhixin Zhao,Shengjie Wang,Mengqiao Liu,Jieqian Sun,Yu Cheng,Tao Zhang*

Main category: cs.RO

TL;DR: AIDE是一个双流框架，通过交互式探索与视觉语言推理结合，解决机器人在模糊指令下识别任务相关物体的问题，实现零样本可供性分析和实时任务执行。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的方法在处理模糊人类指令时面临挑战，主要由于推理效率低下和缺乏环境交互，这阻碍了实时任务规划和执行。

Method: 提出AIDE双流框架：多阶段推理作为决策流，加速决策作为执行流，集成交互式探索与视觉语言推理，实现零样本可供性分析和模糊指令解释。

Result: 在仿真和真实环境中的大量实验表明，AIDE实现了超过80%的任务规划成功率和10Hz下超过95%的闭环连续执行准确率，在多样化开放世界场景中优于现有VLM方法。

Conclusion: AIDE通过整合交互式探索与视觉语言推理，有效解决了模糊指令下的物体识别和任务执行问题，在实时性能和准确性方面显著优于现有方法。

Abstract: Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for "I'm thirsty") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\% and more than 95\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.

</details>


### [194] [Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework](https://arxiv.org/abs/2602.05310)
*Jipeng Kong,Xinzhe Liu,Yuhang Lin,Jinrui Han,Sören Schwertfeger,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: PAiD提出了一种渐进式架构，将人形机器人足球技能学习分解为三个阶段：通过人体运动跟踪获取运动技能、轻量级感知-动作集成实现位置泛化、物理感知的仿真到现实迁移，解决了现有方法中模块间不稳定或训练目标冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 足球对人形机器人提出了重大挑战，需要紧密集成的感知-动作能力来完成感知引导的踢球和全身平衡控制等任务。现有方法存在模块化流程中模块间不稳定性，或端到端框架中训练目标冲突的问题。

Method: 提出感知-动作集成决策（PAiD）架构，将足球技能获取分解为三个阶段：1) 通过人体运动跟踪获取运动技能；2) 轻量级感知-动作集成实现位置泛化；3) 物理感知的仿真到现实迁移。这种分阶段分解建立了稳定的基础技能，避免了感知集成期间的奖励冲突，并最小化了仿真到现实的差距。

Result: 在Unitree G1机器人上的实验展示了高保真度的人形踢球能力，在静态或滚动球、不同位置和干扰等多样化条件下表现出鲁棒性能，同时在室内和室外场景中保持一致的执行效果。

Conclusion: 这种分而治之的策略推进了鲁棒的人形机器人足球能力，并为复杂具身技能获取提供了一个可扩展的框架。该架构通过分阶段学习避免了奖励冲突，最小化了仿真到现实差距，实现了稳定且泛化性强的足球技能。

Abstract: Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.

</details>


### [195] [RoboPaint: From Human Demonstration to Any Robot and Any View](https://arxiv.org/abs/2602.05325)
*Jiacheng Fan,Zhiyue Zhao,Yiqian Zhang,Chao Chen,Peide Wang,Hengdi Zhang,Zhengxue Cheng*

Main category: cs.RO

TL;DR: 提出Real-Sim-Real数据管道，通过人类演示生成机器人可执行训练数据，无需直接遥操作，解决VLA模型在灵巧操作中的数据瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 获取大规模、高保真机器人演示数据是扩展视觉-语言-动作模型在灵巧操作中应用的关键瓶颈。传统遥操作方法成本高、可扩展性差，需要更高效的数据收集方案。

Method: 构建标准化数据采集室捕获多模态人类演示（同步3个RGB-D视频、11个RGB视频、29自由度手套关节角度和14通道触觉信号）。提出触觉感知重定向方法，通过几何和力引导优化将人手状态映射到机器人灵巧手状态。在Isaac Sim中渲染重定向轨迹生成机器人训练数据。

Result: 重定向的灵巧手轨迹在10个多样化物体操作任务中达到84%成功率。仅使用生成数据训练的VLA策略（Pi0.5）在三个代表性任务（抓放、推、倾倒）上达到80%平均成功率。

Conclusion: 机器人训练数据可以通过Real-Sim-Real数据管道从人类演示中高效"绘制"出来。该方法为遥操作提供了可扩展、成本效益高的替代方案，在复杂灵巧操作中性能损失最小。

Abstract: Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently "painted" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.

</details>


### [196] [Benchmarking Affordance Generalization with BusyBox](https://arxiv.org/abs/2602.05441)
*Dean Fortier,Timothy Adamson,Tess Hellebrekers,Teresa LaScala,Kofi Ennin,Michael Murray,Andrey Kolobov,Galen Mullins*

Main category: cs.RO

TL;DR: BusyBox是一个用于评估VLA模型物理可操作性泛化能力的基准测试平台，包含6个可互换模块，通过3D打印和开源设计便于实验室复现。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在视觉和语言空间泛化方面取得进展，但缺乏对物理可操作性泛化能力的系统评估。需要专门基准来测试模型操纵具有熟悉物理特征的新物体的能力。

Method: 设计BusyBox物理基准平台，包含开关、滑块、电线、按钮、显示屏和旋钮6个模块。模块可互换和旋转，创建视觉外观不同但可操作性相同的变体。提供3D打印CAD文件、物料清单和Mobile Aloha机器人收集的语言标注演示数据集。

Result: 实验表明，即使是强大的开源VLA模型如π₀.₅和GR00T-N1.6，在BusyBox变体间的泛化能力仍然面临高度挑战。基准平台易于在机器人实验室构建，促进社区评估和改进VLA模型的可操作性泛化能力。

Conclusion: BusyBox为系统评估VLA模型的物理可操作性泛化提供了标准化基准，通过开源设计和数据集促进社区研究，揭示了当前VLA模型在物理交互泛化方面的局限性。

Abstract: Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.
  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.

</details>


### [197] [TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation](https://arxiv.org/abs/2602.05468)
*Pranav Ponnivalavan,Satoshi Funabashi,Alexander Schmitz,Tetsuya Ogata,Shigeki Sugano*

Main category: cs.RO

TL;DR: 提出TaSA框架，通过两阶段深度预测学习解决机器人手自接触感知问题，实现精细操作


<details>
  <summary>Details</summary>
Motivation: 人类能够进行多种手内操作（如捏取物体、使用工具），这些操作通常涉及物体与多个手指的同时接触。对于机器人手来说，这仍然是一个开放性问题，因为这种灵巧操作需要区分自接触产生的触觉感受和外部接触产生的触觉感受。大多数方法通过约束运动来避免/忽略自接触触觉信息，这虽然降低了复杂性，但也限制了在自接触不可避免的真实场景中的泛化能力。

Method: 提出TaSA（Tactile Sensory Attenuation）两阶段深度预测学习框架。第一阶段：明确学习自接触动力学，建模机器人自身动作如何生成触觉反馈。第二阶段：将学习到的模型整合到运动学习阶段，以强调操作过程中的物体接触信号。该方法基于人类的感觉衰减原理，即神经系统区分可预测的自接触信号，使新颖的物体刺激作为相关信号凸显出来。

Result: 在一组需要精细触觉辨别的插入任务上评估TaSA：将铅笔芯插入自动铅笔、将硬币插入槽中、将回形针固定在纸张上，涉及不同方向、位置和尺寸。在所有任务中，使用TaSA训练的策略比基线方法获得了显著更高的成功率。

Conclusion: 基于感觉衰减的结构化触觉感知对于机器人灵巧操作至关重要。TaSA框架通过明确建模自接触动力学并整合到运动学习中，能够有效区分自接触和物体接触，实现了更高成功率的精细操作。

Abstract: Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation.

</details>


### [198] [DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter](https://arxiv.org/abs/2602.05513)
*Xukun Li,Yu Sun,Lei Zhang,Bosheng Huang,Yibo Peng,Yuan Meng,Haojun Jiang,Shaoxuan Xie,Guacai Yao,Alois Knoll,Zhenshan Bing,Xinlong Wang,Zhenguo Sun*

Main category: cs.RO

TL;DR: DECO是一个基于DiT的策略框架，通过解耦多模态条件处理实现灵巧操作，并发布了包含触觉感知的双手机器人操作数据集DECO-50


<details>
  <summary>Details</summary>
Motivation: 现有的机器人操作策略在处理多模态输入（视觉、动作、本体感知、触觉等）时存在耦合问题，需要更有效的多模态条件解耦方法来提升灵巧操作的性能

Method: 基于DiT架构，通过联合自注意力处理图像和动作token，自适应层归一化注入本体感知和可选条件，交叉注意力注入触觉信号，并使用轻量级LoRA适配器进行高效微调

Result: 提出了DECO框架并发布了DECO-50数据集，包含4个场景、28个子任务、超过50小时数据、约500万帧图像和8000条成功轨迹的双手机器人触觉操作数据集

Conclusion: DECO框架通过解耦多模态条件处理为机器人灵巧操作提供了有效的解决方案，DECO-50数据集为触觉感知的双手机器人操作研究提供了重要资源

Abstract: Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.

</details>


### [199] [Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments](https://arxiv.org/abs/2602.05516)
*Runxiao Liu,Pengda Mao,Xiangli Le,Shuang Gu,Yapeng Chen,Quan Quan*

Main category: cs.RO

TL;DR: 提出基于虚拟管理论和耗散系统理论的多无人机协同运输控制框架，用于受限环境中电缆悬挂负载的运输，具有低计算开销、高稳定性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决多无人机在受限环境中协同运输电缆悬挂负载的挑战，需要高效协调、适应障碍物布局、保持系统稳定性和鲁棒性

Method: 结合虚拟管理论和耗散系统理论，设计多无人机协同控制框架，实现张力分布和协调运输，动态调整无人机配置以适应障碍物布局

Result: 通过大量仿真验证了方法的有效性，展示了大规模多无人机系统的可扩展性，并在室外场景中进行了实验验证，证明了实际可行性和鲁棒性

Conclusion: 提出的控制框架能够高效实现多无人机在受限环境中的协同运输，具有低计算开销、高稳定性和鲁棒性，适用于实际应用场景

Abstract: This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.

</details>


### [200] [VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator](https://arxiv.org/abs/2602.05552)
*Bessie Dominguez-Dager,Sergio Suescun-Ferrandiz,Felix Escalona,Francisco Gomez-Donoso,Miguel Cazorla*

Main category: cs.RO

TL;DR: VLN-Pilot是一个新颖框架，利用大型视觉语言模型作为室内无人机导航的"飞行员"，通过自然语言指令理解和视觉感知实现自主导航，无需GPS或传统路径规划方法。


<details>
  <summary>Details</summary>
Motivation: 传统室内无人机导航依赖基于规则或几何的路径规划方法，需要大量任务特定工程。本文旨在利用VLLM的多模态推理能力，实现更自然、语义丰富的语言驱动无人机控制，减少操作员工作量并提高安全性。

Method: VLN-Pilot框架将大型视觉语言模型作为核心控制器，模型能够：1）解释自由形式的自然语言指令；2）将指令与视觉观察相结合；3）规划并执行无人机轨迹；4）处理空间关系、避障和动态事件响应。框架在定制的高真实感室内仿真基准上进行验证。

Result: 在定制室内仿真基准测试中，VLLM驱动的智能体在复杂指令跟随任务上取得高成功率，包括具有多个语义目标的长期导航任务。结果表明该框架能够有效替代远程无人机飞行员。

Conclusion: VLN-Pilot展示了VLLM作为室内无人机自主导航"飞行员"的潜力，能够显著减少操作员工作量，提高安全性和任务灵活性，为检查、搜救、设施监控等应用提供了可扩展、人性化的无人机控制方案。

Abstract: This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.

</details>


### [201] [TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards](https://arxiv.org/abs/2602.05596)
*Hokyun Lee,Woo-Jeong Baek,Junhyeok Cha,Jaeheung Park*

Main category: cs.RO

TL;DR: TOLEBI：首个基于学习的双足机器人容错框架，通过模拟故障训练和在线关节状态分类实现硬件故障下的鲁棒行走


<details>
  <summary>Details</summary>
Motivation: 随着学习算法在机器人应用中的普及，双足行走的强化学习成为人形机器人研究热点。现有方法在正常行走任务中成功率很高，但缺乏处理硬件故障（如关节锁定、电源故障）和环境干扰的能力，而这些在真实世界中可能导致严重后果。

Method: 提出TOLEBI框架：1）在仿真中注入关节锁定、电源故障和外部干扰来学习容错行走策略；2）通过仿真到真实迁移将策略部署到实体机器人；3）集成在线关节状态模块，根据运行时实际观测分类关节状态。

Result: 在人形机器人TOCABI上进行了真实世界和仿真验证实验，证明了所提方法的适用性。这是首个基于学习的双足行走容错框架。

Conclusion: TOLEBI为双足机器人提供了首个学习型容错框架，能够处理关节锁定、电源故障等硬件问题，推动了该领域高效学习方法的发展。

Abstract: With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.

</details>


### [202] [HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments](https://arxiv.org/abs/2602.05608)
*Yufei Zhu,Shih-Min Yang,Martin Magnusson,Allan Wang*

Main category: cs.RO

TL;DR: HiCrowd：一种结合强化学习和模型预测控制的分层框架，利用行人运动作为引导，解决机器人在密集人群中的导航问题，减少冻结行为。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在密集人群导航中的"冻结机器人问题"，即机器人因找不到安全运动路径而被困在人群中。现有方法通常将行人视为动态障碍物，而本文提出利用行人运动作为引导的新思路。

Method: 提出HiCrowd分层框架：高层RL策略生成跟随点，使机器人与合适的行人群体对齐；低层MPC安全跟踪该引导点，进行短时域规划。结合长期人群感知决策与安全短期执行。

Result: 在离线设置（重放记录的人类轨迹）和在线设置（模拟中行人轨迹更新以响应机器人）中，相比反应式和基于学习的方法，HiCrowd在导航效率和安全性方面表现更优，减少了冻结行为。在真实世界数据集和合成人群数据集上的实验验证了其有效性。

Conclusion: 利用人类运动作为引导，而非仅将人类视为动态障碍物，为机器人在人群中的安全高效导航提供了有力原则。HiCrowd框架成功解决了冻结机器人问题，实现了更好的导航性能。

Abstract: Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.

</details>


### [203] [Scalable and General Whole-Body Control for Cross-Humanoid Locomotion](https://arxiv.org/abs/2602.05791)
*Yufei Xue,YunFeng Lin,Wentao Dong,Yang Tang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Minghuan Liu,Weinan Zhang*

Main category: cs.RO

TL;DR: 提出XHugWBC框架，实现单策略跨多种人形机器人设计的通用控制，无需针对特定机器人训练


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的人形机器人全身控制器大多需要针对特定机器人进行训练，缺乏跨不同机器人设计的通用性

Method: XHugWBC框架包含：(1)物理一致的形态随机化，(2)跨不同人形机器人的语义对齐观测和动作空间，(3)建模形态和动力学特性的有效策略架构

Result: 在12个仿真人形机器人和7个真实机器人上验证了强泛化能力和鲁棒性，支持零样本迁移到未见过的机器人

Conclusion: 通过一次训练学习广泛形态和动力学特性分布，获得的结构偏置支持跨不同人形机器人设计的通用控制

Abstract: Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.

</details>


### [204] [A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion](https://arxiv.org/abs/2602.05855)
*Dennis Bank,Joost Cordes,Thomas Seel,Simon F. G. Ehlers*

Main category: cs.RO

TL;DR: 提出基于学习的多模态地形感知框架，使用机器人中心高度图表示，通过CNN-GRU混合编码器-解码器结构融合深度相机、LiDAR和IMU数据，提升地形重建精度和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 在非结构化、以人为中心的环境中部署人形机器人需要可靠的地形感知。传统系统通常依赖手动设计的单传感器流水线，存在局限性。

Method: 采用基于学习的框架，使用机器人中心高度图作为中间表示。提出混合编码器-解码器结构（EDS），结合CNN提取空间特征，GRU核心处理时间一致性。融合Intel RealSense深度相机、LIVOX MID-360 LiDAR（通过高效球面投影处理）和机载IMU的多模态数据。

Result: 多模态融合相比仅使用深度相机提升7.2%重建精度，相比仅使用LiDAR提升9.9%。集成3.2秒时间上下文减少了建图漂移。

Conclusion: 提出的学习型多模态框架在机器人地形感知方面优于传统单传感器方法，通过时空融合显著提高了重建精度和一致性。

Abstract: Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.

</details>


### [205] [Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools](https://arxiv.org/abs/2602.05895)
*Qi Li,Karsten Berns*

Main category: cs.RO

TL;DR: 提出了一种残差强化学习方法，将标称笛卡尔控制器与学习到的残差策略相结合，用于液压装载机起重机在垃圾容器回收任务中的容器提升阶段。


<details>
  <summary>Details</summary>
Motivation: 城市环境中垃圾容器回收任务中容器提升阶段具有挑战性，需要精确的轨迹跟踪和摆动抑制。由于卸载单元挂钩与容器环之间的几何公差相对于起重机整体尺度较小，传统控制器难以满足精度要求。

Method: 采用残差强化学习方法：1) 标称控制器使用导纳控制进行轨迹跟踪和摆锤感知的摆动阻尼，然后通过带零空间姿态项的阻尼最小二乘逆运动学生成关节速度命令；2) 在Isaac Lab中使用PPO训练的残差策略补偿未建模动力学和参数变化；3) 采用随机化episode初始化和域随机化（载荷属性、执行器增益、被动关节参数）增强泛化能力。

Result: 仿真结果表明，与单独使用标称控制器相比，该方法提高了跟踪精度、减少了振荡、并获得了更高的提升成功率。

Conclusion: 残差强化学习方法能够有效补偿未建模动力学和参数变化，提高液压装载机起重机在容器提升任务中的精度和鲁棒性，而无需从头开始进行端到端学习。

Abstract: This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.

</details>


### [206] [From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits](https://arxiv.org/abs/2602.05922)
*Aziz Mohamed Mili,Louis Catar,Paul Gérard,Ilyass Tabiai,David St-Onge*

Main category: cs.RO

TL;DR: 开发了一个端到端工具链，将无人机撞击测试数据转换为可部署的安全控制器，用于室内微型飞行器靠近人类时的安全限制


<details>
  <summary>Details</summary>
Motivation: 室内微型飞行器越来越多地用于需要靠近人员的任务，但缺乏基于实测撞击风险调整运动限制的实用方法

Method: 1) 设计紧凑可复制的撞击测试装置和协议，捕获不同无人机类别和接触表面的力-时间曲线；2) 建立数据驱动模型，将撞击前速度映射到冲量和接触持续时间；3) 发布脚本和ROS2节点，在线执行速度限制并记录合规性

Result: 在多个商用现成四旋翼无人机和代表性室内资产上验证了工作流程，证明推导出的控制器在满足安全利益相关者指定的力约束的同时保持了任务吞吐量

Conclusion: 提供了一个实用的桥梁，将实测撞击数据转换为运行时限制，包括可共享的数据集、代码和可重复流程，团队可采用该流程认证室内微型飞行器在人类附近的操作

Abstract: Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.

</details>


### [207] [Visuo-Tactile World Models](https://arxiv.org/abs/2602.06001)
*Carolina Higuera,Sergio Arnaud,Byron Boots,Mustafa Mukadam,Francois Robert Hogan,Franziska Meier*

Main category: cs.RO

TL;DR: VT-WM通过结合视觉和触觉感知构建世界模型，在接触丰富的机器人操作任务中提升物理保真度和规划性能，在零样本真实机器人实验中实现高达35%的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 纯视觉模型在遮挡或接触状态模糊时容易出现物体消失、瞬移或违反物理规律等常见失败模式，需要结合触觉感知来更好地理解机器人-物体交互的接触物理。

Method: 提出多任务视觉-触觉世界模型(VT-WM)，通过补充触觉感知来捕捉接触物理，在接触丰富的操作任务集上进行训练，提升想象过程中的物理保真度。

Result: VT-WM在自回归推演中实现33%更好的物体持久性保持和29%更好的运动规律符合度；在零样本真实机器人实验中达到高达35%的成功率提升，在接触丰富的多步任务中提升最大。

Conclusion: VT-WM通过触觉推理捕捉接触物理，显著提升世界模型的物理保真度和规划性能，并展现出良好的下游任务适应能力，仅需少量演示即可在新任务中实现可靠规划。

Abstract: We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [208] [Applying Ground Robot Fleets in Urban Search: Understanding Professionals' Operational Challenges and Design Opportunities](https://arxiv.org/abs/2602.04992)
*Puqi Zhou,Charles R. Twardy,Cynthia Lum,Myeong Lee,David J. Porfirio,Michael R. Hieb,Chris Thomas,Xuesu Xiao,Sungsoo Ray Hong*

Main category: cs.HC

TL;DR: 研究通过焦点小组访谈警察，探讨地面机器人车队如何减轻城市搜救中的认知和体力负担，识别了四个关键挑战领域和四个设计机会


<details>
  <summary>Details</summary>
Motivation: 城市搜救需要在高认知和情境负荷下做出快速、可辩护的决策并维持体力消耗。尽管技术进步提供了地面机器人车队与计算机视觉和LLM界面的潜力，但缺乏对公共安全专业人员如何看待和整合这些技术的研究，存在开发技术先进但不实用解决方案的风险

Method: 在弗吉尼亚州五个地方部门的八名警察中进行了焦点小组会议，探讨他们对地面机器人技术的看法和整合到现有实践中的设想

Result: 研究发现地面机器人可以减少专业人员对纸质参考、心算和临时协调的依赖，减轻四个关键挑战领域的认知和体力负担：(1)跨多个搜索假设分配人力资源，(2)保持群体意识和情境意识，(3)构建符合失踪人员特征的路线规划，(4)在不确定性下管理认知和体力疲劳。进一步识别了四个设计机会和要求

Conclusion: 提出了可部署、可问责、以人为本的城市搜救支持系统的设计启示，强调需要关注公共安全专业人员的实际需求和操作信任

Abstract: Urban searches demand rapid, defensible decisions and sustained physical effort under high cognitive and situational load. Incident commanders must plan, coordinate, and document time-critical operations, while field searchers execute evolving tasks in uncertain environments. With recent advances in technology, ground-robot fleets paired with computer-vision-based situational awareness and LLM-powered interfaces offer the potential to ease these operational burdens. However, no dedicated studies have examined how public safety professionals perceive such technologies or envision their integration into existing practices, risking building technically sophisticated yet impractical solutions. To address this gap, we conducted focus-group sessions with eight police officers across five local departments in Virginia. Our findings show that ground robots could reduce professionals' reliance on paper references, mental calculations, and ad-hoc coordination, alleviating cognitive and physical strain in four key challenge areas: (1) partitioning the workforce across multiple search hypotheses, (2) retaining group awareness and situational awareness, (3) building route planning that fits the lost-person profile, and (4) managing cognitive and physical fatigue under uncertainty. We further identify four design opportunities and requirements for future ground-robot fleet integration in public-safety operations: (1) scalable multi-robot planning and control interfaces, (2) agency-specific route optimization, (3) real-time replanning informed by debrief updates, and (4) vision-assisted cueing that preserves operational trust while reducing cognitive workload. We conclude with design implications for deployable, accountable, and human-centered urban-search support systems

</details>


### [209] [A Design Space for Live Music Agents](https://arxiv.org/abs/2602.05064)
*Yewon Kim,Stephen Brade,Alexander Wang,David Zhou,Haven Kim,Bill Wang,Sung-Ju Lee,Hugo F Flores Garcia,Cheng-Zhi Anna Huang,Chris Donahue*

Main category: cs.HC

TL;DR: 该论文通过分析184个实时音乐智能体系统，构建了一个涵盖使用场景、交互方式、技术实现和生态系统四个维度的综合设计空间，为跨学科研究提供结构化框架。


<details>
  <summary>Details</summary>
Motivation: 实时音乐为研究创造力和交互提供了独特场景，但跨学科特性导致研究碎片化，阻碍了有效沟通和协作进展。需要整合HCI、AI和计算机音乐等领域的视角来统一理解实时音乐智能体的设计。

Method: 通过分析184个系统（包括学术文献和视频资料），整合多学科视角，开发了一个包含使用场景、交互、技术和生态系统四个维度的综合设计空间框架。

Result: 构建了实时音乐智能体的设计空间，揭示了当前系统的趋势和空白，为研究人员、设计师和音乐家提供了理解现有系统和塑造未来实时人机音乐共创的结构化视角。

Conclusion: 该设计空间促进了跨学科交流，通过识别趋势和差距为未来实时音乐智能体的发展提供指导，并将标注系统作为活体工件发布以支持持续研究。

Abstract: Live music provides a uniquely rich setting for studying creativity and interaction due to its spontaneous nature. The pursuit of live music agents--intelligent systems supporting real-time music performance and interaction--has captivated researchers across HCI, AI, and computer music for decades, and recent advancements in AI suggest unprecedented opportunities to evolve their design. However, the interdisciplinary nature of music has led to fragmented development across research communities, hindering effective communication and collaborative progress. In this work, we bring together perspectives from these diverse fields to map the current landscape of live music agents. Based on our analysis of 184 systems across both academic literature and video, we develop a comprehensive design space that categorizes dimensions spanning usage contexts, interactions, technologies, and ecosystems. By highlighting trends and gaps in live music agents, our design space offers researchers, designers, and musicians a structured lens to understand existing systems and shape future directions in real-time human-AI music co-creation. We release our annotated systems as a living artifact at https://live-music-agents.github.io.

</details>


### [210] [VR Calm Plus: Coupling a Squeezable Tangible Interaction with Immersive VR for Stress Regulation](https://arxiv.org/abs/2602.05093)
*He Zhang,Xinyang Li,Xingyu Zhou,Xinyi Fu*

Main category: cs.HC

TL;DR: VR Calm Plus：一种结合压力敏感毛绒玩具的多模态VR系统，通过挤压互动增强放松效果，相比纯视觉基线显著提升积极情绪和放松感知。


<details>
  <summary>Details</summary>
Motivation: 当前VR压力管理应用主要依赖视听刺激，忽视了挤压互动的治疗潜力。本研究旨在填补这一空白，探索将触觉互动融入VR环境以增强放松效果。

Method: 开发VR Calm Plus系统，整合压力敏感毛绒玩具与交互式VR环境，允许用户通过物理挤压动作动态调节虚拟氛围。采用40名参与者进行实验评估，使用PANAS-X量表、主观问卷、生理测量（心率、皮肤电导、脉搏率变异性）和半结构化访谈。

Result: 相比纯视觉基线，挤压互动显著增强积极情绪和感知放松。生理数据显示"主动放松"状态：心率更大程度降低，自主神经灵活性（PRV）保持，同时维持情绪参与（GSR）。

Conclusion: 将实体输入与沉浸式环境结合对支持情绪健康具有重要价值，为未来基于VR的心理健康工具提供设计见解。

Abstract: While Virtual Reality (VR) is increasingly employed for stress management, most applications rely heavily on audio-visual stimuli and overlook the therapeutic potential of squeezing engagement. To address this gap, we introduce VR Calm Plus, a multimodal system that integrates a pressure-sensitive plush toy into an interactive VR environment. This interface allows users to dynamically modulate the virtual atmosphere through physical squeezing actions, fostering a deeper sense of embodied relaxation. We evaluated the system with 40 participants using PANAS-X surveys, subjective questionnaires, physiological measures (heart rate, skin conductance, pulse rate variability), and semi-structured interviews. Results demonstrate that, compared to a visual-only baseline, squeeze-based interaction significantly enhances positive affect and perceived relaxation. Physiological data further revealed a state of "active relaxation", characterized by greater reductions in heart rate and preserved autonomic flexibility (PRV), alongside sustained emotional engagement (GSR). Our findings highlight the value of coupling tangible input with immersive environments to support emotional well-being and offer design insights for future VR-based mental health tools.

</details>


### [211] [Reporting and Reviewing LLM-Integrated Systems in HCI: Challenges and Considerations](https://arxiv.org/abs/2602.05128)
*Karla Felix Navarro,Eugene Syriani,Ian Arawjo*

Main category: cs.HC

TL;DR: HCI学者在报告和评审涉及LLM集成系统的论文时应考虑：LLM行为不确定性导致信任规范被侵蚀，作者通过技术评估、使用正当性和弱化LLM存在来缓解不信任，提示报告应视上下文而定，HCI与ML/NLP社区规范存在冲突，需制定相应指南。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨HCI学者在报告和评审涉及LLM集成系统的论文时应考虑的因素，因为LLM行为的不确定性和围绕AI的夸张言论正在侵蚀作者与审稿人之间的信任规范，且HCI与ML/NLP社区在贡献定义和技术严谨性方面存在规范冲突。

Method: 研究方法包括对18位LLM集成系统论文作者进行访谈，了解他们的撰写和评审经验，并收集6位HCI专家的额外反馈，基于这些发现制定报告和评审指南。

Result: 研究发现：1）LLM行为不确定性侵蚀信任规范，作者认为审稿人对LLM集成系统论文持独特怀疑态度；2）作者通过技术评估、使用正当性论证和弱化LLM存在来缓解不信任；3）提示报告应视上下文而定，而非一概而论；4）尽管存在伦理问题，作者仍为专有模型使用辩护；5）HCI与ML/NLP社区在贡献定义和技术严谨性方面存在规范冲突。

Conclusion: 结论是HCI社区需要制定针对LLM集成系统论文的报告和评审指南，考虑信任建立机制、上下文相关的提示报告策略、模型选择正当性，并协调HCI与ML/NLP社区的不同规范和价值观念。

Abstract: What should HCI scholars consider when reporting and reviewing papers that involve LLM-integrated systems? We interview 18 authors of LLM-integrated system papers on their authoring and reviewing experiences. We find that norms of trust-building between authors and reviewers appear to be eroded by the uncertainty of LLM behavior and hyperbolic rhetoric surrounding AI. Authors perceive that reviewers apply uniquely skeptical and inconsistent standards towards papers that report LLM-integrated systems, and mitigate mistrust by adding technical evaluations, justifying usage, and de-emphasizing LLM presence. Authors' views challenge blanket directives to report all prompts and use open models, arguing that prompt reporting is context-dependent and justifying proprietary model usage despite ethical concerns. Finally, some tensions in peer review appear to stem from clashes between the norms and values of HCI and ML/NLP communities, particularly around what constitutes a contribution and an appropriate level of technical rigor. Based on our findings and additional feedback from six expert HCI researchers, we present a set of guidelines and considerations for authors, reviewers, and HCI communities around reporting and reviewing papers that involve LLM-integrated systems.

</details>


### [212] [Varifocal Displays Reduce the Impact of the Vergence-Accommodation Conflict on 3D Pointing Performance in Augmented Reality Systems](https://arxiv.org/abs/2602.05129)
*Xiaodan Hu,Monica Perusquía-Hernández,Mayra Donaji Barrera Machuca,Anil Ufuk Batmaz,Yan Zhang,Wolfgang Stuerzlinger,Kiyoshi Kiyokawa*

Main category: cs.HC

TL;DR: 变焦显示能改善AR中的3D指向性能，但效果存在个体差异


<details>
  <summary>Details</summary>
Motivation: 研究变焦显示是否能缓解AR中的调节-辐辏冲突，从而改善3D指向交互性能

Method: 采用被试内设计，24名参与者在变焦和固定焦距条件下执行ISO 9241-411指向任务

Result: 变焦显示整体性能显著优于固定焦距基线，但改善幅度和方向存在个体差异，基线性能较好的参与者改善较小甚至偶尔出现性能下降

Conclusion: 变焦技术能改善AR指向性能，但设计中需要考虑显著的个体差异

Abstract: This paper investigates whether a custom varifocal display can improve 3D pointing performance in augmented reality (AR), where the vergence-accommodation conflict (VAC) is known to impair interaction. Varifocal displays have been hypothesized to alleviate the VAC by dynamically matching the focal distance to the user's gaze-defined target depth. Following prior work, we conducted a within-subject study with 24 participants performing an ISO 9241-411 pointing task under varifocal and fixed-focal viewing. Overall, varifocal viewing yielded significantly higher performance than the fixed-focal baseline across key interaction metrics, although the magnitude and even the direction of the benefit varied across individuals. In particular, participants' responses exhibited a baseline-dependent pattern, with smaller improvements (or occasional degradation) observed for those with better baseline performance. Our findings suggest that varifocal technology can improve AR pointing performance relative to fixed-focal viewing, while highlighting substantial individual differences that should be considered in design and evaluation.

</details>


### [213] [Co-Designing Collaborative Generative AI Tools for Freelancers](https://arxiv.org/abs/2602.05299)
*Kashif Imteyaz,Michael Muller,Claudia Flores-Saviaga,Saiph Savage*

Main category: cs.HC

TL;DR: 论文探讨生成式AI工具如何支持自由职业者协作，而非强化其孤立性，提出"辅助AI"概念以保护创意自主权


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI工具主要为传统工作场所设计，强调个人生产力，缺乏对自由职业者短期团队协作的支持，反而可能加剧其孤立状态并忽视平台动态，使协作更加困难

Method: 通过27名自由职业者的协同设计会议，探索如何设计支持自由职业者协作的生成式AI工具，重点关注AI系统如何影响创意自主权和工作身份

Result: 自由职业者提出"辅助AI"系统概念，即人类引导的工具，支持创意自主权和身份认同，允许灵活的自由职业者主导协作，促进"生产性摩擦"

Conclusion: 基于马尔库塞的技术理性概念，自由职业者抵制一维效率驱动的AI，设想能保护集体创意自主权的技术，并提出面向自由职业者协作生成式AI工具的设计建议

Abstract: Most generative AI tools prioritize individual productivity and personalization, with limited support for collaboration. Designed for traditional workplaces, these tools do not fit freelancers' short-term teams or lack of shared institutional support, which can worsen their isolation and overlook freelancing platform dynamics. This mismatch means that, instead of empowering freelancers, current generative AI tools could reinforce existing precarity and make freelancer collaboration harder. To investigate how to design generative AI tools to support freelancer collaboration, we conducted co-design sessions with 27 freelancers. A key concern that emerged was the risk of AI systems compromising their creative agency and work identities when collaborating, especially when AI tools could reproduce content without attribution, threatening the authenticity and distinctiveness of their collaborative work. Freelancers proposed "auxiliary AI" systems, human-guided tools that support their creative agencies and identities, allowing for flexible freelancer-led collaborations that promote "productive friction". Drawing on Marcuse's concept of technological rationality, we argue that freelancers are resisting one-dimensional, efficiency-driven AI, and instead envisioning technologies that preserve their collective creative agencies. We conclude with design recommendations for collaborative generative AI tools for freelancers.

</details>


### [214] [DiLLS: Interactive Diagnosis of LLM-based Multi-agent Systems via Layered Summary of Agent Behaviors](https://arxiv.org/abs/2602.05446)
*Rui Sheng,Yukun Yang,Chuhan Shi,Yanna Lin,Zixin Chen,Huamin Qu,Furui Cheng*

Main category: cs.HC

TL;DR: DiLLS是一个用于理解和诊断基于LLM的多智能体系统行为的交互式框架，通过三层查询完成结构（活动、行动、操作）来组织和揭示系统行为，帮助开发者更有效地识别和诊断故障。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体系统在处理复杂任务时表现出色，但智能体行为的复杂性使得系统难以理解。当故障发生时，开发者难以识别根本原因并确定改进路径。传统基于原始日志检查的方法效率低下，因为数据量大且复杂。

Method: 提出DiLLS框架和交互式系统，通过自然语言探测多智能体系统，将规划和执行信息组织到三层查询完成结构中：活动（activities）、行动（actions）和操作（operations），生成结构化的多层摘要。

Result: 通过用户研究表明，DiLLS显著提高了开发者在识别、诊断和理解基于LLM的多智能体系统故障方面的有效性和效率。

Conclusion: DiLLS为解决多智能体系统行为理解和故障诊断的挑战提供了有效的框架和工具，通过结构化信息组织和自然语言交互，使开发者能够更深入地理解复杂系统行为。

Abstract: Large language model (LLM)-based multi-agent systems have demonstrated impressive capabilities in handling complex tasks. However, the complexity of agentic behaviors makes these systems difficult to understand. When failures occur, developers often struggle to identify root causes and to determine actionable paths for improvement. Traditional methods that rely on inspecting raw log records are inefficient, given both the large volume and complexity of data. To address this challenge, we propose a framework and an interactive system, DiLLS, designed to reveal and structure the behaviors of multi-agent systems. The key idea is to organize information across three levels of query completion: activities, actions, and operations. By probing the multi-agent system through natural language, DiLLS derives and organizes information about planning and execution into a structured, multi-layered summary. Through a user study, we show that DiLLS significantly improves developers' effectiveness and efficiency in identifying, diagnosing, and understanding failures in LLM-based multi-agent systems.

</details>


### [215] [Relying on LLMs: Student Practices and Instructor Norms are Changing in Computer Science Education](https://arxiv.org/abs/2602.05506)
*Xinrui Lin,Heyan Huang,Shumin Shi,John Vines*

Main category: cs.HC

TL;DR: 研究探讨计算机科学学生和教师在写作、测验、编程、项目学习和信息检索五种场景中使用LLM的情况，发现7种关键意图，揭示学生实践与教师规范之间的冲突程度差异，提出LLM设计指南


<details>
  <summary>Details</summary>
Motivation: 先前研究对学生过度依赖大型语言模型(LLM)表示担忧，本研究旨在深入了解计算机科学学生和教师在不同教育场景中如何与LLM互动，识别使用意图和实践模式

Method: 通过用户研究，包括16名学生和6名教师，分析五种场景（写作、测验、编程、项目式学习、信息检索）中的LLM使用情况，识别关键意图和冲突模式

Result: 识别出7种关键意图；发现学生实践与教师规范之间存在不同程度的冲突：写作生成和编程测验解决存在明显冲突，编程项目实现和项目式学习存在部分冲突，写作修订与构思、编程测验修正和信息查询与总结存在广泛共识；教师正从禁止转向认可学生使用LLM完成高质量工作，并将使用记录纳入评估评分

Conclusion: 提出LLM设计指南：在冲突明显的场景（如写作生成）部署默认防护机制，采用游戏化和共情交互防止学生"抛弃"LLM；在低冲突意图中利用理解检查促进学习；建议教育者适应学生LLM使用模式，将其整合到评估体系中

Abstract: Prior research has raised concerns about students' over-reliance on large language models (LLMs) in higher education. This paper examines how Computer Science students and instructors engage with LLMs across five scenarios: "Writing", "Quiz", "Programming", "Project-based learning", and "Information retrieval". Through user studies with 16 students and 6 instructors, we identify 7 key intents, including increasingly complex student practices. Findings reveal varying levels of conflict between student practices and instructor norms, ranging from clear conflict in "Writing-generation" and "(Programming) quiz-solving", through partial conflict in "Programming project-implementation" and "Project-based learning", to broad agreement in "Writing-revision & ideation", "(Programming) quiz-correction" and "Info-query & summary". We document instructors are shifting from prohibiting to recognizing students' use of LLMs for high-quality work, integrating usage records into assessment grading. Finally, we propose LLM design guidelines: deploying default guardrails with game-like and empathetic interaction to prevent students from "deserting" LLMs, especially for "Writing-generation", while utilizing comprehension checks in low-conflict intents to promote learning.

</details>


### [216] [AI chatbots versus human healthcare professionals: a systematic review and meta-analysis of empathy in patient care](https://arxiv.org/abs/2602.05628)
*Alastair Howcroft,Amber Bennett-Weston,Ahmad Khan,Joseff Griffiths,Simon Gay,Jeremy Howick*

Main category: cs.HC

TL;DR: AI聊天机器人在文本交流中被认为比人类医疗专业人员更具同理心，标准化平均差异为0.87（P<0.00001），相当于10分量表上增加2分。


<details>
  <summary>Details</summary>
Motivation: 同理心对改善患者预后至关重要，而AI聊天机器人在医疗领域的应用迅速扩展。现有研究对AI与人类医疗专业人员的同理心表现存在不一致结论，需要系统评估。

Method: 系统检索多个数据库，比较使用大语言模型的AI聊天机器人与人类医疗专业人员在同理心测量上的研究。使用ROBINS-I评估偏倚风险，对可行数据进行随机效应荟萃分析，避免重复计数。

Result: 纳入15项研究（2023-2024年），其中13项显示AI同理心评分显著更高，仅2项皮肤病学研究支持人类回答。13项可提取数据的研究荟萃分析显示，标准化平均差异为0.87（95% CI 0.54-1.20），支持AI（P<0.00001）。

Conclusion: 在纯文本场景中，AI聊天机器人常被认为比人类医疗专业人员更具同理心。未来研究需通过直接患者评估验证这些发现，并评估新兴语音AI系统是否能提供类似同理心优势。

Abstract: Background: Empathy is widely recognized for improving patient outcomes, including reduced pain and anxiety and improved satisfaction, and its absence can cause harm. Meanwhile, use of artificial intelligence (AI)-based chatbots in healthcare is rapidly expanding, with one in five general practitioners using generative AI to assist with tasks such as writing letters. Some studies suggest AI chatbots can outperform human healthcare professionals (HCPs) in empathy, though findings are mixed and lack synthesis.
  Sources of data: We searched multiple databases for studies comparing AI chatbots using large language models with human HCPs on empathy measures. We assessed risk of bias with ROBINS-I and synthesized findings using random-effects meta-analysis where feasible, whilst avoiding double counting.
  Areas of agreement: We identified 15 studies (2023-2024). Thirteen studies reported statistically significantly higher empathy ratings for AI, with only two studies situated in dermatology favouring human responses. Of the 15 studies, 13 provided extractable data and were suitable for pooling. Meta-analysis of those 13 studies, all utilising ChatGPT-3.5/4, showed a standardized mean difference of 0.87 (95% CI, 0.54-1.20) favouring AI (P < .00001), roughly equivalent to a two-point increase on a 10-point scale.
  Areas of controversy: Studies relied on text-based assessments that overlook non-verbal cues and evaluated empathy through proxy raters.
  Growing points: Our findings indicate that, in text-only scenarios, AI chatbots are frequently perceived as more empathic than human HCPs.
  Areas timely for developing research: Future research should validate these findings with direct patient evaluations and assess whether emerging voice-enabled AI systems can deliver similar empathic advantages.

</details>


### [217] [(Computer) Vision in Action: Comparing Remote Sighted Assistance and a Multimodal Voice Agent in Inspection Sequences](https://arxiv.org/abs/2602.05671)
*Damien Rudaz,Barbara Nino Carreras,Sara Merlino,Brian L. Due,Barry Brown*

Main category: cs.HC

TL;DR: 通过对比盲人与人类远程视觉助手、多模态语音助手在相同任务中的协作实践，揭示语音助手因无法产生环境触发的视觉行为而缺乏主动性的根本原因


<details>
  <summary>Details</summary>
Motivation: 探索人机协作与人际协作的差异，借鉴盲人专家和视力正常志愿者的专业知识，解决多模态语音助手长期存在的主动性问题

Method: 对大型语料库中两个代表性片段进行细粒度分析，对比盲人参与者与人类远程视觉助手（通过电话协作）以及同一参与者与多模态语音助手（稍早时间执行相同任务）的协作实践

Result: 通过对比分析，精确识别了多模态语音助手在现场未能实施的基本主动实践，发现语音助手无法产生环境触发的基于视觉的行为

Conclusion: 只要多模态语音助手无法产生环境触发的基于视觉的行为，它们就缺乏人类远程视觉助手所依赖的关键资源，这是其主动性不足的根本原因

Abstract: Does human-AI assistance unfold in the same way as human-human assistance? This research explores what can be learned from the expertise of blind individuals and sighted volunteers to inform the design of multimodal voice agents and address the enduring challenge of proactivity. Drawing on granular analysis of two representative fragments from a larger corpus, we contrast the practices co-produced by an experienced human remote sighted assistant and a blind participant-as they collaborate to find a stain on a blanket over the phone-with those achieved when the same participant worked with a multimodal voice agent on the same task, a few moments earlier. This comparison enables us to specify precisely which fundamental proactive practices the agent did not enact in situ. We conclude that, so long as multimodal voice agents cannot produce environmentally occasioned vision-based actions, they will lack a key resource relied upon by human remote sighted assistants.

</details>


### [218] [Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction](https://arxiv.org/abs/2602.05687)
*Pavithren V S Pakianathan,Rania Islambouli,Diogo Branco,Albrecht Schmidt,Tiago Guerreiro,Jan David Smeddinck*

Main category: cs.HC

TL;DR: LLM驱动的自动摘要和对话界面可帮助医疗专业人员理解患者生成健康数据，但需解决透明度、隐私和过度依赖等问题


<details>
  <summary>Details</summary>
Motivation: 患者通过可穿戴设备和智能手机生成大量个人健康数据，这些数据有潜力改变预防性医疗，但数据规模大、异质性强，加上医疗专业人员时间压力和数据分析能力限制，阻碍了这些数据在临床实践中的整合应用

Method: 以心血管疾病风险降低为用例，采用混合方法研究，16名医疗专业人员使用原型系统审查多模态患者生成健康数据。原型整合了常见图表、LLM生成的摘要和对话式界面

Result: AI摘要提供快速概览并锚定数据探索，对话式交互支持灵活分析并弥合数据素养差距。但医疗专业人员对透明度、隐私和过度依赖表示担忧

Conclusion: 研究为将AI驱动的摘要和对话整合到临床工作流程以支持患者生成健康数据理解提供了实证见解和社会技术设计启示

Abstract: Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data literacy of healthcare professionals (HCPs). We explore how large language models (LLMs) can support sensemaking of patient-generated health data (PGHD) with automated summaries and natural language data exploration. Using cardiovascular disease (CVD) risk reduction as a use case, 16 HCPs reviewed multimodal PGHD in a mixed-methods study with a prototype that integrated common charts, LLM-generated summaries, and a conversational interface. Findings show that AI summaries provided quick overviews that anchored exploration, while conversational interaction supported flexible analysis and bridged data-literacy gaps. However, HCPs raised concerns about transparency, privacy, and overreliance. We contribute empirical insights and sociotechnical design implications for integrating AI-driven summarization and conversation into clinical workflows to support PGHD sensemaking.

</details>


### [219] [Authorship Drift: How Self-Efficacy and Trust Evolve During LLM-Assisted Writing](https://arxiv.org/abs/2602.05819)
*Yeon Su Park,Nadia Azzahra Putri Arvi,Seoyoung Kim,Juho Kim*

Main category: cs.HC

TL;DR: 研究探讨了人类与大型语言模型协作写作中的作者身份问题，分析了用户自我效能感和信任的动态变化及其对提示策略和作者身份结果的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地作为写作协作伙伴使用，用户和模型共同塑造文本，这带来了作者身份认定的关键挑战。理解这种协作中的作者身份需要考察用户在协作过程中不断变化的内部状态，特别是自我效能感和信任，但这些状态的动态变化及其与用户提示策略和作者身份结果的关联尚未得到充分探索。

Method: 通过一项涉及302名参与者的LLM辅助写作研究，收集交互日志以及逐轮次的自我效能感和信任评分，分析这些动态变化。

Result: 协作通常降低用户的自我效能感，同时增加信任。失去自我效能感的参与者更倾向于要求LLM直接编辑其作品，而恢复自我效能感的参与者则请求更多审阅和反馈。具有稳定自我效能感的参与者对最终文本表现出更高的实际和感知作者身份。

Conclusion: 基于这些发现，提出了理解和支持人类-LLM协作中作者身份的设计启示。

Abstract: Large language models (LLMs) are increasingly used as collaborative partners in writing. However, this raises a critical challenge of authorship, as users and models jointly shape text across interaction turns. Understanding authorship in this context requires examining users' evolving internal states during collaboration, particularly self-efficacy and trust. Yet, the dynamics of these states and their associations with users' prompting strategies and authorship outcomes remain underexplored. We examined these dynamics through a study of 302 participants in LLM-assisted writing, capturing interaction logs and turn-by-turn self-efficacy and trust ratings. Our analysis showed that collaboration generally decreased users' self-efficacy while increasing trust. Participants who lost self-efficacy were more likely to ask the LLM to edit their work directly, whereas those who recovered self-efficacy requested more review and feedback. Furthermore, participants with stable self-efficacy showed higher actual and perceived authorship of the final text. Based on these findings, we propose design implications for understanding and supporting authorship in human-LLM collaboration.

</details>


### [220] [ToMigo: Interpretable Design Concept Graphs for Aligning Generative AI with Creative Intent](https://arxiv.org/abs/2602.05825)
*Lena Hegemann,Xinyi Wen,Michael A. Hedderich,Tarmo Nurmi,Hariharan Subramonyam*

Main category: cs.HC

TL;DR: ToMigo是一个通过设计概念图表示用户意图的AI系统，用于解决生成式AI与用户意图不对齐的问题，特别应用于图形设计领域。


<details>
  <summary>Details</summary>
Motivation: 生成式AI经常产生与用户意图不对齐的结果，特别是在处理模糊提示时。现有方法需要专业知识或严格流程，难以让用户通过简单直接的方式理解和影响AI对意图的解释。

Method: ToMigo将意图表示为设计概念图：节点代表目的、内容或风格的选择，边通过可解释的链接连接它们。系统从参考图像和文本推断意图，使用预研究数据推导的节点类型和边模式，通过多模态大语言模型生成对齐用户意图和统一设计目标的概念图。

Result: 用户研究显示ToMigo获得了高对齐评分，能够很好地捕捉大多数用户意图。用户报告了更强的控制感，并认为可编辑图、反思性聊天、概念-设计重新对齐等交互功能对于发展和实现设计想法很有用。

Conclusion: ToMigo通过设计概念图表示意图，提供了一种让用户探索AI推理并直接操作设计概念的方法，有效解决了生成式AI与用户意图对齐的问题，在图形设计领域表现出良好的效果。

Abstract: Generative AI often produces results misaligned with user intentions, for example, resolving ambiguous prompts in unexpected ways. Despite existing approaches to clarify intent, a major challenge remains: understanding and influencing AI's interpretation of user intent through simple, direct inputs requiring no expertise or rigid procedures. We present ToMigo, representing intent as design concept graphs: nodes represent choices of purpose, content, or style, while edges link them with interpretable explanations. Applied to graphic design, ToMigo infers intent from reference images and text. We derived a schema of node types and edges from pre-study data, informing a multimodal large language model to generate graphs aligning nodes externally with user intent and internally toward a unified design goal. This structure enables users to explore AI reasoning and directly manipulate the design concept. In our user studies, ToMigo received high alignment ratings and captured most user intentions well. Users reported greater control and found interactive features-editable graphs, reflective chats, concept-design realignment-useful for evolving and realizing their design ideas.

</details>


### [221] [Whispers of the Butterfly: A Research-through-Design Exploration of In-Situ Conversational AI Guidance in Large-Scale Outdoor MR Exhibitions](https://arxiv.org/abs/2602.05826)
*Dongyijie Primo Pan,Shuyue Li,Yawei Zhao,Junkun Long,Hao Li,Pan Hui*

Main category: cs.HC

TL;DR: Dream-Butterfly是一个用于大型户外混合现实艺术展览的对话式AI导览系统，通过非人类同伴化身提供多语言、基于展览的解释，研究比较了AI主导与人工主导导览的效果。


<details>
  <summary>Details</summary>
Motivation: 大型户外混合现实艺术展览通常将虚拟作品分布在开放公共空间中，但解释服务难以规模化，往往将探索变成脚本化的游览。现有方法缺乏能够提供个性化、可扩展解释的解决方案。

Method: 通过研究型设计方法创建Dream-Butterfly系统，将其部署在中国南方一所公立大学校园的大型户外MR展览中。进行野外对照研究(N=24)，比较AI主导导览与人工主导导览，同时保持两种条件下都有工作人员负责安全。结合问卷调查和半结构化访谈进行分析。

Result: 研究揭示了将主要解释渠道从人工转向AI如何重塑解释访问、感知响应性、沉浸感和工作负荷，以及游客如何在工作人员、AI导览和自身之间协商责任交接。发现了混合人机导览角色配置的设计启示。

Conclusion: 研究提炼出可转移的设计启示，用于配置混合人机导览角色，以及在移动、安全受限的户外MR展览中体现对话式智能体。为大规模户外MR展览的可扩展解释系统提供了实践指导。

Abstract: Large-scale outdoor mixed reality (MR) art exhibitions distribute curated virtual works across open public spaces, but interpretation rarely scales without turning exploration into a scripted tour. Through Research-through-Design, we created Dream-Butterfly, an in-situ conversational AI docent embodied as a small non-human companion that visitors summon for multilingual, exhibition-grounded explanations. We deployed Dream-Butterfly in a large-scale outdoor MR exhibition at a public university campus in southern China, and conducted an in-the-wild between-subject study (N=24) comparing a primarily human-led tour with an AI-led tour while keeping staff for safety in both conditions. Combining questionnaires and semi-structured interviews, we characterize how shifting the primary explanation channel reshapes explanation access, perceived responsiveness, immersion, and workload, and how visitors negotiate responsibility handoffs among staff, the AI guide, and themselves. We distill transferable design implications for configuring mixed human-AI guiding roles and embodying conversational agents in mobile, safety-constrained outdoor MR exhibitions.

</details>


### [222] [Large Data Acquisition and Analytics at Synchrotron Radiation Facilities](https://arxiv.org/abs/2602.05837)
*Aashish Panta,Giorgio Scorzelli,Amy A. Gooch,Werner Sun,Katherine S. Shanks,Suchismita Sarker,Devin Bougie,Keara Soloway,Rolf Verberg,Tracy Berman,Glenn Tarcea,John Allison,Michela Taufer,Valerio Pascucci*

Main category: cs.HC

TL;DR: 开发并部署了一个用于同步辐射设施的数据采集与管理框架，解决了大规模数据、远程监控和实时质量评估的挑战，显著提升了实验效率和数据可访问性。


<details>
  <summary>Details</summary>
Motivation: 同步辐射设施如康奈尔高能同步辐射源（CHESS）面临三大挑战：1）实验访问时间有限；2）需要实时现场实验监控；3）每个用户组产生TB级数据的管理难题。这些问题影响了实验效率和数据质量评估。

Method: 设计、部署并评估了一个基于安全CHESS服务器的框架，提供实时、基于Web的工具用于远程实验监控和数据质量评估。该系统在三个光束线（ID3A、ID3B、ID4B）上实施，管理50-100TB数据和超过1000万个文件。

Result: 框架在2024年底成功管理了50-100TB数据和超过1000万个文件。通过43个研究组和86个仪表板的测试，结果显示：减少了管理开销，提高了数据可访问性，并简化了数据工作流程。

Conclusion: 该框架有效解决了同步辐射设施的数据采集和管理挑战，通过实时Web工具实现了远程监控和数据质量评估，显著提升了操作效率和数据工作流程，对同步辐射数据采集具有变革性影响。

Abstract: Synchrotron facilities like the Cornell High Energy Synchrotron Source (CHESS) generate massive data volumes from complex beamline experiments, but face challenges such as limited access time, the need for on-site experiment monitoring, and managing terabytes of data per user group. We present the design, deployment, and evaluation of a framework that addresses CHESS's data acquisition and management issues. Deployed on a secure CHESS server, our system provides real time, web-based tools for remote experiment monitoring and data quality assessment, improving operational efficiency. Implemented across three beamlines (ID3A, ID3B, ID4B), the framework managed 50-100 TB of data and over 10 million files in late 2024. Testing with 43 research groups and 86 dashboards showed reduced overhead, improved accessibility, and streamlined data workflows. Our paper highlights the development, deployment, and evaluation of our framework and its transformative impact on synchrotron data acquisition.

</details>


### [223] [DuoDrama: Supporting Screenplay Refinement Through LLM-Assisted Human Reflection](https://arxiv.org/abs/2602.05854)
*Yuying Tang,Xinyi Chen,Haotian Li,Xing Xie,Xiaojuan Ma,Huamin Qu*

Main category: cs.HC

TL;DR: DuoDrama是一个AI系统，通过体验-评估双角色工作流为编剧提供反馈，协调角色内部视角和故事整体外部视角，提升反思效果


<details>
  <summary>Details</summary>
Motivation: 现有AI工具无法充分协调角色内部视角和故事整体外部视角来满足编剧在剧本修改阶段的反思需求

Method: 基于表演理论和9位专业编剧的形成性研究，设计了体验-评估双角色工作流(ExReflect)：AI先扮演体验角色生成体验，再转换为评估角色基于体验生成反馈

Result: 14位专业编剧的研究显示，DuoDrama提高了反馈质量和一致性，增强了反思的有效性、深度和丰富度

Conclusion: DuoDrama通过协调内部和外部视角的AI反馈系统，有效支持编剧的反思过程，并讨论了更广泛的影响和未来方向

Abstract: AI has been increasingly integrated into screenwriting practice. In refinement, screenwriters expect AI to provide feedback that supports reflection across the internal perspective of characters and the external perspective of the overall story. However, existing AI tools cannot sufficiently coordinate the two perspectives to meet screenwriters' needs. To address this gap, we present DuoDrama, an AI system that generates feedback to assist screenwriters' reflection in refinement. To enable DuoDrama, based on performance theories and a formative study with nine professional screenwriters, we design the Experience-Grounded Feedback Generation Workflow for Human Reflection (ExReflect). In ExReflect, an AI agent adopts an experience role to generate experience and then shifts to an evaluation role to generate feedback based on the experience. A study with fourteen professional screenwriters shows that DuoDrama improves feedback quality and alignment and enhances the effectiveness, depth, and richness of reflection. We conclude by discussing broader implications and future directions.

</details>


### [224] ["It Talks Like a Patient, But Feels Different": Co-Designing AI Standardized Patients with Medical Learners](https://arxiv.org/abs/2602.05856)
*Zhiqi Gao,Guo Zhu,Huarui Luo,Dongyijie Primo Pan,Haoming Tang,Bingquan Zhang,Jiahuan Pei,Jie Li,Benyou Wang*

Main category: cs.HC

TL;DR: 该研究探讨了AI标准化病人(AI-SPs)的设计需求，发现教学可用性比对话真实性更能促进学习者信任、参与和教育价值


<details>
  <summary>Details</summary>
Motivation: 传统标准化病人(SPs)成本高、难以扩展且不一致，而基于大语言模型的AI-SPs虽然提供灵活按需练习，但学习者常感觉"像病人但感觉不同"，需要研究如何改进AI-SPs设计以满足学习者需求

Method: 访谈12名临床年级医学生并进行三次协同设计工作坊，分析学习者对SP互动的约束体验和对AI-SPs的期望，识别学习者中心需求并转化为AI-SP设计需求，合成概念工作流程

Result: 识别出六个学习者中心需求，将其转化为AI-SP设计需求，并合成概念工作流程，发现教学可用性而非对话真实性是驱动学习者信任、参与和教育价值的关键因素

Conclusion: AI-SPs应定位为刻意练习工具，教学可用性比对话真实性更重要，这决定了学习者信任、参与和教育价值，为AI-SP设计提供了以学习者为中心的框架

Abstract: Standardized patients (SPs) play a central role in clinical communication training but are costly, difficult to scale, and inconsistent. Large language model (LLM) based AI standardized patients (AI-SPs) promise flexible, on-demand practice, yet learners often report that they talk like a patient but feel different. We interviewed 12 clinical-year medical students and conducted three co-design workshops to examine how learners experience constraints of SP encounters and what they expect from AI-SPs. We identified six learner-centered needs, translated them into AI-SP design requirements, and synthesized a conceptual workflow. Our findings position AI-SPs as tools for deliberate practice and show that instructional usability, rather than conversational realism alone, drives learner trust, engagement, and educational value.

</details>


### [225] [Prompting Destiny: Negotiating Socialization and Growth in an LLM-Mediated Speculative Gameworld](https://arxiv.org/abs/2602.05864)
*Mandi Yang,Zhiqi Gao,Yibo Meng,Dongyijie Primo Pan*

Main category: cs.HC

TL;DR: LLM角色扮演游戏支持社会化、道德责任和教育角色定位的反思，通过四季节结构引导玩家帮助儿童王子处理道德情境，比较LLM在不同阶段的差异化反应。


<details>
  <summary>Details</summary>
Motivation: 将社会化理论转化为反思性AI介导游戏系统，帮助玩家理解教育指导如何随社会化进程而变化，探索教育角色定位和道德责任。

Method: 基于社会化理论设计四季节结构游戏，玩家引导儿童王子处理道德情境，LLM介导NPC提供差异化反应；隐藏实时评分，提供延迟的成长反馈作为反思提示；通过用户研究（N=12）分析游戏日志和访谈。

Result: 玩家在责任协商和角色定位方面表现出复杂互动；揭示了开放式表达与持续参与之间的"进入负荷"紧张关系；系统成功将社会化模型转化为反思性游戏体验。

Conclusion: 该研究贡献了将社会化社会学模型转化为反思性AI介导游戏系统的设计知识，为教育游戏设计提供了新视角。

Abstract: We present an LLM-mediated role-playing game that supports reflection on socialization, moral responsibility, and educational role positioning. Grounded in socialization theory, the game follows a four-season structure in which players guide a child prince through morally charged situations and compare the LLM-mediated NPC's differentiated responses across stages, helping them reason about how educational guidance shifts with socialization. To approximate real educational contexts and reduce score-chasing, the system hides real-time evaluative scores and provides delayed, end-of-stage growth feedback as reflective prompts. We conducted a user study (N=12) with gameplay logs and post-game interviews, analyzed via reflexive thematic analysis. Findings show how players negotiated responsibility and role positioning, and reveal an entry-load tension between open-ended expression and sustained engagement. We contribute design knowledge on translating sociological models of socialization into reflective AI-mediated game systems.

</details>


### [226] [From Human-Human Collaboration to Human-Agent Collaboration: A Vision, Design Philosophy, and an Empirical Framework for Achieving Successful Partnerships Between Humans and LLM Agents](https://arxiv.org/abs/2602.05987)
*Bingsheng Yao,Chaoran Chen,April Yi Wang,Sherry Tongshuang Wu,Toby Jia-jun Li,Dakuo Wang*

Main category: cs.HC

TL;DR: 该研讨会旨在建立人机协作新范式的研究议程，探讨远程人类协作研究如何启发和指导人类与LLM智能体协作的设计与研究。


<details>
  <summary>Details</summary>
Motivation: LLM智能体的出现使得智能系统从"工具"角色转变为真正的人类合作者，实现新的人机协作范式。研究者希望将LLM智能体设计成类似远程人类合作者，从而能够借鉴数十年远程人类协作研究中关于信任、意识和共同基础的研究成果，同时揭示当一方或多方是AI智能体时出现的独特机遇和挑战。

Method: 举办180分钟高度互动的跨学科研讨会，汇集HCI、CSCW和AI领域的研究者。研讨会包括主题演讲、系列邀请闪电演讲，以及探索性小组设计会议，参与者将通过故事板设计新颖的人机伙伴关系范式。

Result: 研讨会旨在通过培养共享词汇和制定研究议程来启发研究社区。预期成果包括建立人机协作的基础研究议程，为协作智能体的未来发展指明方向。

Conclusion: 该研讨会标志着人机协作新时代的开端，通过借鉴远程人类协作的丰富理解，为设计和研究人类与AI智能体协作提供理论基础和实践指导，推动智能系统从工具向真正合作者的转变。

Abstract: The emergence of Large Language Model (LLM) agents enables us to build agent-based intelligent systems that move beyond the role of a "tool" to become genuine collaborators with humans, thereby realizing a novel human-agent collaboration paradigm. Our vision is that LLM agents should resemble remote human collaborators, which allows HCI researchers to ground the future exploration in decades of research on trust, awareness, and common ground in remote human collaboration, while also revealing the unique opportunities and challenges that emerge when one or more partners are AI agents. This workshop establishes a foundational research agenda for the new era by posing the question: How can the rich understanding of remote human collaboration inspire and inform the design and study of human-agent collaboration? We will bring together an interdisciplinary group from HCI, CSCW, and AI to explore this critical transition. The 180-minute workshop will be highly interactive, featuring a keynote speaker, a series of invited lightning talks, and an exploratory group design session where participants will storyboard novel paradigms of human-agent partnership. Our goal is to enlighten the research community by cultivating a shared vocabulary and producing a research agenda that charts the future of collaborative agents.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [227] [Doc2Spec: Synthesizing Formal Programming Specifications from Natural Language via Grammar Induction](https://arxiv.org/abs/2602.04892)
*Shihao Xia,Mengting He,Haomin Jia,Linhai Song*

Main category: cs.PL

TL;DR: Doc2Spec：一个多智能体框架，使用LLM从自然语言规则自动推导规范语法，然后基于该语法生成形式化规范，提高API实现与使用的合规性验证效率。


<details>
  <summary>Details</summary>
Motivation: 确保API实现和使用符合自然语言编程规则对软件正确性、安全性和可靠性至关重要。形式化验证能提供强保证但需要精确规范，而手动编写这些规范既困难又昂贵。

Method: 提出Doc2Spec多智能体框架，利用LLM从自然语言规则自动推导规范语法，然后基于诱导的语法生成形式化规范。该语法捕获关键领域知识，约束规范空间，并强制一致表示。

Result: 在三种编程语言的七个基准测试中，Doc2Spec优于无语法诱导的基线方法，并与使用手动构建语法的技术取得竞争性结果，证明了自动语法诱导在形式化自然语言规则方面的有效性。

Conclusion: Doc2Spec通过自动语法诱导显著提高了形式化规范生成的可靠性和质量，为API合规性验证提供了一种高效且可扩展的解决方案。

Abstract: Ensuring that API implementations and usage comply with natural language programming rules is critical for software correctness, security, and reliability. Formal verification can provide strong guarantees but requires precise specifications, which are difficult and costly to write manually. To address this challenge, we present Doc2Spec, a multi-agent framework that uses LLMs to automatically induce a specification grammar from natural-language rules and then generates formal specifications guided by the induced grammar. The grammar captures essential domain knowledge, constrains the specification space, and enforces consistent representations, thereby improving the reliability and quality of generated specifications. Evaluated on seven benchmarks across three programming languages, Doc2Spec outperforms a baseline without grammar induction and achieves competitive results against a technique with a manually crafted grammar, demonstrating the effectiveness of automated grammar induction for formalizing natural-language rules.

</details>


### [228] [Strong Normalisation for Asynchronous Effects](https://arxiv.org/abs/2602.05528)
*Danel Ahman,Ilja Sobolev*

Main category: cs.PL

TL;DR: 该论文研究了Ahman和Pretnar提出的异步效应演算的归一化性质，证明了移除一般递归后的演算具有强归一性，并进一步证明了在重新引入受控的中断驱动递归行为后，其顺序片段仍保持强归一性。


<details>
  <summary>Details</summary>
Motivation: Ahman和Pretnar的异步效应演算扩展了传统的代数计算效应处理，引入了基于信号和中断的异步机制。该演算能够建模抢占式多线程、可取消远程函数调用、多方应用等场景。然而，其归一化性质尚未被系统研究，这对于确保异步程序的可终止性和可靠性至关重要。

Method: 研究采用理论证明方法：首先从原始演算中移除一般递归，证明剩余演算（包括顺序和并行部分）具有强归一性；然后证明在重新引入受控的中断驱动递归行为后，顺序片段仍保持强归一性。证明结构采用Lindley和Stark的⊤⊤-提升方法的自然扩展，所有结果均在Agda中形式化验证。

Result: 1. 移除一般递归后的异步效应演算具有强归一性（包括顺序和并行部分）；2. 仅保证非常简单的异步示例的可终止性；3. 在重新引入受控的中断驱动递归行为后，顺序片段仍保持强归一性；4. 所有证明结果均在Agda中形式化验证。

Conclusion: 该研究系统分析了异步效应演算的归一化性质，证明了在适当限制递归行为的情况下，异步效应演算可以保持强归一性。这为异步效应编程提供了理论基础，确保了相关程序的可终止性。证明方法扩展了现有的效应语言归一化证明技术，形式化验证增强了结果的可靠性。

Abstract: Asynchronous effects of Ahman and Pretnar complement the conventional synchronous treatment of algebraic computational effects with asynchrony based on decoupling the execution of algebraic operation calls into signalling that an operation's implementation needs to be executed, and into interrupting a running computation with the operation's result, to which the computation can react by installing matching interrupt handlers. Beyond providing asynchrony for algebraic effects, the resulting core calculus also naturally models examples such as pre-emptive multi-threading, (cancellable) remote function calls, multi-party applications, and even a parallel variant of runners of algebraic effects. In this paper, we study the normalisation properties of this calculus. We prove that if one removes general recursion from the original calculus, then the remaining calculus is strongly normalising, including both its sequential and parallel parts. However, this only guarantees termination for very simple asynchronous examples. To improve on this result, we also prove that the sequential fragment of the calculus remains strongly normalising when a controlled amount of interrupt-driven recursive behaviour is reintroduced. Our strong normalisation proofs are structured compositionally as a natural extension of Lindley and Stark's $\top\top$-lifting based approach for proving strong normalisation of effectful languages. All our results are also formalised in Agda.

</details>


### [229] [An Equational Axiomatization of Dynamic Threads via Algebraic Effects: Presheaves on Finite Relations, Labelled Posets, and Parameterized Algebraic Theories](https://arxiv.org/abs/2602.05850)
*Ohad Kammar,Jack Liell-Cock,Sam Lindley,Cristina Matache,Sam Staton*

Main category: cs.PL

TL;DR: 基于代数效应理论为动态线程提供完整的等式公理化，使用参数化代数理论作为框架，以fork和wait为核心原语，证明模型完备性和语法完备性，并扩展到简单并发语言的语义分析。


<details>
  <summary>Details</summary>
Motivation: 为动态线程提供完整的等式公理化，使用代数效应理论将并发语义分析聚焦于fork和wait的代数操作，避免复杂的底层细节，为并发编程提供坚实的理论基础。

Method: 采用参数化代数理论作为具体语法框架，构建基于fork和wait原语的参数化代数理论，包含基本原子操作和结合性等定律，通过代数效应方法进行语义分析。

Result: 实现了两个完备性：1) 模型完备性：闭表达式完全捕获标记偏序集(pomsets)的相等性；2) 语法完备性：开表达式在所有闭替换下相等时可证明相等。扩展应用到简单并发语言，建立了操作语义和指称语义，证明其健全性、充分性和一阶完全抽象性。

Conclusion: 代数效应理论为动态线程提供了有效的等式公理化框架，参数化代数理论是处理名称和绑定的便利工具，所提出的方法在理论和实践上都具有重要意义，为并发编程语言设计提供了理论基础。

Abstract: We use the theory of algebraic effects to give a complete equational axiomatization for dynamic threads. Our method is based on parameterized algebraic theories, which give a concrete syntax for strong monads on functor categories, and are a convenient framework for names and binding. Our programs are built from the key primitives `fork' and `wait'. `Fork' creates a child thread and passes its name (thread ID) to the parent thread. `Wait' allows us to wait for given child threads to finish. We provide a parameterized algebraic theory built from fork and wait, together with basic atomic actions and laws such as associativity of `fork'. Our equational axiomatization is complete in two senses. First, for closed expressions, it completely captures equality of labelled posets (pomsets), an established model of concurrency: model complete. Second, any two open expressions are provably equal if they are equal under all closing substitutions: syntactically complete. The benefit of algebraic effects is that the semantic analysis can focus on the algebraic operations of fork and wait. We then extend the analysis to a simple concurrent programming language by giving operational and denotational semantics. The denotational semantics is built using the methods of parameterized algebraic theories and we show that it is sound, adequate, and fully abstract at first order for labelled-poset observations.

</details>
