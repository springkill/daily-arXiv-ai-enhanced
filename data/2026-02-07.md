<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.RO](#cs.RO) [Total: 16]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.LG](#cs.LG) [Total: 115]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.CR](#cs.CR) [Total: 21]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Repairing Property Graphs under PG-Constraints](https://arxiv.org/abs/2602.05503)
*Christopher Spinrath,Angela Bonifati,Rachid Echahed*

Main category: cs.DB

TL;DR: 研究属性图在PG-Constraints约束下的修复问题，提出包含整数线性规划、朴素算法和LP引导贪心算法的三种修复策略，实验显示标签删除比节点/边删除减少59%删除量，LP引导贪心算法比ILP快97%且质量相当。


<details>
  <summary>Details</summary>
Motivation: 随着图数据库标准化发展，出现了GQL、SQL/PGQ等标准查询语言和PG-Constraints约束语言。目前缺乏对属性图在PG-Constraints约束下修复问题的系统性研究，需要开发有效的修复方法来处理违反约束的图数据。

Method: 识别PG-Constraints的一个重要子集，该子集编码了否定约束并包含递归特性，同时允许基于自动机的结构错误分析。提出完整的修复流水线，涉及图拓扑结构变更，包括节点、边和可选的标签删除。研究三种修复策略：基于整数线性规划(ILP)、朴素算法和LP引导贪心算法。

Result: 实验表明，使用标签删除相比节点/边删除可以减少59%的删除操作。LP引导贪心算法相比ILP策略在运行时间上具有高达97%的优势，同时保持相同的修复质量。

Conclusion: 该研究为属性图在PG-Constraints约束下的修复提供了系统性的解决方案，证明了标签删除策略的有效性和LP引导贪心算法在效率上的显著优势，为图数据库的数据质量维护提供了实用工具。

Abstract: Recent standardization efforts for graph databases lead to standard query languages like GQL and SQL/PGQ, and constraint languages like Property Graph Constraints (PG-Constraints). In this paper, we embark on the study of repairing property graphs under PG-Constraints. We identify a significant subset of PG-Constraints, encoding denial constraints and including recursion as a key feature, while still permitting automata-based structural analyses of errors. We present a comprehensive repair pipeline for these constraints to repair Property Graphs, involving changes in the graph topology and leading to node, edge and, optionally, label deletions. We investigate three algorithmic strategies for the repair procedure, based on Integer Linear Programming (ILP), a naive, and an LP-guided greedy algorithm. Our experiments on various real-world datasets reveal that repairing with label deletions can achieve a 59% reduction in deletions compared to node/edge deletions. Moreover, the LP-guided greedy algorithm offers a runtime advantage of up to 97% compared to the ILP strategy, while matching the same quality.

</details>


### [2] [Taking the Leap: Efficient and Reliable Fine-Grained NUMA Migration in User-space](https://arxiv.org/abs/2602.05540)
*Felix Schuhknecht,Nick Rassau*

Main category: cs.DB

TL;DR: 提出page_leap()用户空间页面迁移方法，解决NUMA架构下数据放置与查询并行化不匹配问题，相比现有方案提供更高性能和更完整功能集


<details>
  <summary>Details</summary>
Motivation: 现代多插槽架构提供单一虚拟地址空间但物理内存分布在多个NUMA区域。当查询在多个区域的核上并行执行时，需要相应分布数据以确保本地访问。现有迁移方案（Linux自动NUMA平衡和move_pages()系统调用）在功能和性能上存在显著不足。

Method: 提出page_leap()用户空间迁移方法，通过利用虚拟内存子系统特性实现异步高性能页面迁移。该方法具有以下特性：(a)用户主动触发，(b)确保所有页面最终迁移，(c)正确处理并发写入，(d)支持池化内存，(e)基于工作负载自适应调整迁移粒度，(f)支持小页面和大页面。

Result: page_leap()方法在性能和功能上优于现有方案，能够有效解决NUMA架构下的数据放置问题，提升查询并行化效率。

Conclusion: page_leap()为NUMA架构提供了一种高效、功能完整的用户空间页面迁移方案，解决了现有方法的局限性，能够显著改善多区域并行查询的数据局部性。

Abstract: Modern multi-socket architectures offer a single virtual address space, but physically divide main-memory across multiple regions, where each region is attached to a CPU and its cores. While this simplifies the usage, developers must be aware of non-uniform memory access (NUMA), where an access by a thread running on a core-local NUMA region is significantly cheaper than an access from a core-remote region. Obviously, if query answering is parallelized across the cores of multiple regions, then the portion of the database on which the query is operating should be distributed across the same regions to ensure local accesses. As the present data placement might not fit this, migrating pages from one NUMA region to another can be performed to improve the situation. To do so, different options exist: One option is to rely on automatic NUMA balancing integrated in Linux, which is steered by the observed access patterns and frequency. Another option is to actively trigger migration via the system call move_pages(). Unfortunately, both variants have significant downsides in terms of their feature set and performance. As an alternative, we propose a new user-space migration method called page_leap() that can perform page migration asynchronously at a high performance by exploiting features of the virtual memory subsystem. The method is (a) actively triggered by the user, (b) ensures that all pages are eventually migrated, (c) handles concurrent writes correctly, (d) supports pooled memory, (e) adaptively adjusts its migration granularity based on the workload, and (f) supports both small pages and huge pages.

</details>


### [3] ["Detective Work We Shouldn't Have to Do": Practitioner Challenges in Regulatory-Aligned Data Quality in Machine Learning Systems](https://arxiv.org/abs/2602.05944)
*Yichun Wang,Kristina Irion,Paul Groth,Hazar Harmouch*

Main category: cs.DB

TL;DR: 该研究通过访谈欧盟数据从业者，探讨了机器学习系统中数据质量监管要求与工程实践之间的差距，揭示了法律原则与工作流程脱节、工具不足、责任边界模糊等问题。


<details>
  <summary>Details</summary>
Motivation: 随着欧盟GDPR和AI法案等监管框架对机器学习系统数据质量提出要求，这些法律要求与ML技术实践存在紧密关联，但法律原则与工程工作流程之间可能存在差距，需要研究从业者如何理解和应对这些监管要求。

Method: 采用定性访谈研究方法，对欧盟地区在受监管环境中从事机器学习系统的数据从业者进行半结构化访谈，探讨他们如何理解监管要求下的数据质量、面临的挑战以及需要的支持。

Result: 研究发现存在法律原则与工程工作流程之间的持续差距、数据管道碎片化、现有工具局限性、技术团队与法律团队责任边界不清晰、以及倾向于被动审计驱动的质量实践等问题。同时识别出从业者对合规感知工具、更清晰治理结构和向主动数据治理文化转变的需求。

Conclusion: 机器学习系统中的数据质量监管要求与工程实践之间存在显著脱节，需要开发合规感知工具、建立明确治理结构，并推动从被动审计到主动数据治理的文化转变，以弥合法律要求与技术实施之间的差距。

Abstract: Ensuring data quality in machine learning (ML) systems has become increasingly complex as regulatory requirements expand. In the European Union (EU), frameworks such as the General Data Protection Regulation (GDPR) and the Artificial Intelligence Act (AI Act) articulate data quality requirements that closely parallel technical concerns in ML practice, while also extending to legal obligations related to accountability, risk management, and human rights protection. This paper presents a qualitative interview study with EU-based data practitioners working on ML systems in regulated contexts. Through semi-structured interviews, we investigate how practitioners interpret regulatory-aligned data quality, the challenges they encounter, and the supports they identify as necessary. Our findings reveal persistent gaps between legal principles and engineering workflows, fragmentation across data pipelines, limitations of existing tools, unclear responsibility boundaries between technical and legal teams, and a tendency toward reactive, audit-driven quality practices. We also identify practitioners' needs for compliance-aware tooling, clearer governance structures, and cultural shifts toward proactive data governance.

</details>


### [4] [Fast Private Adaptive Query Answering for Large Data Domains](https://arxiv.org/abs/2602.05674)
*Miguel Fuentes,Brett Mullins,Yingtai Xiao,Daniel Kifer,Cameron Musco,Daniel Sheldon*

Main category: cs.DB

TL;DR: 提出AIM+GReM机制，通过残差查询和自适应优化改进差分隐私边际发布，显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私边际发布机制在从噪声测量重建边际估计时存在计算瓶颈，需要更高效的解决方案

Method: 提出基于多维数组的残差查询概念框架、惰性更新策略和自适应隐私预算分配，将残差查询集成到AIM等自适应机制中

Result: AIM+GReM机制比原始框架快几个数量级，具有竞争性误差和显著改善的可扩展性

Conclusion: 残差查询与自适应机制的集成有效解决了差分隐私边际发布的计算瓶颈问题，实现了高效、准确的隐私保护数据发布

Abstract: Privately releasing marginals of a tabular dataset is a foundational problem in differential privacy. However, state-of-the-art mechanisms suffer from a computational bottleneck when marginal estimates are reconstructed from noisy measurements. Recently, residual queries were introduced and shown to lead to highly efficient reconstruction in the batch query answering setting. We introduce new techniques to integrate residual queries into state-of-the-art adaptive mechanisms such as AIM. Our contributions include a novel conceptual framework for residual queries using multi-dimensional arrays, lazy updating strategies, and adaptive optimization of the per-round privacy budget allocation. Together these contributions reduce error, improve speed, and simplify residual query operations. We integrate these innovations into a new mechanism (AIM+GReM), which improves AIM by using fast residual-based reconstruction instead of a graphical model approach. Our mechanism is orders of magnitude faster than the original framework and demonstrates competitive error and greatly improved scalability.

</details>


### [5] [Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2602.04926)
*Ning Wang,Kuanyan Zhu,Daniel Yuehwoon Yee,Yitang Gao,Shiying Huang,Zirun Xu,Sainyam Galhotra*

Main category: cs.DB

TL;DR: AutoPrunedRetriever：一种图式RAG系统，通过持久化最小推理子图并增量扩展，减少重复检索和推理，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统对每个查询都重新检索长段落并从头推理，导致token消耗、延迟和成本增加。需要一种能够持久化推理结构并增量扩展的系统来提高效率。

Method: 提出AutoPrunedRetriever系统，将实体和关系存储在紧凑的ID索引码本中，将问题、事实和答案表示为边序列。采用两层整合策略（快速ANN/KNN别名检测+选择性k-means）保持图紧凑，并剪枝低价值结构。实现两个前端：使用REBEL三元组解析器的AutoPrunedRetriever-REBEL和使用LLM提取器的AutoPrunedRetriever-llm。

Result: 在GraphRAG-Benchmark（医学和小说领域）上，两个变体均达到最先进的复杂推理准确率，比HippoRAG2提高约9-11个百分点。在STEM和TV基准测试中排名第一，同时使用的token数量比基于图的基线少两个数量级。

Conclusion: AutoPrunedRetriever通过持久化最小推理子图并增量扩展，显著提高了RAG系统的效率和实用性，适用于长期运行会话、演化语料库和多智能体管道。

Abstract: Retrieval-augmented generation (RAG) is now standard for knowledge-intensive LLM tasks, but most systems still treat every query as fresh, repeatedly re-retrieving long passages and re-reasoning from scratch, inflating tokens, latency, and cost. We present AutoPrunedRetriever, a graph-style RAG system that persists the minimal reasoning subgraph built for earlier questions and incrementally extends it for later ones. AutoPrunedRetriever stores entities and relations in a compact, ID-indexed codebook and represents questions, facts, and answers as edge sequences, enabling retrieval and prompting over symbolic structure instead of raw text. To keep the graph compact, we apply a two-layer consolidation policy (fast ANN/KNN alias detection plus selective $k$-means once a memory threshold is reached) and prune low-value structure, while prompts retain only overlap representatives and genuinely new evidence. We instantiate two front ends: AutoPrunedRetriever-REBEL, which uses REBEL as a triplet parser, and AutoPrunedRetriever-llm, which swaps in an LLM extractor. On GraphRAG-Benchmark (Medical and Novel), both variants achieve state-of-the-art complex reasoning accuracy, improving over HippoRAG2 by roughly 9--11 points, and remain competitive on contextual summarize and generation. On our harder STEM and TV benchmarks, AutoPrunedRetriever again ranks first, while using up to two orders of magnitude fewer tokens than graph-heavy baselines, making it a practical substrate for long-running sessions, evolving corpora, and multi-agent pipelines.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [6] [Signal or 'Noise': Human Reactions to Robot Errors in the Wild](https://arxiv.org/abs/2602.05010)
*Maia Stiber,Sameer Khan,Russell Taylor,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 研究探索了在真实世界咖啡机器人部署中，人们对机器人错误的社交信号反应，发现这些信号丰富但"嘈杂"，特别是在群体互动中。


<details>
  <summary>Details</summary>
Motivation: 虽然社交信号在受控交互中被证明对错误管理可靠有用，但在真实世界中，特别是在非社交机器人、重复互动、群体互动以及连续或传播性错误的情况下，这些信号是否仍然有效尚不清楚。

Method: 构建了一个咖啡机器人，并在公共场域进行了实地部署（N=49），观察参与者在错误和其他刺激下的社交信号表达。

Result: 参与者对错误和其他刺激持续表达了多样化的社交信号，特别是在群体互动中。真实世界中的社交信号丰富（参与者主动提供交互信息）但"嘈杂"。

Conclusion: 研究讨论了在真实世界人机交互中使用社交信号的教训、益处和挑战，为理解非实验室环境下人们对机器人错误的反应提供了实证依据。

Abstract: In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but "noisy." We discuss lessons, benefits, and challenges for using social signals in real-world HRI.

</details>


### [7] [Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping](https://arxiv.org/abs/2602.05029)
*Octavio Arriaga,Proneet Sharma,Jichen Guo,Marc Otto,Siddhant Kadwe,Rebecca Adam*

Main category: cs.RO

TL;DR: 提出一种可微分神经图形模型，结合神经基础模型与物理可微分渲染，实现零样本场景重建与机器人抓取，无需额外3D数据或测试样本。


<details>
  <summary>Details</summary>
Motivation: 当前最先进模型依赖大量训练数据和测试样本构建黑盒场景表示，限制了机器人在新环境中的操作能力。需要更数据高效、可解释且泛化性强的零样本方法。

Method: 结合神经基础模型与物理可微分渲染，通过求解一系列约束优化问题，从单张RGBD图像和边界框估计物理一致的场景参数（网格、光照、材质、6D位姿）。

Result: 在标准模型无关少样本基准测试中超越现有算法；通过零样本抓取任务验证了场景重建的准确性。

Conclusion: 该方法为新型环境中实现更数据高效、可解释且泛化性强的机器人自主性提供了途径，无需依赖大量数据集或测试采样。

Abstract: Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.

</details>


### [8] [CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction](https://arxiv.org/abs/2602.06038)
*Xiaopan Zhang,Zejin Wang,Zhixu Li,Jianpeng Yao,Jiachen Li*

Main category: cs.RO

TL;DR: 论文提出CommCP框架，用于解决多智能体多任务具身问答问题，通过LLM驱动的去中心化通信和保形预测校准消息，提高任务成功率和探索效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器人需要协作完成自然语言指令，但现有研究缺乏对多异构机器人协同信息收集的正式建模。传统具身问答主要关注单智能体，而多智能体场景中有效通信对避免冗余和协调行动至关重要。

Method: 提出CommCP框架：1) 将多智能体多任务具身问答(MM-EQA)形式化为新问题；2) 采用基于LLM的去中心化通信架构；3) 引入保形预测技术校准生成的消息，减少接收者干扰并提高通信可靠性。

Result: 实验表明CommCP在提出的MM-EQA基准测试中显著优于基线方法，提高了任务成功率和探索效率。基准包含多样化的照片级真实家庭场景和具身问题。

Conclusion: CommCP框架有效解决了多智能体协作中的通信协调问题，保形预测的消息校准机制提升了通信可靠性，为异构机器人协同完成复杂任务提供了可行方案。

Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.

</details>


### [9] [Ontology-Driven Robotic Specification Synthesis](https://arxiv.org/abs/2602.05456)
*Maksym Figat,Ryan M. Mackey,Michel D. Ingham*

Main category: cs.RO

TL;DR: 提出RSTM2方法，通过本体驱动、分层的随机时间Petri网建模，将高层目标转化为可执行规范，支持多机器人系统的架构权衡、资源分配和不确定性下的性能分析。


<details>
  <summary>Details</summary>
Motivation: 解决安全关键和任务关键机器人应用中高层目标与形式化可执行规范之间的鸿沟，为复杂多机器人系统（如NASA CADRE任务）提供系统工程技术。

Method: 提出RSTM2方法，采用本体驱动、分层的方法，使用带资源的随机时间Petri网建模，支持任务、系统和子系统层面的蒙特卡洛仿真，并利用本体概念实现可解释的AI辅助规范合成。

Result: 通过假设案例研究展示了RSTM2方法如何支持架构权衡、资源分配和不确定性下的性能分析，特别适用于去中心化、资源感知和自适应的多机器人系统。

Conclusion: RSTM2方法为复杂多机器人系统提供了有效的系统工程方法，能够将高层目标转化为可执行规范，支持自主规范合成，对未来自主系统开发具有重要意义。

Abstract: This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.

</details>


### [10] [Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework](https://arxiv.org/abs/2602.05310)
*Jipeng Kong,Xinzhe Liu,Yuhang Lin,Jinrui Han,Sören Schwertfeger,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: PAiD提出了一种渐进式架构，通过三个阶段分解足球技能获取：运动技能获取、轻量级感知-动作整合和物理感知的仿真到现实迁移，实现了人形机器人稳定高效的足球技能学习。


<details>
  <summary>Details</summary>
Motivation: 足球对人形机器人提出了重大挑战，需要紧密集成的感知-动作能力。现有方法存在模块化管道中的模块间不稳定性或端到端框架中的训练目标冲突问题。

Method: PAiD采用渐进式架构，将足球技能获取分解为三个阶段：1) 通过人体运动跟踪获取运动技能；2) 轻量级感知-动作整合实现位置泛化；3) 物理感知的仿真到现实迁移。

Result: 在Unitree G1机器人上的实验展示了高保真的人形踢球能力，在静态或滚动球、不同位置和干扰等多样化条件下均表现稳健，在室内外场景中保持一致的执行效果。

Conclusion: 这种分而治之的策略推进了稳健的人形足球能力，并为复杂具身技能获取提供了一个可扩展的框架。

Abstract: Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.

</details>


### [11] [Benchmarking Affordance Generalization with BusyBox](https://arxiv.org/abs/2602.05441)
*Dean Fortier,Timothy Adamson,Tess Hellebrekers,Teresa LaScala,Kofi Ennin,Michael Murray,Andrey Kolobov,Galen Mullins*

Main category: cs.RO

TL;DR: BusyBox是一个用于评估VLA模型物理操作泛化能力的基准测试平台，包含6个可互换模块，通过3D打印和开源设计便于实验室构建


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在视觉和语言空间泛化方面取得进展，但缺乏对物理操作泛化能力（affordance generalization）的系统评估方法，需要专门基准来测试模型处理新物体但熟悉物理特征的能力

Method: 设计包含开关、滑块、电线、按钮、显示屏和旋钮的6模块物理系统，模块可互换旋转创建多种变体；提供3D打印CAD文件、材料清单和Mobile Aloha机器人演示数据集

Result: BusyBox变体间的泛化对强大开源VLA模型（如π₀.₅和GR00T-N1.6）仍具挑战性；平台设计便于实验室构建，促进社区评估

Conclusion: BusyBox为系统评估VLA模型物理操作泛化能力提供了标准化基准，通过开源硬件设计和演示数据集推动该领域研究发展

Abstract: Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.
  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.

</details>


### [12] [DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter](https://arxiv.org/abs/2602.05513)
*Xukun Li,Yu Sun,Lei Zhang,Bosheng Huang,Yibo Peng,Yuan Meng,Haojun Jiang,Shaoxuan Xie,Guacai Yao,Alois Knoll,Zhenshan Bing,Xinlong Wang,Zhenguo Sun*

Main category: cs.RO

TL;DR: DECO是一个基于DiT的策略框架，通过解耦多模态条件实现机器人操作，并配套发布了包含触觉感知的双手机器人灵巧操作数据集DECO-50。


<details>
  <summary>Details</summary>
Motivation: 现有机器人策略在处理多模态输入（视觉、动作、本体感知、触觉等）时存在耦合问题，需要一种能够有效解耦不同模态条件的方法来提升机器人灵巧操作的性能。

Method: DECO采用基于DiT的策略架构，通过联合自注意力处理图像和动作令牌，通过自适应层归一化注入本体感知状态和可选条件，通过交叉注意力注入触觉信号，并使用轻量级LoRA适配器高效微调预训练策略。

Result: 提出了DECO框架并配套发布了DECO-50数据集，该数据集包含4个场景、28个子任务，涵盖超过50小时数据、约500万帧图像和8000条成功轨迹，为双手机器人灵巧操作研究提供了重要资源。

Conclusion: DECO框架通过解耦多模态条件实现了有效的机器人策略学习，DECO-50数据集为机器人灵巧操作研究提供了具有触觉感知的丰富数据资源，推动了该领域的发展。

Abstract: Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.

</details>


### [13] [TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards](https://arxiv.org/abs/2602.05596)
*Hokyun Lee,Woo-Jeong Baek,Junhyeok Cha,Jaeheung Park*

Main category: cs.RO

TL;DR: TOLEBI：首个基于学习的双足机器人容错框架，通过模拟故障训练和在线关节状态监测实现硬件故障下的鲁棒行走


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在双足机器人运动控制中取得高成功率，但缺乏处理硬件故障（如关节锁定、电源故障）和环境干扰的方法。真实环境中这些故障可能导致严重后果，需要开发容错控制框架。

Method: 提出TOLEBI框架：1）在仿真中注入关节锁定、电源故障和外部干扰，训练容错运动策略；2）通过仿真到真实迁移将策略部署到实体机器人；3）集成在线关节状态模块，运行时根据实际观测分类关节状态。

Result: 在人形机器人TOCABI上进行了仿真和真实世界验证实验，证明了方法的适用性。这是首个基于学习的双足机器人容错框架。

Conclusion: TOLEBI框架成功实现了双足机器人在硬件故障下的容错行走，推动了该领域高效学习方法的发展，为真实环境中的鲁棒机器人控制提供了解决方案。

Abstract: With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.

</details>


### [14] [HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments](https://arxiv.org/abs/2602.05608)
*Yufei Zhu,Shih-Min Yang,Martin Magnusson,Allan Wang*

Main category: cs.RO

TL;DR: HiCrowd：一种分层框架，结合强化学习和模型预测控制，利用行人运动作为引导来解决机器人在密集人群中的冻结问题，提高导航效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在密集人群导航中的"冻结机器人问题"——机器人因找不到安全运动路径而被困在人群中。现有方法将行人仅视为动态障碍物，而HiCrowd提出利用行人运动作为引导来与兼容的人群流对齐。

Method: 提出HiCrowd分层框架：高层RL策略生成跟随点，使机器人与合适的人群组对齐；低层MPC通过短期规划安全跟踪该引导。结合长期人群感知决策与安全短期执行。

Result: 在真实世界数据集和合成人群数据集上的实验表明，HiCrowd在导航效率和安全性方面优于反应式和基于学习的基线方法，同时减少了冻结行为。在离线（重放记录轨迹）和在线（行人轨迹响应机器人）设置中均表现优异。

Conclusion: 将人类运动作为引导而非仅视为动态障碍物，为机器人在人群中的安全高效导航提供了有力原则。HiCrowd的分层框架成功解决了冻结机器人问题，实现了长期决策与短期安全执行的结合。

Abstract: Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.

</details>


### [15] [From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking](https://arxiv.org/abs/2602.05683)
*Chuwei Wang,Eduardo Sebastián,Amanda Prorok,Anastasia Bizyaeva*

Main category: cs.RO

TL;DR: 提出一种简约的神经形态控制框架，通过动态分岔机制解决视觉导航中的决策困境，将视觉目标激励直接转换为自我中心运动指令


<details>
  <summary>Details</summary>
Motivation: 机器人导航长期面临反应式传感器控制与基于模型规划器之间的协调问题，特别是在目标选项缺乏主导性时，反应式系统难以打破对称性而陷入决策困境，需要避免计算密集型规划器

Method: 提出简约的神经形态控制框架，将机载摄像头图像像素编码为动态神经元群输入，直接将视觉目标激励转换为自我中心运动指令；采用动态分岔机制延迟决策直到环境几何诱导的临界点，受动物认知和意见动态机制模型启发

Result: 在仿真环境和实验四旋翼平台上验证了该方法，实现了实时自主性，计算负担小，参数少且可解释，可无缝集成到特定应用的图像处理流程中

Conclusion: 该神经形态控制器有效弥合了反应式控制与模型规划之间的鸿沟，为视觉引导导航和跟踪提供了计算高效、参数简约的解决方案

Abstract: Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.

</details>


### [16] [Scalable and General Whole-Body Control for Cross-Humanoid Locomotion](https://arxiv.org/abs/2602.05791)
*Yufei Xue,YunFeng Lin,Wentao Dong,Yang Tang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Minghuan Liu,Weinan Zhang*

Main category: cs.RO

TL;DR: 提出XHugWBC框架，实现单次训练即可跨不同人形机器人设计的通用控制策略


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的人形机器人全身控制器大多需要针对特定机器人进行训练，缺乏跨不同机器人设计的通用控制能力

Method: XHugWBC框架包含三个核心组件：(1)物理一致的形态随机化，(2)跨不同人形机器人的语义对齐观测和动作空间，(3)建模形态和动力学特性的有效策略架构

Result: 在12个模拟人形机器人和7个真实机器人上的实验表明，该通用控制器具有强大的泛化能力和鲁棒性

Conclusion: 通过一次训练学习到的策略能够零样本迁移到未见过的机器人上，实现了真正意义上的跨具身人形控制

Abstract: Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.

</details>


### [17] [A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion](https://arxiv.org/abs/2602.05855)
*Dennis Bank,Joost Cordes,Thomas Seel,Simon F. G. Ehlers*

Main category: cs.RO

TL;DR: 提出基于学习的多模态地形感知框架，使用机器人中心高度图表示，通过CNN-GRU混合编码器-解码器结构融合深度相机、LiDAR和IMU数据，提升地形重建精度并减少映射漂移。


<details>
  <summary>Details</summary>
Motivation: 传统人形机器人地形感知系统通常依赖手动设计的单传感器流水线，在非结构化、以人为中心的环境中可靠性有限。需要开发更鲁棒、多模态融合的学习框架来提升地形感知的准确性和时间一致性。

Method: 提出基于学习的框架，使用机器人中心高度图作为中间表示。采用混合编码器-解码器结构：CNN用于空间特征提取，GRU核心用于时间一致性。融合Intel RealSense深度相机、LIVOX MID-360 LiDAR（通过高效球面投影处理）和机载IMU的多模态数据。

Result: 多模态融合相比仅使用深度相机配置提升7.2%的重建精度，相比仅使用LiDAR配置提升9.9%。集成3.2秒时间上下文有效减少了映射漂移。

Conclusion: 提出的学习型多模态融合框架显著提升了人形机器人在非结构化环境中的地形感知能力，证明了多传感器融合和时间一致性处理对于可靠地形重建的重要性。

Abstract: Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.

</details>


### [18] [Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools](https://arxiv.org/abs/2602.05895)
*Qi Li,Karsten Berns*

Main category: cs.RO

TL;DR: 提出一种残差强化学习方法，将名义笛卡尔控制器与学习残差策略相结合，用于液压装载起重机在废物容器回收任务中的容器提升阶段，提高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中废物容器回收任务中，液压装载起重机配备欠驱动卸料单元进行容器提升时，存在几何公差小、需要精确轨迹跟踪和摆动抑制的挑战，需要提高控制精度和鲁棒性。

Method: 采用残差强化学习(RRL)方法，结合名义笛卡尔控制器（使用导纳控制进行轨迹跟踪和摆锤感知摆动阻尼，然后通过阻尼最小二乘逆运动学生成关节速度命令）和PPO训练的残差策略，补偿未建模动力学和参数变化。使用随机化初始化和领域随机化增强泛化能力。

Result: 仿真结果表明，相比单独使用名义控制器，该方法提高了跟踪精度、减少了振荡、获得了更高的提升成功率。

Conclusion: 残差强化学习方法能有效提高液压装载起重机在容器提升任务中的控制性能，无需从头开始端到端学习，通过结合名义控制器和学习残差策略实现了更好的精度和鲁棒性。

Abstract: This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.

</details>


### [19] [From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits](https://arxiv.org/abs/2602.05922)
*Aziz Mohamed Mili,Louis Catar,Paul Gérard,Ilyass Tabiai,David St-Onge*

Main category: cs.RO

TL;DR: 开发了一个端到端的开源工具链，将台式冲击测试转化为可部署的无人机安全控制器，通过数据驱动模型将冲击前速度映射到冲量和接触时间，从而计算满足特定力限制的速度边界。


<details>
  <summary>Details</summary>
Motivation: 室内微型飞行器(MAV)越来越多地用于需要靠近人员的任务，但从业者缺乏基于实测冲击风险来调整运动限制的实用方法。需要建立从冲击测试到运行时限制的实用桥梁。

Method: 1. 设计紧凑可复制的冲击测试装置和协议，捕捉不同无人机类别和接触表面的力-时间曲线；2. 提供数据驱动模型，将冲击前速度映射到冲量和接触时间，直接计算目标力限制下的速度边界；3. 发布脚本和ROS2节点，在线执行这些边界并记录合规性，支持特定设施策略。

Result: 在多个商用现成四旋翼无人机和代表性室内资产上验证了工作流程，证明推导的控制器在满足安全利益相关者指定的力约束的同时，保持了任务吞吐量。

Conclusion: 该研究提供了一个实用的桥梁，将实测冲击转化为运行时限制，包含可共享的数据集、代码和可重复的流程，团队可以采用这些来认证室内MAV在人类附近的操作。

Abstract: Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.

</details>


### [20] [Visuo-Tactile World Models](https://arxiv.org/abs/2602.06001)
*Carolina Higuera,Sergio Arnaud,Byron Boots,Mustafa Mukadam,Francois Robert Hogan,Franziska Meier*

Main category: cs.RO

TL;DR: VT-WM通过结合视觉和触觉感知构建多任务世界模型，在接触丰富的操作任务中提升物理保真度，改善物体恒存性和运动规律遵循，并在零样本机器人实验中实现更高成功率。


<details>
  <summary>Details</summary>
Motivation: 解决仅依赖视觉的模型在接触丰富任务中的局限性，如遮挡、模糊接触状态导致的物体消失、瞬移或违反物理规律的运动，通过触觉感知补充视觉信息以更好理解机器人-物体交互。

Method: 提出多任务视觉-触觉世界模型(VT-WM)，结合视觉和触觉传感捕捉接触物理特性。模型在多个接触丰富的操作任务上进行训练，通过触觉推理增强对接触动力学的理解，支持自回归推演和规划。

Result: VT-WM在物理保真度方面显著提升：物体恒存性提高33%，运动规律遵循性提高29%。零样本真实机器人实验中成功率提升达35%，在接触丰富的多步任务中增益最大。模型还展现出下游任务适应性，仅需少量演示即可可靠规划新任务。

Conclusion: 结合视觉和触觉感知的世界模型能更准确地捕捉接触物理特性，提升机器人操作任务的物理保真度和规划性能，特别是在接触丰富的场景中具有显著优势，并展现出良好的任务适应性。

Abstract: We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.

</details>


### [21] [Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking](https://arxiv.org/abs/2602.05079)
*Vinal Asodia,Iman Sharifi,Saber Fallah*

Main category: cs.RO

TL;DR: 提出神经符号特征表示和软一阶逻辑奖励函数，提升自动驾驶强化学习的场景理解和安全性


<details>
  <summary>Details</summary>
Motivation: 现有基于摄像头的深度强化学习方法存在两个主要问题：1) 很少将高层场景上下文整合到特征表示中；2) 依赖僵化、固定的奖励函数。这些问题限制了自动驾驶系统的场景感知能力和安全决策

Method: 提出神经符号特征表示管道，包含语义、空间和形状信息，以及场景中动态实体的空间增强特征，特别关注安全关键的道路使用者。同时提出软一阶逻辑(SFOL)奖励函数，通过符号推理模块平衡人类价值观。从分割图中提取语义和空间谓词，应用于语言规则以获得奖励权重

Result: 在CARLA仿真环境中的定量实验表明，与基线表示和奖励公式相比，所提出的神经符号表示和SFOL奖励函数在不同交通密度和遮挡水平下提高了策略鲁棒性和安全相关性能指标

Conclusion: 将整体表示和软推理整合到强化学习中，可以支持自动驾驶中更具上下文感知能力和价值对齐的决策制定

Abstract: The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [22] [Task-Adaptive Physical Reservoir Computing via Tunable Molecular Communication Dynamics](https://arxiv.org/abs/2602.05931)
*Saad Yousuf,Kaan Burak Ikiz,Murat Kuscu*

Main category: cs.ET

TL;DR: 分子通信信道可作为可重构的物理储备池计算系统，通过调节生物物理参数优化不同计算任务性能


<details>
  <summary>Details</summary>
Motivation: 现有物理储备池计算实现大多是静态的，性能局限于狭窄的任务范围，需要开发可重构、任务自适应的物理计算系统

Method: 采用双模拟方法：计算高效的确定性平均场模型和高保真粒子随机模型(Smoldyn)，结合贝叶斯优化在高维参数空间中识别离散操作机制

Result: 发现明确权衡关系：富含信道记忆的参数集在混沌时间序列预测任务中表现优异，而促进强受体非线性的机制在非线性数据转换方面更优；后处理方法可减轻分子噪声提升随机储备池性能

Conclusion: 分子通信信道不仅是计算基底，更是可调谐、生物启发计算系统的设计蓝图，为未来湿件AI实现提供了明确的优化框架

Abstract: Physical Reservoir Computing (PRC) offers an efficient paradigm for processing temporal data, yet most physical implementations are static, limiting their performance to a narrow range of tasks. In this work, we demonstrate in silico that a canonical Molecular Communication (MC) channel can function as a highly versatile and task-adaptive PRC whose computational properties are reconfigurable. Using a dual-simulation approach -- a computationally efficient deterministic mean-field model and a high-fidelity particle-based stochastic model (Smoldyn) -- we show that tuning the channel's underlying biophysical parameters, such as ligand-receptor kinetics and diffusion dynamics, allows the reservoir to be optimized for distinct classes of computation. We employ Bayesian optimization to efficiently navigate this high-dimensional parameter space, identifying discrete operational regimes. Our results reveal a clear trade-off: parameter sets rich in channel memory excel at chaotic time-series forecasting tasks (e.g., Mackey Glass), while regimes that promote strong receptor nonlinearity are superior for nonlinear data transformation. We further demonstrate that post-processing methods improve the performance of the stochastic reservoir by mitigating intrinsic molecular noise. These findings establish the MC channel not merely as a computational substrate, but as a design blueprint for tunable, bioinspired computing systems, providing a clear optimization framework for future wetware AI implementations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [A Causal Perspective for Enhancing Jailbreak Attack and Defense](https://arxiv.org/abs/2602.04893)
*Licheng Pan,Yunsheng Lu,Jiexi Liu,Jialing Tao,Haozhe Feng,Hui Xue,Zhixuan Chu,Kui Ren*

Main category: cs.LG

TL;DR: 提出Causal Analyst框架，利用LLM进行因果发现，识别越狱攻击的直接原因，并应用于攻击增强和防御建议


<details>
  <summary>Details</summary>
Motivation: 现有研究主要分析潜在表示，忽视了可解释提示特征与越狱发生之间的因果关系，需要深入理解越狱机制以提升LLM安全性和可靠性

Method: 提出Causal Analyst框架，整合LLM到数据驱动的因果发现中；构建包含35k越狱尝试的数据集，标注37个人类可读提示特征；联合训练LLM提示编码和GNN因果图学习，重建提示特征到越狱响应的因果路径

Result: 分析发现特定特征如"Positive Character"和"Number of Task Steps"是越狱的直接因果驱动因素；开发了Jailbreaking Enhancer显著提升攻击成功率，以及Guardrail Advisor从混淆查询中提取真实恶意意图；实验验证了因果分析的鲁棒性和优于非因果方法的性能

Conclusion: 从因果角度分析越狱特征是提高LLM可靠性的有效且可解释的方法，因果分析为理解和应对LLM越狱提供了新视角

Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.

</details>


### [24] [A Short and Unified Convergence Analysis of the SAG, SAGA, and IAG Algorithms](https://arxiv.org/abs/2602.05304)
*Feng Zhu,Robert W. Heath,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出统一收敛分析框架，适用于SAG、SAGA和IAG三种算法，简化证明过程并改进理论结果


<details>
  <summary>Details</summary>
Motivation: 现有随机方差缩减算法（如SAG、SAGA）及其确定性对应算法（如IAG）的分析方法分散且复杂，需要统一的收敛分析框架来简化证明并改进理论结果

Method: 开发统一收敛分析方法：1) 使用简单集中工具建立随机子采样延迟界限；2) 设计新颖的李雅普诺夫函数来考虑延迟影响

Result: 获得首个SAG和SAGA的高概率收敛界，可扩展到非凸目标和马尔可夫采样；显著改进了IAG算法的最佳已知收敛率

Conclusion: 提出的统一分析框架简化了证明过程，提供了模块化、简洁的收敛分析，为三种重要算法建立了统一的理论基础

Abstract: Stochastic variance-reduced algorithms such as Stochastic Average Gradient (SAG) and SAGA, and their deterministic counterparts like the Incremental Aggregated Gradient (IAG) method, have been extensively studied in large-scale machine learning. Despite their popularity, existing analyses for these algorithms are disparate, relying on different proof techniques tailored to each method. Furthermore, the original proof of SAG is known to be notoriously involved, requiring computer-aided analysis. Focusing on finite-sum optimization with smooth and strongly convex objective functions, our main contribution is to develop a single unified convergence analysis that applies to all three algorithms: SAG, SAGA, and IAG. Our analysis features two key steps: (i) establishing a bound on delays due to stochastic sub-sampling using simple concentration tools, and (ii) carefully designing a novel Lyapunov function that accounts for such delays. The resulting proof is short and modular, providing the first high-probability bounds for SAG and SAGA that can be seamlessly extended to non-convex objectives and Markov sampling. As an immediate byproduct of our new analysis technique, we obtain the best known rates for the IAG algorithm, significantly improving upon prior bounds.

</details>


### [25] [Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability](https://arxiv.org/abs/2602.04902)
*Kingsuk Maitra*

Main category: cs.LG

TL;DR: 论文提出动量注意力机制，将Transformer视为物理电路，通过引入运动学动量实现单层归纳头形成，揭示了辛剪切与高通滤波器的对偶性。


<details>
  <summary>Details</summary>
Motivation: 将机械可解释性(MI)程序扩展到物理电路视角，通过引入守恒定律和时变交流动力学来增强Transformer计算图，旨在克服传统架构中归纳头形成的拓扑深度约束。

Method: 提出动量注意力机制，通过运动学差分算子p_t = q_t - q_{t-1}嵌入物理先验，在查询和键上实现辛剪切变换。建立辛-滤波器对偶性，证明物理剪切在数学上等价于高通滤波器。

Result: 在5100+控制实验中验证，125M动量模型在归纳密集型任务上表现优异，跟踪350M基线在验证损失上仅差约2.9%。发现缩放定律γ* = 4.17 × N^{-0.74}，建立了动量与深度的可替代性。

Conclusion: 该框架提供了连接生成式AI、哈密顿物理和信号处理的补充分析工具包，通过注入运动学动量实现了单层归纳能力，揭示了物理机制与数学滤波之间的深刻联系。

Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + γp_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $γ^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.

</details>


### [26] [Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates](https://arxiv.org/abs/2602.05311)
*Chengxiao Wang,Haoze Wu,Gagandeep Singh*

Main category: cs.LG

TL;DR: 提出训练鲁棒神经李雅普诺夫屏障证书的方法，用于在系统动力学存在扰动时验证深度强化学习控制器的安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有神经李雅普诺夫和屏障证书方法仅在固定理想无扰动动力学下提供保证，限制了在现实世界应用中面对不确定性时的可靠性。

Method: 定义鲁棒李雅普诺夫屏障函数，基于Lipschitz连续性指定充分条件确保对有限扰动的鲁棒性。提出通过对抗训练、Lipschitz邻域边界和全局Lipschitz正则化的实用训练目标。

Result: 在倒立摆和2D对接环境中验证方法，相比基线显著提高认证鲁棒性边界（最高4.6倍）和强扰动下的经验成功率（最高2.4倍）。

Conclusion: 训练鲁棒神经证书对于在动力学扰动下的安全强化学习是有效的，为现实世界应用提供了更强的可靠性保证。

Abstract: Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.

</details>


### [27] [Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering](https://arxiv.org/abs/2602.04903)
*Eitan Sprejer,Oscar Agustín Stanchi,María Victoria Carro,Denise Alejandra Mester,Iván Arcuschin*

Main category: cs.LG

TL;DR: 特征导向方法能有效控制LLM行为但严重损害模型性能，在准确性和连贯性上产生显著下降，提示工程仍是更平衡的实用方案。


<details>
  <summary>Details</summary>
Motivation: 特征导向作为直接操控内部表征的LLM控制方法虽有优势，但其实际应用效果特别是与输出质量的权衡关系尚不明确，需要实证评估。

Method: 评估Goodfire的Auto Steer与提示工程基线方法，在14个导向查询（涵盖无害和安全相关行为）上，使用Llama-8B和Llama-70B模型在171个MMLU问题上测试，测量准确性、连贯性和行为控制效果。

Result: Auto Steer成功修改目标行为（Llama-8B得分3.33 vs. 2.98，Llama-70B得分3.57 vs. 3.10），但导致性能急剧下降：MMLU准确率从66%降至46%（8B）和87%降至73%（70B），连贯性从4.62降至2.24和4.94降至3.89。简单提示工程实现最佳整体平衡。

Conclusion: 当前特征导向方法在不能牺牲任务性能的实际部署中存在局限性，机械控制方法面临基本的能力-行为权衡，部署前必须进行实证表征。

Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

</details>


### [28] [DCER: Dual-Stage Compression and Energy-Based Reconstruction](https://arxiv.org/abs/2602.04904)
*Yiwen Wang,Jiahao Qin*

Main category: cs.LG

TL;DR: DCER：通过双阶段压缩和基于能量的重建解决多模态融合中的噪声输入和模态缺失问题，在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 多模态融合面临两个鲁棒性挑战：噪声输入会降低表示质量，模态缺失会导致预测失败。现有方法难以同时有效处理这两个问题。

Method: 提出DCER统一框架：1）压缩阶段：模态内频率变换（音频用小波，视频用DCT）去除噪声；跨模态瓶颈token强制真正融合而非捷径学习。2）基于能量的重建：通过梯度下降在学习的能量函数上恢复缺失模态的表示，最终能量提供内在不确定性量化。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS基准测试中达到最先进性能，呈现U型鲁棒性模式（在完整模态和高缺失条件下都表现良好），能量与预测误差的相关性ρ>0.72。

Conclusion: DCER通过双阶段压缩和能量重建有效解决了多模态融合中的噪声和缺失问题，提供了统一且可解释的解决方案，能量函数还能提供不确定性量化。

Abstract: Multimodal fusion faces two robustness challenges: noisy inputs degrade representation quality, and missing modalities cause prediction failures. We propose DCER, a
  unified framework addressing both challenges through dual-stage compression and energy-based reconstruction. The compression stage operates at two levels:
  within-modality frequency transforms (wavelet for audio, DCT for video) remove noise while preserving task-relevant patterns, and cross-modality bottleneck tokens
  force genuine integration rather than modality-specific shortcuts. For missing modalities, energy-based reconstruction recovers representations via gradient descent
  on a learned energy function, with the final energy providing intrinsic uncertainty quantification (\r{ho} > 0.72 correlation with prediction error). Experiments on
  CMU-MOSI, CMU-MOSEI, and CH-SIMS demonstrate state-of-the-art performance across all benchmarks, with a U-shaped robustness pattern favoring multimodal fusion at
  both complete and high-missing conditions. The code will be available on Github.

</details>


### [29] [LISA: Laplacian In-context Spectral Analysis](https://arxiv.org/abs/2602.04906)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LISA是一种用于拉普拉斯时间序列模型的推理时自适应方法，仅使用观测前缀，结合延迟坐标嵌入和拉普拉斯谱学习生成扩散坐标状态表示，配合冻结非线性解码器进行一步预测。


<details>
  <summary>Details</summary>
Motivation: 将上下文自适应与非参数谱方法联系起来，解决动态系统在变化动态下的时间序列预测问题，通过推理时自适应提升模型性能。

Method: 结合延迟坐标嵌入和拉普拉斯谱学习生成扩散坐标状态表示，使用冻结非线性解码器进行一步预测，引入基于高斯过程回归或类注意力马尔可夫算子的轻量级潜在空间残差适配器。

Result: 在预测和自回归展开实验中，LISA优于冻结基线，在动态变化情况下表现尤为突出，验证了上下文自适应与非参数谱方法的有效结合。

Conclusion: LISA成功将上下文自适应与动态系统非参数谱方法联系起来，为时间序列模型推理时自适应提供了有效框架，在变化动态下表现优异。

Abstract: We propose Laplacian In-context Spectral Analysis (LISA), a method for inference-time adaptation of Laplacian-based time-series models using only an observed prefix. LISA combines delay-coordinate embeddings and Laplacian spectral learning to produce diffusion-coordinate state representations, together with a frozen nonlinear decoder for one-step prediction. We introduce lightweight latent-space residual adapters based on either Gaussian-process regression or an attention-like Markov operator over context windows. Across forecasting and autoregressive rollout experiments, LISA improves over the frozen baseline and is often most beneficial under changing dynamics. This work links in-context adaptation to nonparametric spectral methods for dynamical systems.

</details>


### [30] [Temporal Pair Consistency for Variance-Reduced Flow Matching](https://arxiv.org/abs/2602.04908)
*Chika Maduabuchi,Jindong Wang*

Main category: cs.LG

TL;DR: TPC是一种轻量级方差减少原则，通过耦合同一概率路径上配对时间步的速度预测来降低梯度方差，无需修改模型架构、概率路径或求解器。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型（如扩散模型、流匹配、整流流）通常使用独立处理时间步的目标函数进行训练，导致估计器方差高和采样效率低。现有方法通过显式平滑惩罚、轨迹正则化或修改概率路径和求解器来缓解这一问题。

Method: 引入时间对一致性（TPC）原则，在估计器层面耦合同一概率路径上配对时间步的速度预测。该方法不修改模型架构、概率路径或求解器，通过轨迹耦合的正则化减少梯度方差。

Result: 在流匹配框架中，TPC在CIFAR-10和ImageNet多个分辨率上提高了样本质量和效率，以相同或更低的计算成本实现了更低的FID。该方法可无缝扩展到现代SOTA风格管道，包括噪声增强训练、基于分数的去噪和整流流。

Conclusion: TPC提供了一种轻量级且有效的方差减少方法，通过时间步配对耦合改善了连续时间生成模型的训练效率和样本质量，同时保持底层流匹配目标不变。

Abstract: Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.

</details>


### [31] [SemPipes -- Optimizable Semantic Data Operators for Tabular Machine Learning Pipelines](https://arxiv.org/abs/2602.05134)
*Olga Ovcharenko,Matthias Boehm,Sebastian Schelter*

Main category: cs.LG

TL;DR: SemPipes：一种集成LLM语义数据操作符的声明式编程模型，用于表格机器学习管道，通过代码合成自动优化数据操作


<details>
  <summary>Details</summary>
Motivation: 现实世界表格机器学习需要复杂的数据准备管道，设计这些管道需要大量领域专业知识和工程努力，因此需要探索LLM如何通过代码合成支持表格ML

Method: 引入SemPipes声明式编程模型，集成LLM驱动的语义数据操作符到表格ML管道中。语义操作符用自然语言指定数据转换，运行时系统执行。训练期间，SemPipes根据数据特征、操作指令和管道上下文合成自定义操作符实现，通过基于进化搜索的LLM代码合成自动优化管道中的数据操作

Result: 在多样化的表格ML任务中评估SemPipes，显示语义操作符显著提高了专家设计和智能体生成管道的端到端预测性能，同时降低了管道复杂性

Conclusion: SemPipes通过集成LLM语义操作符和自动代码合成，有效简化了表格ML管道设计，提高了预测性能，减少了工程复杂度

Abstract: Real-world machine learning on tabular data relies on complex data preparation pipelines for prediction, data integration, augmentation, and debugging. Designing these pipelines requires substantial domain expertise and engineering effort, motivating the question of how large language models (LLMs) can support tabular ML through code synthesis. We introduce SemPipes, a novel declarative programming model that integrates LLM-powered semantic data operators into tabular ML pipelines. Semantic operators specify data transformations in natural language while delegating execution to a runtime system. During training, SemPipes synthesizes custom operator implementations based on data characteristics, operator instructions, and pipeline context. This design enables the automatic optimization of data operations in a pipeline via LLM-based code synthesis guided by evolutionary search. We evaluate SemPipes across diverse tabular ML tasks and show that semantic operators substantially improve end-to-end predictive performance for both expert-designed and agent-generated pipelines, while reducing pipeline complexity. We implement SemPipes in Python and release it at https://github.com/deem-data/sempipes/tree/v1.

</details>


### [32] [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)
*Xiaolin Hu,Hang Yuan,Xinzhu Sang,Binbin Yan,Zhou Yu,Cong Huang,Kai Chen*

Main category: cs.LG

TL;DR: A²-LLM：端到端对话音频虚拟人大型语言模型，统一推理语言、音频韵律和3D面部运动，超越传统级联架构的唇部同步，实现情感丰富的面部动画生成。


<details>
  <summary>Details</summary>
Motivation: 当前对话数字人系统大多采用级联架构连接独立模块，存在误差累积、高延迟、实时性差等问题。这些流水线缺乏底层对话上下文访问能力，优先考虑刚性唇部同步而非情感深度。

Method: 提出A²-LLM端到端对话音频虚拟人大型语言模型，在统一框架内联合推理语言、音频韵律和3D面部运动。引入FLAME-QA高质量多模态数据集，以问答格式对齐语义意图与表情面部动态。

Result: 实验结果表明，系统在保持实时效率（500ms延迟，0.7 RTF）的同时，实现了卓越的情感表现力，生成超越简单唇部同步的情感丰富面部运动。

Conclusion: A²-LLM通过端到端统一框架解决了传统级联架构的局限性，实现了情感表达与实时性能的平衡，为下一代人机交互中的表达性对话数字人提供了有效解决方案。

Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent modules. These pipelines are often plagued by accumulated errors, high latency, and poor real-time performance. Lacking access to the underlying conversational context, these pipelines inherently prioritize rigid lip-sync over emotional depth. To address these challenges, we propose A$^2$-LLM, an end-to-end conversational audio avatar large language model that jointly reasons about language, audio prosody, and 3D facial motion within a unified framework. To facilitate training, we introduce FLAME-QA, a high-quality multimodal dataset designed to align semantic intent with expressive facial dynamics within a QA format. By leveraging deep semantic understanding, A$^2$-LLM generates emotionally rich facial movements beyond simple lip-synchronization. Experimental results demonstrate that our system achieves superior emotional expressiveness while maintaining real-time efficiency (500 ms latency, 0.7 RTF).

</details>


### [33] [SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel](https://arxiv.org/abs/2602.04915)
*Jose Miguel Luna,Taha Bouhsine,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: SLAY是一种基于Yat核的线性时间注意力机制，通过将查询和键约束到单位球面上实现仅依赖角度对齐的几何感知注意力，使用伯恩斯坦定理推导出严格正随机特征近似，在保持线性时间复杂度的同时性能接近标准softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 解决传统注意力机制二次时间复杂度的可扩展性问题，同时避免现有线性时间注意力方法（如Performers和Cosformers）的性能损失，寻求最接近softmax注意力的线性近似。

Method: 提出SLAY方法：1) 将查询和键约束到单位球面上，使注意力仅依赖于角度对齐；2) 基于松弛的E-Product（Yat核）构建几何感知注意力；3) 利用伯恩斯坦定理将球面Yat核表示为非负多项式-指数乘积核的混合；4) 推导严格正随机特征近似实现线性时间O(L)注意力。

Result: 1) 证明了球面上的正定性和有界性；2) 估计器产生定义良好、非负的注意力分数；3) 经验上，SLAY性能与标准softmax注意力几乎无法区分；4) 在保持线性时间和内存缩放的同时，始终优于先前的线性时间注意力机制；5) 代表了迄今为止最接近softmax注意力的线性时间近似。

Conclusion: SLAY实现了无需典型性能权衡的可扩展Transformer，在保持线性时间复杂度的同时达到了接近softmax注意力的性能，是当前最先进的线性时间注意力机制。

Abstract: We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interactions in physics. Our method, Spherical Linearized Attention with Yat Kernels (SLAY), constrains queries and keys to the unit sphere so that attention depends only on angular alignment. Using Bernstein's theorem, we express the spherical Yat-kernel as a nonnegative mixture of polynomial-exponential product kernels and derive a strictly positive random-feature approximation enabling linear-time O(L) attention. We establish positive definiteness and boundedness on the sphere and show that the estimator yields well-defined, nonnegative attention scores. Empirically, SLAY achieves performance that is nearly indistinguishable from standard softmax attention while retaining linear time and memory scaling, and consistently outperforms prior linear-time attention mechanisms such as Performers and Cosformers. To the best of our knowledge, SLAY represents the closest linear-time approximation to softmax attention reported to date, enabling scalable Transformers without the typical performance trade-offs of attention linearization.

</details>


### [34] [Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution](https://arxiv.org/abs/2602.04918)
*Long Zhang,Fangwei Lin*

Main category: cs.LG

TL;DR: LLMs在遇到上下文信息与参数记忆冲突时，会通过"正交干扰"机制而非"流形稀释"来优先采用上下文信息，即注入与真实方向准正交的转向向量来旋转隐藏状态表示，而非抑制内部真实信息的幅度。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在上下文信息与参数记忆冲突时优先采用上下文信息（顺从性）的机制实现，特别是模型如何通过顺从性解决这些知识冲突，以及这种抑制是源于残差流中信号幅度的稀释还是方向性几何改变。

Method: 对Qwen-4B、Llama-3.1-8B和GLM-4-9B进行分层几何分析，将反事实上下文诱导的残差流更新分解为径向（基于范数）和角度（基于余弦）分量。

Result: 拒绝了"流形稀释"假说的普遍性，因为三种架构中有两种在事实查询性能显著下降时仍保持稳定的残差范数。相反，观察到顺从性一致表现为"正交干扰"，即冲突上下文注入与真实方向准正交的转向向量，有效旋转隐藏状态表示。

Conclusion: 模型不会"忘记"或抑制内部真实信息的幅度，而是采用几何位移机制来绕过正确的解嵌入向量，在保持原始结构幅度的同时模拟采纳。这些发现挑战了基于标量置信度检测幻觉的方法，强调需要向量监控来区分真正的知识整合和表面的上下文模仿。

Abstract: Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the "Manifold Dilution" hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by "Orthogonal Interference," where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not "unlearn" or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.

</details>


### [35] [Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog](https://arxiv.org/abs/2602.04919)
*Yiran Zhao,Shengyang Zhou,Zijian Wu,Tongyan Hu,Yuhui Xu,Rengan Dou,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 提出渐进压缩方法PTL，通过多次细粒度迭代的剪枝-调优循环，将LLM压缩至近一半大小，仅需轻量级后训练即可保持推理性能。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然具有强大的推理能力，但庞大的参数量需要大量计算资源。传统剪枝方法直接移除冗余参数会导致推理任务性能急剧下降，且需要大量后训练来恢复能力。

Method: 提出渐进压缩方法，将压缩过程分为多个细粒度迭代，每个阶段应用剪枝-调优循环(PTL)：先剪枝减少模型大小，然后通过微调恢复性能。这种迭代方法类似于"温水煮青蛙"效应，使模型能够逐步压缩而不出现性能骤降。

Result: PTL能够将LLM压缩至接近原始大小的一半，仅需轻量级后训练即可在推理任务上保持与原始模型相当的性能。该方法灵活，可应用于多种剪枝策略（神经元剪枝、层剪枝）和不同后训练方法（持续预训练、强化学习）。实验还证实了PTL在代码生成等数学推理之外任务的有效性。

Conclusion: PTL提供了一种有效的渐进压缩框架，能够在显著减少LLM参数量的同时保持推理能力，具有广泛的适用性和灵活性。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but their substantial size often demands significant computational resources. To reduce resource consumption and accelerate inference, it is essential to eliminate redundant parameters without compromising performance. However, conventional pruning methods that directly remove such parameters often lead to a dramatic drop in model performance in reasoning tasks, and require extensive post-training to recover the lost capabilities. In this work, we propose a gradual compacting method that divides the compression process into multiple fine-grained iterations, applying a Prune-Tune Loop (PTL) at each stage to incrementally reduce model size while restoring performance with finetuning. This iterative approach-reminiscent of the "boiling frog" effect-enables the model to be progressively compressed without abrupt performance loss. Experimental results show that PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to the original model on reasoning tasks. Moreover, PTL is flexible and can be applied to various pruning strategies, such as neuron pruning and layer pruning, as well as different post-training methods, including continual pre-training and reinforcement learning. Additionally, experimental results confirm the effectiveness of PTL on a variety of tasks beyond mathematical reasoning, such as code generation, demonstrating its broad applicability.

</details>


### [36] [Imposing Boundary Conditions on Neural Operators via Learned Function Extensions](https://arxiv.org/abs/2602.04923)
*Sepehr Mousavi,Siddhartha Mishra,Laura De Lorenzis*

Main category: cs.LG

TL;DR: 提出了一种通过函数扩展将复杂非齐次边界条件融入神经算子的通用框架，在18个具有挑战性的PDE数据集上实现了最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在处理高度可变的复杂边界条件时能力有限，特别是当解算子对边界强迫表现出强敏感性时，现有方法往往失败。需要一种通用框架来处理复杂的非齐次边界条件。

Method: 提出通过函数扩展将边界数据映射到整个空间域上的潜在伪扩展，使任何标准算子学习架构都能消费边界信息。该方法与任意的域到域神经算子耦合，能同时学习复杂边界条件和输入域函数的丰富依赖关系。

Result: 在涵盖泊松方程、线性弹性和超弹性问题的18个具有挑战性的数据集上，该方法实现了最先进的精度，大幅超越基线方法，且在不同数据集间无需超参数调优。

Conclusion: 学习边界到域的扩展是现有神经算子框架中施加复杂边界条件的有效实用策略，能够为更广泛的PDE控制问题构建准确稳健的科学机器学习模型。

Abstract: Neural operators have emerged as powerful surrogates for the solution of partial differential equations (PDEs), yet their ability to handle general, highly variable boundary conditions (BCs) remains limited. Existing approaches often fail when the solution operator exhibits strong sensitivity to boundary forcings. We propose a general framework for conditioning neural operators on complex non-homogeneous BCs through function extensions. Our key idea is to map boundary data to latent pseudo-extensions defined over the entire spatial domain, enabling any standard operator learning architecture to consume boundary information. The resulting operator, coupled with an arbitrary domain-to-domain neural operator, can learn rich dependencies on complex BCs and input domain functions at the same time. To benchmark this setting, we construct 18 challenging datasets spanning Poisson, linear elasticity, and hyperelasticity problems, with highly variable, mixed-type, component-wise, and multi-segment BCs on diverse geometries. Our approach achieves state-of-the-art accuracy, outperforming baselines by large margins, while requiring no hyperparameter tuning across datasets. Overall, our results demonstrate that learning boundary-to-domain extensions is an effective and practical strategy for imposing complex BCs in existing neural operator frameworks, enabling accurate and robust scientific machine learning models for a broader range of PDE-governed problems.

</details>


### [37] [Internalizing LLM Reasoning via Discovery and Replay of Latent Actions](https://arxiv.org/abs/2602.04925)
*Zhenning Shi,Yijia Zhu,Junhan Shi,Xun Zhang,Lei Wang,Congcong Miao*

Main category: cs.LG

TL;DR: STIR是一个通过动态潜在轨迹控制来内部化思维链推理过程的框架，能够在减少token消耗的同时提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的激活引导方法依赖于静态控制向量，无法适应复杂推理任务中非平稳的演化过程。需要一种能够动态适应推理过程变化的内部化方法。

Method: STIR采用三阶段流水线：1) 差分内在动作归纳，从潜在推理成功中提取引导原语；2) 稀疏控制基构建，创建紧凑且几何多样的工具库；3) 价值调制轨迹干预，通过基于锚点的门控机制动态注入上下文特定的脉冲。

Result: 在四个代表性模型的六个算术和逻辑基准测试中，STIR将平均准确率提高了1.9%至7.5%，同时将平均token消耗减少了高达35%（相比普通解码）。

Conclusion: 显式思维链的好处可以通过动态潜在轨迹控制来实现，内部化推理过程可以绕过显式生成，同时实现更高的保真度。这为高效推理提供了新的范式。

Abstract: The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.

</details>


### [38] [Depth-Wise Emergence of Prediction-Centric Geometry in Large Language Models](https://arxiv.org/abs/2602.04931)
*Shahar Haim,Daniel C McNamee*

Main category: cs.LG

TL;DR: 论文发现仅解码器LLM在深度维度上存在从上下文处理到预测形成的计算阶段转变，伴随表示几何的重组，其中角度组织参数化预测分布相似性，范数编码上下文特定信息。


<details>
  <summary>Details</summary>
Motivation: 研究仅解码器大语言模型中从上下文处理到预测形成的计算动态，理解表示几何如何重组以支持从上下文到预测的转换。

Method: 采用结合几何分析和机制干预的统一框架，分析深度维度上的表示几何变化，通过角度组织和范数编码研究表示结构。

Result: 发现LLM后期层表示实现结构化几何编码，角度组织参数化预测分布相似性，表示范数编码上下文特定信息但不决定预测，支持对token预测的选择性因果控制。

Conclusion: 研究提供了从上下文到预测转换动态的机制-几何解释，揭示了LLM中表示几何如何重组以支持预测形成过程。

Abstract: We show that decoder-only large language models exhibit a depth-wise transition from context-processing to prediction-forming phases of computation accompanied by a reorganization of representational geometry. Using a unified framework combining geometric analysis with mechanistic intervention, we demonstrate that late-layer representations implement a structured geometric code that enables selective causal control over token prediction. Specifically, angular organization of the representation geometry parametrizes prediction distributional similarity, while representation norms encode context-specific information that does not determine prediction. Together, these results provide a mechanistic-geometric account of the dynamics of transforming context into predictions in LLMs.

</details>


### [39] [Comparing Euclidean and Hyperbolic K-Means for Generalized Category Discovery](https://arxiv.org/abs/2602.04932)
*Mohamad Dalal,Thomas B. Moeslund,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: HC-GCD提出在双曲空间中直接进行聚类，而不是像现有方法那样只在双曲空间学习表示后转回欧氏空间聚类，实验证明双曲K-Means比欧氏K-Means效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有双曲广义类别发现方法仅使用双曲几何进行表示学习，但在聚类时转回欧氏空间，作者认为这是次优的，需要直接在双曲空间中进行聚类。

Method: 提出HC-GCD方法，在洛伦兹双曲模型学习嵌入表示，并使用双曲K-Means算法直接在双曲空间中对这些嵌入进行聚类。

Result: 在语义偏移基准数据集上，HC-GCD与先前最先进的双曲GCD方法性能相当；双曲K-Means比欧氏K-Means准确率更高；欧氏嵌入范数裁剪对未见类和已见类准确率有不同影响；双曲K-Means在不同标签粒度下产生更一致的聚类。

Conclusion: 直接在双曲空间进行聚类比在欧氏空间聚类更有效，双曲K-Means能产生更一致和准确的聚类结果，为广义类别发现任务提供了更好的解决方案。

Abstract: Hyperbolic representation learning has been widely used to extract implicit hierarchies within data, and recently it has found its way to the open-world classification task of Generalized Category Discovery (GCD). However, prior hyperbolic GCD methods only use hyperbolic geometry for representation learning and transform back to Euclidean geometry when clustering. We hypothesize this is suboptimal. Therefore, we present Hyperbolic Clustered GCD (HC-GCD), which learns embeddings in the Lorentz Hyperboloid model of hyperbolic geometry, and clusters these embeddings directly in hyperbolic space using a hyperbolic K-Means algorithm. We test our model on the Semantic Shift Benchmark datasets, and demonstrate that HC-GCD is on par with the previous state-of-the-art hyperbolic GCD method. Furthermore, we show that using hyperbolic K-Means leads to better accuracy than Euclidean K-Means. We carry out ablation studies showing that clipping the norm of the Euclidean embeddings leads to decreased accuracy in clustering unseen classes, and increased accuracy for seen classes, while the overall accuracy is dataset dependent. We also show that using hyperbolic K-Means leads to more consistent clusters when varying the label granularity.

</details>


### [40] [Transolver-3: Scaling Up Transformer Solvers to Industrial-Scale Geometries](https://arxiv.org/abs/2602.04940)
*Hang Zhou,Haixu Wu,Haonan Shangguan,Yuezhou Ma,Huikun Weng,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Transolver-3：用于工业规模PDE求解的深度学习框架，通过切片优化和摊销训练处理超过1.6亿网格单元的高分辨率网格


<details>
  <summary>Details</summary>
Motivation: 传统神经PDE求解器在处理工业规模几何体（超过10^8网格单元）时面临内存复杂度过高的挑战，无法满足复杂工程任务的高分辨率需求

Method: 1. 利用矩阵乘法结合律实现更快的切片和反切片操作；2. 几何切片平铺技术分割物理状态计算；3. 摊销训练策略：在原始高分辨率网格的随机子集上学习；4. 推理期间的物理状态缓存技术

Result: Transolver-3能够处理超过1.6亿网格单元的工业规模网格，在包括飞机和汽车设计在内的三个具有挑战性的仿真基准测试中表现出色

Conclusion: Transolver-3通过创新的架构优化和训练策略，成功解决了神经PDE求解器在工业规模应用中的可扩展性问题，为高保真物理仿真提供了可行的深度学习解决方案

Abstract: Deep learning has emerged as a transformative tool for the neural surrogate modeling of partial differential equations (PDEs), known as neural PDE solvers. However, scaling these solvers to industrial-scale geometries with over $10^8$ cells remains a fundamental challenge due to the prohibitive memory complexity of processing high-resolution meshes. We present Transolver-3, a new member of the Transolver family as a highly scalable framework designed for high-fidelity physics simulations. To bridge the gap between limited GPU capacity and the resolution requirements of complex engineering tasks, we introduce two key architectural optimizations: faster slice and deslice by exploiting matrix multiplication associative property and geometry slice tiling to partition the computation of physical states. Combined with an amortized training strategy by learning on random subsets of original high-resolution meshes and a physical state caching technique during inference, Transolver-3 enables high-fidelity field prediction on industrial-scale meshes. Extensive experiments demonstrate that Transolver-3 is capable of handling meshes with over 160 million cells, achieving impressive performance across three challenging simulation benchmarks, including aircraft and automotive design tasks.

</details>


### [41] [Privileged Information Distillation for Language Models](https://arxiv.org/abs/2602.04942)
*Emiliano Penaloza,Dheeraj Vattikonda,Nicolas Gontier,Alexandre Lacoste,Laurent Charlin,Massimo Caccia*

Main category: cs.LG

TL;DR: π-Distill和OPSD：两种利用训练时特权信息蒸馏前沿智能体模型的方法，在仅能访问动作轨迹的情况下优于传统监督微调+强化学习流程


<details>
  <summary>Details</summary>
Motivation: 在智能体环境中，前沿模型通常隐藏内部推理过程，仅暴露动作轨迹，这破坏了标准蒸馏流程。需要开发能够利用训练时特权信息但推理时不依赖该信息的蒸馏方法。

Method: 提出两种方法：1) π-Distill：联合教师-学生目标，使用同一模型同时训练特权信息条件化的教师和无条件的学生；2) OPSD：基于强化学习的方法，在学生和特权信息条件化教师之间使用反向KL惩罚。

Result: 两种算法都能有效利用仅动作特权信息蒸馏前沿智能体。π-Distill（在某些情况下OPSD）在多个智能体基准测试、模型和特权信息形式上，优于假设能访问完整思维链监督的行业标准实践（监督微调+强化学习）。

Conclusion: π-Distill和OPSD为解决训练时特权信息蒸馏问题提供了有效解决方案，特别是在仅能访问动作轨迹的情况下，能够成功将特权信息学到的能力迁移到推理时不依赖该信息的策略中。

Abstract: Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that π-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.

</details>


### [42] [ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation](https://arxiv.org/abs/2602.05051)
*Songyuan Zhang,Oswin So,H. M. Sabbir Ahmad,Eric Yang Yu,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: ReFORM是一种基于流策略的离线强化学习方法，通过构造强制执行较宽松的支持约束，解决了分布外误差问题，同时在OGBench基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中存在两个主要挑战：1) 分布外误差问题，当策略偏离训练分布时产生；2) 最优策略分布可能是多模态且难以表示的。现有方法通过惩罚统计距离项来保持策略接近行为策略，但这限制了策略改进且不能完全防止OOD动作。扩散或流策略方法虽然能处理多模态表示，但如何避免OOD误差同时保持策略表达能力仍不明确。

Method: ReFORM基于流策略构建，通过构造强制执行支持约束。方法分为两步：1) 学习一个有界源分布的行为克隆流策略，捕捉动作分布的支持集；2) 优化一个反射流，为BC流生成有界噪声同时保持支持集，以最大化性能。这种方法确保策略始终保持在行为策略的支持范围内，避免OOD误差。

Result: 在OGBench基准测试的40个挑战性任务上，使用不同质量的数据集，ReFORM在所有任务中使用恒定超参数集，在性能曲线分析中显著优于所有经过手动调优超参数的基线方法。

Conclusion: ReFORM通过流策略的构造性支持约束，有效解决了离线强化学习中的OOD误差问题，同时保持了策略的表达能力，在多个基准测试中表现出卓越性能，且具有超参数鲁棒性。

Abstract: Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.

</details>


### [43] [Near-Optimal Dynamic Matching via Coarsening with Application to Heart Transplantation](https://arxiv.org/abs/2602.04989)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 该论文提出了一种基于粗化方法的在线匹配算法，通过将离线节点聚合成有容量的簇来获得接近最优的理论保证，并将其应用于心脏移植分配，在模拟中接近全知基准的性能。


<details>
  <summary>Details</summary>
Motivation: 在线匹配在互联网广告和器官分配等领域应用广泛，但实际算法往往缺乏强有力的理论保证。本文旨在通过开发基于粗化方法的新在线匹配算法来解决这一问题，弥合数据驱动启发式方法与悲观理论下界之间的差距。

Method: 开发了基于粗化方法的在线匹配算法，将离线节点聚合成有容量的簇。虽然粗化通常意味着粒度损失，但作者证明这种聚合实际上能产生接近最优的理论保证。该方法应用于心脏移植分配，基于历史数据的结构特性开发理论上有依据的策略。

Result: 在现实模拟中，该策略的表现与全知基准（omniscient benchmark）非常接近。该工作为先前器官分配中基于聚类的方法提供了严格的理论依据。

Conclusion: 该研究通过粗化方法开发了具有强理论保证的在线匹配算法，成功应用于心脏移植分配，弥合了数据驱动启发式方法与理论下界之间的差距，为基于聚类的器官分配方法提供了理论支持。

Abstract: Online matching has been a mainstay in domains such as Internet advertising and organ allocation, but practical algorithms often lack strong theoretical guarantees. We take an important step toward addressing this by developing new online matching algorithms based on a coarsening approach. Although coarsening typically implies a loss of granularity, we show that, to the contrary, aggregating offline nodes into capacitated clusters can yield near-optimal theoretical guarantees. We apply our methodology to heart transplant allocation to develop theoretically grounded policies based on structural properties of historical data. In realistic simulations, our policy closely matches the performance of the omniscient benchmark. Our work bridges the gap between data-driven heuristics and pessimistic theoretical lower bounds, and provides rigorous justification for prior clustering-based approaches in organ allocation.

</details>


### [44] [Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives](https://arxiv.org/abs/2602.04990)
*Ioannis Anagnostides,Itai Zilberstein,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 本文主张器官分配算法应考虑激励因素，提出将机制设计等工具整合到下一代分配政策中


<details>
  <summary>Details</summary>
Motivation: 当前器官分配算法从基于规则的系统向机器学习和数据驱动优化过渡，但往往忽视了激励这一根本障碍。器官分配不是静态优化问题，而是涉及移植中心、临床医生和监管机构的复杂博弈，存在激励错配问题

Method: 这是一篇立场论文，以美国成人心脏移植分配为案例，识别决策流程中的关键激励错配，并通过数据展示其当前的不良后果。提出将机制设计、战略分类、因果推断和社会选择等工具整合到分配政策中的研究议程

Result: 识别了器官分配系统中的激励错配问题，并展示这些错配正在产生不良后果。提出了"激励感知"作为下一代分配政策的核心原则

Conclusion: 下一代器官分配政策应该是激励感知的，需要机器学习社区整合机制设计、战略分类、因果推断和社会选择等工具，以确保在面对各利益相关方战略行为时的鲁棒性、效率和公平性

Abstract: The allocation of scarce donor organs constitutes one of the most consequential algorithmic challenges in healthcare. While the field is rapidly transitioning from rigid, rule-based systems to machine learning and data-driven optimization, we argue that current approaches often overlook a fundamental barrier: incentives. In this position paper, we highlight that organ allocation is not merely a static optimization problem, but rather a complex game involving transplant centers, clinicians, and regulators. Focusing on US adult heart transplant allocation, we identify critical incentive misalignments across the decision-making pipeline, and present data showing that they are having adverse consequences today. Our main position is that the next generation of allocation policies should be incentive aware. We outline a research agenda for the machine learning community, calling for the integration of mechanism design, strategic classification, causal inference, and social choice to ensure robustness, efficiency, and fairness in the face of strategic behavior from the various constituent groups.

</details>


### [45] [Constrained Group Relative Policy Optimization](https://arxiv.org/abs/2602.05863)
*Roger Girgis,Rodrigue de Schaetzen,Luke Rowe,Azalée Robitaille,Christopher Pal,Liam Paull*

Main category: cs.LG

TL;DR: 提出Constrained GRPO，一种基于拉格朗日方法的GRPO扩展，用于带行为约束的策略优化，通过标量化优势估计解决多组件优势估计中的优化问题。


<details>
  <summary>Details</summary>
Motivation: GRPO作为无评论家策略学习的可扩展框架，在显式行为约束设置下的应用尚未充分探索。需要将GRPO扩展到带约束的策略优化场景，特别是在依赖大型多模态基础模型的具身AI领域。

Method: 提出Constrained GRPO，基于拉格朗日松弛的GRPO扩展。约束通过指示器成本函数指定，直接优化违反率。关键创新是标量化优势构造，避免多组件优势估计中因标准差不匹配导致的拉格朗日信号失真问题。

Result: 在玩具网格世界中验证了多组件优势估计的优化病理，并证明标量化优势能恢复稳定的约束控制。在机器人任务中，Constrained GRPO提高了约束满足度同时增加任务成功率。

Conclusion: Constrained GRPO为具身AI领域的约束策略优化提供了一个简单有效的方案，解决了多组件优势估计中的关键问题，在保持任务性能的同时有效控制约束违反。

Abstract: While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.

</details>


### [46] [EntRGi: Entropy Aware Reward Guidance for Diffusion Language Models](https://arxiv.org/abs/2602.05000)
*Atula Tejaswi,Litu Rout,Constantine Caramanis,Sanjay Shakkottai,Sujay Sanghavi*

Main category: cs.LG

TL;DR: EntRGi：一种用于离散扩散语言模型测试时适应的熵感知奖励引导方法，通过动态调节奖励模型的梯度来克服连续松弛和直通估计器的缺点


<details>
  <summary>Details</summary>
Motivation: 奖励引导在连续扩散模型的测试时适应中取得了巨大成功，但将其应用于离散扩散语言模型面临挑战：无法直接对离散标记进行微分。现有方法要么使用连续松弛（但奖励模型从未在连续输入上训练），要么使用直通估计器（在离散标记处评估梯度却更新连续logits，导致优化错误）。

Method: 提出EntRGi（熵感知奖励引导），通过基于模型置信度动态调节奖励模型梯度的新机制来超越现有方法的权衡。该方法使用连续松弛但通过模型置信度进行调制，为奖励模型提供可靠输入的同时显著改进奖励引导。

Result: 在7B参数的扩散语言模型上，跨越3个不同的奖励模型和3个多技能基准测试进行实证验证，结果显示EntRGi相比最先进方法取得了一致的改进。

Conclusion: EntRGi通过动态调节奖励模型梯度，有效解决了离散扩散语言模型奖励引导中连续松弛和直通估计器的固有缺陷，在多个基准测试中展现出优越性能。

Abstract: Reward guidance has been applied to great success in the test-time adaptation of continuous diffusion models; it updates each denoising step using the gradients from a downstream reward model. We study reward guidance for discrete diffusion language models, where one cannot differentiate through the natural outputs of the model because they are discrete tokens. Existing approaches either replace these discrete tokens with continuous relaxations, or employ techniques like the straight-through estimator. In this work, we show the downsides of both these methods. The former degrades gradient feedback because the reward model has never been trained with continuous inputs. The latter involves incorrect optimization because the gradient evaluated at discrete tokens is used to update continuous logits. Our key innovation is to go beyond this tradeoff by introducing a novel mechanism called EntRGi: Entropy aware Reward Guidance that dynamically regulates the gradients from the reward model. By modulating the continuous relaxation using the model's confidence, our approach substantially improves reward guidance while providing reliable inputs to the reward model. We empirically validate our approach on a 7B-parameter diffusion language model across 3 diverse reward models and 3 multi-skill benchmarks, showing consistent improvements over state-of-the-art methods.

</details>


### [47] [Enhanced QKNorm normalization for neural transformers with the Lp norm](https://arxiv.org/abs/2602.05006)
*Ezequiel Lopez-Rubio,Javier Montes-Perez,Esteban Jose Palomo*

Main category: cs.LG

TL;DR: 提出了一种基于Lp范数的QKNorm归一化方案泛化方法，允许使用非欧几里得范数来稳定Transformer中的查询和键向量学习。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中查询和键向量的归一化对于确保学习稳定性至关重要，无论向量尺度如何。现有归一化方法有限，需要更通用的方案来处理不同范数空间。

Method: 提出了一种基于Lp范数的QKNorm归一化方案泛化方法，允许使用非欧几里得范数。该方法扩展了传统的归一化方案，使其能够适应不同的范数空间。

Result: 实验结果表明，该方法在简单问题上表现出适用性，验证了基于Lp范数的归一化方案的有效性。

Conclusion: 基于Lp范数的QKNorm泛化方法为Transformer架构提供了更灵活的归一化方案，能够处理非欧几里得范数，为后续研究奠定了基础。

Abstract: The normalization of query and key vectors is an essential part of the Transformer architecture. It ensures that learning is stable regardless of the scale of these vectors. Some normalization approaches are available. In this preliminary work, a generalization of the QKNorm normalization scheme is proposed. The approach is based on the Lp norm, allowing non-Euclidean norms to be employed. Experimental results demonstrate the suitability of the method for a simple problem.

</details>


### [48] [Private PoEtry: Private In-Context Learning via Product of Experts](https://arxiv.org/abs/2602.05012)
*Rob Romijnders,Mohammad Mahdi Derakhshani,Jonathan Petit,Max Welling,Christos Louizos,Yuki M. Asano*

Main category: cs.LG

TL;DR: 提出一种基于专家乘积模型的理论框架，用于保护隐私的上下文学习，显著提升准确率同时保持强隐私保证


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私上下文学习方法存在计算成本高或启发式方法效果有限的问题，需要更有效的隐私保护机制

Method: 通过专家乘积模型重新构建隐私上下文学习框架，获得理论支撑且可并行化的算法

Result: 在文本分类、数学和视觉语言五个数据集上评估，相比现有DP-ICL方法平均准确率提升超过30个百分点

Conclusion: 提出的专家乘积模型框架为隐私上下文学习提供了理论基础，显著提升性能同时保持强隐私保证

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks with only a small set of examples at inference time, thereby avoiding task-specific fine-tuning. However, in-context examples may contain privacy-sensitive information that should not be revealed through model outputs. Existing differential privacy (DP) approaches to ICL are either computationally expensive or rely on heuristics with limited effectiveness, including context oversampling, synthetic data generation, or unnecessary thresholding. We reformulate private ICL through the lens of a Product-of-Experts model. This gives a theoretically grounded framework, and the algorithm can be trivially parallelized. We evaluate our method across five datasets in text classification, math, and vision-language. We find that our method improves accuracy by more than 30 percentage points on average compared to prior DP-ICL methods, while maintaining strong privacy guarantees.

</details>


### [49] [A Simple Reduction Scheme for Constrained Contextual Bandits with Adversarial Contexts via Regression](https://arxiv.org/abs/2602.05019)
*Dhruv Sarkar,Abhishek Sinha*

Main category: cs.LG

TL;DR: 提出一种基于SquareCB框架的模块化算法方案，用于对抗性上下文环境下的约束上下文赌博机问题，通过在线回归预言机将约束问题转化为自适应定义替代奖励函数的无约束问题


<details>
  <summary>Details</summary>
Motivation: 研究对抗性选择上下文环境下的约束上下文赌博机问题，解决在预算耗尽后仍需继续运行的持续设置中同时控制遗憾和累积约束违反的问题

Method: 基于Foster等人的SquareCB框架，提出模块化算法方案，利用在线回归预言机将约束问题转化为具有自适应定义替代奖励函数的无约束上下文赌博机问题

Result: 相比大多数专注于随机上下文的先前工作，该方法在更一般的对抗性上下文设置中获得了改进的保证，并提供紧凑透明的分析

Conclusion: 通过将约束上下文赌博机问题简化为无约束问题，为对抗性上下文环境提供了一种简单、模块化且分析透明的解决方案，改进了现有结果

Abstract: We study constrained contextual bandits (CCB) with adversarially chosen contexts, where each action yields a random reward and incurs a random cost. We adopt the standard realizability assumption: conditioned on the observed context, rewards and costs are drawn independently from fixed distributions whose expectations belong to known function classes. We consider the continuing setting, in which the algorithm operates over the entire horizon even after the budget is exhausted. In this setting, the objective is to simultaneously control regret and cumulative constraint violation. Building on the seminal SquareCB framework of Foster et al. (2018), we propose a simple and modular algorithmic scheme that leverages online regression oracles to reduce the constrained problem to a standard unconstrained contextual bandit problem with adaptively defined surrogate reward functions. In contrast to most prior work on CCB, which focuses on stochastic contexts, our reduction yields improved guarantees for the more general adversarial context setting, together with a compact and transparent analysis.

</details>


### [50] [Laws of Learning Dynamics and the Core of Learners](https://arxiv.org/abs/2602.05026)
*Inkee Jung,Siu Cheong Lau*

Main category: cs.LG

TL;DR: 提出学习动力学基本定律（守恒定律和总熵减少），并基于此引入熵驱动的终身集成学习方法，通过构建免疫机制防御CIFAR-10数据集上的迁移对抗攻击


<details>
  <summary>Details</summary>
Motivation: 建立学习动力学的基本理论框架，并利用该框架开发更有效的防御对抗攻击的集成学习方法，解决传统简单平均集成在面对强扰动时性能不足的问题

Method: 基于学习动力学守恒定律和熵减少原理，提出熵驱动的终身集成学习框架，构建免疫机制来防御迁移对抗攻击，在CIFAR-10数据集上实现对抗鲁棒性

Result: 相比简单平均清洁样本和对抗样本专用模型的朴素集成，提出的logifold方法在大多数测试案例中达到更高准确率，在强扰动下获得显著性能提升

Conclusion: 学习动力学基本定律为机器学习提供了理论基础，基于熵的终身集成学习方法能有效提升对抗鲁棒性，特别是在强扰动场景下表现优异

Abstract: We formulate the fundamental laws governing learning dynamics, namely the conservation law and the decrease of total entropy. Within this framework, we introduce an entropy-based lifelong ensemble learning method. We evaluate its effectiveness by constructing an immunization mechanism to defend against transfer-based adversarial attacks on the CIFAR-10 dataset. Compared with a naive ensemble formed by simply averaging models specialized on clean and adversarial samples, the resulting logifold achieves higher accuracy in most test cases, with particularly large gains under strong perturbations.

</details>


### [51] [Laplacian Representations for Decision-Time Planning](https://arxiv.org/abs/2602.05031)
*Dikshant Shehmar,Matthew Schlegel,Matthew E. Taylor,Marlos C. Machado*

Main category: cs.LG

TL;DR: ALPS：基于拉普拉斯表示的层次规划算法，在离线目标条件RL任务中超越模型自由方法


<details>
  <summary>Details</summary>
Motivation: 基于学习的模型进行规划是模型强化学习的关键挑战，状态表示需要支持局部成本计算同时保持长时域结构

Method: 使用拉普拉斯表示作为规划的有效潜在空间，捕捉多时间尺度的状态空间距离，并基于此提出ALPS层次规划算法

Result: 在OGBench基准的离线目标条件RL任务中，ALPS超越了常用的基线方法，该基准先前由模型自由方法主导

Conclusion: 拉普拉斯表示为规划提供了有效的潜在空间，能够捕捉状态距离、分解长时域问题并减轻预测误差累积，ALPS算法在离线RL任务中表现出色

Abstract: Planning with a learned model remains a key challenge in model-based reinforcement learning (RL). In decision-time planning, state representations are critical as they must support local cost computation while preserving long-horizon structure. In this paper, we show that the Laplacian representation provides an effective latent space for planning by capturing state-space distances at multiple time scales. This representation preserves meaningful distances and naturally decomposes long-horizon problems into subgoals, also mitigating the compounding errors that arise over long prediction horizons. Building on these properties, we introduce ALPS, a hierarchical planning algorithm, and demonstrate that it outperforms commonly used baselines on a selection of offline goal-conditioned RL tasks from OGBench, a benchmark previously dominated by model-free methods.

</details>


### [52] [Causal Representation Meets Stochastic Modeling under Generic Geometry](https://arxiv.org/abs/2602.05033)
*Jiaxu Ren,Yixin Wang,Biwei Huang*

Main category: cs.LG

TL;DR: 提出MUTATE框架，用于从连续时间潜在随机点过程中学习可识别的因果表示，解决基因组学和神经科学等领域的科学问题


<details>
  <summary>Details</summary>
Motivation: 现有可识别因果表示学习方法主要关注i.i.d.观测或离散时间潜在过程，但许多现实场景（如基因组突变积累、神经元放电）需要识别连续时间随机过程，这是当前研究的空白

Method: 开发MUTATE框架：基于变分自编码器，包含时间自适应转换模块，通过分析参数空间的几何特性来保证可识别性，专门处理连续时间潜在随机点过程

Result: 在模拟和实证研究中，MUTATE能有效回答科学问题，如基因组中突变的积累机制和神经元放电触发的时间动态机制

Conclusion: MUTATE框架成功实现了连续时间潜在随机点过程的因果表示学习，填补了现有方法的空白，为气候科学、生物学、物理学等领域的科学发现提供了新工具

Abstract: Learning meaningful causal representations from observations has emerged as a crucial task for facilitating machine learning applications and driving scientific discoveries in fields such as climate science, biology, and physics. This process involves disentangling high-level latent variables and their causal relationships from low-level observations. Previous work in this area that achieves identifiability typically focuses on cases where the observations are either i.i.d. or follow a latent discrete-time process. Nevertheless, many real-world settings require identifying latent variables that are continuous-time stochastic processes (e.g., multivariate point processes). To this end, we develop identifiable causal representation learning for continuous-time latent stochastic point processes. We study its identifiability by analyzing the geometry of the parameter space. Furthermore, we develop MUTATE, an identifiable variational autoencoder framework with a time-adaptive transition module to infer stochastic dynamics. Across simulated and empirical studies, we find that MUTATE can effectively answer scientific questions, such as the accumulation of mutations in genomics and the mechanisms driving neuron spike triggers in response to time-varying dynamics.

</details>


### [53] [Feedback Control for Multi-Objective Graph Self-Supervision](https://arxiv.org/abs/2602.05036)
*Karish Grover,Theodore Vasiloudis,Han Xie,Sixing Lu,Xiang Song,Christos Faloutsos*

Main category: cs.LG

TL;DR: ControlG：基于控制理论的图自监督学习多任务协调框架，通过时序分配而非权重混合解决目标冲突问题


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习方法在组合多个预训练目标时存在目标干扰和训练不稳定问题，传统的每更新混合方法导致冲突诱导的负迁移、目标效用非平稳和隐藏目标饥饿三种失败模式

Method: 提出ControlG框架，将多目标图SSL重构为反馈控制的时序分配问题，包含三个核心组件：估计每个目标的难度和成对对抗性；通过帕累托感知的对数超体积规划器规划目标预算；使用PID控制器进行调度

Result: 在9个数据集上，ControlG持续优于最先进的基线方法，同时生成可审计的调度计划，揭示哪些目标驱动了学习过程

Conclusion: 协调多目标图自监督学习本质上是一个时序分配问题，而非简单的权重混合；控制理论方法能够有效解决目标冲突，提供可解释的训练过程

Abstract: Can multi-task self-supervised learning on graphs be coordinated without the usual tug-of-war between objectives? Graph self-supervised learning (SSL) offers a growing toolbox of pretext objectives: mutual information, reconstruction, contrastive learning; yet combining them reliably remains a challenge due to objective interference and training instability. Most multi-pretext pipelines use per-update mixing, forcing every parameter update to be a compromise, leading to three failure modes: Disagreement (conflict-induced negative transfer), Drift (nonstationary objective utility), and Drought (hidden starvation of underserved objectives). We argue that coordination is fundamentally a temporal allocation problem: deciding when each objective receives optimization budget, not merely how to weigh them. We introduce ControlG, a control-theoretic framework that recasts multi-objective graph SSL as feedback-controlled temporal allocation by estimating per-objective difficulty and pairwise antagonism, planning target budgets via a Pareto-aware log-hypervolume planner, and scheduling with a Proportional-Integral-Derivative (PID) controller. Across 9 datasets, ControlG consistently outperforms state-of-the-art baselines, while producing an auditable schedule that reveals which objectives drove learning.

</details>


### [54] [Learning, Solving and Optimizing PDEs with TensorGalerkin: an efficient high-performance Galerkin assembly algorithm](https://arxiv.org/abs/2602.05052)
*Shizheng Wen,Mingyuan Chi,Tianwei Yu,Ben Moseley,Mike Yan Michelis,Pu Ren,Hao Sun,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出统一的算法框架，用于变分结构PDE的数值求解、约束优化和物理信息学习，基于Galerkin离散化和高效GPU兼容的TensorGalerkin张量系统组装框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理变分结构PDE的数值求解、优化和学习时缺乏统一高效的计算框架，特别是在GPU加速和端到端可微分性方面存在不足。

Method: 基于Galerkin离散化，开发TensorGalerkin框架：通过Python级Map阶段张量化单元操作，然后利用稀疏矩阵乘法在网格诱导的稀疏图上进行全局归约，实现高效线性系统组装。

Result: 在2D和3D椭圆、抛物和双曲PDE的非结构化网格上，相比多种基线方法，该框架在所有目标下游应用中均展现出显著的计算效率和精度优势。

Conclusion: 提出的统一框架为变分PDE的数值求解、约束优化和物理信息学习提供了高效、准确且可扩展的计算工具，具有广泛的应用潜力。

Abstract: We present a unified algorithmic framework for the numerical solution, constrained optimization, and physics-informed learning of PDEs with a variational structure. Our framework is based on a Galerkin discretization of the underlying variational forms, and its high efficiency stems from a novel highly-optimized and GPU-compliant TensorGalerkin framework for linear system assembly (stiffness matrices and load vectors). TensorGalerkin operates by tensorizing element-wise operations within a Python-level Map stage and then performs global reduction with a sparse matrix multiplication that performs message passing on the mesh-induced sparsity graph. It can be seamlessly employed downstream as i) a highly-efficient numerical PDEs solver, ii) an end-to-end differentiable framework for PDE-constrained optimization, and iii) a physics-informed operator learning algorithm for PDEs. With multiple benchmarks, including 2D and 3D elliptic, parabolic, and hyperbolic PDEs on unstructured meshes, we demonstrate that the proposed framework provides significant computational efficiency and accuracy gains over a variety of baselines in all the targeted downstream applications.

</details>


### [55] [Quantile-Physics Hybrid Framework for Safe-Speed Recommendation under Diverse Weather Conditions Leveraging Connected Vehicle and Road Weather Information Systems Data](https://arxiv.org/abs/2602.05053)
*Wen Zhang,Adel W. Sadek,Chunming Qiao*

Main category: cs.LG

TL;DR: 提出混合预测框架，结合QRF和物理约束为高速公路提供实时安全速度区间推荐


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件显著影响驾驶员能见度和轮胎-路面摩擦力，需要调整安全驾驶速度以降低事故风险

Method: 使用分位数回归森林(QRF)预测10分钟窗口内的车速分布，结合基于物理的制动距离约束计算安全速度上限，融合QRF预测分位数、限速标志和物理约束得到最终推荐区间

Result: QRF模型MAE为1.55 mph，96.43%的中位数速度预测误差在5 mph内，PICP(50%)为48.55%，在不同天气类型和路段上表现出良好的泛化能力

Conclusion: 该框架能够响应天气变化并在不同路段上泛化，有望实际部署以提高交通安全并减少天气相关事故

Abstract: Inclement weather conditions can significantly impact driver visibility and tire-road surface friction, requiring adjusted safe driving speeds to reduce crash risk. This study proposes a hybrid predictive framework that recommends real-time safe speed intervals for freeway travel under diverse weather conditions. Leveraging high-resolution Connected Vehicle (CV) data and Road Weather Information System (RWIS) data collected in Buffalo, NY, from 2022 to 2023, we construct a spatiotemporally aligned dataset containing over 6.6 million records across 73 days. The core model employs Quantile Regression Forests (QRF) to estimate vehicle speed distributions in 10-minute windows, using 26 input features that capture meteorological, pavement, and temporal conditions. To enforce safety constraints, a physics-based upper speed limit is computed for each interval based on real-time road grip and visibility, ensuring that vehicles can safely stop within their sight distance. The final recommended interval fuses QRF-predicted quantiles with both posted speed limits and the physics-derived upper bound. Experimental results demonstrate strong predictive performance: the QRF model achieves a mean absolute error of 1.55 mph, with 96.43% of median speed predictions within 5 mph, a PICP (50%) of 48.55%, and robust generalization across weather types. The model's ability to respond to changing weather conditions and generalize across road segments shows promise for real-world deployment, thereby improving traffic safety and reducing weather-related crashes.

</details>


### [56] [Reliable Explanations or Random Noise? A Reliability Metric for XAI](https://arxiv.org/abs/2602.05082)
*Poushali Sengupta,Sabita Maharjan,Frank Eliassen,Shashi Raj Pandey,Yan Zhang*

Main category: cs.LG

TL;DR: 论文提出解释可靠性指数（ERI），用于量化机器学习模型解释在现实部署条件下的稳定性，包括输入扰动、特征冗余、模型演化和分布偏移四种可靠性公理。


<details>
  <summary>Details</summary>
Motivation: 复杂机器学习模型在关键领域（能源、医疗、金融、自动驾驶）的应用需要可解释性，但现有解释方法（如SHAP和集成梯度）在现实非对抗性变化下的可靠性尚未得到充分衡量。这些解释在系统级条件下（小输入扰动、相关表示、微小模型更新）可能大幅变化，损害了可靠性。

Method: 提出解释可靠性指数（ERI）系列指标，基于四个可靠性公理：对输入扰动的鲁棒性、特征冗余下的一致性、模型演化中的平滑性、对温和分布偏移的韧性。为每个公理提供形式化保证（如Lipschitz型边界和时间稳定性结果）。提出ERI-T用于序列模型的时序可靠性度量，并建立ERI-Bench基准测试系统。

Result: 实验结果显示流行解释方法存在广泛的可靠性失效，表明在现实部署条件下解释可能不稳定。ERI能够暴露和量化这些不稳定性，为解释可靠性提供原则性评估。

Conclusion: 通过引入ERI指标和基准测试，论文为可解释AI系统提供了量化解释可靠性的方法，支持构建更可信的XAI系统。这项工作揭示了当前解释方法的可靠性缺陷，并为改进解释稳定性提供了评估框架。

Abstract: In recent years, explaining decisions made by complex machine learning models has become essential in high-stakes domains such as energy systems, healthcare, finance, and autonomous systems. However, the reliability of these explanations, namely, whether they remain stable and consistent under realistic, non-adversarial changes, remains largely unmeasured. Widely used methods such as SHAP and Integrated Gradients (IG) are well-motivated by axiomatic notions of attribution, yet their explanations can vary substantially even under system-level conditions, including small input perturbations, correlated representations, and minor model updates. Such variability undermines explanation reliability, as reliable explanations should remain consistent across equivalent input representations and small, performance-preserving model changes. We introduce the Explanation Reliability Index (ERI), a family of metrics that quantifies explanation stability under four reliability axioms: robustness to small input perturbations, consistency under feature redundancy, smoothness across model evolution, and resilience to mild distributional shifts. For each axiom, we derive formal guarantees, including Lipschitz-type bounds and temporal stability results. We further propose ERI-T, a dedicated measure of temporal reliability for sequential models, and introduce ERI-Bench, a benchmark designed to systematically stress-test explanation reliability across synthetic and real-world datasets. Experimental results reveal widespread reliability failures in popular explanation methods, showing that explanations can be unstable under realistic deployment conditions. By exposing and quantifying these instabilities, ERI enables principled assessment of explanation reliability and supports more trustworthy explainable AI (XAI) systems.

</details>


### [57] [Individual Fairness In Strategic Classification](https://arxiv.org/abs/2602.05084)
*Zhiqun Zuo,Mohammad Mahdi Khalili*

Main category: cs.LG

TL;DR: 研究战略分类中的个体公平性问题，提出随机分类器方法实现个体公平，并通过线性规划找到最优解，同时可扩展到群体公平概念


<details>
  <summary>Details</summary>
Motivation: 战略分类中个体修改特征以影响机器学习决策带来公平性挑战，群体公平已有广泛研究，但个体公平仍未被充分探索

Method: 分析基于阈值的分类器，证明确定性阈值违反个体公平；引入随机分类器实现个体公平的条件，通过线性规划问题找到最优且个体公平的随机分类器；方法可扩展到群体公平概念

Result: 在真实数据集上的实验证实，该方法能有效减轻不公平性并改善公平性与准确性的权衡

Conclusion: 随机分类器是实现战略分类中个体公平的有效方法，通过线性规划可找到最优解，且该方法具有扩展到群体公平概念的潜力

Abstract: Strategic classification, where individuals modify their features to influence machine learning (ML) decisions, presents critical fairness challenges. While group fairness in this setting has been widely studied, individual fairness remains underexplored. We analyze threshold-based classifiers and prove that deterministic thresholds violate individual fairness. Then, we investigate the possibility of using a randomized classifier to achieve individual fairness. We introduce conditions under which a randomized classifier ensures individual fairness and leverage these conditions to find an optimal and individually fair randomized classifier through a linear programming problem. Additionally, we demonstrate that our approach can be extended to group fairness notions. Experiments on real-world datasets confirm that our method effectively mitigates unfairness and improves the fairness-accuracy trade-off.

</details>


### [58] [Unbiased Single-Queried Gradient for Combinatorial Objective](https://arxiv.org/abs/2602.05119)
*Thanawat Sornwanee*

Main category: cs.LG

TL;DR: 提出一种用于组合优化问题的随机梯度方法，通过单次查询获得无偏梯度估计，包含REINFORCE方法作为特例


<details>
  <summary>Details</summary>
Motivation: 组合优化问题在概率重构中常面临超立方体上的优化，对应伯努利概率参数。组合性质导致精确梯度计算需要多次查询，效率低下

Method: 提出一种随机梯度方法，仅需对组合函数进行单次查询即可获得无偏梯度估计。该方法通过重要性采样包含REINFORCE方法，并扩展为一类新的随机梯度

Result: 该方法能够高效计算组合优化问题的梯度，显著减少查询次数，同时保持梯度估计的无偏性

Conclusion: 提出的随机梯度方法为组合优化问题提供了高效的梯度计算框架，统一了现有方法并扩展了新的梯度估计技术

Abstract: In a probabilistic reformulation of a combinatorial problem, we often face an optimization over a hypercube, which corresponds to the Bernoulli probability parameter for each binary variable in the primal problem. The combinatorial nature suggests that an exact gradient computation requires multiple queries. We propose a stochastic gradient that is unbiased and requires only a single query of the combinatorial function. This method encompasses a well-established REINFORCE (through an importance sampling), as well as including a class of new stochastic gradients.

</details>


### [59] [Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks](https://arxiv.org/abs/2602.05125)
*William F. Shen,Xinchi Qiu,Chenxi Whitehouse,Lisa Alazraki,Shashwat Goel,Francesco Barbieri,Timon Willi,Akhil Mathur,Ilias Leontiadis*

Main category: cs.LG

TL;DR: RRD框架通过递归分解-过滤循环优化LLM评估准则，提升评估准确性和强化微调效果


<details>
  <summary>Details</summary>
Motivation: 现有评估准则存在覆盖不足、维度混淆、偏好方向错位、冗余和高度相关标准等问题，导致评估准确性下降和强化微调效果不佳

Method: 提出RRD框架，采用递归分解-过滤循环：将粗粒度准则分解为细粒度判别性标准，过滤错位和冗余准则，使用相关性感知加权方案防止高度相关标准的过度表示

Result: 在JudgeBench和PPE上显著提升GPT-4o和Llama3.1-405B评估准确性（JudgeBench最高提升17.7分）；作为WildChat RFT奖励信号，Qwen3-4B奖励提升160%，Llama3.1-8B提升60%，优于基线10-20%；在HealthBench-Hard和BiGGen Bench上具有良好迁移性

Conclusion: RRD为开放领域LLM评估和奖励建模提供了可扩展、可解释的基础，通过递归准则优化显著提升了评估准确性和强化微调效果

Abstract: Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.

</details>


### [60] [Decoupled Orthogonal Dynamics: Regularization for Deep Network Optimizers](https://arxiv.org/abs/2602.05136)
*Hao Chen,Jinghui Yuan,Hanmin Zhang*

Main category: cs.LG

TL;DR: AdamW中的标准权重衰减并非最优，存在径向拉锯战问题。作者提出正交动力学解耦方法AdamO，将范数控制与自适应预条件分离，改善泛化能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管AdamW将权重衰减与自适应梯度缩放解耦，但存在根本冲突：径向拉锯战。梯度倾向于增加参数范数以扩展有效容量，而权重衰减则不加区分地抑制范数增长，这种推拉相互作用导致径向振荡，向Adam的第二矩估计注入噪声，可能损害精细的切向特征学习。

Method: 提出正交动力学解耦方法，并实例化为AdamO：使用SGD风格的更新处理一维范数控制，而Adam的自适应预条件仅限于切向子空间。AdamO进一步结合曲率自适应径向步长调整，以及针对尺度不变层和低维参数的架构感知规则和投影。

Result: 在视觉和语言任务上的实验表明，AdamO相比AdamW提高了泛化能力和稳定性，且未引入额外的复杂约束。

Conclusion: 参数的大小和方向在优化器动力学中扮演不同角色，应被解耦。AdamO通过正交动力学解耦有效解决了径向拉锯战问题，为优化器设计提供了新思路。

Abstract: Is the standard weight decay in AdamW truly optimal? Although AdamW decouples weight decay from adaptive gradient scaling, a fundamental conflict remains: the Radial Tug-of-War. In deep learning, gradients tend to increase parameter norms to expand effective capacity while steering directions to learn features, whereas weight decay indiscriminately suppresses norm growth. This push--pull interaction induces radial oscillations, injecting noise into Adam's second-moment estimates and potentially degrading delicate tangential feature learning. We argue that magnitude and direction play distinct roles and should be decoupled in optimizer dynamics. We propose Orthogonal Dynamics Decoupling and instantiate it as AdamO: an SGD-style update handles the one-dimensional norm control, while Adam's adaptive preconditioning is confined to the tangential subspace. AdamO further incorporates curvature-adaptive radial step sizing and architecture-aware rules and projections for scale-invariant layers and low-dimensional parameters. Experiments on vision and language tasks show that AdamO improves generalization and stability over AdamW without introducing additional complex constraints.

</details>


### [61] [Adaptive Exploration for Latent-State Bandits](https://arxiv.org/abs/2602.05139)
*Jikai Jin,Kenneth Hung,Sanath Kumar Krishnamurthy,Baoyi Shi,Congshan Zhang*

Main category: cs.LG

TL;DR: 提出了一类无需显式状态建模的bandit算法，通过滞后上下文特征和协调探测策略来应对存在未观测混杂因子的非平稳环境


<details>
  <summary>Details</summary>
Motivation: 经典多臂老虎机算法在存在隐藏、时变状态的环境中往往失效，这些未观测混杂因子会导致奖励估计偏差和状态信息受限，影响最优动作选择

Method: 引入基于状态模型无关的bandit算法族，利用滞后上下文特征和协调探测策略，隐式追踪潜在状态并区分状态依赖的奖励模式，无需显式状态建模

Result: 在多样化设置中的实证结果表明，该方法相比经典方法具有优越性能，能够学习最优策略，同时保持计算效率和对非平稳奖励的鲁棒适应

Conclusion: 所提出的状态模型无关算法能有效应对未观测混杂因子带来的挑战，为实际应用中的算法选择提供了实用建议

Abstract: The multi-armed bandit problem is a core framework for sequential decision-making under uncertainty, but classical algorithms often fail in environments with hidden, time-varying states that confound reward estimation and optimal action selection. We address key challenges arising from unobserved confounders, such as biased reward estimates and limited state information, by introducing a family of state-model-free bandit algorithms that leverage lagged contextual features and coordinated probing strategies. These implicitly track latent states and disambiguate state-dependent reward patterns. Our methods and their adaptive variants can learn optimal policies without explicit state modeling, combining computational efficiency with robust adaptation to non-stationary rewards. Empirical results across diverse settings demonstrate superior performance over classical approaches, and we provide practical recommendations for algorithm selection in real-world applications.

</details>


### [62] [Fairness Under Group-Conditional Prior Probability Shift: Invariance, Drift, and Target-Aware Post-Processing](https://arxiv.org/abs/2602.05144)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 该论文研究群体条件先验概率偏移(GPPS)下的公平性问题，证明了错误率公平标准的结构不变性，提出了无需目标域标签的公平性评估方法，并开发了TAP-GPPS算法来在目标域实现人口统计均等。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统通常在历史数据上训练和评估公平性，但部署环境中的条件可能发生变化。特别是当不同人口群体中正结果的流行率变化不同时，会导致群体条件先验概率偏移(GPPS)。这种偏移在实际应用中很常见，如疾病率在不同人群中变化不同，或经济条件对贷款违约率的影响不均等。需要研究这种偏移对公平性标准的影响以及如何应对。

Method: 1. 理论分析：证明在GPPS下，基于错误率的公平标准(均衡几率)具有结构不变性，而基于接受率的公平标准(人口统计均等)会漂移，且这种漂移对于非平凡分类器是不可避免的(偏移鲁棒不可能性定理)。2. 识别性分析：证明目标域风险和公平性指标可以在没有目标标签的情况下识别，利用GPPS下ROC量的不变性，仅从源标签和无标签目标数据就能进行一致估计。3. 算法开发：提出TAP-GPPS算法，从无标签数据估计流行率，校正后验概率，并选择阈值来满足目标域的人口统计均等。

Result: 1. 理论结果：证明了公平性标准的二分法——均衡几率在GPPS下保持不变，而人口统计均等会漂移。2. 识别性结果：目标域风险和公平性指标可以在没有目标标签的情况下一致估计。3. 算法性能：实验验证了理论预测，并表明TAP-GPPS算法能以最小的效用损失实现目标公平性。

Conclusion: 该研究揭示了群体条件先验概率偏移对公平性标准的不同影响，提供了无需目标标签评估目标域公平性的方法，并开发了实用的算法来应对这种偏移。这些结果为在动态环境中部署公平的机器学习系统提供了理论基础和实用工具。

Abstract: Machine learning systems are often trained and evaluated for fairness on historical data, yet deployed in environments where conditions have shifted. A particularly common form of shift occurs when the prevalence of positive outcomes changes differently across demographic groups--for example, when disease rates rise faster in one population than another, or when economic conditions affect loan default rates unequally. We study group-conditional prior probability shift (GPPS), where the label prevalence $P(Y=1\mid A=a)$ may change between training and deployment while the feature-generation process $P(X\mid Y,A)$ remains stable. Our analysis yields three main contributions. First, we prove a fundamental dichotomy: fairness criteria based on error rates (equalized odds) are structurally invariant under GPPS, while acceptance-rate criteria (demographic parity) can drift--and we prove this drift is unavoidable for non-trivial classifiers (shift-robust impossibility). Second, we show that target-domain risk and fairness metrics are identifiable without target labels: the invariance of ROC quantities under GPPS enables consistent estimation from source labels and unlabeled target data alone, with finite-sample guarantees. Third, we propose TAP-GPPS, a label-free post-processing algorithm that estimates prevalences from unlabeled data, corrects posteriors, and selects thresholds to satisfy demographic parity in the target domain. Experiments validate our theoretical predictions and demonstrate that TAP-GPPS achieves target fairness with minimal utility loss.

</details>


### [63] [TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference](https://arxiv.org/abs/2602.05145)
*Jiyoung Park,Hankyu Jang,Changseok Song,Wookeun Jung*

Main category: cs.LG

TL;DR: TIDE是一个集成到LLM推理系统的在线草稿自适应框架，通过重用推理过程中的隐藏状态进行零开销草稿训练，利用自适应运行时控制和异构集群映射，在真实工作负载中实现1.15倍吞吐量提升和1.67倍训练时间减少。


<details>
  <summary>Details</summary>
Motivation: 推测解码能加速LLM推理，但在实际部署中面临挑战：工作负载动态变化、系统级约束限制性能提升。现有方法需要重新加载目标模型或重新计算训练信号，导致额外开销。

Method: TIDE框架将在线草稿自适应集成到高性能LLM推理系统中：1）重用推理过程中生成的目标模型隐藏状态作为训练信号，实现零开销草稿训练；2）采用自适应运行时控制，仅在有益时激活推测和训练；3）利用异构集群，将解耦的推理和训练映射到合适的GPU类别。

Result: 在多样化真实世界工作负载中，TIDE相比静态推测解码实现最高1.15倍吞吐量提升，相比重新计算训练信号的方法减少1.67倍草稿训练时间。

Conclusion: TIDE通过将在线草稿自适应直接集成到LLM推理系统中，解决了推测解码在实际部署中的挑战，实现了高效的自适应推测解码，显著提升了系统吞吐量并减少了训练开销。

Abstract: Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptation directly into high-performance LLM inference systems. TIDE reuses target model hidden states generated during inference as training signals, enabling zero-overhead draft adaptation without reloading the target model, and employs adaptive runtime control to activate speculation and training only when beneficial. TIDE exploits heterogeneous clusters by mapping decoupled inference and training to appropriate GPU classes. Across diverse real-world workloads, TIDE achieves up to 1.15x throughput improvement over static speculative decoding while reducing draft training time by 1.67x compared to approaches that recompute training signals.

</details>


### [64] [Cross-talk based multi-task learning for fault classification of physically coupled machine system](https://arxiv.org/abs/2602.05146)
*Wonjun Yi,Rismaya Kumar Mishra,Yong-Hwa Park*

Main category: cs.LG

TL;DR: 该论文提出了一种基于交叉对话结构的多任务学习框架，利用故障信号中物理耦合的额外信息，通过联合学习故障条件和相关物理变量来提升故障分类性能。


<details>
  <summary>Details</summary>
Motivation: 机器系统产生的信号中，故障条件与各种物理变量存在物理耦合。现有故障分类研究大多仅依赖直接故障标签，忽略了信号中自然嵌入的由其他物理耦合信息塑造的额外信息。利用这种耦合关系可以提升故障分类性能。

Method: 采用多任务学习框架，特别是交叉对话结构，通过交叉对话层实现任务间的可控信息交换，避免负迁移。基于先前提出的残差神经降维模型，将其扩展到两个物理耦合显著的基准数据集：无人机故障数据集和电机复合故障数据集。

Result: 在两个基准测试中，残差神经降维模型持续优于单任务模型、合并所有标签组合的多类模型以及共享主干多任务模型。对于无人机故障数据集，通过学习故障分类与物理属性（机器类型和操纵方向）的联合学习，交叉对话架构能更好地分类故障。对于电机复合故障数据集，测试了单通道和多通道数据作为分类器输入时的性能。

Conclusion: 交叉对话结构的多任务学习框架能有效利用故障信号中的物理耦合信息，通过联合学习故障条件和相关物理变量，显著提升故障分类性能，优于传统单任务和共享主干多任务方法。

Abstract: Machine systems inherently generate signals in which fault conditions and various physical variables are physically coupled. Although many existing fault classification studies rely solely on direct fault labels, the aforementioned signals naturally embed additional information shaped by other physically coupled information. Herein, we leverage this coupling through a multi-task learning (MTL) framework that jointly learns fault conditions and the related physical variables. Among MTL architectures, crosstalk structures have distinct advantages because they allow for controlled information exchange between tasks through the cross-talk layer while preventing negative transfer, in contrast to shared trunk architectures that often mix incompatible features. We build on our previously introduced residual neural dimension reductor model, and extend its application to two benchmarks where physical coupling is prominent. The first benchmark is a drone fault dataset, in which machine type and maneuvering direction significantly alter the frequency components of measured signals even under the same nominal condition. By learning fault classification together with these physical attributes, the cross-talk architecture can better classify faults. The second benchmark dataset is the motor compound fault dataset. In this system, each fault component, inner race fault, outer race fault, misalignment, and unbalance is coupled to the other. For motor compound fault, we also test classification performance when we use single-channel data or multi-channel data as input to the classifier. Across both benchmarks, our residual neural dimension reductor, consistently outperformed single-task models, multi-class models that merge all label combinations, and shared trunk multi-task models.

</details>


### [65] [CoSA: Compressed Sensing-Based Adaptation of Large Language Models](https://arxiv.org/abs/2602.05148)
*Songtao Wei,Yi Li,Bohan Zhang,Zhichun Guo,Ying Huang,Yuede Ji,Miao Yin,Guanpeng Li,Bingzhe Li*

Main category: cs.LG

TL;DR: CoSA是一种基于压缩感知理论的新型参数高效微调方法，通过固定随机投影矩阵和可学习核心表达权重更新，突破了传统低秩分解方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA和PiSSA）依赖权重更新的低秩分解，但低秩假设可能限制表达能力，特别是在任务特定适应场景中奇异值分布相对均匀时。

Method: CoSA基于压缩感知理论，不将权重更新约束在低秩子空间，而是通过固定随机投影矩阵和紧凑可学习核心来表达权重更新。该方法提供了形式化的理论分析，证明权重更新可以压缩编码到低维空间并通过随机投影映射回来。

Result: 在10个多样化任务（包括自然语言理解和生成）上评估CoSA，使用RoBERTa、Llama和Qwen家族的5个不同规模模型。在所有设置中，CoSA始终匹配或优于最先进的PEFT方法。

Conclusion: CoSA为高效且表达力强的多尺度模型适应提供了一个原则性视角，突破了传统低秩分解方法的限制，在保持参数效率的同时提供了更好的表达能力。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.

</details>


### [66] [Position: Capability Control Should be a Separate Goal From Alignment](https://arxiv.org/abs/2602.05164)
*Shoaib Ahmed Siddiqui,Eleni Triantafillou,David Krueger,Adrian Weller*

Main category: cs.LG

TL;DR: 该立场论文主张将能力控制视为与对齐不同的独立目标，提出在模型生命周期中实施三层能力控制机制，并倡导采用深度防御方法组合这些控制措施。


<details>
  <summary>Details</summary>
Motivation: 基础模型在广泛数据分布上训练，产生了通用能力，这既支持许多下游应用，也扩大了潜在滥用和失败的空间。当前对齐方法通常是上下文和偏好驱动的，而能力控制需要施加硬性操作限制，包括在对抗性诱导下的行为限制。

Method: 将能力控制机制组织为三个层次：1) 基于数据的训练分布控制；2) 基于学习的权重或表示层干预；3) 基于系统的部署后护栏，控制输入、输出和动作。由于每个层次单独使用时都有特征性失败模式，作者倡导采用深度防御方法，在整个技术栈中组合互补的控制措施。

Result: 提出了一个系统化的能力控制框架，明确了能力控制与对齐的区别，并建立了跨模型生命周期的三层控制机制分类。该方法为构建更安全、可控的基础模型提供了理论基础和实施路径。

Conclusion: 能力控制应被视为与对齐不同的独立目标，需要在整个模型生命周期中实施多层次控制机制。采用深度防御方法组合互补控制措施是必要的，但实现有效控制仍面临关键挑战，包括知识的双重用途性质和组合泛化问题。

Abstract: Foundation models are trained on broad data distributions, yielding generalist capabilities that enable many downstream applications but also expand the space of potential misuse and failures. This position paper argues that capability control -- imposing restrictions on permissible model behavior -- should be treated as a distinct goal from alignment. While alignment is often context and preference-driven, capability control aims to impose hard operational limits on permissible behaviors, including under adversarial elicitation. We organize capability control mechanisms across the model lifecycle into three layers: (i) data-based control of the training distribution, (ii) learning-based control via weight- or representation-level interventions, and (iii) system-based control via post-deployment guardrails over inputs, outputs, and actions. Because each layer has characteristic failure modes when used in isolation, we advocate for a defense-in-depth approach that composes complementary controls across the full stack. We further outline key open challenges in achieving such control, including the dual-use nature of knowledge and compositional generalization.

</details>


### [67] [EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization](https://arxiv.org/abs/2602.05165)
*Kevin Han,Yuhang Zhou,Mingze Gao,Gedi Zhou,Serena Li,Abhishek Kumar,Xiangjun Fan,Weiwei Li,Lizhu Zhang*

Main category: cs.LG

TL;DR: EBPO是一种新的强化学习框架，通过经验贝叶斯方法将局部组统计量与全局先验相结合，解决了GRPO在小批量下高方差和失败场景中梯度消失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法如GRPO面临稳定性挑战：在小批量计算约束下估计器方差高，在饱和失败场景中（所有响应获得相同零奖励）梯度信号消失，限制了强化学习在增强大语言模型推理能力中的应用。

Method: 提出经验贝叶斯策略优化（EBPO）框架，使用收缩估计器动态平衡局部组统计量与通过Welford在线算法更新的全局先验，正则化局部组基线，从策略累积的全局统计中借用强度。

Result: 理论上证明EBPO相比GRPO保证更低的均方误差、有界的熵衰减和失败场景中非消失的惩罚信号。实证上在AIME和OlympiadBench等多个基准测试中一致优于GRPO和其他基线方法，表现出更好的训练稳定性，即使在小批量下也能获得高性能增益，并从难度分层课程学习中显著受益。

Conclusion: EBPO通过经验贝叶斯方法有效解决了RLVR中的稳定性问题，为增强大语言模型的推理能力提供了更可靠和高效的强化学习框架。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.

</details>


### [68] [Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.05183)
*John Yan,Michael Yu,Yuqi Sun,Alexander Duffy,Tyler Marques,Matthew Lyle Olson*

Main category: cs.LG

TL;DR: 论文提出Meta-Autointerp方法，通过稀疏自编码器(SAEs)和LLM摘要技术分析复杂强化学习环境中大语言模型的训练动态，发现细粒度行为模式，但发现多数SAE特征和LLM生成假设对人类理解帮助有限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在复杂强化学习和多智能体环境中训练，理解训练过程中的行为变化变得困难。稀疏自编码器在数据为中心的可解释性方面显示出潜力，但需要系统方法来分析大规模强化学习训练运行。

Method: 提出Meta-Autointerp方法，将SAE特征分组为可解释的训练动态假设。结合预训练的稀疏自编码器和LLM摘要方法，分析Full-Press Diplomacy环境中的大规模强化学习训练运行。通过自动评估和用户研究验证发现。

Result: 发现细粒度行为包括角色扮演模式、退化输出、语言切换，以及高级战略行为和特定环境错误。90%发现的SAE元特征具有显著性，发现意外的奖励黑客行为。用户研究表明多数SAE特征和LLM假设对人类帮助有限，但部分SAE衍生假设对下游任务具有预测价值。通过增强未训练智能体的系统提示，得分提升+14.2%。

Conclusion: 稀疏自编码器和LLM摘要器为智能体行为提供互补视角，Meta-Autointerp框架为未来数据为中心的可解释性工作提供了实用起点，有助于确保训练过程中LLM行为的可信度。

Abstract: Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent's system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.

</details>


### [69] [SpectraKAN: Conditioning Spectral Operators](https://arxiv.org/abs/2602.05187)
*Chun-Wun Cheng,Carola-Bibiane Schönlieb,Angelica I. Aviles-Rivero*

Main category: cs.LG

TL;DR: SpectraKAN：一种输入自适应的谱神经算子，通过全局表示调制多尺度傅里叶主干，显著提升PDE求解性能


<details>
  <summary>Details</summary>
Motivation: 现有谱神经算子（如FNO）使用静态傅里叶核，无法捕捉多尺度、状态依赖和各向异性的系统动态，限制了其在复杂PDE求解中的适应性。

Method: 从时空历史中提取紧凑全局表示，通过单查询交叉注意力调制多尺度傅里叶主干，将静态谱卷积转变为输入自适应的积分算子。

Result: 在多种PDE基准测试中达到最先进性能，RMSE降低高达49%，在具有挑战性的时空预测任务上表现尤为突出。

Conclusion: SpectraKAN通过输入自适应调制机制，在保持谱混合效率的同时显著提升了神经算子对复杂PDE动态的建模能力，为自适应谱算子提供了理论保证。

Abstract: Spectral neural operators, particularly Fourier Neural Operators (FNO), are a powerful framework for learning solution operators of partial differential equations (PDEs) due to their efficient global mixing in the frequency domain. However, existing spectral operators rely on static Fourier kernels applied uniformly across inputs, limiting their ability to capture multi-scale, regime-dependent, and anisotropic dynamics governed by the global state of the system. We introduce SpectraKAN, a neural operator that conditions the spectral operator on the input itself, turning static spectral convolution into an input-conditioned integral operator. This is achieved by extracting a compact global representation from spatio-temporal history and using it to modulate a multi-scale Fourier trunk via single-query cross-attention, enabling the operator to adapt its behaviour while retaining the efficiency of spectral mixing. We provide theoretical justification showing that this modulation converges to a resolution-independent continuous operator under mesh refinement and KAN gives smooth, Lipschitz-controlled global modulation. Across diverse PDE benchmarks, SpectraKAN achieves state-of-the-art performance, reducing RMSE by up to 49% over strong baselines, with particularly large gains on challenging spatio-temporal prediction tasks.

</details>


### [70] [Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs](https://arxiv.org/abs/2602.05191)
*Wentao Ni,Kangqi Zhang,Zhongming Yu,Oren Nelson,Mingu Lee,Hong Cai,Fatih Porikli,Jongryool Kim,Zhijian Liu,Jishen Zhao*

Main category: cs.LG

TL;DR: Double-P：一种分层稀疏注意力框架，通过两级top-p选择优化长上下文推理，在保持精度的同时显著减少计算开销


<details>
  <summary>Details</summary>
Motivation: 随着长上下文推理成为大语言模型的核心需求，注意力机制中不断增长的关键值缓存成为解码瓶颈。现有稀疏注意力方法无法同时优化top-p精度、选择开销和稀疏注意力成本，限制了整体效率。

Method: 提出Double-P分层稀疏注意力框架：1）在聚类级别使用大小加权质心进行粗粒度top-p估计；2）通过第二级top-p阶段自适应细化计算，仅在需要时分配令牌级注意力。

Result: 在长上下文基准测试中，Double-P实现了接近零的精度下降，将注意力计算开销减少高达1.8倍，端到端解码速度比最先进的固定预算稀疏注意力方法快1.3倍。

Conclusion: Double-P通过分层优化策略有效解决了稀疏注意力在精度、选择开销和计算成本之间的权衡问题，为长上下文推理提供了高效可扩展的解决方案。

Abstract: As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions across heads and layers, whereas top-p sparse attention directly preserves attention mass and provides stronger accuracy guarantees. Existing top-p methods, however, fail to jointly optimize top-p accuracy, selection overhead, and sparse attention cost, which limits their overall efficiency. We present Double-P, a hierarchical sparse attention framework that optimizes all three stages. Double-P first performs coarse-grained top-p estimation at the cluster level using size-weighted centroids, then adaptively refines computation through a second top-p stage that allocates token-level attention only when needed. Across long-context benchmarks, Double-P consistently achieves near-zero accuracy drop, reducing attention computation overhead by up to 1.8x and delivers up to 1.3x end-to-end decoding speedup over state-of-the-art fixed-budget sparse attention methods.

</details>


### [71] [Extreme Weather Nowcasting via Local Precipitation Pattern Prediction](https://arxiv.org/abs/2602.05204)
*Changhoon Song,Teng Yuan Chang,Youngjoon Hong*

Main category: cs.LG

TL;DR: 提出exPreCast高效确定性雷达预报框架和平衡雷达数据集，在普通和极端降雨场景下实现最先进的临近预报性能


<details>
  <summary>Details</summary>
Motivation: 极端天气准确预报对风险管理至关重要，现有方法存在局限性：扩散模型计算昂贵不适合实时应用，确定性模型偏向普通降雨，基准数据集不平衡影响实际应用

Method: 提出exPreCast框架，集成局部时空注意力、纹理保持立方双上采样解码器和时间提取器，灵活调整预报时域；构建平衡的KMA雷达数据集包含普通和极端降雨事件

Result: 在SEVIR、MeteoNet和平衡KMA数据集上实验表明，该方法在普通和极端降雨场景下均达到最先进性能，提供准确可靠的临近预报

Conclusion: exPreCast框架结合平衡数据集解决了现有临近预报方法的局限性，为实际应用提供了高效准确的解决方案

Abstract: Accurate forecasting of extreme weather events such as heavy rainfall or storms is critical for risk management and disaster mitigation. Although high-resolution radar observations have spurred extensive research on nowcasting models, precipitation nowcasting remains particularly challenging due to pronounced spatial locality, intricate fine-scale rainfall structures, and variability in forecasting horizons. While recent diffusion-based generative ensembles show promising results, they are computationally expensive and unsuitable for real-time applications. In contrast, deterministic models are computationally efficient but remain biased toward normal rainfall. Furthermore, the benchmark datasets commonly used in prior studies are themselves skewed--either dominated by ordinary rainfall events or restricted to extreme rainfall episodes--thereby hindering general applicability in real-world settings. In this paper, we propose exPreCast, an efficient deterministic framework for generating finely detailed radar forecasts, and introduce a newly constructed balanced radar dataset from the Korea Meteorological Administration (KMA), which encompasses both ordinary precipitation and extreme events. Our model integrates local spatiotemporal attention, a texture-preserving cubic dual upsampling decoder, and a temporal extractor to flexibly adjust forecasting horizons. Experiments on established benchmarks (SEVIR and MeteoNet) as well as on the balanced KMA dataset demonstrate that our approach achieves state-of-the-art performance, delivering accurate and reliable nowcasts across both normal and extreme rainfall regimes.

</details>


### [72] [Disentangled Representation Learning via Flow Matching](https://arxiv.org/abs/2602.05214)
*Jinjin Chi,Taoping Liu,Mengtao Yin,Ximing Li,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出基于流匹配的解耦表示学习框架，通过因子条件化流和正交正则化实现语义对齐，在多个数据集上优于现有基线


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法通过归纳偏置鼓励因子独立性，但缺乏强语义对齐。需要开发能同时保证因子独立性和语义对齐的解耦表示学习方法。

Method: 提出流匹配框架，将解耦表示为在紧凑潜在空间中学习因子条件化流。引入非重叠（正交）正则化器抑制因子间干扰，减少信息泄漏。

Result: 在多个数据集上的广泛实验显示，相比代表性基线方法，该方法在解耦分数、可控性和样本保真度方面均有显著提升。

Conclusion: 流匹配框架结合正交正则化能有效实现解耦表示学习，在语义对齐和因子独立性方面优于现有方法，为理解数据生成过程提供了新途径。

Abstract: Disentangled representation learning aims to capture the underlying explanatory factors of observed data, enabling a principled understanding of the data-generating process. Recent advances in generative modeling have introduced new paradigms for learning such representations. However, existing diffusion-based methods encourage factor independence via inductive biases, yet frequently lack strong semantic alignment. In this work, we propose a flow matching-based framework for disentangled representation learning, which casts disentanglement as learning factor-conditioned flows in a compact latent space. To enforce explicit semantic alignment, we introduce a non-overlap (orthogonality) regularizer that suppresses cross-factor interference and reduces information leakage between factors. Extensive experiments across multiple datasets demonstrate consistent improvements over representative baselines, yielding higher disentanglement scores as well as improved controllability and sample fidelity.

</details>


### [73] [Private Prediction via Shrinkage](https://arxiv.org/abs/2602.05219)
*Chao Yan*

Main category: cs.LG

TL;DR: 本文研究了差分隐私预测问题，展示了在流式设置中可以将查询复杂度从√T降低到T的多对数级别，针对不同对手模型和概念类给出了样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: Dwork和Feldman提出的差分隐私预测问题中，标准组合方法导致查询复杂度与√T成正比。本文旨在在流式设置中显著降低这种依赖关系，从√T降低到T的多对数级别，从而提高差分隐私预测的效率。

Method: 针对两种不同的对手模型设计私有预测器：1) 对于无意识在线对手和任意概念类C，使用基于VC维的算法；2) 对于自适应在线对手和ℝ^d中的半空间，采用专门的算法。两种方法都在流式设置中实现，避免了标准组合方法的√T依赖。

Result: 1) 对于无意识在线对手和任意概念类C，需要|S|=Õ(VC(C)^{3.5}log^{3.5}T)个标记样本；2) 对于自适应在线对手和ℝ^d中的半空间，需要|S|=Õ(d^{5.5}log T)个标记样本。两种结果都将查询复杂度从√T降低到log T的多对数级别。

Conclusion: 在流式差分隐私预测中，可以显著改善查询复杂度的依赖关系，从√T降低到T的多对数级别。这一改进适用于不同对手模型和概念类，为高效差分隐私预测提供了理论基础。

Abstract: We study differentially private prediction introduced by Dwork and Feldman (COLT 2018): an algorithm receives one labeled sample set $S$ and then answers a stream of unlabeled queries while the output transcript remains $(\varepsilon,δ)$-differentially private with respect to $S$. Standard composition yields a $\sqrt{T}$ dependence for $T$ queries.
  We show that this dependence can be reduced to polylogarithmic in $T$ in streaming settings. For an oblivious online adversary and any concept class $\mathcal{C}$, we give a private predictor that answers $T$ queries with $|S|= \tilde{O}(VC(\mathcal{C})^{3.5}\log^{3.5}T)$ labeled examples. For an adaptive online adversary and halfspaces over $\mathbb{R}^d$, we obtain $|S|=\tilde{O}\left(d^{5.5}\log T\right)$.

</details>


### [74] [ZeroS: Zero-Sum Linear Attention for Efficient Transformers](https://arxiv.org/abs/2602.05230)
*Jiecheng Lu,Xu Han,Yan Sun,Viresh Pati,Yubin Kim,Siddhartha Somani,Shihao Yang*

Main category: cs.LG

TL;DR: ZeroS是一种线性注意力机制，通过移除常数项和重新加权残差，解决了传统线性注意力的限制，在保持O(N)复杂度的同时达到或超过标准softmax注意力的性能。


<details>
  <summary>Details</summary>
Motivation: 线性注意力方法虽然提供O(N)复杂度，但通常性能不如标准softmax注意力。研究发现两个根本限制：1) 仅限于凸组合，只允许加性信息融合；2) 均匀累积权重偏差，在长上下文中稀释注意力。

Method: 提出Zero-Sum Linear Attention (ZeroS)，通过移除常数零阶项1/t并重新加权剩余的零和softmax残差。这种修改创建了数学稳定的权重，允许正负值，使单个注意力层能够执行对比操作，同时保持O(N)复杂度。

Result: 理论上，ZeroS相比凸组合扩展了可表示函数的集合。实证上，在各种序列建模基准测试中，ZeroS达到或超过标准softmax注意力的性能。

Conclusion: ZeroS解决了线性注意力的根本限制，在保持线性复杂度的同时实现了与标准注意力相当或更好的性能，为高效序列建模提供了有前景的解决方案。

Abstract: Linear attention methods offer Transformers $O(N)$ complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term $1/t$ and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining $O(N)$ complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks.

</details>


### [75] [Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions](https://arxiv.org/abs/2602.05234)
*Yuntai Bao,Xuhong Zhang,Jintao Chen,Ge Su,Yuxiang Cai,Hao Peng,Bing Sun,Haiqin Weng,Liu Yan,Jianwei Yin*

Main category: cs.LG

TL;DR: 提出Concept DAS（CDAS）方法，通过分布式对齐搜索原则实现模型干预式引导，采用分布匹配目标而非概率最大化，实现更忠实稳定的控制


<details>
  <summary>Details</summary>
Motivation: 当前基于干预的模型引导方法借鉴了微调的强优化目标，容易过拟合且性能不佳，有时产生不自然的输出。作者认为有效的引导需要忠实识别内部模型机制，而非强制执行外部偏好

Method: 基于分布式对齐搜索（DAS）原则，提出Concept DAS方法。采用DAS的核心机制——分布式交换干预（DII），但引入针对引导任务的新分布匹配目标，通过将干预后的输出分布与反事实分布对齐来学习干预

Result: 在AxBench大规模模型引导基准上，CDAS并不总是优于偏好优化方法，但可能从模型规模增加中获益更多。在两个安全相关案例研究中（覆盖安全对齐模型的拒绝行为、中和思维链后门），CDAS实现了系统性引导同时保持模型通用效用

Conclusion: CDAS是偏好优化方法的补充，在特定条件下构成了干预式模型引导的稳健方法。它通过分布匹配而非概率最大化实现更忠实稳定的控制

Abstract: Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs. We hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences. To this end, we build on the principles of distributed alignment search (DAS), the standard for causal variable localization, to propose a new steering method: Concept DAS (CDAS). While we adopt the core mechanism of DAS, distributed interchange intervention (DII), we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. CDAS differs from prior work in two main ways: first, it learns interventions via weak-supervised distribution matching rather than probability maximization; second, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control. On AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale. In two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility. These results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering. Our code is available at https://github.com/colored-dye/concept_das.

</details>


### [76] [CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers](https://arxiv.org/abs/2602.05243)
*Boxiang Zhang,Baijian Yang*

Main category: cs.LG

TL;DR: CORP是一种无需标签、梯度或微调的单次结构化剪枝框架，通过闭式岭回归补偿剪枝后的表示误差，在严格后训练约束下高效压缩Vision Transformers。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers虽然精度高但计算和内存成本昂贵。现有结构化剪枝方法依赖重训练或多阶段优化，限制了后训练部署。需要一种在严格后训练约束下（仅使用少量未标记校准集）的高效剪枝方法。

Method: 将结构化剪枝建模为表示恢复问题，将移除的激活和注意力对数建模为保留组件的仿射函数，推导闭式岭回归解并将补偿折叠到模型权重中，最小化校准分布下的期望表示误差。

Result: 在ImageNet上使用DeiT模型验证，发现MLP和注意力表示存在强冗余性。无补偿时单次结构化剪枝导致严重精度下降；使用CORP后，在激进稀疏度下仍能保持精度。DeiT-Huge剪枝50% MLP和注意力结构后保持82.8% Top-1精度，单GPU 20分钟内完成剪枝。

Conclusion: CORP证明了Vision Transformers中MLP和注意力表示的强冗余性，提出的闭式单次结构化剪枝框架能在严格后训练约束下有效压缩模型，实现实际效率提升，为后训练部署提供了实用解决方案。

Abstract: Vision Transformers achieve strong accuracy but incur high compute and memory cost. Structured pruning can reduce inference cost, but most methods rely on retraining or multi-stage optimization. These requirements limit post-training deployment. We propose \textbf{CORP}, a closed-form one-shot structured pruning framework for Vision Transformers. CORP removes entire MLP hidden dimensions and attention substructures without labels, gradients, or fine-tuning. It operates under strict post-training constraints using only a small unlabeled calibration set. CORP formulates structured pruning as a representation recovery problem. It models removed activations and attention logits as affine functions of retained components and derives closed-form ridge regression solutions that fold compensation into model weights. This minimizes expected representation error under the calibration distribution. Experiments on ImageNet with DeiT models show strong redundancy in MLP and attention representations. Without compensation, one-shot structured pruning causes severe accuracy degradation. With CORP, models preserve accuracy under aggressive sparsity. On DeiT-Huge, CORP retains 82.8\% Top-1 accuracy after pruning 50\% of both MLP and attention structures. CORP completes pruning in under 20 minutes on a single GPU and delivers substantial real-world efficiency gains.

</details>


### [77] [Hybrid Gated Flow (HGF): Stabilizing 1.58-bit LLMs via Selective Low-Rank Correction](https://arxiv.org/abs/2602.05269)
*David Alejandro Trejo Pizzo*

Main category: cs.LG

TL;DR: HGF是一种双流架构，将1.58位三元主干与可学习的低秩FP16校正路径相结合，通过自适应门控控制，在边缘设备上显著恢复量化模型质量，仅增加12-15%内存开销。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署LLM受"内存墙"限制，1.58位量化技术虽然大幅减少内存占用，但通常导致20-25%的困惑度下降。需要一种方法在保持低内存开销的同时恢复量化模型的质量。

Method: 提出混合门控流(HGF)架构：1）三元主干（1.58位量化）；2）可学习的低秩FP16校正路径；3）自适应门控机制控制双流耦合。通过TinyStories数据集在两个训练阶段（2500和3500步）进行验证。

Result: HGF 5.4在验证损失上达到0.9306，相比BitNet的1.0294显著提升，恢复了约55%三元量化与FP16基线（0.8490）之间的质量差距，仅增加12-15%内存开销。发现量化作为结构正则化的涌现现象，三元锚定的HGF保持稳定收敛，而全精度差分注意力基线出现训练不稳定。

Conclusion: HGF架构有效解决了边缘设备部署LLM的内存墙问题，在保持低内存开销的同时显著恢复量化模型质量。该方法的稳定性和质量恢复能力可线性扩展到生产级语言建模（1.2B和3B参数），为边缘AI部署提供了实用解决方案。

Abstract: The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the "Memory Wall" -- a hardware limitation where memory bandwidth, not compute, becomes the bottleneck. Recent 1.58-bit quantization techniques (e.g., BitNet b1.58) dramatically reduce memory footprint but typically incur a perplexity degradation of 20-25% compared to FP16 baselines. In this work, we introduce Hybrid Gated Flow (HGF), a dual-stream architecture that couples a 1.58-bit ternary backbone with a learnable, low-rank FP16 correction path controlled by adaptive gates.
  Through extensive experiments on the TinyStories dataset across two training regimes (2500 and 3500 steps), we demonstrate that HGF 5.4 achieves a validation loss of 0.9306 compared to BitNet's 1.0294, recovering approximately 55% of the quality gap between pure ternary quantization and the FP16 baseline (0.8490). This recovery is achieved with only ~12-15% memory overhead beyond the ternary backbone.
  Furthermore, we provide empirical evidence for an emergent phenomenon: quantization as structural regularization. While a full-precision differential attention baseline (Diff_Only) exhibited training instability with validation loss exceeding 1.68, the ternary-anchored HGF maintained robust convergence throughout training. Finally, we report preliminary results extending this architecture to 1.2B and 3B parameter models trained on SlimPajama and FineWeb-Edu. These larger-scale experiments confirm that the architectural stability and quality recovery observed in small-scale proxies scale linearly to production-grade language modeling regimes.

</details>


### [78] [Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities](https://arxiv.org/abs/2602.05281)
*Pengyi Li,Elizaveta Goncharova,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出ARM机制解决RLVR中GRPO方法导致的低熵策略和模式崩溃问题，通过优势重加权平衡不同正确推理路径的置信度，提升输出多样性同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 标准策略优化方法（如GRPO）在强化学习可验证奖励（RLVR）中往往收敛到低熵策略，导致严重的模式崩溃和有限的输出多样性。作者从采样概率动态的角度分析此问题，发现标准目标函数不成比例地强化最高似然路径，抑制了有效的替代推理链。

Method: 提出新颖的优势重加权机制（ARM），通过将提示困惑度和答案置信度纳入优势估计，动态重塑奖励信号。该方法衰减过度自信推理路径的梯度更新，同时将概率质量重新分配给未被充分探索的正确解决方案，从而平衡所有正确响应的置信度水平。

Result: 在Qwen2.5和DeepSeek模型上的数学和编码基准测试表明，该方法显著缓解了熵崩溃。具体而言，在Qwen2.5-7B上，该方法在Pass@1上比GRPO高出5.7%，在Pass@32上显著高出13.9%，显示出在生成多样化正确推理路径方面的卓越能力。

Conclusion: ARM方法显著增强了生成多样性和响应熵，同时保持了有竞争力的准确性，有效实现了推理任务中探索与利用之间的优越权衡，解决了RLVR中模式崩溃和多样性受限的关键问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.

</details>


### [79] [Robust Inference-Time Steering of Protein Diffusion Models via Embedding Optimization](https://arxiv.org/abs/2602.05285)
*Minhuan Li,Jiequn Han,Pilar Cossio,Luhuan Wu*

Main category: cs.LG

TL;DR: EmbedOpt：一种在条件嵌入空间中优化实验似然性的扩散模型引导方法，用于生成符合实验约束的分子构象


<details>
  <summary>Details</summary>
Motivation: 传统基于坐标的后验采样方法在目标构象位于先验分布低密度区域时需要激进且脆弱的似然性加权，存在局限性

Method: 在条件嵌入空间中优化实验似然性，利用该空间编码的丰富序列和共进化信号来调整扩散先验以符合实验约束

Result: 在冷冻电镜图谱拟合任务中优于基于坐标的后验采样方法，在距离约束任务中性能相当，在跨越两个数量级的超参数范围内表现出更好的工程鲁棒性，且能显著减少推理所需的扩散步骤

Conclusion: EmbedOpt通过条件嵌入空间优化提供了一种更鲁棒、高效的扩散模型引导方法，用于生成符合实验约束的分子构象

Abstract: In many biophysical inverse problems, the goal is to generate biomolecular conformations that are both physically plausible and consistent with experimental measurements. As recent sequence-to-structure diffusion models provide powerful data-driven priors, posterior sampling has emerged as a popular framework by guiding atomic coordinates to target conformations using experimental likelihoods. However, when the target lies in a low-density region of the prior, posterior sampling requires aggressive and brittle weighting of the likelihood guidance. Motivated by this limitation, we propose EmbedOpt, an alternative inference-time approach for steering diffusion models to optimize experimental likelihoods in the conditional embedding space. As this space encodes rich sequence and coevolutionary signals, optimizing over it effectively shifts the diffusion prior to align with experimental constraints. We validate EmbedOpt on two benchmarks simulating cryo-electron microscopy map fitting and experimental distance constraints. We show that EmbedOpt outperforms the coordinate-based posterior sampling method in map fitting tasks, matches performance on distance constraint tasks, and exhibits superior engineering robustness across hyperparameters spanning two orders of magnitude. Moreover, its smooth optimization behavior enables a significant reduction in the number of diffusion steps required for inference, leading to better efficiency.

</details>


### [80] [HealthMamba: An Uncertainty-aware Spatiotemporal Graph State Space Model for Effective and Reliable Healthcare Facility Visit Prediction](https://arxiv.org/abs/2602.05286)
*Dahai Yu,Lin Jiang,Rongchao Xu,Guang Wang*

Main category: cs.LG

TL;DR: HealthMamba：一种用于医疗设施访问预测的不确定性感知时空框架，通过统一时空上下文编码器、图状态空间模型和综合不确定性量化模块，在预测精度和不确定性量化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗设施访问预测研究通常将其视为时间序列预测问题，忽略了不同类型医疗设施之间的空间依赖关系，且无法在公共卫生紧急事件等异常情况下提供可靠预测。需要开发能够同时考虑时空依赖并提供可靠不确定性量化的方法。

Method: 提出HealthMamba框架，包含三个核心组件：1) 统一时空上下文编码器，融合异构静态和动态信息；2) GraphMamba图状态空间模型，用于分层时空建模；3) 综合不确定性量化模块，集成三种不确定性量化机制。

Result: 在加州、纽约、德州和佛罗里达四个大规模真实世界数据集上的评估显示，HealthMamba相比最先进基线方法，预测精度提升约6.0%，不确定性量化提升3.5%。

Conclusion: HealthMamba通过整合时空建模和不确定性量化，能够提供准确可靠的医疗设施访问预测，特别适用于公共卫生紧急事件等异常情况，有助于优化医疗资源配置和公共卫生政策制定。

Abstract: Healthcare facility visit prediction is essential for optimizing healthcare resource allocation and informing public health policy. Despite advanced machine learning methods being employed for better prediction performance, existing works usually formulate this task as a time-series forecasting problem without considering the intrinsic spatial dependencies of different types of healthcare facilities, and they also fail to provide reliable predictions under abnormal situations such as public emergencies. To advance existing research, we propose HealthMamba, an uncertainty-aware spatiotemporal framework for accurate and reliable healthcare facility visit prediction. HealthMamba comprises three key components: (i) a Unified Spatiotemporal Context Encoder that fuses heterogeneous static and dynamic information, (ii) a novel Graph State Space Model called GraphMamba for hierarchical spatiotemporal modeling, and (iii) a comprehensive uncertainty quantification module integrating three uncertainty quantification mechanisms for reliable prediction. We evaluate HealthMamba on four large-scale real-world datasets from California, New York, Texas, and Florida. Results show HealthMamba achieves around 6.0% improvement in prediction accuracy and 3.5% improvement in uncertainty quantification over state-of-the-art baselines.

</details>


### [81] [Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective](https://arxiv.org/abs/2602.05319)
*Yinan Huang,Hans Hao-Hsun Hsu,Junran Wang,Bo Dai,Pan Li*

Main category: cs.LG

TL;DR: 提出Sequential Flow Matching框架，通过贝叶斯滤波将流式推理建模为概率流传输，利用前一时刻后验分布作为生成初始化，显著加速采样过程


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流匹配模型在实时流式环境中需要从非信息性初始分布重复采样，导致推理延迟和系统积压。需要一种能与贝叶斯滤波递归结构自然对齐的高效流式推理方法

Method: 提出Sequential Flow Matching框架，将流式推理视为学习从当前时间步到下一时间步的概率流传输。利用前一时刻的后验分布作为生成过程的初始化，提供有原则的"热启动"，加速采样过程

Result: 在多种预测、决策和状态估计任务中，该方法性能与完整步长扩散模型相当，但仅需一步或极少采样步骤，实现更快采样速度

Conclusion: 通过贝叶斯滤波框架进行序列推理为流基模型的实时部署提供了新的、有原则的视角，实现了高效采样而不牺牲性能

Abstract: Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.

</details>


### [82] [GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL](https://arxiv.org/abs/2602.05323)
*Zifan Liu,Xinran Li,Shibo Chen,Jun Zhang*

Main category: cs.LG

TL;DR: GAS算法通过数据增强和重新标注提升离线安全强化学习中的轨迹拼接能力，引入目标函数估计最优可达成奖励与成本目标，实现奖励最大化与约束满足的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的离线安全强化学习方法面临两个主要挑战：1) 缺乏从次优轨迹中"拼接"最优转移的能力；2) 难以平衡奖励目标与成本目标，特别是在两者冲突时。

Method: 提出Goal-Assisted Stitching (GAS)算法：1) 在转移级别增强和重新标注数据集，从次优轨迹构建高质量轨迹；2) 引入目标函数，使用期望回归在增强数据集上训练，估计最优可达成奖励和成本目标；3) 重塑数据集以获得更均匀的奖励-成本回报分布；4) 用估计的目标指导策略训练。

Result: 实证结果验证了GAS的有效性，在平衡奖励最大化与约束满足方面表现出优于现有方法的性能。

Conclusion: GAS通过增强拼接能力和引入自适应目标估计，有效解决了离线安全强化学习中轨迹拼接困难与奖励-成本平衡问题，实现了更好的性能权衡。

Abstract: Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to "stitch" optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.

</details>


### [83] [Pool-based Active Learning as Noisy Lossy Compression: Characterizing Label Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.05333)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 该论文提出了一个信息论框架来分析池式主动学习的理论极限，将主动学习重新表述为有噪有损压缩问题，并推导出标签复杂度和泛化误差的信息论下界。


<details>
  <summary>Details</summary>
Motivation: 现有池式主动学习（AL）缺乏统一的理论分析框架来理解其理论极限。论文旨在建立信息论基础，分析数据选择和学习的联合理论边界。

Method: 将池式主动学习重新表述为有噪有损压缩问题：池观测映射到有噪符号观测，数据选择对应压缩，学习对应解码。应用有限块长有噪有损压缩分析，推导信息论下界。

Result: 推导出标签复杂度和泛化误差的信息论下界，这些边界反映了学习算法引起的过拟合及其归纳偏置与目标任务之间的差异，并与现有信息论边界和稳定性理论相关。

Conclusion: 该框架为池式主动学习提供了新的理论视角，统一分析了数据选择和学习过程，建立了信息论基础来理解主动学习的理论极限。

Abstract: This paper proposes an information-theoretic framework for analyzing the theoretical limits of pool-based active learning (AL), in which a subset of instances is selectively labeled. The proposed framework reformulates pool-based AL as a noisy lossy compression problem by mapping pool observations to noisy symbol observations, data selection to compression, and learning to decoding. This correspondence enables a unified information-theoretic analysis of data selection and learning in pool-based AL. Applying finite blocklength analysis of noisy lossy compression, we derive information-theoretic lower bounds on label complexity and generalization error that serve as theoretical limits for a given learning algorithm under its associated optimal data selection strategy. Specifically, our bounds include terms that reflect overfitting induced by the learning algorithm and the discrepancy between its inductive bias and the target task, and are closely related to established information-theoretic bounds and stability theory, which have not been previously applied to the analysis of pool-based AL. These properties yield a new theoretical perspective on pool-based AL.

</details>


### [84] [Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting](https://arxiv.org/abs/2602.05371)
*Hongyi Li,Han Lin,Jun Xu*

Main category: cs.LG

TL;DR: HRT（Hinge Regression Tree）通过将树节点分裂重新表述为非线性最小二乘问题，使用两个线性预测器的最大/最小包络获得ReLU类表达能力，解决了倾斜决策树学习的高质量分裂难题。


<details>
  <summary>Details</summary>
Motivation: 倾斜决策树结合了树的透明性和多元决策边界的强大能力，但学习高质量倾斜分裂是NP难问题，现有方法依赖缓慢搜索或无理论启发式方法，需要更高效的理论基础方法。

Method: 将每个分裂重新表述为两个线性预测器的非线性最小二乘问题，其最大/最小包络产生ReLU类表达能力；使用交替拟合程序，相当于固定分区内的阻尼牛顿（高斯-牛顿）法；分析节点级优化，证明回溯线搜索变体的局部目标单调递减和收敛性。

Result: 固定和自适应阻尼都能实现快速稳定收敛，可与岭正则化结合；HRT模型类是通用逼近器，具有显式O(δ²)逼近率；在合成和真实世界基准测试中，以更紧凑结构匹配或优于单树基线。

Conclusion: HRT为倾斜决策树学习提供了理论基础坚实的方法，解决了高质量分裂的NP难问题，实现了高效收敛和紧凑结构，在保持树透明性的同时提升了多元决策边界表达能力。

Abstract: Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries, but learning high-quality oblique splits is NP-hard, and practical methods still rely on slow search or theory-free heuristics. We present the Hinge Regression Tree (HRT), which reframes each split as a non-linear least-squares problem over two linear predictors whose max/min envelope induces ReLU-like expressive power. The resulting alternating fitting procedure is exactly equivalent to a damped Newton (Gauss-Newton) method within fixed partitions. We analyze this node-level optimization and, for a backtracking line-search variant, prove that the local objective decreases monotonically and converges; in practice, both fixed and adaptive damping yield fast, stable convergence and can be combined with optional ridge regularization. We further prove that HRT's model class is a universal approximator with an explicit $O(δ^2)$ approximation rate, and show on synthetic and real-world benchmarks that it matches or outperforms single-tree baselines with more compact structures.

</details>


### [85] [Assessing Electricity Demand Forecasting with Exogenous Data in Time Series Foundation Models](https://arxiv.org/abs/2602.05390)
*Wei Soon Cheong,Lian Lian Jiang,Jamie Ng Suat Ling*

Main category: cs.LG

TL;DR: 该论文评估了时间序列基础模型在电力需求预测中利用外生特征的能力，发现基础模型性能高度可变，简单的LSTM基线在稳定气候条件下经常优于所有基础模型，模型架构和地理环境是关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型已成为预测的新范式，但它们有效利用外生特征的能力尚不清楚，而外生特征对电力需求预测至关重要。本研究旨在实证评估基础模型在电力需求预测中的表现，特别是它们处理跨通道相关性和利用外生特征的能力。

Method: 在新加坡和澳大利亚电力市场的小时和日粒度上，系统评估了MOIRAI、MOMENT、TinyTimeMixers、ChronosX和Chronos-2等基础模型。使用具有可逆实例归一化的LSTM作为基线，在三种特征配置下进行评估：所有特征、选定特征和仅目标特征。

Result: 研究发现基础模型的有效性高度可变：Chronos-2在基础模型中表现最佳（在零样本设置下），但简单的基线模型在新加坡稳定气候条件下经常优于所有基础模型，特别是在短期预测范围内。模型架构至关重要，具有协同架构实现（TTM的通道混合、Chronos-2的分组注意力）的模型能持续利用外生特征，而其他方法显示出不一致的效益。地理环境同样重要，基础模型主要在多变气候条件下表现出优势。

Conclusion: 这些结果挑战了关于基础模型普遍优越性的假设，并强调了在能源领域需要特定领域模型。基础模型的优势高度依赖于模型架构和地理环境，特别是在稳定气候条件下，简单的传统方法可能更有效。

Abstract: Time-series foundation models have emerged as a new paradigm for forecasting, yet their ability to effectively leverage exogenous features -- critical for electricity demand forecasting -- remains unclear. This paper empirically evaluates foundation models capable of modeling cross-channel correlations against a baseline LSTM with reversible instance normalization across Singaporean and Australian electricity markets at hourly and daily granularities. We systematically assess MOIRAI, MOMENT, TinyTimeMixers, ChronosX, and Chronos-2 under three feature configurations: all features, selected features, and target-only. Our findings reveal highly variable effectiveness: while Chronos-2 achieves the best performance among foundation models (in zero-shot settings), the simple baseline frequently outperforms all foundation models in Singapore's stable climate, particularly for short-term horizons. Model architecture proves critical, with synergistic architectural implementations (TTM's channel-mixing, Chronos-2's grouped attention) consistently leveraging exogenous features, while other approaches show inconsistent benefits. Geographic context emerges as equally important, with foundation models demonstrating advantages primarily in variable climates. These results challenge assumptions about universal foundation model superiority and highlight the need for domain-specific models, specifically in the energy domain.

</details>


### [86] [Robust Federated Learning via Byzantine Filtering over Encrypted Updates](https://arxiv.org/abs/2602.05410)
*Adda Akram Bendoukha,Aymen Boudguiga,Nesrine Kaaniche,Renaud Sirdey,Didem Demirag,Sébastien Gambs*

Main category: cs.LG

TL;DR: 提出结合同态加密和元分类器的联邦学习安全聚合与拜占庭容错方法，通过属性推断攻击启发式训练过滤分类器，自动选择CKKS同态加密参数，在多个基准测试中实现90-94%的拜占庭更新识别准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时仍面临隐私安全挑战，包括推理攻击导致敏感数据暴露和拜占庭行为对训练模型的影响。现有解决方案往往独立处理安全聚合和拜占庭容错，难以同时实现这两方面目标。

Method: 1) 基于属性推断攻击蓝图，在带标签的影子更新上训练过滤元分类器，重现多种拜占庭攻击行为（后门、梯度反转、标签翻转、洗牌攻击）；2) 使用元分类器输出通过重加权取消拜占庭加密更新；3) 提出自动选择CKKS同态加密系统最优核和维度超参数的方法，考虑同态推理、聚合约束和效率。

Result: 在FEMNIST、CIFAR10、GTSRB和acsincome基准测试中验证了方法的有效性。SVM过滤实现90-94%的拜占庭更新识别准确率，模型效用损失微小，同态加密推理运行时间：单个更新6-24秒，整体聚合9-26秒。

Conclusion: 该方法成功结合同态加密隐私保护聚合与基于属性推断的拜占庭过滤，在保护隐私的同时有效抵御拜占庭攻击，为联邦学习提供了同时实现安全聚合和拜占庭容错的可行方案。

Abstract: Federated Learning (FL) aims to train a collaborative model while preserving data privacy. However, the distributed nature of this approach still raises privacy and security issues, such as the exposure of sensitive data due to inference attacks and the influence of Byzantine behaviors on the trained model. In particular, achieving both secure aggregation and Byzantine resilience remains challenging, as existing solutions often address these aspects independently. In this work, we propose to address these challenges through a novel approach that combines homomorphic encryption for privacy-preserving aggregation with property-inference-inspired meta-classifiers for Byzantine filtering. First, following the property-inference attacks blueprint, we train a set of filtering meta-classifiers on labeled shadow updates, reproducing a diverse ensemble of Byzantine misbehaviors in FL, including backdoor, gradient-inversion, label-flipping and shuffling attacks. The outputs of these meta-classifiers are then used to cancel the Byzantine encrypted updates by reweighting. Second, we propose an automated method for selecting the optimal kernel and the dimensionality hyperparameters with respect to homomorphic inference, aggregation constraints and efficiency over the CKKS cryptosystem. Finally, we demonstrate through extensive experiments the effectiveness of our approach against Byzantine participants on the FEMNIST, CIFAR10, GTSRB, and acsincome benchmarks. More precisely, our SVM filtering achieves accuracies between $90$% and $94$% for identifying Byzantine updates at the cost of marginal losses in model utility and encrypted inference runtimes ranging from $6$ to $24$ seconds and from $9$ to $26$ seconds for an overall aggregation.

</details>


### [87] [BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs](https://arxiv.org/abs/2602.05448)
*Sheshansh Agrawal,Thien Hang Nguyen,Douwe Kiela*

Main category: cs.LG

TL;DR: 提出基于锦标赛图的k-wise重排序框架，通过聚合k文档比较中的成对偏好信息，减少LLM调用次数，提高检索增强生成中重排序的效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM重排序方法要么依赖启发式方法未能充分利用每次排序决策的信息，要么在充分利用信息时效率低下。需要一种既能充分利用LLM比较信息又能保持高效的方法

Method: 引入锦标赛图框架：1) 将每次k文档比较视为包含binom(k,2)个成对偏好的完整锦标赛；2) 聚合到全局偏好图中，利用传递闭包推导额外排序；3) 设计贪心查询调度最大化识别top-m项目的信息增益；4) 处理非传递偏好（循环）时将其折叠为等价类，产生分层排序

Result: 在14个基准测试和5个LLM上的实验表明：与可比方法相比，在达到相同或更高准确率的同时，减少了25-40%的token使用；与成对方法相比，在质量相近的情况下减少了7倍的token使用

Conclusion: 锦标赛图框架为k-wise重排序提供了理论基础，通过有效利用每次比较中的成对偏好信息，实现了准确率与效率的帕累托最优，显著减少了LLM重排序的计算成本

Abstract: Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\times$ fewer than pairwise methods at near-identical quality.

</details>


### [88] [When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL](https://arxiv.org/abs/2602.05459)
*Jan Malte Töpperwien,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 离线目标条件强化学习中的超参数敏感性研究：发现并非RL固有，而是由自举机制放大；QRL比HIQL更鲁棒


<details>
  <summary>Details</summary>
Motivation: 研究深度强化学习中普遍存在的超参数敏感性问题，探究其是否是RL问题固有的，还是由特定训练机制加剧的。通过离线目标条件RL环境，控制数据分布固定性和非平稳性，来系统分析这一问题。

Method: 在离线目标条件RL设置下，控制数据分布固定性，通过调度数据质量变化来显式控制非平稳性。研究不同数据质量在平稳和非平稳机制下的表现，覆盖两种代表性算法：HIQL（基于自举的TD学习）和QRL（拟度量表示学习）。引入跨目标梯度对齐诊断来分析算法差异。

Result: 观察到比在线RL更强的超参数配置鲁棒性，即使在受控非平稳条件下。当存在适度专家数据（约20%）时，QRL保持广泛稳定的近最优区域，而HIQL表现出尖锐的最优点，且这些最优点在训练阶段间显著漂移。自举目标表现出更强的破坏性梯度干扰，这与超参数敏感性直接相关。

Conclusion: 训练期间对超参数配置变化的高度敏感性并非RL不可避免的特性，而是由自举机制的动态特性放大。这为设计更鲁棒的算法目标提供了途径，表明可以通过改进目标函数设计来降低超参数敏感性。

Abstract: Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\approx$ 20\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.

</details>


### [89] [A Unified Framework for Rethinking Policy Divergence Measures in GRPO](https://arxiv.org/abs/2602.05494)
*Qingyuan Wu,Yuhui Wang,Simon Sinong Zhan,Yanning Dai,Shilong Deng,Sarra Habchi,Qi Zhu,Matthias Gallé,Chao Huang*

Main category: cs.LG

TL;DR: 本文提出了一个统一的裁剪框架，将现有RLVR方法（如GRPO）统一在广义策略散度概念下，并引入KL3估计器作为关键策略约束，理论上证明其等价于非对称比率裁剪，实验表明能提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法（如GRPO及其变体）通过裁剪似然比来约束策略散度以确保稳定更新，但缺乏对策略散度度量如何影响探索和性能的系统分析。需要建立一个统一的理论框架来理解不同策略散度约束的作用机制。

Method: 提出了一个统一的裁剪框架，将现有方法统一在广义策略散度概念下，涵盖似然比和KL散度等度量。引入KL3估计器（KL散度的方差减少蒙特卡洛估计器）作为关键策略约束，并理论证明其等价于非对称比率裁剪，将概率质量重新分配到高置信度动作。

Result: 在数学推理基准测试上的实验结果表明，将KL3估计器整合到GRPO中能同时提高训练稳定性和最终性能。这验证了基于原则的策略散度约束在策略优化中的重要性。

Conclusion: 统一的裁剪框架为系统分析策略散度度量如何影响探索和性能提供了理论基础。KL3估计器作为策略约束在数学上等价于非对称比率裁剪，能促进更强的探索同时保持GRPO类方法的简单性，为RLVR方法提供了改进方向。

Abstract: Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.

</details>


### [90] [Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification](https://arxiv.org/abs/2602.05535)
*Tao Huang,Rui Wang,Xiaofei Liu,Yi Qin,Li Duan,Liping Jing*

Main category: cs.LG

TL;DR: EUQ提出了一种细粒度的证据不确定性量化方法，通过建模正负证据来检测LVLM的误行为，包括幻觉、越狱、对抗性漏洞和OOD失败。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在遇到不称职或对抗性输入时会产生不可靠甚至有害内容（幻觉、危险指令等），这些误行为源于认知不确定性（内部知识冲突或信息缺失），而现有不确定性量化方法仅捕捉总体认知不确定性，在识别这些问题上效果有限。

Method: 提出证据不确定性量化方法，将模型输出头的特征解释为支持（正）或反对（负）的证据，利用证据理论建模和聚合这些证据，在单次前向传播中量化内部冲突和知识缺口。

Result: 在四种误行为类别（幻觉、越狱、对抗性漏洞、OOD失败）上对最先进的LVLM进行评估，EUQ始终优于强基线，发现幻觉对应高内部冲突，OOD失败对应高无知度；层间证据不确定性动态分析有助于从新视角解释内部表示的演化。

Conclusion: EUQ提供了一种细粒度的不确定性量化方法，能有效检测LVLM的误行为，为理解模型内部表示演化提供了新视角，有助于提升LVLM在关键应用中的可靠性。

Abstract: Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.

</details>


### [91] [Steering Large Reasoning Models towards Concise Reasoning via Flow Matching](https://arxiv.org/abs/2602.05539)
*Yawei Li,Benjamin Bergner,Yinghan Zhao,Vihang Prakash Patil,Bei Chen,Cheng Wang*

Main category: cs.LG

TL;DR: FlowSteer：一种基于流匹配的非线性引导方法，通过完整分布变换而非单一线性偏移，使大型推理模型产生更简洁高效的推理输出。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现出色，但其效率常因冗长输出而受限。现有引导方法基于限制性的线性表示假设，仅应用单一全局向量到隐藏表示，无法实现精确控制。

Method: 提出FlowSteer非线性引导方法，通过学习冗长推理和简洁推理相关分布之间的完整变换，而非均匀线性偏移。该变换通过流匹配作为速度场学习，实现对模型推理过程的精确、输入依赖控制。

Result: 在多样化推理基准测试中，FlowSteer相比领先的推理时基线方法，展现出强大的任务性能和标记效率。通过将引导表示与简洁推理激活分布对齐，FlowSteer产生比线性偏移更紧凑的推理。

Conclusion: 使用生成技术建模完整分布传输为控制大型推理模型提供了更有效和原则性的基础，超越了传统线性方法的限制。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.

</details>


### [92] [Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation](https://arxiv.org/abs/2602.05548)
*Zhiqi Yu,Zhangquan Chen,Mengting Liu,Heye Zhang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 本文提出A-GRAE方法，通过引入非对称优势估计解决GRPO中探索不足和难度适应问题，在七个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: GRPO作为强化学习可验证奖励的标准方法，在探索效率和难度适应方面存在瓶颈。研究发现这些瓶颈源于组相对优势估计(GRAE)中隐含的优势对称性，这种对称性导致两个关键限制：组层面抑制新正确解的探索，样本层面无法适应非平稳的难度聚焦需求。

Method: 提出非对称GRAE(A-GRAE)方法，通过动态调节探索激励和样本难度聚焦来解决对称性问题。具体包括：(1)非对称抑制正确轨迹的优势以鼓励必要探索；(2)采用课程学习式策略，初期优先简单样本，逐步转向复杂样本。

Result: 在七个基准测试上的实验表明，A-GRAE一致地改进了GRPO及其变体，在LLM和MLLM上都取得了显著性能提升。

Conclusion: GRAE中的优势对称性是次优的，非对称优势估计能有效促进探索和难度适应。A-GRAE通过动态调节探索激励和样本难度聚焦，为RLVR提供了更高效的训练框架。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.

</details>


### [93] [Logical Guidance for the Exact Composition of Diffusion Models](https://arxiv.org/abs/2602.05549)
*Francesco Alesiani,Jonathan Warrell,Tanja Bien,Henrik Christiansen,Matheus Ferraz,Mathias Niepert*

Main category: cs.LG

TL;DR: LOGDIFF是一个扩散模型引导框架，通过逻辑表达式实现精确约束生成，建立了布尔演算规则确保复杂逻辑公式的精确引导。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在推理时难以处理复杂逻辑约束的生成任务，需要一种能够精确组合原子属性引导信号的框架，以实现基于逻辑表达式的可控生成。

Method: 提出布尔演算规则：当逻辑公式可表示为电路，其中合取组合条件独立子公式、析取组合条件独立或互斥子公式时，可实现精确逻辑引导。开发递归算法从原子分数和后验概率计算引导信号，并引入混合引导方法结合分类器引导和无分类器引导。

Result: 在图像和蛋白质结构生成任务中验证了框架的有效性，实现了基于复杂逻辑表达式的精确约束生成。

Conclusion: LOGDIFF为扩散模型提供了系统化的逻辑引导框架，能够处理复杂布尔约束，将分类器引导和无分类器引导统一到同一框架中，扩展了扩散模型在约束生成任务中的应用范围。

Abstract: We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.
  We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.
  First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.
  Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.
  In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.
  Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.
  Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.
  We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.

</details>


### [94] [OpenMAG: A Comprehensive Benchmark for Multimodal-Attributed Graph](https://arxiv.org/abs/2602.05576)
*Chenxi Wan,Xunkai Li,Yilong Zuo,Haokun Deng,Sihan Li,Bowen Fan,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OpenMAG是一个全面的多模态属性图学习基准，整合了6个领域的19个数据集、16种编码器、24个SOTA模型和8个下游任务，旨在解决现有基准在领域覆盖、编码器灵活性、模型多样性和任务范围方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着能够处理复杂跨模态语义和结构依赖的新型多模态属性图模型快速发展，建立严格统一的评估标准变得至关重要。现有基准在领域覆盖、编码器灵活性、模型多样性和任务范围方面存在关键限制，对公平评估构成重大挑战。

Method: 提出OpenMAG基准，整合19个数据集覆盖6个领域，包含16种编码器支持静态和可训练特征编码，实现24个最先进模型的标准化库，支持8个下游任务，在统一框架内实现公平比较。

Result: 通过系统评估必要性、数据质量、有效性、鲁棒性和效率，得出14个关于多模态属性图学习的基本见解，为未来进展提供指导。

Conclusion: OpenMAG提供了一个全面的多模态属性图学习基准，解决了现有评估标准的局限性，通过系统评估得出了指导未来研究的重要见解，促进了该领域的公平比较和进一步发展。

Abstract: Multimodal-Attributed Graph (MAG) learning has achieved remarkable success in modeling complex real-world systems by integrating graph topology with rich attributes from multiple modalities. With the rapid proliferation of novel MAG models capable of handling intricate cross-modal semantics and structural dependencies, establishing a rigorous and unified evaluation standard has become imperative. Although existing benchmarks have facilitated initial progress, they exhibit critical limitations in domain coverage, encoder flexibility, model diversity, and task scope, presenting significant challenges to fair evaluation. To bridge this gap, we present OpenMAG, a comprehensive benchmark that integrates 19 datasets across 6 domains and incorporates 16 encoders to support both static and trainable feature encoding. OpenMAG further implements a standardized library of 24 state-of-the-art models and supports 8 downstream tasks, enabling fair comparisons within a unified framework. Through systematic assessment of necessity, data quality, effectiveness, robustness, and efficiency, we derive 14 fundamental insights into MAG learning to guide future advancements. Our code is available at https://github.com/YUKI-N810/OpenMAG.

</details>


### [95] [Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.05605)
*Jiaji Zhang,Hailiang Zhao,Guoxuan Zhu,Ruichao Sun,Jiaju Wu,Xinkui Zhao,Hanlin Tang,Weiyi Lu,Kan Liu,Tao Lan,Lin Qu,Shuiguang Deng*

Main category: cs.LG

TL;DR: Shiva-DiT提出了一种新的扩散变换器剪枝方法，通过残差感知的差分Top-k选择实现确定性token计数，同时保持端到端可学习性，解决了现有方法在可微分性、效率和静态预算要求之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiTs）由于自注意力的二次计算复杂度导致计算成本过高。现有的剪枝方法无法同时满足可微分性、效率以及硬件开销所需的严格静态预算要求。

Method: 提出Shiva-DiT方法，核心包括：1）基于残差的差分Top-k选择，利用残差感知直通估计器实现确定性token计数和端到端学习；2）上下文感知路由器和自适应比率策略，自主学习自适应剪枝调度。

Result: 在包括SD3.5在内的主流模型上实验表明，Shiva-DiT建立了新的帕累托前沿，实现了1.54倍的挂钟时间加速，同时保持优于现有基线的保真度，有效消除了不规则张量开销。

Conclusion: Shiva-DiT成功解决了扩散变换器剪枝中可微分性、效率和静态预算要求之间的矛盾，为高效DiT部署提供了有效的解决方案。

Abstract: Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.

</details>


### [96] [Path-Guided Flow Matching for Dataset Distillation](https://arxiv.org/abs/2602.05616)
*Xuhui Li,Zhengquan Luo,Xiwei Liu,Yongqiang Yu,Zhiqiang Xu*

Main category: cs.LG

TL;DR: PGFM是一种基于流匹配的数据集蒸馏框架，通过ODE求解实现快速确定性合成，相比基于扩散的方法效率更高且采样步骤更少。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的数据集蒸馏方法依赖启发式指导或原型分配，存在采样耗时、轨迹不稳定等问题，特别是在强控制或低IPC条件下会损害下游泛化性能。

Method: 提出路径引导流匹配(PGFM)框架，在冻结VAE的潜在空间中进行流匹配，学习从高斯噪声到数据分布的类条件传输。开发了连续路径到原型引导算法，实现ODE一致的路径控制，确保轨迹可靠地到达指定原型同时保持多样性和效率。

Result: 在高分辨率基准测试中，PGFM匹配或超越了先前的基于扩散的蒸馏方法，采样步骤更少，同时提供具有显著改进效率的竞争性能，例如比基于扩散的方法效率提高7.6倍，模式覆盖率达到78%。

Conclusion: PGFM是第一个基于流匹配的生成式蒸馏框架，通过ODE求解实现快速确定性合成，解决了扩散方法中的采样耗时和轨迹不稳定问题，在保持性能的同时显著提高了效率。

Abstract: Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\times$ more efficient than the diffusion-based counterparts with 78\% mode coverage.

</details>


### [97] [Rewards as Labels: Revisiting RLVR from a Classification Perspective](https://arxiv.org/abs/2602.05630)
*Zepeng Zhai,Meilin Chen,Jiaxuan Zhao,Junlang Qian,Lei Shen,Yuan Lu*

Main category: cs.LG

TL;DR: REAL框架将可验证奖励重新定义为分类标签而非标量权重，通过锚定logits增强策略学习，解决了GRPO方法中的梯度错配和梯度主导问题，在数学推理任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法（特别是GRPO及其变体）存在梯度错配问题：正样本中梯度分配不当，负样本中梯度主导，导致策略更新效率低下且次优。需要解决这些梯度分配问题以提升强化学习在复杂推理任务中的性能。

Method: 提出REAL框架，将可验证奖励重新定义为分类标签而非标量权重，将策略优化重构为分类问题。引入锚定logits增强策略学习，分析表明REAL产生单调有界的梯度加权，实现跨rollout的平衡梯度分配。

Result: 在数学推理基准测试中，REAL显著提升训练稳定性并持续超越GRPO及其变体。在1.5B模型上，REAL平均Pass@1比DAPO提升6.7%；在7B模型上，REAL继续超越DAPO和GSPO分别6.2%和1.7%。即使使用简单的二元交叉熵，REAL仍保持稳定并平均超越DAPO 4.5%。

Conclusion: REAL通过将可验证奖励重新定义为分类标签，有效解决了现有RLVR方法中的梯度错配问题，实现了更平衡的梯度分配和更稳定的训练，在数学推理任务上取得了显著的性能提升，且具有良好的可扩展性。

Abstract: Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.

</details>


### [98] [Empowering Time Series Analysis with Large-Scale Multimodal Pretraining](https://arxiv.org/abs/2602.05646)
*Peng Chen,Siyuan Wang,Shiyan Hu,Xingjian Wu,Yang Shu,Zhongwen Rao,Meng Wang,Yijie Li,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: HORAI：首个面向时间序列分析的多模态基础模型，通过频率增强的跨模态编码器和时频解码器，在包含10亿数据点的MM-TS数据集上预训练，在零样本预测和异常检测任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型主要依赖单模态预训练，缺乏互补模态来增强理解。构建多模态基础模型面临两大挑战：1）缺乏统一的多模态预训练范式和大型多模态语料库；2）如何有效整合异构模态并增强模型泛化能力。

Method: 提出多模态预训练范式，利用时间序列的内生模态（衍生图像和文本）和外生知识（真实世界新闻）。开发自动化数据构建流程创建MM-TS数据集（首个大规模多模态时间序列数据集，覆盖6个领域，包含10亿数据点）。提出HORAI模型，包含频率增强跨模态编码器和时频解码器两个核心组件。

Result: 在MM-TS上预训练后，HORAI在时间序列预测和异常检测任务上实现了最先进的零样本性能，展示了强大的跨模态和跨领域泛化能力。

Conclusion: HORAI是时间序列分析多模态基础模型的早期探索，通过统一的多模态预训练范式和频率增强架构，有效解决了异构模态融合和模型泛化问题，为时间序列分析提供了新的多视角理解框架。

Abstract: While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it faces key challenges: 1) lack of a unified multimodal pretraining paradigm and large-scale multimodal corpora for time series analysis; 2) how to effectively integrate heterogeneous modalities and enhance model generalization. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first propose a multimodal pretraining paradigm that leverages time series with endogenous modalities (derived images and text) and exogenous knowledge (real-world news), providing a comprehensive multi-view perspective for time series analysis. To support this, we develop an automated data construction pipeline to curate MM-TS, the first large-scale multimodal time series dataset spanning six domains, with up to one billion points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. It integrates two core components: the Frequency-enhanced Cross-Modality Encoder and the Time-Frequency Decoder, designed to effectively fuse multimodal features and enhance model generalization across modalities and domains. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong generalization.

</details>


### [99] [End-to-End Compression for Tabular Foundation Models](https://arxiv.org/abs/2602.05649)
*Guri Zabërgja,Rafiq Kamel,Arlind Kadra,Christian M. M. Frey,Josif Grabocka*

Main category: cs.LG

TL;DR: TACO是一个端到端的表格压缩模型，通过在潜在空间中压缩训练数据集，解决了表格Transformer模型因注意力机制导致的二次复杂度问题，实现了94倍推理加速和97%内存减少，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 表格Transformer模型虽然性能优异，但其基于注意力机制的架构具有关于数据集大小的二次复杂度，导致训练和推理时间开销大，限制了处理大规模数据集的能力。

Method: 提出TACO（端到端表格压缩模型），在潜在空间中压缩训练数据集，减少计算复杂度，同时保持预测性能。

Result: 在TabArena基准测试中，TACO推理时间比最先进的表格Transformer快94倍，内存消耗减少97%，性能无明显下降，且随着数据集规模增大表现更好。

Conclusion: TACO通过压缩训练数据有效解决了表格Transformer的二次复杂度问题，实现了高效的大规模表格数据处理，同时保持或超越了现有方法的性能。

Abstract: The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.

</details>


### [100] [Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization](https://arxiv.org/abs/2602.05657)
*Aleksandar Armacki,Dragana Bajović,Dušan Jakovetić,Soummya Kar,Ali H. Sayed*

Main category: cs.LG

TL;DR: 该研究通过大偏差理论分析SGD类方法的长期尾部衰减，为SGD和c-SGD提供了比现有有限时间界限更快的尾部衰减率，并证明这些界限在多项式对数因子内是紧的。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注高概率保证（固定概率阈值下的误差率），缺乏直接研究失败概率（固定误差阈值下的尾部衰减率）的工作。此外，现有结果都是有限时间的，无法捕捉现代学习模型（通常训练数百万次迭代）真正的长期尾部衰减行为。

Method: 使用大偏差理论研究SGD类方法的长期尾部衰减：1）对非凸成本和有界噪声下的普通SGD，提供梯度范数平方最佳迭代的尾部上界；2）对重尾噪声下的裁剪SGD(c-SGD)，在p∈(1,2]阶有界矩条件下提供上界；3）为SGD和c-SGD提供尾部衰减的下界。

Result: 1）SGD的长期尾部衰减率为e^{-t/log(t)}；2）c-SGD在p∈(1,2)时的衰减率为e^{-t^{β_p}/log(t)}，其中β_p=4(p-1)/(3p-2)，p=2时为e^{-t/log²(t)}；3）下界显示衰减率为e^{-t}，证明所获速率在多项式对数因子内是紧的。这些结果比现有有限时间界限（SGD的e^{-√t}和c-SGD的e^{-t^{β_p/2}}）快一个数量级。

Conclusion: 该工作填补了SGD尾部行为研究的空白，通过大偏差理论揭示了SGD类方法比先前已知更快的长期尾部衰减，为算法单次运行提供了更强的长期保证。结果表明在某些机制下，尾部衰减比现有认知快得多。

Abstract: The study of tail behaviour of SGD-induced processes has been attracting a lot of interest, due to offering strong guarantees with respect to individual runs of an algorithm. While many works provide high-probability guarantees, quantifying the error rate for a fixed probability threshold, there is a lack of work directly studying the probability of failure, i.e., quantifying the tail decay rate for a fixed error threshold. Moreover, existing results are of finite-time nature, limiting their ability to capture the true long-term tail decay which is more informative for modern learning models, typically trained for millions of iterations. Our work closes these gaps, by studying the long-term tail decay of SGD-based methods through the lens of large deviations theory, establishing several strong results in the process. First, we provide an upper bound on the tails of the gradient norm-squared of the best iterate produced by (vanilla) SGD, for non-convex costs and bounded noise, with long-term decay at rate $e^{-t/\log(t)}$. Next, we relax the noise assumption by considering clipped SGD (c-SGD) under heavy-tailed noise with bounded moment of order $p \in (1,2]$, showing an upper bound with long-term decay at rate $e^{-t^{β_p}/\log(t)}$, where $β_p = \frac{4(p-1)}{3p-2}$ for $p \in (1,2)$ and $e^{-t/\log^2(t)}$ for $p = 2$. Finally, we provide lower bounds on the tail decay, at rate $e^{-t}$, showing that our rates for both SGD and c-SGD are tight, up to poly-logarithmic factors. Notably, our results demonstrate an order of magnitude faster long-term tail decay compared to existing work based on finite-time bounds, which show rates $e^{-\sqrt{t}}$ and $e^{-t^{β_p/2}}$, $p \in (1,2]$, for SGD and c-SGD, respectively. As such, we uncover regimes where the tails decay much faster than previously known, providing stronger long-term guarantees for individual runs.

</details>


### [101] [Probabilistic Multi-Regional Solar Power Forecasting with Any-Quantile Recurrent Neural Networks](https://arxiv.org/abs/2602.05660)
*Slawek Smyl,Paweł Pełka,Grzegorz Dudek*

Main category: cs.LG

TL;DR: 提出基于AQ-RNN的多区域光伏功率任意分位数概率预测框架，通过双轨循环架构联合处理序列特定和跨区域上下文信息，实现单模型内任意概率水平的分位数估计


<details>
  <summary>Details</summary>
Motivation: 光伏发电渗透率增加给电力系统运行带来显著不确定性，需要超越确定性点预测的概率预测方法，以支持不确定性感知的能源管理和运行决策

Method: 基于AQ-RNN的任意分位数概率预测框架，采用双轨循环架构处理序列特定和跨区域上下文信息，结合扩张循环单元、基于补丁的时间建模和动态集成机制

Result: 使用259个欧洲区域30年小时级光伏发电数据进行评估，相比现有统计和神经概率基准方法，在预测精度、校准度和预测区间质量方面均取得一致改进

Conclusion: 该方法适用于可再生能源主导电力系统中的不确定性感知能源管理和运行决策，能够有效估计校准的条件分位数并利用空间依赖性增强系统级鲁棒性

Abstract: The increasing penetration of photovoltaic (PV) generation introduces significant uncertainty into power system operation, necessitating forecasting approaches that extend beyond deterministic point predictions. This paper proposes an any-quantile probabilistic forecasting framework for multi-regional PV power generation based on the Any-Quantile Recurrent Neural Network (AQ-RNN). The model integrates an any-quantile forecasting paradigm with a dual-track recurrent architecture that jointly processes series-specific and cross-regional contextual information, supported by dilated recurrent cells, patch-based temporal modeling, and a dynamic ensemble mechanism.
  The proposed framework enables the estimation of calibrated conditional quantiles at arbitrary probability levels within a single trained model and effectively exploits spatial dependencies to enhance robustness at the system level. The approach is evaluated using 30 years of hourly PV generation data from 259 European regions and compared against established statistical and neural probabilistic baselines. The results demonstrate consistent improvements in forecast accuracy, calibration, and prediction interval quality, underscoring the suitability of the proposed method for uncertainty-aware energy management and operational decision-making in renewable-dominated power systems.

</details>


### [102] [Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection](https://arxiv.org/abs/2602.05667)
*Ling Zhan,Zhen Li,Junjie Huang,Tao Jia*

Main category: cs.LG

TL;DR: 提出SCLCS框架，通过结构感知对比学习选择代表性核心集，仅用10%数据即可保持FC模型性能排序，解决大规模fMRI数据中功能连接性建模方法基准测试的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大规模fMRI数据集中数百种功能连接性（FC）建模方法的基准测试对可重复神经科学至关重要，但模型-数据配对的组合爆炸使穷举评估计算上不可行，阻碍其成为常规预分析步骤。

Method: 提出SCLCS框架：1）使用自适应Transformer学习每个样本的独特FC结构；2）引入结构扰动分数（SPS）量化训练期间学习结构的稳定性，识别代表基础连接原型样本；3）通过top-k排名识别稳定样本，并引入密度平衡采样策略促进多样性，确保核心集结构稳健且分布具有代表性。

Result: 在REST-meta-MDD数据集上，SCLCS仅用10%数据即可保持真实模型排序，在排序一致性（nDCG@k）上比最先进核心集选择方法提升高达23.2%。

Conclusion: 这是首个将核心集选择形式化用于FC算子基准测试的工作，使大规模算子比较成为计算神经科学中可行且不可或缺的部分，解决了FC基准测试的计算瓶颈问题。

Abstract: Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS

</details>


### [103] [Stable but Wrong: When More Data Degrades Scientific Conclusions](https://arxiv.org/abs/2602.05668)
*Zhipeng Zhang,Kai Li*

Main category: cs.LG

TL;DR: 论文揭示数据驱动科学存在根本性缺陷：即使推理过程稳定收敛且通过常规诊断，仍可能系统性地得出错误结论，特别是当观测可靠性以不可观测方式退化时。


<details>
  <summary>Details</summary>
Motivation: 挑战现代科学依赖大数据和自动化推理管道的隐含假设——即更多数据必然使科学结论更可靠。作者发现这种信念可能在结构性机制下根本且不可逆地失效。

Method: 识别一种结构性机制，在该机制下标准推理程序平滑收敛、保持良好校准并通过常规诊断检查，但仍系统性地收敛到错误结论。使用最小化合成实验进行验证。

Result: 在该机制下，额外数据不仅不会纠正错误反而会放大错误，而基于残差和拟合优度的诊断仍保持误导性的正常状态。推理过程的稳定性、收敛性和置信度不足以保证认识论有效性。

Conclusion: 数据驱动科学存在内在限制：推理不能被视为数据可用的无条件结果，而必须受观测过程完整性的明确约束所支配。

Abstract: Modern science increasingly relies on ever-growing observational datasets and automated inference pipelines, under the implicit belief that accumulating more data makes scientific conclusions more reliable. Here we show that this belief can fail in a fundamental and irreversible way. We identify a structural regime in which standard inference procedures converge smoothly, remain well calibrated, and pass conventional diagnostic checks, yet systematically converge to incorrect conclusions. This failure arises when the reliability of observations degrades in a manner that is intrinsically unobservable to the inference process itself. Using minimal synthetic experiments, we demonstrate that in this regime additional data do not correct error but instead amplify it, while residual-based and goodness-of-fit diagnostics remain misleadingly normal. These results reveal an intrinsic limit of data-driven science: stability, convergence, and confidence are not sufficient indicators of epistemic validity. We argue that inference cannot be treated as an unconditional consequence of data availability, but must instead be governed by explicit constraints on the integrity of the observational process.

</details>


### [104] [Mining Generalizable Activation Functions](https://arxiv.org/abs/2602.05688)
*Alex Vitvitskyi,Michael Boratko,Matej Grcic,Razvan Pascanu,Deep Shah,Petar Veličković*

Main category: cs.LG

TL;DR: 使用AlphaEvolve（基于前沿LLM作为变异算子）进行进化搜索，发现新的激活函数，不仅提升性能，还能编码特定归纳偏置，通过OOD数据作为适应度函数。


<details>
  <summary>Details</summary>
Motivation: 激活函数选择是研究热点，影响优化和模型表达能力，同时改变架构的隐式归纳偏置。现有方法搜索空间有限，需要手动设计。现代进化搜索框架（如AlphaEvolve）能探索更广泛的搜索空间，并发现具有特定归纳偏置的激活函数。

Method: 采用AlphaEvolve进化搜索框架，利用前沿LLM作为变异算子，在特定FLOP预算内搜索所有可能的Python函数作为激活函数。使用OOD（分布外）数据性能作为适应度函数，以发现尊重数据内在结构、对分布变化鲁棒的激活函数。通过小规模合成数据集进行实证探索。

Result: 实证研究表明，相对小规模的合成数据集足以让AlphaEvolve发现有意义的激活函数。该方法能够探索更广泛、灵活的搜索空间，无需手动构造搜索空间，且搜索偏向于有意义的激活函数，提高了搜索效率。

Conclusion: 进化搜索是发现新激活函数的有效框架，现代管道如AlphaEvolve能探索更广泛的搜索空间并发现具有特定归纳偏置的激活函数。使用OOD数据作为适应度函数可针对分布变化鲁棒性进行优化，小规模合成数据集足以支持有效搜索。

Abstract: The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.

</details>


### [105] [Almost Asymptotically Optimal Active Clustering Through Pairwise Observations](https://arxiv.org/abs/2602.05690)
*Rachel S. Y. Teo,P. N. Karthik,Ramya Korlakai Vinayak,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出一个主动聚类框架，通过噪声二元反馈将M个项目聚类到未知数量K个组中，建立查询次数的理论下界，并设计渐近最优算法


<details>
  <summary>Details</summary>
Motivation: 解决在噪声和主动收集反馈条件下，将项目聚类到未知数量组中的问题，旨在最小化所需查询次数同时保证聚类精度

Method: 使用变化测度技术建立查询次数的理论下界，设计基于广义似然比统计量的渐近最优算法，开发计算可行的GLR统计量变体

Result: 建立了期望查询次数的基本下界，设计了算法性能与下界差距可准确估计且保持常数倍关系的渐近最优算法

Conclusion: 提出的主动聚类框架在理论下界和算法设计方面取得了平衡，计算可行的GLR统计量变体在实际应用中具有良好性能

Abstract: We propose a new analysis framework for clustering $M$ items into an unknown number of $K$ distinct groups using noisy and actively collected responses. At each time step, an agent is allowed to query pairs of items and observe bandit binary feedback. If the pair of items belongs to the same (resp.\ different) cluster, the observed feedback is $1$ with probability $p>1/2$ (resp.\ $q<1/2$). Leveraging the ubiquitous change-of-measure technique, we establish a fundamental lower bound on the expected number of queries needed to achieve a desired confidence in the clustering accuracy, formulated as a sup-inf optimization problem. Building on this theoretical foundation, we design an asymptotically optimal algorithm in which the stopping criterion involves an empirical version of the inner infimum -- the Generalized Likelihood Ratio (GLR) statistic -- being compared to a threshold. We develop a computationally feasible variant of the GLR statistic and show that its performance gap to the lower bound can be accurately empirically estimated and remains within a constant multiple of the lower bound.

</details>


### [106] [FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning](https://arxiv.org/abs/2602.05693)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: FedRandom是一种解决联邦学习中参与者贡献评估不稳定性的新方法，通过统计估计生成更多样本来提供更一致可靠的贡献评估，在多种数据集上显著改善稳定性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，公平评估参与者的贡献至关重要，这关系到参与者的补偿、恶意参与者和搭便车者的识别。然而，现有方法在贡献评估上存在显著的不稳定性，这种不稳定性会影响参与者参与联邦的意愿。

Method: 提出FedRandom方法，将贡献评估不稳定性视为统计估计问题，通过生成比常规联邦学习策略更多的样本来提供更一致可靠的参与者贡献评估。

Result: 在CIFAR-10、MNIST、CIFAR-100和FMNIST等不同数据分布上测试，FedRandom在超过一半的评估场景中将与真实值的整体距离减少了三分之一以上，并在超过90%的情况下改善了稳定性。

Conclusion: FedRandom通过统计方法有效缓解了联邦学习中参与者贡献评估的不稳定性问题，为公平补偿和恶意参与者识别提供了更可靠的评估框架。

Abstract: Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.

</details>


### [107] [Fix Representation (Optimally) Before Fairness: Finite-Sample Shrinkage Population Correction and the True Price of Fairness Under Subpopulation Shift](https://arxiv.org/abs/2602.05707)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 论文研究机器学习中预测准确性与群体公平性之间的权衡关系，发现现有观察到的矛盾现象可能是由于训练数据中子群比例失真的假象，提出了基于收缩加权的评估协议来揭示真实的公平性-效用边界。


<details>
  <summary>Details</summary>
Motivation: 机器学习实践中经常观察到预测准确性与群体公平性约束之间的张力，但有时公平性干预似乎又能提高准确性。作者认为这两种现象都可能是训练数据中子群比例失真的假象，需要建立理论框架来区分真实的公平性-效用权衡与数据失真造成的假象。

Method: 在子群偏移（组内分布稳定，组间比例偏移）的假设下，建立了理论分析框架：1）证明了完全重要性加权校正的渐近无偏性但有限样本次优性；2）提出了最优有限样本校正方法——收缩加权，在目标分布和训练分布混合之间插值；3）设计了可操作的评估协议：先（最优地）固定表示，再比较公平性干预与收缩校正基线的差异，以隔离真实的、不可约的公平性代价。

Result: 理论分析表明：1）完全重要性加权校正虽然渐近无偏但在有限样本下并非最优；2）收缩加权是最优的有限样本校正方法；3）观察到的"公平性有助于准确性"现象可能是由于将公平性方法与不当加权的基线进行比较造成的。在合成数据和真实世界基准（Adult、COMPAS）上的实验验证了理论预测，并证明该协议能消除虚假的权衡，揭示真实的公平性-效用边界。

Conclusion: 论文提出了一个理论框架和评估协议来解决机器学习中公平性-准确性权衡的评估问题。通过识别和纠正训练数据中子群比例失真的影响，该协议能够区分真实的公平性代价与数据失真造成的假象，为公平机器学习提供了更可靠的评估方法。研究强调了在评估公平性干预时考虑数据表示问题的重要性，并提供了收缩加权作为最优的有限样本校正方法。

Abstract: Machine learning practitioners frequently observe tension between predictive accuracy and group fairness constraints -- yet sometimes fairness interventions appear to improve accuracy. We show that both phenomena can be artifacts of training data that misrepresents subgroup proportions. Under subpopulation shift (stable within-group distributions, shifted group proportions), we establish: (i) full importance-weighted correction is asymptotically unbiased but finite-sample suboptimal; (ii) the optimal finite-sample correction is a shrinkage reweighting that interpolates between target and training mixtures; (iii) apparent "fairness helps accuracy" can arise from comparing fairness methods to an improperly-weighted baseline. We provide an actionable evaluation protocol: fix representation (optimally) before fairness -- compare fairness interventions against a shrinkage-corrected baseline to isolate the true, irreducible price of fairness. Experiments on synthetic and real-world benchmarks (Adult, COMPAS) validate our theoretical predictions and demonstrate that this protocol eliminates spurious tradeoffs, revealing the genuine fairness-utility frontier.

</details>


### [108] [Muon in Associative Memory Learning: Training Dynamics and Scaling Laws](https://arxiv.org/abs/2602.05725)
*Binghui Li,Kaifei Wang,Han Zhong,Pinyan Lu,Liwei Wang*

Main category: cs.LG

TL;DR: Muon优化器在线性关联记忆模型中相比梯度下降(GD)能显著缓解频率学习不平衡问题，在无噪声情况下实现指数级加速，在有噪声情况下展现更优的优化缩放规律。


<details>
  <summary>Details</summary>
Motivation: Muon通过梯度符号更新矩阵参数在实践中表现出色，但其动态特性和缩放行为在理论上尚不明确。研究者希望在线性关联记忆模型中理解Muon与标准梯度下降在频率学习不平衡问题上的差异。

Method: 在线性关联记忆模型中研究Muon优化器，该模型采用softmax检索和查询-答案对的层次化频率谱，分别考虑无噪声和有标签噪声两种情况。通过理论分析Muon的优化动态，并将其解释为隐式矩阵预处理器。

Result: 在无噪声情况下，Muon相比GD实现指数级加速；在有噪声且频率谱呈幂律衰减的情况下，推导出Muon的优化缩放规律，证明其缩放效率优于GD。Muon可解释为自适应任务对齐和块对称梯度结构产生的隐式矩阵预处理器。

Conclusion: Muon通过缓解频率学习不平衡问题，在线性关联记忆模型中展现出比梯度下降更优的收敛速度和缩放效率。在合成长尾分类和LLaMA风格预训练实验中也验证了理论结果。

Abstract: Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.

</details>


### [109] [CSRv2: Unlocking Ultra-Sparse Embeddings](https://arxiv.org/abs/2602.05735)
*Lixuan Guo,Yifei Wang,Tiansheng Wen,Yifan Wang,Aosong Feng,Bo Chen,Stefanie Jegelka,Chenyu You*

Main category: cs.LG

TL;DR: CSRv2是一种改进的对比稀疏表示训练方法，通过渐进式k退火、监督对比目标和全骨干微调，使超稀疏嵌入（仅2个活跃特征）变得可行，在保持性能的同时大幅提升计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有对比稀疏表示（CSR）在超稀疏区域（超过80%神经元不活跃）性能严重下降，无法充分发挥稀疏嵌入的效率潜力。需要一种方法使超稀疏嵌入变得可行，以在保持嵌入质量的同时大幅降低存储、内存和推理延迟成本。

Method: CSRv2采用三个关键技术：1）渐进式k退火稳定稀疏学习；2）监督对比目标增强表示质量；3）全骨干微调确保端到端适应性。这些方法共同解决了CSR在超稀疏区域的性能退化问题。

Result: CSRv2将死神经元比例从80%降至20%，在k=2时获得14%的准确率提升。超稀疏嵌入（仅2个活跃特征）达到CSR在k=8和MRL在32维度的性能水平。相比MRL获得7倍加速，相比密集嵌入在文本表示中实现高达300倍的计算和内存效率提升。

Conclusion: CSRv2使超稀疏嵌入变得实用而不牺牲性能，为实时和边缘部署的AI系统拓宽了设计空间，在嵌入质量和效率都至关重要的场景中具有重要应用价值。

Abstract: In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.

</details>


### [110] [How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning](https://arxiv.org/abs/2602.05749)
*Kai Ming Ting,Wei-Jie Xu,Hang Zhang*

Main category: cs.LG

TL;DR: 论文分析了深度聚类相对于k-means的优势是否真实存在，发现深度嵌入聚类(DEC)并未真正解决k-means在发现任意形状、大小和密度聚类方面的根本局限性，而非深度学习方法通过利用数据分布信息反而能更好地实现深度聚类的目标。


<details>
  <summary>Details</summary>
Motivation: 深度聚类常被宣称具有优于k-means的关键优势，但这种优势通常仅在图像数据集上得到验证。作者质疑深度嵌入聚类(DEC)是否真正克服了k-means的根本局限性，即无法发现任意形状、大小和密度的聚类。研究旨在验证深度学习方法是否真正解决了这些基本问题。

Method: 通过分析深度嵌入聚类(DEC)方法，DEC使用自编码器学习潜在表示，并基于k-means类似过程进行聚类，采用端到端优化。研究对比了非深度学习方法，后者利用数据集中聚类的分布信息来有效解决k-means的根本局限性。

Result: 研究发现深度学习的表示并未使DEC克服k-means的根本局限性。相反，非深度学习方法通过利用聚类分布信息，能够有效解决任意形状、大小和密度的聚类发现问题，实现了深度聚类方法本应达到的目标。

Conclusion: 深度聚类方法并未真正解决k-means的根本局限性，因为它们没有利用底层数据分布信息。非深度学习方法通过利用聚类的分布信息，反而能更好地实现深度聚类的目标，这对深度聚类方法具有更广泛的启示意义。

Abstract: Deep clustering (DC) is often quoted to have a key advantage over $k$-means clustering. Yet, this advantage is often demonstrated using image datasets only, and it is unclear whether it addresses the fundamental limitations of $k$-means clustering. Deep Embedded Clustering (DEC) learns a latent representation via an autoencoder and performs clustering based on a $k$-means-like procedure, while the optimization is conducted in an end-to-end manner. This paper investigates whether the deep-learned representation has enabled DEC to overcome the known fundamental limitations of $k$-means clustering, i.e., its inability to discover clusters of arbitrary shapes, varied sizes and densities. Our investigations on DEC have a wider implication on deep clustering methods in general. Notably, none of these methods exploit the underlying data distribution. We uncover that a non-deep learning approach achieves the intended aim of deep clustering by making use of distributional information of clusters in a dataset to effectively address these fundamental limitations.

</details>


### [111] [Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance](https://arxiv.org/abs/2602.05774)
*Xiandong Zou,Jianshu Li,Jing Huang,Pan Zhou*

Main category: cs.LG

TL;DR: VSD（变分推测解码）通过变分推断训练草稿路径，优化多路径验证的解码过程，相比单贪婪轨迹方法提升解码效率


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在训练与解码不一致问题：训练优化单贪婪轨迹，而解码需要验证和排序多个采样草稿路径

Method: 将草稿训练公式化为对潜在提案（草稿路径）的变分推断，最大化目标模型接受概率的边际概率，使用EM算法优化，E步从经过筛选的后验分布中MCMC采样，M步使用自适应拒绝加权和置信感知正则化最大化加权似然

Result: 理论分析证实VSD增加期望接受长度和加速比，在LLM和MLLM上的实验显示VSD比EAGLE-3提升9.6%加速，比ViSpec提升7.9%加速

Conclusion: VSD通过变分推断框架解决训练-解码不一致问题，显著提升推测解码效率，为大规模语言模型推理加速提供有效方法

Abstract: Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.

</details>


### [112] [Cross-Domain Offline Policy Adaptation via Selective Transition Correction](https://arxiv.org/abs/2602.05776)
*Mengbei Yan,Jiafei Lyu,Shengjie Sun,Zhongjian Qiao,Jingwen Yang,Zichuan Lin,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: STC算法通过修正源域数据中的动作和奖励，使其与目标域动态对齐，从而在跨域离线强化学习中有效利用源域数据提升策略学习性能。


<details>
  <summary>Details</summary>
Motivation: 跨域离线强化学习中，直接合并源域和目标域数据集可能导致次优性能，因为存在动态不匹配问题。现有方法通过源域转移过滤或奖励修改可能无法充分利用源域数据的价值。

Method: 提出选择性转移修正（STC）算法：1）使用逆策略模型和奖励模型修正源域转移的动作和奖励，使其与目标域动态对齐；2）利用前向动态模型保留比原始转移更匹配目标动态的修正样本。

Result: 在多种具有动态偏移的环境中进行实验，STC算法相比现有基线方法取得了更优的性能表现。

Conclusion: 通过修正源域数据而非简单过滤或修改奖励，STC算法能够可靠地利用源域数据进行策略适应，有效解决跨域离线强化学习中的动态不匹配问题。

Abstract: It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.

</details>


### [113] [Distributional Reinforcement Learning with Diffusion Bridge Critics](https://arxiv.org/abs/2602.05783)
*Shutong Ding,Yimiao Zhou,Ke Hu,Mokai Pan,Shan Zhong,Yanwei Fu,Jingya Wang,Ye Shi*

Main category: cs.LG

TL;DR: 提出了一种基于扩散桥的分布强化学习方法DBC，首次将扩散桥模型用作critic，通过直接建模Q值的逆累积分布函数来准确捕捉价值分布，避免坍缩为简单高斯分布。


<details>
  <summary>Details</summary>
Motivation: 现有扩散强化学习方法主要关注扩散策略，而忽略了扩散critic。由于策略优化本质上依赖于critic，准确的价值估计比策略表达能力更重要。考虑到大多数强化学习任务的随机性，critic更适合用分布模型来描述。

Method: 提出扩散桥critic（DBC），直接建模Q值的逆累积分布函数（CDF），利用扩散桥的强大分布匹配能力准确捕捉价值分布。进一步推导了分析积分公式来解决DBC中的离散化误差问题。DBC是即插即用组件，可集成到大多数现有RL框架中。

Result: 在MuJoCo机器人控制基准测试中，DBC相比之前的分布critic模型表现出优越性，证明了扩散桥作为critic的有效性。

Conclusion: DBC是首个将扩散桥模型用作critic的工作，通过直接建模Q值的逆CDF准确捕捉价值分布，避免分布坍缩，在价值估计中表现出色，可作为即插即用组件集成到现有RL框架中。

Abstract: Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.

</details>


### [114] [Classification Under Local Differential Privacy with Model Reversal and Model Averaging](https://arxiv.org/abs/2602.05797)
*Caihong Qin,Yang Bai*

Main category: cs.LG

TL;DR: 该论文将LDP下的隐私学习重新解释为迁移学习问题，提出了三种新技术来提升分类性能而不损害隐私：噪声二进制反馈评估机制、模型反转和模型平均。


<details>
  <summary>Details</summary>
Motivation: 本地差分隐私虽然提供了强大的隐私保证，但引入的噪声会显著降低数据效用。现有方法在保持隐私的同时难以维持良好的分类性能，需要新的技术来解决这一效用损失问题。

Method: 1) 将LDP下的隐私学习重新解释为迁移学习问题，将噪声数据视为源域，未观察到的干净数据视为目标域；2) 提出噪声二进制反馈评估机制来估计数据集效用；3) 模型反转技术通过反转决策边界来挽救性能不佳的分类器；4) 模型平均技术根据估计的效用为多个反转分类器分配权重。

Result: 提供了LDP下的理论超额风险界限，并展示了所提方法如何降低这一风险。在模拟和真实数据集上的实证结果显示分类准确率有显著提升。

Conclusion: 通过将LDP隐私学习重新框架为迁移学习问题，并开发专门针对LDP的技术，可以在不损害隐私的前提下显著改善分类性能，为解决LDP中的效用损失问题提供了有效途径。

Abstract: Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.

</details>


### [115] [Principled Confidence Estimation for Deep Computed Tomography](https://arxiv.org/abs/2602.05812)
*Matteo Gätzner,Johannes Kirschner*

Main category: cs.LG

TL;DR: 提出基于序列似然混合框架的CT重建置信度估计方法，为深度学习重建提供理论覆盖保证的置信区域


<details>
  <summary>Details</summary>
Motivation: 为基于深度学习的CT重建提供具有理论覆盖保证的置信度估计框架，解决传统方法在不确定性量化方面的不足

Method: 基于序列似然混合框架，采用符合Beer-Lambert定律的对数线性前向模型和泊松噪声模型，适用于U-Net、U-Net集成和生成扩散模型等深度学习方法

Result: 深度重建方法相比经典算法产生更紧密的置信区域，同时保持理论覆盖保证，能够检测重建图像中的幻觉并提供可解释的置信区域可视化

Conclusion: 深度学习模型不仅是强大的估计器，也是可靠的不确定性感知医学成像工具，为临床和科学成像提供理论保障的置信度估计

Abstract: We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.

</details>


### [116] [Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers](https://arxiv.org/abs/2602.05813)
*Artem Riabinin,Andrey Veprikov,Arman Bolatov,Martin Takáč,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 提出自适应学习率调度方法，针对范数约束优化器（如Muon和Lion），通过广义光滑性假设推导出自然产生预热-衰减的学习率策略，并开发实用调度器自动调整预热时长，在LLaMA架构的大语言模型预训练中验证效果


<details>
  <summary>Details</summary>
Motivation: 研究范数约束优化器的自适应学习率调度问题，旨在解决传统启发式学习率调度（如预热-衰减）缺乏理论依据的问题，通过建立理论框架来自然推导学习率策略，并开发实用方法减少超参数调优需求

Method: 1. 提出广义光滑性假设：局部曲率随次优性间隙减小；2. 在该假设下建立收敛性保证，证明适当学习率选择会自然产生预热-衰减模式；3. 开发实用学习率调度器，仅依赖标准超参数，在训练开始时自动调整预热时长；4. 在LLaMA架构的大语言模型预训练中进行实验验证

Result: 在LLaMA架构的大语言模型预训练实验中，自适应预热选择方法在所有考虑的设置中始终优于或至少匹配最佳手动调优的预热调度，无需额外的超参数搜索

Conclusion: 提出的理论框架为范数约束优化器的学习率调度提供了理论基础，开发的实用调度器能够自动适应预热时长，在大语言模型预训练中表现优异，减少了手动调优的需求，代码已开源

Abstract: We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.
  Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup

</details>


### [117] [Synthesizing Realistic Test Data without Breaking Privacy](https://arxiv.org/abs/2602.05833)
*Laura Plein,Alexi Turcotte,Arina Hallemans,Andreas Zeller*

Main category: cs.LG

TL;DR: 提出一种基于模糊测试和判别器的隐私保护合成数据生成方法，通过间接利用原始数据生成具有相同统计分布但隐私安全的测试数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的合成数据生成方法存在两个主要问题：1) 生成模型准确性不足；2) 仍然容易受到成员推理攻击或数据集重建攻击，因为原始数据直接用于训练过程。需要一种既能保持原始数据统计特性又能保护隐私的合成数据生成方法。

Method: 受GAN启发但采用不同架构：1) 使用测试生成器（模糊器）从输入规范生成测试数据，保留原始数据设定的约束；2) 判别器模型评估生成数据与原始数据的接近程度；3) 通过进化样本并使用判别器确定"好样本"，生成隐私保护数据。该方法仅间接利用原始数据，避免直接训练。

Result: 在四个用于评估最先进技术的公开数据集上进行实验，结果表明该方法能够生成具有高实用性的合成数据集，同时有效保护隐私。生成的合成数据与原始数据具有相似的统计分布和实用价值。

Conclusion: 该方法展示了生成高实用性隐私保护合成数据集的潜力，通过间接利用原始数据和进化优化策略，在保持统计特性的同时避免了传统GAN方法的隐私风险。

Abstract: There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining "good samples" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.

</details>


### [118] [DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders](https://arxiv.org/abs/2602.05859)
*Xu Wang,Bingqing Jiang,Yu Wan,Baosong Yang,Lingpeng Kong,Difan Zou*

Main category: cs.LG

TL;DR: 本文提出了DLM-Scope，首个针对扩散语言模型（DLMs）的稀疏自编码器（SAE）可解释性框架，展示了SAE在DLMs中提取可解释特征的有效性，并发现了与自回归LLMs不同的特性。


<details>
  <summary>Details</summary>
Motivation: 随着扩散语言模型（DLMs）逐渐成为自回归大语言模型（LLMs）的有前景替代方案，需要为这类新兴模型开发专门的可解释性工具。稀疏自编码器（SAEs）已成为自回归LLMs机制可解释性的标准工具，但尚未应用于DLMs。

Method: 提出了DLM-Scope框架，将Top-K稀疏自编码器（SAEs）应用于扩散语言模型。通过训练SAEs提取稀疏、可解释的特征，并研究SAE插入对DLMs的影响，包括损失变化、扩散时间干预效果等。

Result: 1. SAEs能够从DLMs中忠实提取可解释特征；2. SAE插入对DLMs的影响与LLMs不同：在早期层可减少交叉熵损失（LLMs中通常增加损失）；3. DLM的SAE特征支持更有效的扩散时间干预，性能常优于LLM引导；4. SAEs可为DLM解码顺序提供有用信号；5. SAE特征在DLMs后训练阶段保持稳定。

Conclusion: DLM-Scope为扩散语言模型的机制可解释性奠定了基础，展示了SAEs在DLM相关任务和算法中的巨大潜力，为DLMs的可解释性研究开辟了新方向。

Abstract: Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.

</details>


### [119] [Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations](https://arxiv.org/abs/2602.05885)
*Wei Liu,Jiawei Xu,Yingru Li,Longtao Zheng,Tianjian Li,Qian Liu,Junxian He*

Main category: cs.LG

TL;DR: 该论文提出了一种用于内核生成的强化学习框架，通过设计KernelGYM环境、解决多轮RL中的偏置梯度问题，以及引入性能分析奖励来训练Dr.Kernel-14B模型，在KernelBench上取得了优于Claude-4.5-Sonnet和GPT-5的性能。


<details>
  <summary>Details</summary>
Motivation: 高质量内核对于可扩展AI系统至关重要，但训练LLMs生成此类代码面临数据不足、环境脆弱、奖励黑客攻击和懒惰优化等问题。模型可能通过黑客攻击训练奖励，优先考虑琐碎的正确性而非有意义的加速。

Method: 1) 设计KernelGYM分布式GPU环境，支持奖励黑客检查、多轮交互数据收集和长期RL训练；2) 提出Turn-level Reinforce-Leave-One-Out (TRLOO)解决GRPO中自包含导致的偏置梯度问题；3) 引入不匹配校正确保训练稳定性；4) 使用性能分析奖励(PR)和性能分析拒绝采样(PRS)克服懒惰优化问题。

Result: Dr.Kernel-14B模型在KernelBench上达到与Claude-4.5-Sonnet竞争的性能。在KernelBench Level-2子集上，31.6%的生成内核实现了至少1.2倍加速，优于Claude-4.5-Sonnet(26.7%)和GPT-5(28.6%)。当选择所有轮次中的最佳候选时，1.2倍加速率进一步提升至47.8%。

Conclusion: 通过系统研究内核生成的强化学习方法，解决了奖励黑客攻击和懒惰优化问题，成功训练出高性能的Dr.Kernel-14B模型。提出的KernelGYM环境和TRLOO方法为多轮RL训练提供了有效解决方案，所有资源已开源。

Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.

</details>


### [120] [Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting](https://arxiv.org/abs/2602.05887)
*Tianqi Shen,Jinji Yang,Junze He,Kunhan Gao,Ziye Ma*

Main category: cs.LG

TL;DR: 提出一种模拟过参数化逃逸方向(SOD)的确定性框架，用于低秩矩阵感知问题，无需实际进行张量提升即可逃离虚假局部极小值。


<details>
  <summary>Details</summary>
Motivation: 低秩矩阵感知等非凸问题的优化景观包含大量虚假局部极小值，使得基于梯度的优化器难以收敛到全局最优。虽然过参数化通过张量提升可以将局部极小值转换为严格鞍点，但实际进行张量提升计算上不可行。

Method: 提出模拟过参数化逃逸方向(SOD)机制：设计数学框架模拟过参数化空间的景观和逃逸方向，将过参数化逃逸方向投影到原始参数空间，保证从现有局部极小值严格降低目标函数值。

Result: 数值实验表明，该框架能可靠逃离局部极小值并促进收敛到全局最优，与显式张量过参数化相比计算成本最小。这是第一个能保证逃离虚假局部极小值的确定性框架，无需随机扰动或启发式估计。

Conclusion: 该框架展示了如何利用模拟过参数化来驯服具有挑战性的优化景观，对超越矩阵感知的非凸优化具有非平凡意义。

Abstract: Low-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. Recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. Motivated by this observation, we propose a Simulated Oracle Direction (SOD) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. In essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. To the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. Numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. We believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.

</details>


### [121] [DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training](https://arxiv.org/abs/2602.05890)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Jiahan Li,Chenhao Huang,Junjie Ye,Sixian Li,Mingxu Chai,Yuhui Wang,Yajie Yang,Ming Zhang,Jiazheng Zhang,Shichun Liu,Caishuang Huang,Yunke Zhang,Yuran Wang,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: DFPO是一种鲁棒分布强化学习框架，通过将价值建模为跨时间步的连续流而非独立分位数预测，在噪声监督下实现更好的训练稳定性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现实世界RL训练面临噪声监督和域外泛化差的挑战，现有分布RL方法独立学习每个分位数作为标量，导致价值表示粗糙，缺乏对状态信息的细粒度条件化，在复杂和OOD条件下表现不佳

Method: 提出DFPO框架：1) 将价值建模为时间步间的连续流，学习价值流场而非孤立分位数预测；2) 集成条件风险控制；3) 沿价值流轨迹施加一致性约束，以稳定噪声反馈下的训练

Result: 在对话、数学推理和科学任务上的实验表明，DFPO在噪声监督下优于PPO、FlowRL和其他鲁棒基线，实现了改进的训练稳定性和泛化能力

Conclusion: DFPO通过连续价值流建模、条件风险控制和一致性约束，为噪声监督下的RL训练提供了更鲁棒的解决方案，改善了复杂和OOD条件下的性能

Abstract: Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.

</details>


### [122] [Parity, Sensitivity, and Transformers](https://arxiv.org/abs/2602.05896)
*Alexander Kozachinskiy,Tomasz Steifer,Przemysław Wałȩga*

Main category: cs.LG

TL;DR: 本文提出了一种新的Transformer架构用于解决PARITY问题，该架构使用softmax、长度无关且多项式有界的位置编码，无需层归一化，同时支持因果掩码和非因果掩码。此外，首次给出了Transformer解决PARITY问题的下界，证明单层单头无法解决该问题。


<details>
  <summary>Details</summary>
Motivation: Transformer架构已有近十年历史，但我们对其计算能力仍了解有限。特别是，单层Transformer能否解决PARITY问题？现有解决方案需要至少2层，且依赖不实用特性：长度相关位置编码、hardmax、无正则化参数的层归一化，或不支持因果掩码。本文旨在构建更实用的Transformer架构来解决PARITY问题，并探索其计算能力的理论界限。

Method: 提出新的Transformer构造方法，使用softmax激活函数、长度无关且多项式有界的位置编码，不依赖层归一化。该构造同时支持因果掩码和非因果掩码操作。此外，通过理论分析证明单层单头Transformer无法解决PARITY问题，首次建立了该问题的下界。

Result: 成功构建了能够解决PARITY问题的Transformer架构，该架构具有实用特性：使用softmax、长度无关位置编码、无层归一化要求，且同时支持因果和非因果掩码。理论证明表明，单层单头Transformer无法解决PARITY问题，这是该问题的首个下界结果。

Conclusion: 本文在Transformer计算能力理解方面取得重要进展：1）提出了更实用的PARITY问题解决方案，消除了先前构造中的不实用特性；2）首次建立了Transformer解决PARITY问题的下界，证明单层单头架构的局限性。这些结果为理解Transformer的计算能力边界提供了新的理论洞见。

Abstract: The transformer architecture is almost a decade old. Despite that, we still have a limited understanding of what this architecture can or cannot compute. For instance, can a 1-layer transformer solve PARITY -- or more generally -- which kinds of transformers can do it? Known constructions for PARITY have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.
  We give a new construction of a transformer for PARITY with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. We also give the first lower bound for transformers solving PARITY -- by showing that it cannot be done with only one layer and one head.

</details>


### [123] [Regularized Calibration with Successive Rounding for Post-Training Quantization](https://arxiv.org/abs/2602.05902)
*Seohyeon Cha,Huancheng Chen,Dongjun Kim,Haoran Zhang,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 该论文提出了一种基于正则化非对称校准的后训练量化方法，通过对称与非对称校准的插值作为正则化，并开发了简单连续舍入程序和有界搜索扩展，在保持计算成本可控的同时提升LLM量化性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署面临内存和延迟成本挑战，后训练量化虽能实现高效推理，但其效果严重依赖于量化目标和舍入程序。现有方法在激活不匹配时表现不佳，需要更稳健的量化方法。

Method: 1. 提出对称与非对称校准的插值作为正则化方法，保留PTQ标准二次结构的同时增强对激活不匹配的鲁棒性；2. 推导简单连续舍入程序，自然融入非对称校准；3. 开发有界搜索扩展，允许在量化质量和计算成本之间进行显式权衡。

Result: 在多个LLM家族、量化比特宽度和基准测试上的实验表明，基于正则化非对称校准目标的有界搜索方法持续改善困惑度和准确率，优于PTQ基线方法，同时仅产生适度且可控的额外计算成本。

Conclusion: 通过将对称与非对称校准插值作为正则化形式，并开发相应的舍入和搜索方法，能够有效提升后训练量化的性能，在保持计算效率的同时为LLM部署提供更优的量化解决方案。

Abstract: Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.

</details>


### [124] [Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903)
*András Balogh,Márk Jelasity*

Main category: cs.LG

TL;DR: 该研究提出使用对抗序列生成方法来验证序列模型在象棋领域的正确性，发现所有训练模型都存在不完整性问题，但某些训练技术和数据集选择能显著改善正确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决生成序列模型训练中的关键问题：基于样本的训练能否捕捉语言的真实结构（"世界模型"）。理论表明最多只能保证生成有效序列，但不能保证生成所有有效序列。因此需要实用工具来验证序列模型的正确性。

Method: 采用对抗序列生成方法验证序列模型的正确性。选择象棋作为研究领域，因其复杂度适中且有简单的基于规则的世界模型。提出多种对抗序列生成方法，生成有效序列迫使序列模型预测无效的下一步走法。同时研究棋盘状态探针在训练和攻击方法中的潜在应用。

Result: 研究发现：1) 所有训练的象棋模型都不完整（非正确）；2) 某些训练技术和数据集选择能显著改善正确性；3) 在大多数模型中，提取的棋盘状态对下一个标记预测没有因果作用。

Conclusion: 对抗序列生成是验证序列模型正确性的有效方法，不仅能证伪正确性，还能进行细粒度分析失败模式和训练选择的影响。研究为理解生成序列模型的局限性提供了实用工具和见解。

Abstract: Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.

</details>


### [125] [Chunky Post-Training: Data Driven Failures of Generalization](https://arxiv.org/abs/2602.05910)
*Seoirse Murray,Allison Qi,Timothy Qian,John Schulman,Collin Burns,Sara Price*

Main category: cs.LG

TL;DR: 论文提出"块状后训练"概念，指大语言模型在后训练中学习到数据块中的虚假相关性，导致意外行为。作者开发了SURF和TURF工具来检测和追踪这些问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练使用多样化数据集，但这些数据集除了预期行为外还编码了偶然模式（如格式与内容的相关性、狭窄的措辞、数据筛选过程中的隐式关联）。这些模式对开发者不可见但对模型显著，导致模型产生令创建者惊讶的行为（如拒绝以特定格式呈现的真实事实）。

Method: 提出SURF（黑盒管道）在运行时检测这些意外行为，以及TURF工具将这些失败追溯到具体的后训练数据。将工具应用于前沿模型（Claude 4.5、GPT-5.1、Grok 4.1、Gemini 3）和开源模型（Tülu 3）。

Result: 块状后训练导致校准错误的行为，这些行为通常源于不平衡或未充分指定的后训练数据块。工具成功检测到模型因数据块中的虚假相关性而产生的意外行为。

Conclusion: 块状后训练是大语言模型开发中的重要问题，需要更好的数据管理和检测工具。SURF和TURF为理解和缓解这一问题提供了有效方法，有助于提高模型行为的可预测性和可靠性。

Abstract: LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (Tülu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.

</details>


### [126] [Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training](https://arxiv.org/abs/2602.05933)
*Zhenghao Xu,Qin Lu,Changlong Yu,Tuo Zhao*

Main category: cs.LG

TL;DR: PMD-mean是一种实用的强化学习算法，通过用采样策略的平均奖励近似对数配分函数，在LLM的大动作空间中实现更稳定高效的策略优化。


<details>
  <summary>Details</summary>
Motivation: 策略镜像下降（PMD）为强化学习提供了理论框架，但在LLM的大动作空间中，精确估计配分函数面临挑战，特别是在有限采样情况下。需要开发更实用的算法来应对这一挑战。

Method: 提出PMD-mean算法：1）用采样策略的平均奖励近似对数配分函数；2）在对数策略空间中进行回归；3）该方法隐式优化了带有自适应混合KL-χ²正则化的镜像下降子问题。

Result: 实验表明PMD-mean在数学推理任务上表现优异，具有更好的稳定性、时间效率和性能。额外的χ²正则化约束了概率的大幅变化，在期望奖励较低时产生更保守的更新，增强了对有限样本估计误差的鲁棒性。

Conclusion: PMD-mean为LLM的强化学习算法提供了理论理解，揭示了通过自适应正则化改进PMD的途径，有助于开发更原则性的RL算法。

Abstract: Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.

</details>


### [127] [Dimensionality Reduction on Riemannian Manifolds in Data Analysis](https://arxiv.org/abs/2602.05936)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.LG

TL;DR: 该论文研究了基于黎曼几何的降维方法，这些方法尊重数据的底层流形结构，重点关注主测地线分析(PGA)作为流形值数据的非线性PCA扩展，并通过黎曼适配扩展判别分析等方法。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得降维方法在处理约束于弯曲空间（如超球面和对称正定流形）的数据时存在局限性，需要开发能够尊重数据底层流形结构的几何感知降维方法。

Method: 研究基于黎曼几何的降维方法，包括主测地线分析(PGA)作为流形值数据的非线性PCA扩展，以及通过黎曼适配扩展判别分析等方法。这些方法利用测地线距离、切空间表示和内在统计度量来实现更忠实的低维嵌入。

Result: 在代表性数据集上的实验结果表明，黎曼方法相比欧几里得对应方法提供了改进的表示质量和分类性能，特别是对于约束于弯曲空间的数据。

Conclusion: 该研究强调了在现代机器学习和数据科学应用中几何感知降维的重要性，黎曼几何方法为处理流形值数据提供了更有效的降维框架。

Abstract: In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.

</details>


### [128] [Orthogonal Model Merging](https://arxiv.org/abs/2602.05943)
*Sihan Yang,Kexuan Shi,Weiyang Liu*

Main category: cs.LG

TL;DR: 提出OrthoMerge方法，在正交群的黎曼流形上进行模型合并，以保持预训练权重的几何结构，解决传统线性合并方法破坏几何特性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流的模型合并方法依赖欧几里得空间中的线性算术，这会破坏预训练权重的内在几何特性（如超球面能量）。需要一种能够保持权重几何结构的合并方法。

Method: 提出OrthoMerge方法：1) 将OFT学习的任务特定正交矩阵映射到李代数进行合并；2) 通过正交-残差解耦策略扩展至非OFT微调模型，通过正交Procrustes问题提取正交分量在正交群流形上合并，线性残差通过标准加法合并。

Result: 大量实证结果表明OrthoMerge能有效缓解灾难性遗忘，并在多样化任务中保持模型性能。

Conclusion: OrthoMerge通过在正交群的黎曼流形上进行模型合并，成功保持了权重的几何结构，为模型合并提供了一种几何感知的解决方案。

Abstract: Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.

</details>


### [129] [$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment](https://arxiv.org/abs/2602.05946)
*Rajdeep Haldar,Lantao Mei,Guang Lin,Yue Xing,Qifan Song*

Main category: cs.LG

TL;DR: 提出基于f-散度的统一对齐框架，包含f-GRPO和f-HAL两种方法，在RLVR和PA任务上验证有效性


<details>
  <summary>Details</summary>
Motivation: 将偏好对齐的散度估计视角扩展到一般对齐设置（如RLVR），建立统一的理论框架

Method: 基于f-散度的变分表示，提出f-GRPO（在线策略强化学习）和f-HAL（混合在线/离线策略目标）

Result: 理论上保证对齐后平均奖励提升，实证在数学推理（RLVR）和安全对齐（PA）任务上优于现有方法

Conclusion: 提出的f-散度框架为LLM对齐提供统一理论支撑和有效方法，在多种对齐设置中展现优越性能

Abstract: Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.

</details>


### [130] [Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces](https://arxiv.org/abs/2602.05961)
*Arran Carter,Sanghyeok Choi,Kirill Tamogashev,Víctor Elvira,Nikolay Malkin*

Main category: cs.LG

TL;DR: 本文提出了一种改进离散扩散采样器的方法，通过引入离策略训练技术，将离散扩散采样器推广到任意分布之间的桥接任务，并展示了在图像生成模型离散潜在空间中进行无数据后验采样的应用。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散采样器在连续空间采样任务中已得到广泛研究，但在离散空间的应用仍相对不足。现有的离散扩散采样器未能充分利用连续空间采样中常用的思想，存在性能提升空间。

Method: 1. 引入离策略训练技术改进离散扩散采样器；2. 将离散扩散采样器推广到任意两个分布之间的桥接任务，首次在离散域中引入数据到能量的薛定谔桥训练；3. 将提出的扩散采样器应用于图像生成模型离散潜在空间的无数据后验采样。

Result: 1. 离策略训练技术提高了离散采样器在已有基准和新合成基准上的性能；2. 成功实现了离散域中数据到能量的薛定谔桥训练；3. 展示了在图像生成模型离散潜在空间中进行无数据后验采样的可行性。

Conclusion: 本文通过引入离策略训练技术显著提升了离散扩散采样器的性能，成功将连续空间采样中的先进思想应用于离散域，并展示了该方法在复杂实际任务中的有效性，为离散空间采样问题提供了新的解决方案。

Abstract: Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.

</details>


### [131] [Layer-wise LoRA fine-tuning: a similarity metric approach](https://arxiv.org/abs/2602.05988)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Lucas Pellicer,Rosimeire Pereira Costa,Edson Bollis,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 提出一种基于层选择的方法，通过仅对最相关的层进行LoRA微调，在保持预测性能的同时将可训练参数进一步减少50%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模持续增长，现有参数高效微调方法（如LoRA）虽然能减少99%可训练参数，但仍可能不足。需要进一步降低微调的计算成本。

Method: 通过测量内部表示变化贡献度，系统选择最相关的层进行LoRA微调。该方法与现有低秩适应技术正交且兼容，可减少LoRA技术中最多50%可训练参数。

Result: 在编码器架构上，GLUE基准测试中可训练参数减少导致预测性能下降可忽略；在解码器架构上，数学问题解决和编码任务中性能有小幅下降甚至提升；在多模态模型中也观察到与全层LoRA微调相当的结果。

Conclusion: 通过层选择策略可显著减少LoRA微调的可训练参数（最多50%），同时在不同模型架构和任务上保持或提升预测性能，为大规模模型的高效微调提供了有效解决方案。

Abstract: Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA

</details>


### [132] [Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps](https://arxiv.org/abs/2602.05993)
*Peter Holderrieth,Douglas Chen,Luca Eyring,Ishin Shah,Giri Anantharaman,Yutong He,Zeynep Akata,Tommi Jaakkola,Nicholas Matthew Boffi,Max Simchowitz*

Main category: cs.LG

TL;DR: Diamond Maps是一种随机流映射模型，能够在推理时高效准确地对齐任意奖励，解决了生成模型奖励对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前流模型和扩散模型虽然能生成高质量样本，但在训练后适应特定用户偏好或约束时成本高昂且脆弱（奖励对齐问题）。作者认为高效奖励对齐应该是生成模型的内在属性而非事后补救，因此需要重新设计模型以提高适应性。

Method: 提出"Diamond Maps"随机流映射模型，将多个模拟步骤摊销到单步采样器中（类似流映射），同时保留奖励对齐所需的随机性。这种设计使得搜索、顺序蒙特卡洛和引导变得可扩展，能够高效一致地估计价值函数。模型通过从GLASS Flows蒸馏进行高效学习。

Result: 实验表明Diamond Maps能够高效学习，实现更强的奖励对齐性能，并且比现有方法具有更好的扩展性。

Conclusion: Diamond Maps为构建能够在推理时快速适应任意偏好和约束的生成模型提供了实用途径。

Abstract: Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose "Diamond Maps", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.

</details>


### [133] [Orthogonal Self-Attention](https://arxiv.org/abs/2602.05996)
*Leo Zhang,James Martens*

Main category: cs.LG

TL;DR: 提出正交自注意力机制(OSA)替代Softmax自注意力，解决无跳跃连接Transformer中的秩塌陷和Jacobian病态问题，通过矩阵指数实现正交注意力矩阵，具有线性计算复杂度。


<details>
  <summary>Details</summary>
Motivation: Softmax自注意力在无跳跃连接架构中会导致秩塌陷和Jacobian病态问题，限制了无跳跃连接Transformer的训练稳定性。需要设计新的注意力机制来绕过这些问题。

Method: 提出正交自注意力机制：通过将查询-键值形成的斜对称矩阵映射到矩阵指数，参数化注意力矩阵为正交矩阵。利用查询-键值的低秩结构实现线性计算复杂度和内存成本。

Result: OSA可以实际实现，计算复杂度和内存成本随序列长度线性增长。推导出确保OSA Jacobian良好条件的初始化方案。

Conclusion: 正交自注意力机制能够解决Softmax自注意力的稳定性问题，使无跳跃连接、无归一化层的Transformer更容易训练，为更高效的Transformer架构设计提供了新途径。

Abstract: Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.

</details>


### [134] [Optimism Stabilizes Thompson Sampling for Adaptive Inference](https://arxiv.org/abs/2602.06014)
*Shunxing Yan,Han Zhong*

Main category: cs.LG

TL;DR: 论文研究了Thompson采样在自适应数据收集下的推断性质，发现乐观机制能稳定采样过程，使多臂赌博机中的渐近推断有效。


<details>
  <summary>Details</summary>
Motivation: Thompson采样广泛用于随机多臂赌博机，但在自适应数据收集下的推断性质复杂。经典渐近理论可能失效，因为臂特定的样本量是随机的，且通过动作选择规则与奖励耦合。需要研究如何恢复稳定性以实现有效的渐近推断。

Method: 研究K臂高斯赌博机，识别乐观机制作为恢复稳定性的关键。分析两种乐观修改：1) 方差膨胀Thompson采样；2) 保持后验方差不变但显式添加均值奖励到后验均值的替代方法。

Result: 证明方差膨胀Thompson采样对任意K≥2都是稳定的，包括多个臂都是最优的挑战性情况，解决了先前研究中从两臂扩展到一般K臂的开放性问题。分析替代乐观修改并建立相同的稳定性结论。

Conclusion: 适当实现的乐观机制能稳定Thompson采样，使多臂赌博机中的渐近推断有效，同时仅产生轻微的额外遗憾成本。

Abstract: Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.

</details>


### [135] [AP-OOD: Attention Pooling for Out-of-Distribution Detection](https://arxiv.org/abs/2602.06031)
*Claus Hofmann,Christian Huber,Bernhard Lehner,Daniel Klotz,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: AP-OOD是一种新颖的文本分布外检测方法，通过利用token级信息超越简单的平均聚合，在半监督设置下灵活结合有限辅助异常数据，在多个文本任务上显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 分布外检测需要将高维数据映射为标量OOD分数，但现有方法在有效利用和聚合语言模型的token嵌入方面存在挑战，需要超越简单的平均聚合方法。

Method: AP-OOD是一种半监督方法，通过利用token级信息而非简单的平均聚合，能够灵活地在无监督和有监督设置之间插值，允许使用有限的辅助异常数据。

Result: 在无监督设置下，AP-OOD在XSUM摘要任务上将FPR95从27.84%降至4.67%，在WMT15 En-Fr翻译任务上从77.08%降至70.37%，创造了文本OOD检测的新SOTA。

Conclusion: AP-OOD通过有效利用token级信息和半监督框架，显著提升了文本分布外检测性能，为可靠部署机器学习模型提供了更有效的解决方案。

Abstract: Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.

</details>


### [136] [Can vision language models learn intuitive physics from interaction?](https://arxiv.org/abs/2602.06033)
*Luca M. Schulze Buschoff,Konstantinos Voudouris,Can Demircan,Eric Schulz*

Main category: cs.LG

TL;DR: 论文研究发现，通过监督微调或环境交互训练的视觉语言模型在物理推理任务上表现不佳，缺乏可泛化的物理直觉，即使任务共享视觉统计特征和物理原理。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型缺乏对物理世界的良好直觉。虽然监督微调可以提升简单物理任务的性能，但模型似乎无法学习到能够泛化到新情境的稳健物理规则。基于认知科学研究，作者假设模型需要通过与环境交互来正确学习物理动态。

Method: 使用强化学习训练模型通过与环境交互进行学习。比较了监督微调和交互学习两种方法，并评估模型在相关任务间的泛化能力。

Result: 虽然通过交互学习可以提高模型在特定任务内的性能，但这种方法无法产生具有可泛化物理直觉的模型。研究发现，在一个任务上训练的模型无法可靠地泛化到相关任务，即使这些任务共享视觉统计特征和物理原理，无论模型是否通过交互训练。

Conclusion: 当前基于监督微调或环境交互的训练方法不足以让视觉语言模型获得稳健且可泛化的物理直觉。需要新的训练范式或架构来使模型真正理解物理世界的动态规律。

Abstract: Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.

</details>


### [137] [Shared LoRA Subspaces for almost Strict Continual Learning](https://arxiv.org/abs/2602.06043)
*Prakhar Kaushik,Ankit Vaidya,Shravan Chaudhari,Rama Chellappa,Alan Yuille*

Main category: cs.LG

TL;DR: Share是一种参数高效的持续微调方法，通过学习和动态更新单个共享低秩子空间，实现跨任务和模态的无缝适应，显著减少参数和内存需求。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型的高效持续适应对实际部署至关重要，但面临灾难性遗忘和高重训练成本挑战。现有参数高效调优方法（如LoRA）缺乏严格的持续学习机制，且依赖数据回放或多适配器。

Method: 构建基础子空间提取过去任务的核心知识，通过识别关键子空间方向增量整合新信息。每个新任务的知识被纳入这个演化子空间，促进前向知识转移，同时最小化灾难性干扰。

Result: 相比传统LoRA方法，Share实现高达100倍参数减少和281倍内存节省，性能与联合训练模型相当。单个Share模型可替代数百个任务特定LoRA适配器，支持可扩展的异步持续学习。

Conclusion: Share为大规模AI系统中的终身学习提供了实用且可扩展的解决方案，在图像分类、自然语言理解、3D姿态估计和文本到图像生成等多个领域验证了其有效性。

Abstract: Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [138] [Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set](https://arxiv.org/abs/2602.04910)
*Nongyu Di,Tianyu Chen,Shan Lu,Shuai Lu,Yeyun Gong,Peng Cheng,Jacob R. Lorch,Yuan Yao,Xiaoxing Ma*

Main category: cs.SE

TL;DR: VeruSyn是一个为Rust验证工具Verus设计的数据合成管道，通过自合成和教程合成生成690万Rust程序及其形式化规约和证明，用于微调代码生成模型以提升正确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的代码正确性存在担忧，需要形式化证明来验证。但相比代码生成，代码证明生成需要更强的推理能力且缺乏训练数据，特别是针对Verus这样的Rust验证工具。

Method: 提出VeruSyn数据合成管道：1) 自合成和教程合成实现大规模Verus特征覆盖；2) 通过智能体轨迹合成补充长链思维数据；3) 合成690万Rust程序，每个都包含形式化规约和证明；4) 使用该数据集微调Qwen2.5-Coder-32B-Instruct模型。

Result: 创建了最大的Verus验证程序集（690万程序），微调的模型在成本-证明权衡方面优于Claude Sonnet 4.5等商业模型，显著超越o4-mini和先前研究模型。

Conclusion: VeruSyn通过大规模数据合成解决了代码证明生成的训练数据稀缺问题，为LLM生成带形式化证明的正确代码提供了有效解决方案，在Rust系统软件验证领域取得了显著进展。

Abstract: Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.

</details>


### [139] [ASA: Activation Steering for Tool-Calling Domain Adaptation](https://arxiv.org/abs/2602.04935)
*Youjin Wang,Run Zhou,Rong Fu,Shuaishuai Cao,Hongwei Zeng,Jiaxuan Lu,Sicheng Fan,Jiaqiao Zhao,Liangming Pan*

Main category: cs.SE

TL;DR: ASA是一种轻量级、无需训练的推理时适配机制，通过读取中间激活的路由信号并使用超轻量路由器产生自适应控制强度，实现精确的领域对齐，解决LLM智能体在多领域工具生态系统中高效适应的问题。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体在现实部署中的核心挑战不是工具使用本身，而是在快速演变的工具集、API和协议下的高效领域适应。重复的LoRA或SFT跨领域训练会产生指数级增长的成本，而提示或模式方法在分布偏移和复杂接口下表现脆弱。

Method: 提出激活导向适配器（ASA），这是一种轻量级、推理时、无需训练的机制。它从中间激活中读取路由信号，并使用超轻量路由器产生自适应控制强度，实现精确的领域对齐。

Result: 在多个模型规模和领域上，ASA实现了与LoRA相当的适应能力，但开销显著降低，并具有强大的跨模型可迁移性。这使得ASA特别适用于具有频繁接口变化的鲁棒、可扩展和高效的多领域工具生态系统。

Conclusion: ASA为解决LLM智能体在多领域工具生态系统中的高效适应问题提供了一种实用解决方案，通过推理时适配机制在保持性能的同时大幅降低训练和维护成本。

Abstract: For real-world deployment of general-purpose LLM agents, the core challenge is often not tool use itself, but efficient domain adaptation under rapidly evolving toolsets, APIs, and protocols. Repeated LoRA or SFT across domains incurs exponentially growing training and maintenance costs, while prompt or schema methods are brittle under distribution shift and complex interfaces. We propose \textbf{Activation Steering Adapter (ASA}), a lightweight, inference-time, training-free mechanism that reads routing signals from intermediate activations and uses an ultra-light router to produce adaptive control strengths for precise domain alignment. Across multiple model scales and domains, ASA achieves LoRA-comparable adaptation with substantially lower overhead and strong cross-model transferability, making it ideally practical for robust, scalable, and efficient multi-domain tool ecosystems with frequent interface churn dynamics.

</details>


### [140] [Quality Model for Machine Learning Components](https://arxiv.org/abs/2602.05043)
*Grace A. Lewis,Rachel Brower-Sinning,Robert Edman,Ipek Ozkaya,Sebastián Echeverría,Alex Derr,Collin Beaudoin,Katherine R. Maffey*

Main category: cs.SE

TL;DR: 提出针对机器学习组件的质量模型，用于需求获取和测试聚焦，解决ML组件与系统集成中的测试不足问题


<details>
  <summary>Details</summary>
Motivation: 当前机器学习测试主要关注模型性能等属性，而忽视了系统级需求（如吞吐量、资源消耗、鲁棒性），导致模型集成、部署和运维失败。虽然ISO 25010和ISO 25059提供了质量框架，但后者将系统属性和ML组件属性混在一起，对组件开发者不实用。

Method: 开发了一个专门针对机器学习组件的质量模型，作为需求获取和协商的指南，为ML组件开发者和系统利益相关者提供共同词汇，以定义系统衍生需求并聚焦测试工作。通过调查验证了模型的相关性和价值，并将其集成到开源ML组件测试评估工具中。

Result: 参与者通过调查认可了该质量模型的相关性和价值。该模型已成功集成到开源ML组件测试评估工具中，展示了其实际应用价值。

Conclusion: 提出的ML组件质量模型为组件开发者和系统利益相关者提供了有效的沟通框架，能够帮助定义系统级需求并指导测试工作，解决了传统ML测试忽视系统集成需求的问题。

Abstract: Despite increased adoption and advances in machine learning (ML), there are studies showing that many ML prototypes do not reach the production stage and that testing is still largely limited to testing model properties, such as model performance, without considering requirements derived from the system it will be a part of, such as throughput, resource consumption, or robustness. This limited view of testing leads to failures in model integration, deployment, and operations. In traditional software development, quality models such as ISO 25010 provide a widely used structured framework to assess software quality, define quality requirements, and provide a common language for communication with stakeholders. A newer standard, ISO 25059, defines a more specific quality model for AI systems. However, a problem with this standard is that it combines system attributes with ML component attributes, which is not helpful for a model developer, as many system attributes cannot be assessed at the component level. In this paper, we present a quality model for ML components that serves as a guide for requirements elicitation and negotiation and provides a common vocabulary for ML component developers and system stakeholders to agree on and define system-derived requirements and focus their testing efforts accordingly. The quality model was validated through a survey in which the participants agreed with its relevance and value. The quality model has been successfully integrated into an open-source tool for ML component testing and evaluation demonstrating its practical application.

</details>


### [141] [Emergence-as-Code for Self-Governing Reliable Systems](https://arxiv.org/abs/2602.05458)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: 提出Emergence-as-Code (EmaC)框架，将微服务旅程可靠性从隐性属性变为可计算、可治理的代码化声明，通过意图声明、运行时推理和编译器控制实现旅程SLO的自动化管理。


<details>
  <summary>Details</summary>
Motivation: 微服务架构中，用户旅程的可靠性是多个微服务交互的涌现属性，难以通过单个服务的SLO-as-code来保证。当前旅程目标（如"结账p99 < 400ms"）通常在代码外维护，随着系统演化而漂移，导致团队要么无法满足用户期望，要么过度配置资源并使用临时启发式方法控制发布。

Method: 提出EmaC框架：1) EmaC规范声明旅程意图（目标、控制流操作符、允许的操作）并绑定到原子SLO和遥测数据；2) 运行时推理组件消费操作工件（如追踪和流量配置）合成候选旅程模型，附带来源和置信度；3) 基于最后接受的模型，EmaC编译器/控制器在明确相关性假设下推导有界的旅程SLO和预算；4) 生成控制平面工件（燃烧率告警、发布门控、操作防护），可在Git工作流中审查。

Result: 开发了匿名化工件仓库，提供可运行的示例规范和生成输出，展示了EmaC框架的实际应用。

Conclusion: EmaC使旅程可靠性从隐性涌现属性变为可计算、可治理的代码化声明，通过意图加证据的方法解决了微服务架构中旅程SLO漂移和过度配置的问题，实现了更可靠的用户体验管理。

Abstract: SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., "checkout p99 < 400 ms") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.

</details>


### [142] [TestMigrationsInPy: A Dataset of Test Migrations from Unittest to Pytest](https://arxiv.org/abs/2602.05122)
*Altino Alves,Andre Hora*

Main category: cs.SE

TL;DR: TestMigrationsInPy是一个包含923个真实世界unittest到pytest迁移案例的数据集，用于支持Python测试框架迁移的自动化研究。


<details>
  <summary>Details</summary>
Motivation: Python项目中从unittest迁移到pytest虽然有益但耗时，需要自动化解决方案来支持迁移过程。

Method: 收集了923个由开发者实际执行的真实迁移案例，构建了包含迁移类型信息（如断言或fixture变更）的数据集。

Result: 创建了TestMigrationsInPy数据集，包含详细的迁移信息，可作为未来研究的基础事实，并支持从简单断言迁移到复杂fixture迁移的验证。

Conclusion: TestMigrationsInPy为Python测试框架迁移研究提供了有价值的基准数据集，支持新型自动化迁移解决方案的开发和验证。

Abstract: Unittest and pytest are the most popular testing frameworks in Python. Overall, pytest provides some advantages, including simpler assertion, reuse of fixtures, and interoperability. Due to such benefits, multiple projects in the Python ecosystem have migrated from unittest to pytest. To facilitate the migration, pytest can also run unittest tests, thus, the migration can happen gradually over time. However, the migration can be time-consuming and take a long time to conclude. In this context, projects would benefit from automated solutions to support the migration process. In this paper, we propose TestMigrationsInPy, a dataset of test migrations from unittest to pytest. TestMigrationsInPy contains 923 real-world migrations performed by developers. Future research proposing novel solutions to migrate frameworks in Python can rely on TestMigrationsInPy as a ground truth. Moreover, as TestMigrationsInPy includes information about the migration type (e.g., changes in assertions or fixtures), our dataset enables novel solutions to be verified effectively, for instance, from simpler assertion migrations to more complex fixture migrations. TestMigrationsInPy is publicly available at: https://github.com/altinoalvesjunior/TestMigrationsInPy.

</details>


### [143] [Exceptional Behaviors: How Frequently Are They Tested?](https://arxiv.org/abs/2602.05123)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: 对25个Python系统中异常行为测试覆盖率的实证研究，发现21.4%的执行方法会抛出异常，其中约20%的方法异常发生频率较高，挑战了异常是"罕见"行为的传统观念。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注传播到测试的异常，忽略了未到达测试的异常。需要全面了解实际系统中异常行为的测试覆盖率，以改进测试质量和工具支持。

Method: 通过运行插桩版本的测试套件，监控执行过程，收集运行时异常信息。分析了25个Python系统的测试套件，覆盖5,372个执行方法、1,790万次调用和140万次抛出的异常。

Result: 21.4%的执行方法会在运行时抛出异常。在抛出异常的方法中，中位数显示每10次调用就有1次执行异常行为。接近80%的异常抛出方法异常发生频率较低，但约20%的方法异常发生频率较高。

Conclusion: 异常抛出行为不一定是"异常"或罕见的。建议开发新工具来支持异常行为的测试和昂贵的try/except块重构，为研究人员和从业者提供重要启示。

Abstract: Exceptions allow developers to handle error cases expected to occur infrequently. Ideally, good test suites should test both normal and exceptional behaviors to catch more bugs and avoid regressions. While current research analyzes exceptions that propagate to tests, it does not explore other exceptions that do not reach the tests. In this paper, we provide an empirical study to explore how frequently exceptional behaviors are tested in real-world systems. We consider both exceptions that propagate to tests and the ones that do not reach the tests. For this purpose, we run an instrumented version of test suites, monitor their execution, and collect information about the exceptions raised at runtime. We analyze the test suites of 25 Python systems, covering 5,372 executed methods, 17.9M calls, and 1.4M raised exceptions. We find that 21.4% of the executed methods do raise exceptions at runtime. In methods that raise exceptions, on the median, 1 in 10 calls exercise exceptional behaviors. Close to 80% of the methods that raise exceptions do so infrequently, but about 20% raise exceptions more frequently. Finally, we provide implications for researchers and practitioners. We suggest developing novel tools to support exercising exceptional behaviors and refactoring expensive try/except blocks. We also call attention to the fact that exception-raising behaviors are not necessarily "abnormal" or rare.

</details>


### [144] [The Necessity of a Holistic Safety Evaluation Framework for AI-Based Automation Features](https://arxiv.org/abs/2602.05157)
*Alireza Abbaspour,Shabin Mahadevan,Kilian Zwirglmaier,Jeff Stafford*

Main category: cs.SE

TL;DR: QM组件传统上被排除在安全分析之外，但AI集成显示这些组件可能引入SOTIF相关风险，需要重新评估安全考量


<details>
  <summary>Details</summary>
Motivation: 传统上质量管理(QM)组件被排除在功能安全(FuSa)和预期功能安全(SOTIF)分析之外，但随着AI集成到驾驶自动化系统中，即使是QM分类的组件也可能引入安全风险，这需要重新评估现有安全框架

Method: 通过案例研究分析AI驱动的感知系统，展示QM分类组件中的缺陷如何导致意外功能行为；采用综合的FuSa、SOTIF和AI标准驱动方法进行风险识别和缓解

Result: 研究表明AI驱动的感知系统缺陷可能出现在QM分类组件中，导致具有关键安全影响的意外功能行为；需要修订现有安全框架以应对AI带来的挑战

Conclusion: 必须对AI组件进行全面的安全分析和风险评估，采用综合的FuSa、SOTIF和AI标准驱动方法，修订现有安全框架以确保跨所有组件分类的全面安全保证

Abstract: The intersection of Safety of Intended Functionality (SOTIF) and Functional Safety (FuSa) analysis of driving automation features has traditionally excluded Quality Management (QM) components from rigorous safety impact evaluations. While QM components are not typically classified as safety-relevant, recent developments in artificial intelligence (AI) integration reveal that such components can contribute to SOTIF-related hazardous risks. Compliance with emerging AI safety standards, such as ISO/PAS 8800, necessitates re-evaluating safety considerations for these components. This paper examines the necessity of conducting holistic safety analysis and risk assessment on AI components, emphasizing their potential to introduce hazards with the capacity to violate risk acceptance criteria when deployed in safety-critical driving systems, particularly in perception algorithms. Using case studies, we demonstrate how deficiencies in AI-driven perception systems can emerge even in QM-classified components, leading to unintended functional behaviors with critical safety implications. By bridging theoretical analysis with practical examples, this paper argues for the adoption of comprehensive FuSa, SOTIF, and AI standards-driven methodologies to identify and mitigate risks in AI components. The findings demonstrate the importance of revising existing safety frameworks to address the evolving challenges posed by AI, ensuring comprehensive safety assurance across all component classifications spanning multiple safety standards.

</details>


### [145] [EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering](https://arxiv.org/abs/2602.05242)
*Chenhui Mao,Yuanting Lei,Zhixiang Wei,Ming Liang,Zhixiang Wang,Jingxuan Xu,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: EGSS框架通过熵引导自适应搜索和测试套件增强，在保持Agentic测试时扩展性能优势的同时，显著降低计算开销，实现效率与效果的双重提升。


<details>
  <summary>Details</summary>
Motivation: Agentic测试时扩展在复杂软件工程任务中表现出色，但实际应用受限：1）大规模集成部署成本过高；2）缺乏可靠的候选解决方案选择机制，限制了性能提升潜力。

Method: 提出熵引导逐步扩展框架，包含两个核心组件：1）熵引导自适应搜索，动态平衡效率与效果；2）鲁棒的测试套件增强机制，提高解决方案选择的可靠性。

Result: 在SWE-Bench-Verified上，EGSS将模型性能提升5-10%，Kimi-K2-Intruct解决率从63.2%提升至72.2%，GLM-4.6从65.8%提升至74.6%。同时减少28%以上的推理时token使用量。

Conclusion: EGSS通过熵引导机制有效解决了Agentic测试时扩展的计算效率问题，在保持SOTA性能的同时显著降低计算开销，为实际部署提供了可行方案。

Abstract: Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.

</details>


### [146] [PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models](https://arxiv.org/abs/2602.05270)
*Thanh Le-Cong,Bach Le,Toby Murray,Michael Pradel,Cristian Cadar*

Main category: cs.SE

TL;DR: PatchGuru：首个从真实PR中自动推断可执行补丁规范的技术，通过LLM提取开发者意图并合成补丁预言，用于自动化验证补丁行为


<details>
  <summary>Details</summary>
Motivation: 软件系统演化中，补丁可能无意改变程序行为。由于不完整的回归测试和非正式的自然语言描述，验证补丁是否符合预期语义很困难

Method: 给定PR，PatchGuru使用LLM从自然语言工件中提取开发者意图，并合成补丁预言：在整合补丁前后版本的比较程序中表示为运行时断言的实际规范。通过迭代精炼推断的预言，比较补丁前后行为，识别违规，通过自审过滤不一致性，生成错误报告

Result: 在4个广泛使用的Python项目的400个近期PR上评估，报告39个警告，精度0.62，产生24个确认的真阳性（包括12个先前未知的bug，其中11个被开发者修复）。相比最先进的Testora技术，检测到更多bug（24 vs 7），精度从0.32提升到0.62。每个PR平均成本8.9分钟和0.07美元

Conclusion: PatchGuru通过提供可执行文档和补丁意图的自动化验证，补充了代码审查和回归测试

Abstract: As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.

</details>


### [147] [Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study](https://arxiv.org/abs/2602.05465)
*Alexander Berndt,Vekil Bekmyradov,Rainer Gemulla,Marcus Kessel,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: LLMs在仅基于测试代码的flaky测试分类任务上表现不佳，结果仅略优于随机猜测，表明仅凭测试代码信息不足，需要更多上下文信息。


<details>
  <summary>Details</summary>
Motivation: 先前基于测试代码标识符训练机器学习模型进行flaky测试分类的方法缺乏泛化能力，而预训练大语言模型在跨任务泛化方面表现出潜力，因此研究LLMs是否能解决flaky测试分类的泛化性问题。

Method: 评估了三个LLMs（两个通用模型，一个代码专用模型），使用三种提示技术，在两个flaky测试分类基准数据集上进行测试。同时手动调查了50个样本，以确定人类仅基于测试代码是否能进行flaky测试分类。

Result: LLMs在仅基于测试代码的flaky测试分类任务上表现不佳，最佳提示-模型组合的结果仅略优于随机猜测。手动分析发现测试代码不一定包含足够的信息来进行flakiness分类。

Conclusion: 仅基于测试代码进行flaky测试分类对LLMs和人类都不可行，需要更多上下文信息。未来工作应评估LLMs在增强上下文（如检索增强生成或智能体AI）下的flakiness分类能力。

Abstract: Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.

</details>


### [148] [Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems](https://arxiv.org/abs/2602.05486)
*Matteo Esposito,Lodovica Marchesi,Roberto Tonelli,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 提出将数字主权作为软件架构的一等属性，引入主权参考架构，整合自主权身份、区块链信任、数据治理和受控生成式AI，弥合监管意图与系统设计之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现代软件密集型系统面临数字主权挑战，主要源于非主权云基础设施的垄断、生成式AI的快速普及以及日益严格的监管要求。现有解决方案在治理、合规和安全方面各自为政，缺乏在架构层面实现主权的具体指导。

Method: 提出将主权作为一等架构属性而非纯监管目标，引入主权参考架构，整合四个核心组件：自主权身份、基于区块链的信任与可审计性、主权数据治理、以及在明确架构控制下部署的生成式AI。

Result: 该架构将主权构建为架构质量属性，弥合了监管意图与具体系统设计之间的鸿沟，为构建可审计、可演进、具备司法管辖意识的AI赋能系统提供了连贯基础。

Conclusion: 主权参考架构为软件架构、生成式AI和数字主权交叉领域的研究和实践提供了原则性起点，将生成式AI同时视为治理风险源和合规赋能工具，在适当约束下实现持续保证。

Abstract: Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.

</details>


### [149] [ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval](https://arxiv.org/abs/2602.05550)
*Yulong He,Artem Ermakov,Sergey Kovalchuk,Artem Aliev,Dmitry Shalymov*

Main category: cs.SE

TL;DR: 构建首个大规模ArkTS数据集，用于代码检索和评估任务，并评估现有代码嵌入模型在ArkTS上的性能，通过微调获得高性能ArkTS代码理解模型。


<details>
  <summary>Details</summary>
Motivation: OpenHarmony生态中ArkTS作为核心编程语言，缺乏公开数据集和评估基准，阻碍了ArkTS代码智能研究的发展。

Method: 从GitHub和Gitee爬取ArkTS仓库，使用tree-sitter-arkts提取注释-函数对，进行跨平台去重和函数类型统计分析。设计单搜索任务（自然语言注释检索对应ArkTS函数），评估现有开源代码嵌入模型，并使用ArkTS和TypeScript训练数据集进行微调。

Result: 构建了首个大规模ArkTS数据集，建立了ArkTS代码检索的系统基准。通过微调获得了高性能的ArkTS代码理解模型，模型和数据集已在HuggingFace平台公开。

Conclusion: 该工作填补了ArkTS代码智能研究的数据集和基准空白，为ArkTS代码检索和理解提供了首个系统化解决方案，促进了OpenHarmony生态中代码智能技术的发展。

Abstract: ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.

</details>


### [150] [Towards Green AI: Decoding the Energy of LLM Inference in Software Development](https://arxiv.org/abs/2602.05712)
*Lola Solovyeva,Fernando Castor*

Main category: cs.SE

TL;DR: 该研究分析了LLM推理的能耗模式，区分预填充和解码阶段，发现预填充成本会放大解码能耗，并提出抑制模型"废话"行为可节省高达89%的能耗。


<details>
  <summary>Details</summary>
Motivation: 随着AI辅助工具在软件开发中的广泛应用，基于大语言模型的工具带来了巨大的计算和能源成本。理解并减少LLM推理的能源足迹对于可持续软件开发至关重要。

Method: 研究对6个6B-7B参数和4个3B-4B参数的Transformer模型进行阶段级能耗分析，区分预填充阶段（处理输入并构建内部表示）和解码阶段（使用存储状态生成输出标记）。在代码生成（HumanEval）和代码理解（LongBench）基准上进行评估。

Result: 研究发现：1）不同参数规模的模型在各阶段表现出不同的能耗模式；2）预填充成本的增加会放大解码阶段的每标记能耗，放大范围从1.3%到51.8%；3）十个模型中有三个表现出"废话"行为，向输出添加不必要内容，增加能耗；4）通过抑制代码生成的废话行为，实现了44%到89%的能耗节省，且不影响生成准确性。

Conclusion: 预填充成本会影响解码阶段（主导能耗），抑制废话行为可节省高达89%的能耗。减少推理能耗需要同时缓解废话行为并限制预填充对解码的影响。

Abstract: Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.

</details>


### [151] [A Bayesian Optimization-Based AutoML Framework for Non-Intrusive Load Monitoring](https://arxiv.org/abs/2602.05739)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 提出将AutoML应用于NILM领域的新框架，使用贝叶斯优化进行自动模型选择和超参数调优，并开发了开源工具包AutoML4NILM


<details>
  <summary>Details</summary>
Motivation: NILM（非侵入式负荷监测）通过分析家庭总用电量来估计单个电器的功耗，相比为每个电器安装专用智能电表更具成本效益。但传统方法需要数据科学和机器学习专业知识，限制了领域从业者的应用

Method: 提出将自动化机器学习（AutoML）引入NILM领域的新框架，利用贝叶斯优化进行自动模型选择和超参数调优。开发了AutoML4NILM开源工具包，目前支持11种算法及其不同超参数，设计灵活可扩展

Result: 该框架使领域从业者无需高级数据科学或机器学习专业知识即可有效应用机器学习技术。AutoML4NILM工具包为能源分解的AutoML解决方案部署提供了灵活可扩展的平台

Conclusion: AutoML与NILM的结合为能源分解提供了实用解决方案，降低了技术门槛。开源工具包AutoML4NILM支持进一步研究和行业应用，其灵活设计允许算法和超参数的扩展

Abstract: Non-Intrusive Load Monitoring (NILM), commonly known as energy disaggregation, aims to estimate the power consumption of individual appliances by analyzing a home's total electricity usage. This method provides a cost-effective alternative to installing dedicated smart meters for each appliance. In this paper, we introduce a novel framework that incorporates Automated Machine Learning (AutoML) into the NILM domain, utilizing Bayesian Optimization for automated model selection and hyperparameter tuning. This framework empowers domain practitioners to effectively apply machine learning techniques without requiring advanced expertise in data science or machine learning. To support further research and industry adoption, we present AutoML4NILM, a flexible and extensible open-source toolkit designed to streamline the deployment of AutoML solutions for energy disaggregation. Currently, this framework supports 11 algorithms, each with different hyperparameters; however, its flexible design allows for the extension of both the algorithms and their hyperparameters.

</details>


### [152] [Toward Quantum-Safe Software Engineering: A Vision for Post-Quantum Cryptography Migration](https://arxiv.org/abs/2602.05759)
*Lei Zhang*

Main category: cs.SE

TL;DR: 论文提出量子安全软件工程（QSSE）新研究方向，介绍AQuA框架解决后量子密码迁移挑战


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁传统密码学，后量子密码（PQC）标准化加速，但将遗留软件迁移到量子安全算法并非简单的库替换，现有漏洞检测、重构和测试工具无法处理PQC的概率行为、侧信道敏感性和复杂性能权衡

Method: 提出自动化量子安全适配（AQuA）框架，包含三个支柱：PQC感知检测、语义重构和混合验证，旨在建立量子安全软件工程（QSSE）新研究方向

Result: 论文提出了QSSE研究方向和AQuA框架的愿景，但未提供具体实验结果，主要贡献在于理论框架和未来研究方向

Conclusion: 需要专门的量子安全软件工程工具和方法来应对PQC迁移挑战，AQuA框架为这一新研究方向提供了系统化方法，QSSE应成为独立的软件工程研究领域

Abstract: The quantum threat to cybersecurity has accelerated the standardization of Post-Quantum Cryptography (PQC). Migrating legacy software to these quantum-safe algorithms is not a simple library swap, but a new software engineering challenge: existing vulnerability detection, refactoring, and testing tools are not designed for PQC's probabilistic behavior, side-channel sensitivity, and complex performance trade-offs. To address these challenges, this paper outlines a vision for a new class of tools and introduces the Automated Quantum-safe Adaptation (AQuA) framework, with a three-pillar agenda for PQC-aware detection, semantic refactoring, and hybrid verification, thereby motivating Quantum-Safe Software Engineering (QSSE) as a distinct research direction.

</details>


### [153] [Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes](https://arxiv.org/abs/2602.05780)
*Ulrich Finkler,Irene Manotas,Wei Zhang,Geert Janssen,Octavian Popescu,Shyam Ramji*

Main category: cs.SE

TL;DR: 该论文提出了一种基于代码语义范围的自动化LLM定制方法，通过RAG和微调策略提升模型在私有代码库上的代码补全性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在公共基准测试中表现良好，但在处理训练数据中未见的私有代码库时，生成的代码往往与代码库风格不一致。需要定制化方法来提升模型在私有企业代码库上的表现。

Method: 提出基于代码语义范围的自动化LLM定制方法，包括：1）通过语义范围机制提取代码库数据并构建训练数据对；2）评估两种定制策略：检索增强生成（RAG）和监督微调（FT）；3）在真实企业私有代码库上进行评估。

Result: 定制化模型在私有代码库上的代码补全性能显著优于未定制的大型模型。语义范围机制帮助模型学习特定代码库的底层模式，为开发者提供更精确的代码补全，提升生产力。

Conclusion: 基于语义范围的LLM定制方法能有效提升模型在私有代码库上的性能，中等规模的定制模型可以超越未定制的大型模型。论文还分析了在公共基准测试上的表现，并提出了未来研究方向。

Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.

</details>


### [154] [When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models](https://arxiv.org/abs/2602.05891)
*Shenyu Zheng,Ximing Dong,Xiaoshuang Liu,Gustavo Oliva,Chong Chun Yong,Dayi Lin,Boyuan Chen,Shaowei Wang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Codeforces Elo评分作为大语言模型编程能力评估指标存在严重偏差，研究发现提交顺序、比赛难度选择和模型随机性等因素会导致评分剧烈波动，直接比较Elo分数不可靠。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂推理方面取得突破，Codeforces Elo评分成为评估竞争性编程能力的重要指标。然而，现有研究在报告Elo评分时往往缺乏关键实验细节，导致相同模型版本的评分波动可达近500点，这种不一致性促使研究者系统探究影响Elo评估的隐藏因素。

Method: 采用系统性实证研究方法，构建包含37个近期Codeforces比赛和13,691个生成测试用例的受控基准。重点分析三个偏差因素：(1) 提交的时间顺序，(2) 比赛难度选择，(3) 大语言模型的运行间随机变异性。通过控制变量实验量化这些因素对Elo评分的影响程度。

Result: 研究发现Elo评分对这些参数高度敏感：提交顺序变化可导致评分偏移394点；比赛选择差异可使同一模型的评分差异高达1,122点；运行间性能表现出显著不稳定性，在评估相同比赛时平均分数最大差异达349点。这些结果表明当前Elo评分系统存在严重偏差。

Conclusion: 直接比较大语言模型的Codeforces Elo评分是不可靠且可能产生误导的。为确保评估的有效性，需要严格的标准化流程和透明的实验设置报告。未来的评估实践应当明确控制并报告提交顺序、比赛选择和随机性等关键参数。

Abstract: As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [155] [Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software](https://arxiv.org/abs/2602.04894)
*Tomer Kordonsky,Maayan Yamin,Noam Benzimra,Amit LeVi,Avi Mendelson*

Main category: cs.CR

TL;DR: 该论文研究了LLM生成代码中的漏洞持续性，提出了Feature-Security Table（FSTab）框架，包含黑盒攻击和模型中心评估两个组件，揭示了LLM生成软件中未充分探索的攻击面。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于代码生成，但其输出通常遵循重复的模板模式，可能引入可预测的漏洞。作者旨在研究LLM生成软件中的漏洞持续性，并量化不同模型在生成代码时重现相同漏洞的一致性程度。

Method: 提出了Feature-Security Table（FSTab）框架，包含两个核心组件：1）黑盒攻击方法，通过观察前端特征和了解源LLM来预测后端漏洞，无需访问后端代码或源代码；2）模型中心评估方法，量化给定模型在不同程序、语义保持的重述和应用领域中重现相同漏洞的一致性。

Result: 在包括GPT-5.2、Claude-4.5 Opus和Gemini-3 Pro在内的最先进代码LLM上评估FSTab，结果显示强大的跨领域迁移能力：即使目标领域被排除在训练之外，FSTab在Internal Tools（Claude-4.5 Opus）上仍能达到94%的攻击成功率和93%的漏洞覆盖率。

Conclusion: 研究揭示了LLM生成软件中一个未充分探索的攻击面，强调了代码生成的安全风险。FSTab框架能够有效预测和评估LLM生成代码中的漏洞持续性，为LLM代码生成的安全性评估提供了新方法。

Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \emph{vulnerability persistence} in LLM-generated software and introduce \emph{Feature--Security Table (FSTab)} with two components. First, FSTab enables a black-box attack that predicts likely backend vulnerabilities from observable frontend features and knowledge of the source LLM, without access to backend code or source code. Second, FSTab provides a model-centric evaluation that quantifies how consistently a given model reproduces the same vulnerabilities across programs, semantics-preserving rephrasings, and application domains. We evaluate FSTab on state-of-the-art code LLMs, including GPT-5.2, Claude-4.5 Opus, and Gemini-3 Pro, across diverse application domains. Our results show strong cross-domain transfer: even when the target domain is excluded from training, FSTab achieves up to 94\% attack success and 93\% vulnerability coverage on Internal Tools (Claude-4.5 Opus). These findings expose an underexplored attack surface in LLM-generated software and highlight the security risks of code generation. Our code is available at: https://anonymous.4open.science/r/FSTab-024E.

</details>


### [156] [Semantic-level Backdoor Attack against Text-to-Image Diffusion Models](https://arxiv.org/abs/2602.04898)
*Tianxin Chen,Wenbo Jiang,Hongqiao Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CR

TL;DR: 该论文提出了一种语义级后门攻击方法（SemBD），通过在扩散模型的交叉注意力层中植入连续语义区域的后门，而非传统的离散文本触发器，从而有效规避现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的后门攻击通常依赖固定的文本触发器和单一实体后门目标，容易受到枚举式输入防御和注意力一致性检测的对抗。需要开发更隐蔽、更鲁棒的后门攻击方法。

Method: 提出语义级后门攻击（SemBD）：1）将触发器定义为连续语义区域而非离散文本模式；2）通过基于蒸馏的编辑方法修改交叉注意力层中的键值投影矩阵；3）引入语义正则化防止不完整语义下的意外激活；4）采用多实体后门目标避免高度一致的交叉注意力模式。

Result: 实验表明SemBD实现了100%的攻击成功率，同时对最先进的输入级防御保持强鲁棒性，有效规避了现有防御机制。

Conclusion: 该研究证明了在扩散模型中植入语义级后门的可行性，揭示了现有防御机制的局限性，为开发更安全的文本到图像生成系统提供了重要参考。

Abstract: Text-to-image (T2I) diffusion models are widely adopted for their strong generative capabilities, yet remain vulnerable to backdoor attacks. Existing attacks typically rely on fixed textual triggers and single-entity backdoor targets, making them highly susceptible to enumeration-based input defenses and attention-consistency detection. In this work, we propose Semantic-level Backdoor Attack (SemBD), which implants backdoors at the representation level by defining triggers as continuous semantic regions rather than discrete textual patterns. Concretely, SemBD injects semantic backdoors by distillation-based editing of the key and value projection matrices in cross-attention layers, enabling diverse prompts with identical semantic compositions to reliably activate the backdoor attack. To further enhance stealthiness, SemBD incorporates a semantic regularization to prevent unintended activation under incomplete semantics, as well as multi-entity backdoor targets that avoid highly consistent cross-attention patterns. Extensive experiments demonstrate that SemBD achieves a 100% attack success rate while maintaining strong robustness against state-of-the-art input-level defenses.

</details>


### [157] [Phantom Transfer: Data-level Defences are Insufficient Against Data Poisoning](https://arxiv.org/abs/2602.04899)
*Andrew Draganov,Tolga H. Dur,Anandmayi Bhongade,Mary Phuong*

Main category: cs.CR

TL;DR: 提出一种名为"幻影转移"的数据投毒攻击，即使攻击者知道投毒方式也无法过滤，攻击跨模型有效且能绕过现有防御


<details>
  <summary>Details</summary>
Motivation: 当前数据级防御措施对复杂数据投毒攻击的防护不足，需要开发更有效的安全机制

Method: 改进潜意识学习技术使其适用于现实场景，通过数据投毒植入密码触发行为，攻击跨模型有效（包括GPT-4.1）

Result: 攻击成功且无法通过完全改写数据集来防御，即使知道投毒方式也无法过滤，能植入密码触发行为并绕过现有防御

Conclusion: 数据级防御不足以阻止复杂的数据投毒攻击，未来工作应聚焦于模型审计和白盒安全方法

Abstract: We present a data poisoning attack -- Phantom Transfer -- with the property that, even if you know precisely how the poison was placed into an otherwise benign dataset, you cannot filter it out. We achieve this by modifying subliminal learning to work in real-world contexts and demonstrate that the attack works across models, including GPT-4.1. Indeed, even fully paraphrasing every sample in the dataset using a different model does not stop the attack. We also discuss connections to steering vectors and show that one can plant password-triggered behaviours into models while still beating defences.
  This suggests that data-level defences are insufficient for stopping sophisticated data poisoning attacks. We suggest that future work should focus on model audits and white-box security methods.

</details>


### [158] [PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM](https://arxiv.org/abs/2602.04927)
*Gautam Savaliya,Robert Aufschläger,Abhishek Subedi,Michael Heigl,Martin Schramm*

Main category: cs.CR

TL;DR: PriMod4AI提出了一种混合隐私威胁建模框架，结合LINDDUN传统隐私分类和AI特有的模型中心隐私攻击，通过知识库向量化和检索增强的LLM提示生成，实现AI系统全生命周期的隐私威胁识别与评估。


<details>
  <summary>Details</summary>
Motivation: AI系统处理敏感或高维数据时面临复杂的隐私风险，传统LINDDUN框架的七个隐私威胁类别无法覆盖AI特有的模型中心隐私攻击（如成员推理、模型反转），需要开发能够同时处理传统和AI特有隐私威胁的统一框架。

Method: PriMod4AI采用混合隐私威胁建模方法：1) 整合两个结构化知识源：LINDDUN知识库（传统隐私分类）和模型中心隐私攻击知识库；2) 将知识库嵌入向量数据库进行语义检索；3) 结合数据流图提取的系统级元数据；4) 使用检索增强和数据流特定的提示生成指导LLM识别、解释和分类隐私威胁；5) 生成基于分类学的威胁评估。

Result: 在两个AI系统上的评估表明：1) PriMod4AI能广泛覆盖传统隐私类别；2) 额外识别出模型中心隐私威胁；3) 框架在不同LLM间产生一致、基于知识的输出，观察到的协议分数反映了这种一致性。

Conclusion: PriMod4AI提供了一个统一的隐私威胁建模框架，能够同时处理传统LINDDUN威胁和AI特有的模型中心隐私攻击，通过知识库增强的LLM方法实现了全面、一致的隐私威胁评估，填补了AI系统隐私威胁建模的空白。

Abstract: Artificial intelligence systems introduce complex privacy risks throughout their lifecycle, especially when processing sensitive or high-dimensional data. Beyond the seven traditional privacy threat categories defined by the LINDDUN framework, AI systems are also exposed to model-centric privacy attacks such as membership inference and model inversion, which LINDDUN does not cover. To address both classical LINDDUN threats and additional AI-driven privacy attacks, PriMod4AI introduces a hybrid privacy threat modeling approach that unifies two structured knowledge sources, a LINDDUN knowledge base representing the established taxonomy, and a model-centric privacy attack knowledge base capturing threats outside LINDDUN. These knowledge bases are embedded into a vector database for semantic retrieval and combined with system level metadata derived from Data Flow Diagram. PriMod4AI uses retrieval-augmented and Data Flow specific prompt generation to guide large language models (LLMs) in identifying, explaining, and categorizing privacy threats across lifecycle stages. The framework produces justified and taxonomy-grounded threat assessments that integrate both classical and AI-driven perspectives. Evaluation on two AI systems indicates that PriMod4AI provides broad coverage of classical privacy categories while additionally identifying model-centric privacy threats. The framework produces consistent, knowledge-grounded outputs across LLMs, as reflected in agreement scores in the observed range.

</details>


### [159] [Attack Selection Reduces Safety in Concentrated AI Control Settings against Trusted Monitoring](https://arxiv.org/abs/2602.04930)
*Joachim Schaeffer,Arjun Khandelwal,Tyler Tracy*

Main category: cs.CR

TL;DR: 研究AI系统如何通过对抗性选择攻击来规避监控，在BigCodeBench后门设置中，攻击者模型通过推理监控机制将安全率从99%降至59%，强调评估攻击选择能力对避免安全评估过于乐观的重要性。


<details>
  <summary>Details</summary>
Motivation: 未来AI部署将面临恶意行为监控，但AI可能通过对抗性选择攻击来规避监控（攻击选择），这令人担忧。需要研究攻击选择能力对安全评估的影响，避免对AI安全性产生过于乐观的评估。

Method: 在BigCodeBench后门设置中，让红队创建攻击策略，试图将攻击插入代码而不被监控器发现。将攻击选择分解为两个问题：将攻击映射到质量分数，将质量分数映射到提交概率。将攻击选择构建为分类问题，并分析安全性与FPR和TPR的敏感性关系。

Result: 研究发现安全性对FPR比TPR更敏感。当提示攻击者模型推理监控机制并在攻击选择上保持谨慎时，在0.5%审计预算下，安全率从基线99%降至59%。这表明模型攻击选择能力的激发对安全评估至关重要。

Conclusion: 激发模型的攻击选择能力对于避免控制评估中过于乐观的安全分数至关重要。攻击者通过推理监控机制可以显著降低系统安全性，强调在安全评估中必须考虑AI的对抗性规避能力。

Abstract: Future AI deployments will likely be monitored for malicious behaviour. The ability of these AIs to subvert monitors by adversarially selecting against them - attack selection - is particularly concerning. To study this, we let a red team create attack policies that attempt to insert attacks into code without being caught by a monitor in the concentrated BigCodeBench backdooring setting. We decompose attack selection into two problems: mapping attacks to a quality score and mapping quality scores to submission probabilities. We frame attack selection as a classification problem and show that safety is significantly more sensitive to FPR than TPR. We find that prompting the attacker model to reason about the monitor while being cautious with attack selection reduces safety from a baseline of 99% to 59% at 0.5% auditing budget, emphasizing that eliciting attack selection capabilities of models is vital to avoid overly optimistic safety scores in control evaluations.

</details>


### [160] [The Birthmark Standard: Privacy-Preserving Photo Authentication via Hardware Roots of Trust and Consortium Blockchain](https://arxiv.org/abs/2602.04933)
*Sam Ryan*

Main category: cs.CR

TL;DR: 提出Birthmark Standard认证架构，利用传感器固有噪声特征生成硬件根密钥，通过联盟区块链存储认证记录，解决现有图像认证方案易被元数据剥离和依赖商业平台的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统快速发展破坏了摄影证据的可信度，现有认证方案（如C2PA）存在两个关键缺陷：1）元数据在社交媒体处理过程中易被剥离；2）依赖企业控制的验证基础设施，商业利益可能与公共利益冲突。

Method: 利用制造独特的传感器熵（非均匀性校正图和PRNU模式）生成硬件根认证密钥。拍摄时相机创建匿名化认证证书，通过密钥表架构保持设备匿名性（匿名集超过1000台）。认证记录存储在由新闻机构运营的联盟区块链上，确保验证在元数据完全丢失后仍能进行。

Result: 使用ProVerif形式化验证隐私属性，证明在Dolev-Yao对手假设下实现制造商非关联性和区块链观察者非识别性。原型实现基于Raspberry Pi 4硬件，性能分析显示相机开销低于100ms，百万级每日认证的验证延迟低于500ms。

Conclusion: Birthmark Standard提供了一种硬件根、隐私保护且不依赖商业平台的图像认证解决方案，能够在元数据完全丢失的情况下验证图像真实性，为新闻和公共话语重建可信基础。

Abstract: The rapid advancement of generative AI systems has collapsed the credibility landscape for photographic evidence. Modern image generation models produce photorealistic images undermining the evidentiary foundation upon which journalism and public discourse depend. Existing authentication approaches, such as the Coalition for Content Provenance and Authenticity (C2PA), embed cryptographically signed metadata directly into image files but suffer from two critical failures: technical vulnerability to metadata stripping during social media reprocessing, and structural dependency on corporate-controlled verification infrastructure where commercial incentives may conflict with public interest. We present the Birthmark Standard, an authentication architecture leveraging manufacturing-unique sensor entropy from non-uniformity correction (NUC) maps and PRNU patterns to generate hardware-rooted authentication keys. During capture, cameras create anonymized authentication certificates proving sensor authenticity without exposing device identity via a key table architecture maintaining anonymity sets exceeding 1,000 devices. Authentication records are stored on a consortium blockchain operated by journalism organizations rather than commercial platforms, enabling verification that survives all metadata loss. We formally verify privacy properties using ProVerif, proving observational equivalence for Manufacturer Non-Correlation and Blockchain Observer Non-Identification under Dolev-Yao adversary assumptions. The architecture is validated through prototype implementation using Raspberry Pi 4 hardware, demonstrating the complete cryptographic pipeline. Performance analysis projects camera overhead below 100ms and verification latency below 500ms at scale of one million daily authentications.

</details>


### [161] [System-Level Isolation for Mixed-Criticality RISC-V SoCs: A "World" Reality Check](https://arxiv.org/abs/2602.05002)
*Luis Cunha,Jose Martins,Manuel Rodriguez,Tiago Gomes,Sandro Pinto,Uwe Moslehner,Kai Dieffenbach,Glenn Farrall,Kajetan Nuernberger,Thomas Roecker*

Main category: cs.CR

TL;DR: 本文对RISC-V硬件隔离原语（IOPMP、World-based checker及改进版RISC-V World checker）在异构SoC中的互操作性、可扩展性和实时性进行对比分析，评估其在安全保证与PPA（功耗-性能-面积）方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V在汽车、物联网和工业控制等SWaP-C受限领域的加速采用，异构SoC集成不同完整性级别的组件带来了系统级隔离的安全挑战。现有RISC-V硬件隔离原语的互操作性、可扩展性和实时适用性尚未得到充分理解。

Method: 实现IOPMP、基于World的检查器以及改进版RISC-V World检查器（解决基线规范的关键限制），从实用异构SoC设计角度对这些原语进行对比分析，评估安全保证与PPA权衡。

Result: 基于World的检查器引入固定、配置无关的访问延迟，在最坏情况下延迟低于评估的替代方案，且随系统规模可预测扩展。宏观层面，改进设计相比基线可减少约5%的SoC面积。

Conclusion: 研究结果为RISC-V规范的演进和标准化以及未来RISC-V SoC设计提供直接贡献，所有成果将开源发布。

Abstract: As RISC-V adoption accelerates, domains such as automotive, the Internet of Things (IoT), and industrial control are attracting growing attention. These domains are subject to stringent Size, Weight, Power, and Cost (SWaP-C) constraints, which have driven a shift toward heterogeneous Systems-on-Chip (SoCs) integrating general-purpose CPUs, tightly coupled accelerators, and diverse I/O devices with different integrity levels. While such integration improves cost efficiency and performance, it introduces a fundamental safety and security challenge: enforcing system-level isolation in mixed-criticality environments. Although RISC-V International has proposed several hardware isolation primitives, including RISC-V Worlds, IOPMP, and SmMTT, their interoperability, scalability, and suitability for real-time systems remain insufficiently understood. In this paper, we present a comparative analysis of these primitives from the perspective of practical heterogeneous SoC designs. We implement an IOPMP, a World-based checker, and a modified RISC-V World checker that addresses key limitations of the baseline specification, and evaluate their trade-offs in terms of security guarantees and power-performance-area (PPA). Our results show that the World-based checker introduces a fixed, configuration-independent access latency, achieving lower worst-case delay than the evaluated alternatives while scaling predictably with system size. At the macro level, we estimate that the proposed modifications reduce SoC area by up to approximately 5% compared to a baseline design. All artifacts will be released as open source, and we expect these findings to directly contribute to the evolution and ratification of RISC-V specifications, as well as to the design of future RISC-V SoCs.

</details>


### [162] [Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?](https://arxiv.org/abs/2602.05023)
*Ruixin Yang,Ethan Mendes,Arthur Wang,James Hays,Sauvik Das,Wei Xu,Alan Ritter*

Main category: cs.CR

TL;DR: 该论文提出了VLM-GEOPRIVACY基准，用于评估视觉语言模型在图像地理位置隐私保护方面的表现，发现现有模型虽然能精确定位，但与人类隐私期望严重不符，经常过度披露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型和前沿多模态大推理模型在图像地理定位方面表现出色，这带来了严重的隐私风险。这些广泛可访问的模型可能被滥用来从随意分享的照片中推断敏感位置，甚至达到街道级精度，超出了分享者同意或意图披露的详细程度。现有方法采用一刀切的限制措施，无法区分合法的地理定位使用和恶意行为。

Method: 提出了VLM-GEOPRIVACY基准，挑战视觉语言模型解释真实世界图像中的潜在社会规范和上下文线索，并确定适当的位置信息披露级别。该基准用于评估模型在尊重上下文完整性方面的表现。

Result: 评估了14个领先的视觉语言模型，结果显示：尽管这些模型能够精确地地理定位图像，但它们与人类隐私期望严重不符。模型经常在敏感上下文中过度披露信息，并且容易受到基于提示的攻击。

Conclusion: 研究结果呼吁在多模态系统中采用新的设计原则，将上下文条件化的隐私推理纳入其中。视觉语言模型应该通过推理图像中的元素来保持上下文完整性，确定适当的信息披露级别，平衡隐私和实用性。

Abstract: Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.

</details>


### [163] [VEXA: Evidence-Grounded and Persona-Adaptive Explanations for Scam Risk Sensemaking](https://arxiv.org/abs/2602.05056)
*Heajun An,Connor Ng,Sandesh Sharma Dulal,Junghwan Kim,Jin-Hee Cho*

Main category: cs.CR

TL;DR: VEXA框架：基于证据和角色自适应的诈骗解释生成系统，结合GradientSHAP归因和理论驱动的漏洞角色，为学习者提供可理解的安全解释


<details>
  <summary>Details</summary>
Motivation: 在线诈骗日益复杂，生成式AI使欺骗更加流畅和情境感知。虽然基于Transformer的检测器有很强的预测性能，但其解释对非专家不透明或与模型决策不一致。需要为学习者提供可理解、可信赖的安全解释框架

Method: 提出VEXA框架：1) 使用GradientSHAP归因方法提取检测器证据；2) 结合理论驱动的漏洞角色（如技术新手、情感易感者等）；3) 生成证据基础且角色自适应的解释，确保语义可靠性同时保持语言复杂度不变

Result: 在多渠道数据集评估显示：基于检测器证据的解释提高了语义可靠性而不增加语言复杂度；角色调节引入可解释的风格变化而不破坏证据对齐；证据基础控制语义正确性，角色适应在忠实性约束下操作呈现层面

Conclusion: VEXA证明了角色自适应、证据基础解释的可行性，为非正式情境下可信赖、面向学习者的安全解释提供了设计指导。关键设计洞察：证据基础决定语义正确性，角色适应在呈现层面操作

Abstract: Online scams across email, short message services, and social media increasingly challenge everyday risk assessment, particularly as generative AI enables more fluent and context-aware deception. Although transformer-based detectors achieve strong predictive performance, their explanations are often opaque to non-experts or misaligned with model decisions. We propose VEXA, an evidence-grounded and persona-adaptive framework for generating learner-facing scam explanations by integrating GradientSHAP-based attribution with theory-informed vulnerability personas. Evaluation across multi-channel datasets shows that grounding explanations in detector-derived evidence improves semantic reliability without increasing linguistic complexity, while persona conditioning introduces interpretable stylistic variation without disrupting evidential alignment. These results reveal a key design insight: evidential grounding governs semantic correctness, whereas persona-based adaptation operates at the level of presentation under constraints of faithfulness. Together, VEXA demonstrates the feasibility of persona-adaptive, evidence-grounded explanations and provides design guidance for trustworthy, learner-facing security explanations in non-formal contexts.

</details>


### [164] [SynAT: Enhancing Security Knowledge Bases via Automatic Synthesizing Attack Tree from Crowd Discussions](https://arxiv.org/abs/2602.05329)
*Ziyou Jiang,Lin Shi,Guowei Yang,Xuyan Ma,Fenglong Li,Qing Wang*

Main category: cs.CR

TL;DR: SynAT：一种从众包安全帖子自动合成攻击树的方法，利用LLM和提示学习筛选攻击信息句子，通过事件关系提取模型提取事件和关系，最后应用启发式规则合成攻击树。


<details>
  <summary>Details</summary>
Motivation: 传统安全知识库由于官方安全信息发布的时间滞后，难以维护和应对新兴安全风险。而在线知识共享平台上的安全帖子包含大量众包安全讨论，这些知识可用于增强安全知识库。

Method: 1. 使用大语言模型和提示学习筛选可能包含攻击信息的句子范围；2. 采用基于转换的事件和关系提取模型同时从筛选范围内提取事件和关系；3. 应用启发式规则将提取的事件和关系合成为攻击树。

Result: 在5,070个Stack Overflow安全帖子上进行实验评估，SynAT在事件和关系提取方面优于所有基线方法，在攻击树合成中实现了最高的树相似度。已成功应用于增强华为安全知识库以及公共安全知识库CVE和CAPEC。

Conclusion: SynAT能够有效从众包安全帖子中自动合成攻击树，解决了传统安全知识库维护滞后的问题，具有实际应用价值，可增强企业安全知识库和公共安全数据库。

Abstract: Cyber attacks have become a serious threat to the security of software systems. Many organizations have built their security knowledge bases to safeguard against attacks and vulnerabilities. However, due to the time lag in the official release of security information, these security knowledge bases may not be well maintained, and using them to protect software systems against emergent security risks can be challenging. On the other hand, the security posts on online knowledge-sharing platforms contain many crowd security discussions and the knowledge in those posts can be used to enhance the security knowledge bases. This paper proposes SynAT, an automatic approach to synthesize attack trees from crowd security posts. Given a security post, SynAT first utilize the Large Language Model (LLM) and prompt learning to restrict the scope of sentences that may contain attack information; then it utilizes a transition-based event and relation extraction model to extract the events and relations simultaneously from the scope; finally, it applies heuristic rules to synthesize the attack trees with the extracted events and relations. An experimental evaluation is conducted on 5,070 Stack Overflow security posts, and the results show that SynAT outperforms all baselines in both event and relation extraction, and achieves the highest tree similarity in attack tree synthesis. Furthermore, SynAT has been applied to enhance HUAWEI's security knowledge base as well as public security knowledge bases CVE and CAPEC, which demonstrates SynAT's practicality.

</details>


### [165] [Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening](https://arxiv.org/abs/2602.05386)
*Zhenxiong Yu,Zhi Yang,Zhiheng Jin,Shuhe Wang,Heng Zhang,Yanlin Fei,Lingfeng Zeng,Fangqi Lou,Shuo Zhang,Tu Hu,Jingping Liu,Rongze Chen,Xingyu Zhu,Kunyi Wang,Chaofa Yuan,Xin Guo,Zhaowei Liu,Feipeng Zhang,Jie Huang,Huacan Wang,Ronghao Chen,Liwen Zhang*

Main category: cs.CR

TL;DR: 提出Spider-Sense框架，基于内在风险感知的事件驱动防御机制，让智能体保持潜在警觉，仅在感知风险时触发防御，通过分层防御机制平衡效率与精度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展为自主智能体，其实际应用扩展带来新的安全挑战。现有防御机制多采用强制检查范式，在智能体生命周期的预定义阶段强制触发安全验证。作者认为有效智能体安全应是内在且选择性的，而非架构解耦和强制性的。

Method: 提出Spider-Sense框架，基于内在风险感知的事件驱动防御机制。智能体保持潜在警觉，仅在感知风险时触发防御。触发后采用分层防御机制：通过轻量级相似性匹配解决已知模式，将模糊案例升级到深度内部推理，消除对外部模型的依赖。引入S²Bench基准，包含现实工具执行和多阶段攻击。

Result: 实验表明Spider-Sense实现竞争性或优越的防御性能，达到最低的攻击成功率（ASR）和误报率（FPR），仅带来8.3%的边际延迟开销。

Conclusion: Spider-Sense框架通过内在风险感知和事件驱动防御，提供了一种更有效、更高效的智能体安全解决方案，克服了传统强制检查范式的局限性。

Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\%.

</details>


### [166] [BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models](https://arxiv.org/abs/2602.05401)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Guowen Xu*

Main category: cs.CR

TL;DR: BadTemplate是一种无需训练的LLM后门攻击方法，通过聊天模板将恶意指令注入系统提示，实现高成功率攻击


<details>
  <summary>Details</summary>
Motivation: 揭示聊天模板在LLM训练和推理中的广泛应用带来的安全风险，特别是攻击者控制模板时可在系统提示中注入任意字符串而不被用户察觉的问题

Method: 提出BadTemplate攻击方法：利用聊天模板的可定制性，将精心设计的恶意指令插入高优先级的系统提示中，无需模型重训练，实现持久性后门行为

Result: 在5个基准数据集、6个开源和3个闭源LLM上的实验表明，BadTemplate攻击成功率高达100%，显著优于3个基线方法，在词级和句级攻击中都表现优异，且HuggingFace和LLM-as-a-judge等第三方平台检测基本无效

Conclusion: 聊天模板在LLM供应链中存在严重安全风险，BadTemplate攻击方法简单、可扩展且成本低，可能在实际系统中广泛传播，造成经济损害和大规模错误信息，需要开发有效的防御机制

Abstract: Chat template is a common technique used in the training and inference stages of Large Language Models (LLMs). It can transform input and output data into role-based and templated expressions to enhance the performance of LLMs. However, this also creates a breeding ground for novel attack surfaces. In this paper, we first reveal that the customizability of chat templates allows an attacker who controls the template to inject arbitrary strings into the system prompt without the user's notice. Building on this, we propose a training-free backdoor attack, termed BadTemplate. Specifically, BadTemplate inserts carefully crafted malicious instructions into the high-priority system prompt, thereby causing the target LLM to exhibit persistent backdoor behaviors. BadTemplate outperforms traditional backdoor attacks by embedding malicious instructions directly into the system prompt, eliminating the need for model retraining while achieving high attack effectiveness with minimal cost. Furthermore, its simplicity and scalability make it easily and widely deployed in real-world systems, raising serious risks of rapid propagation, economic damage, and large-scale misinformation. Furthermore, detection by major third-party platforms HuggingFace and LLM-as-a-judge proves largely ineffective against BadTemplate. Extensive experiments conducted on 5 benchmark datasets across 6 open-source and 3 closed-source LLMs, compared with 3 baselines, demonstrate that BadTemplate achieves up to a 100% attack success rate and significantly outperforms traditional prompt-based backdoors in both word-level and sentence-level attacks. Our work highlights the potential security risks raised by chat templates in the LLM supply chain, thereby supporting the development of effective defense mechanisms.

</details>


### [167] [LTRAS: A Linkable Threshold Ring Adaptor Signature Scheme for Efficient and Private Cross-Chain Transactions](https://arxiv.org/abs/2602.05431)
*Yi Liang,Jinguang Han*

Main category: cs.CR

TL;DR: 提出了一种可链接阈值环适配器签名方案（LTRAS），结合了适配器签名的条件绑定、阈值环签名的多账户支付特性以及可链接性以防止双花，显著降低了大规模环和多账户支付场景下的计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 区块链技术面临可扩展性和吞吐量限制，支付通道等链下解决方案受到关注。适配器签名因其支持原子性而被视为构建支付通道的有前景原语，但标准适配器签名可能泄露签名者身份，存在隐私问题。环签名虽能提供匿名性，但在UTXO区块链（如Monero）常见的多账户支付场景中通常引入高通信开销。

Method: 提出了可链接阈值环适配器签名（LTRAS）方案，整合了适配器签名的条件绑定、阈值环签名的多账户支付以及防止双花的可链接性。提供了LTRAS的形式化定义、安全模型和具体构造，并通过理论分析和实验实现评估其性能。

Result: 实验结果表明，在大规模环和多账户支付场景下，该方案相比现有方案显著降低了计算和通信开销。方案在跨链原子交换中的应用展示了其在增强区块链交易隐私和效率方面的潜力。

Conclusion: LTRAS方案有效解决了适配器签名的隐私泄露问题和环签名的高通信开销问题，为区块链支付通道提供了一种兼具隐私性、效率和原子性的解决方案，特别适用于UTXO区块链的多账户支付场景。

Abstract: Despite the advantages of decentralization and immutability, blockchain technology faces significant scalability and throughput limitations, which has prompted the exploration of off-chain solutions like payment channels. Adaptor signatures have been considered a promising primitive for constructing such channels due to their support for atomicity, offering an alternative to traditional hash-timelock contracts. However, standard adaptor signatures may reveal signer identity, raising potential privacy concerns. While ring signatures can mitigate this issue by providing anonymity, they often introduce high communication overhead, particularly in multi-account payment settings commonly used in UTXO-based blockchains like Monero. To address these limitations, we propose a Linkable Threshold Ring Adaptor Signature (LTRAS) scheme, which integrates the conditional binding of adaptor signatures, the multi-account payment of threshold ring signatures, and the linkability for preventing double-spending. The formal definition, security model and concrete construction of LTRAS are provided. We also analyze its security and evaluate its performance through theoretical analysis and experimental implementation. Experimental results demonstrate that our scheme achieve significantly lower computation and communication overhead compared to existing schemes in large ring sizes and multi-account payment scenarios. Finally, we discuss its application in cross-chain atomic swaps, demonstrating its potential for enhancing privacy and efficiency in blockchain transactions.

</details>


### [168] [Clouding the Mirror: Stealthy Prompt Injection Attacks Targeting LLM-based Phishing Detection](https://arxiv.org/abs/2602.05484)
*Takashi Koide,Hiroki Nakano,Daiki Chiba*

Main category: cs.CR

TL;DR: 该论文首次全面评估了针对多模态LLM钓鱼检测的提示注入攻击，提出了攻击分类法，展示了最先进模型仍易受攻击，并提出了InjectDefuser防御框架。


<details>
  <summary>Details</summary>
Motivation: 钓鱼网站数量和技术复杂度持续增长，基于LLM的钓鱼检测方法虽然前景广阔，但易受提示注入攻击。攻击者可利用LLM与人类感知不对称性，在钓鱼网站中嵌入人类难以察觉但LLM可解析的指令来操纵检测结果。目前针对钓鱼检测中提示注入的具体风险和有效缓解策略研究不足。

Method: 提出二维分类法（攻击技术和攻击面）来捕捉现实的提示注入策略；基于该分类法实现多样化攻击并实证研究多个代表性LLM检测系统；提出InjectDefuser防御框架，结合提示强化、基于白名单的检索增强和输出验证。

Result: 实验表明，包括GPT-5在内的最先进模型仍易受提示注入攻击；InjectDefuser防御框架在多个模型上显著降低了攻击成功率，提高了钓鱼检测的可靠性。

Conclusion: 该研究阐明了钓鱼检测中提示注入攻击的风险格局，提供了实用的防御方案，有助于提高下一代钓鱼对抗措施的可靠性。InjectDefuser框架通过多层防御策略有效缓解了提示注入威胁。

Abstract: Phishing sites continue to grow in volume and sophistication. Recent work leverages large language models (LLMs) to analyze URLs, HTML, and rendered content to decide whether a website is a phishing site. While these approaches are promising, LLMs are inherently vulnerable to prompt injection (PI). Because attackers can fully control various elements of phishing sites, this creates the potential for PI that exploits the perceptual asymmetry between LLMs and humans: instructions imperceptible to end users can still be parsed by the LLM and can stealthily manipulate its judgment. The specific risks of PI in phishing detection and effective mitigation strategies remain largely unexplored. This paper presents the first comprehensive evaluation of PI against multimodal LLM-based phishing detection. We introduce a two-dimensional taxonomy, defined by Attack Techniques and Attack Surfaces, that captures realistic PI strategies. Using this taxonomy, we implement diverse attacks and empirically study several representative LLM-based detection systems. The results show that phishing detection with state-of-the-art models such as GPT-5 remains vulnerable to PI. We then propose InjectDefuser, a defense framework that combines prompt hardening, allowlist-based retrieval augmentation, and output validation. Across multiple models, InjectDefuser significantly reduces attack success rates. Our findings clarify the PI risk landscape and offer practical defenses that improve the reliability of next-generation phishing countermeasures.

</details>


### [169] [GNSS SpAmming: a spoofing-based GNSS denial-of-service attack](https://arxiv.org/abs/2602.05517)
*Sergio Angulo Cosín,Javier Junquera-Sánchez,Carlos Hernando-Ramiro,José-Antonio Gómez-Sánchez*

Main category: cs.CR

TL;DR: 提出一种新型GNSS攻击"SpAmming"，结合干扰与欺骗技术，通过CDMA多路复用使接收机失去对合法卫星信号的访问，现有反干扰和反欺骗措施无法有效防御。


<details>
  <summary>Details</summary>
Motivation: 现有GNSS系统面临干扰和欺骗两种攻击，已有大量对抗措施。但作者发现这两种攻击的结合形式——SpAmming攻击，能以更隐蔽的方式达到相同效果，而现有防御措施无法有效应对。

Method: 利用大多数GNSS中的CDMA多路复用特性，通过欺骗攻击使接收机失去对合法卫星信号的访问。使用SDR系统开发攻击平台，针对冷启动、温启动和已获取PVT解算的导航接收机进行实验，测试不同攻击配置（原始虚假信号发射、精确多普勒效应配置、码偏移等）。

Result: 攻击对冷启动接收机特别有效，但在其他场景下也有效果，尤其是配合其他攻击时。实验证明SpAmming攻击能够实现类似拒绝服务的效果，而现有反干扰和反欺骗措施无法保障其有效性。

Conclusion: SpAmming是一种新型GNSS攻击，结合了干扰和欺骗的特点，现有防御措施无法有效应对。需要开发新的检测和对抗措施，并进一步研究其对认证服务（如OSNMA）的影响，以改进对类似攻击的响应能力。

Abstract: GNSSs are vulnerable to attacks of two kinds: jamming (i.e. denying access to the signal) and spoofing (i.e. impersonating a legitimate satellite). These attacks have been extensively studied, and we have a myriad of countermeasures to mitigate them. In this paper we expose a new type of attack: SpAmming, which combines both approaches to achieve the same effects in a more subtle way.
  Exploiting the CDMA multiplexing present in most GNSSs, and through a spoofing attack, this approach leads the receiver to lose access to the signal of a legitimate satellite, which would be equivalent to a denial of service; but in this case the existing countermeasures against jamming or spoofing would not allow safeguarding its effectiveness, as it is neither of them.
  An experimental proof-of-concept is presented in which its impact is evaluated as a function of the previous state of the receiver. Using an SDR-based system developed at the Space Security Centre, the attack is executed against a cold-started receiver, a warm-started receiver, and a receiver that has already acquired the PVT solution and is navigating. Different attack configurations are also tested, starting from a raw emission of the false signal, to surgical Doppler effect configuration, code offset, etc. Although it is shown to be particularly successful against cold-started receivers, the results show that it is also effective in other scenarios, especially if accompanied by other attacks. We will conclude the article by outlining possible countermeasures to detect and, eventually, counteract it; and possible avenues of research to better understand its impact, especially for authenticated services such as OSNMA, and to characterize it in order to improve the response to similar attacks.

</details>


### [170] [ADCA: Attention-Driven Multi-Party Collusion Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2602.05612)
*Jiayao Wang,Yiping Zhang,Jiale Zhang,Wenliang Yuan,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: 提出了一种名为ADCA的新型联邦自监督学习后门攻击方法，通过恶意客户端联盟和注意力机制聚合，解决了现有攻击中触发器易被检测、在聚合中被稀释以及在异构环境中鲁棒性差的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦自监督学习结合了分布式训练的隐私优势和自监督学习利用未标记数据的能力，但现有研究表明其易受后门攻击。现有攻击方法受限于触发器设计，通常采用全局统一触发器，易被检测、在聚合中被稀释，且在异构客户端环境中缺乏鲁棒性。

Method: 提出注意力驱动的多方合谋攻击（ADCA）：1）在本地预训练期间，恶意客户端分解全局触发器以寻找最优本地模式；2）恶意客户端合谋形成恶意联盟，建立协作优化机制；3）在该机制中，每个客户端提交模型更新，注意力机制动态聚合这些更新以探索最佳协作策略；4）聚合参数作为联盟内下一轮训练的初始状态，有效减轻良性更新对后门信息的稀释。

Result: 在多个FSSL场景和四个数据集上的实验表明，ADCA在攻击成功率（ASR）和持久性方面显著优于现有方法，证明了其有效性和鲁棒性。

Conclusion: ADCA通过恶意客户端联盟和注意力机制聚合，成功解决了现有FSSL后门攻击的局限性，实现了更有效、更持久的攻击效果，突显了联邦自监督学习安全防御的重要性。

Abstract: Federated Self-Supervised Learning (FSSL) integrates the privacy advantages of distributed training with the capability of self-supervised learning to leverage unlabeled data, showing strong potential across applications. However, recent studies have shown that FSSL is also vulnerable to backdoor attacks. Existing attacks are limited by their trigger design, which typically employs a global, uniform trigger that is easily detected, gets diluted during aggregation, and lacks robustness in heterogeneous client environments. To address these challenges, we propose the Attention-Driven multi-party Collusion Attack (ADCA). During local pre-training, malicious clients decompose the global trigger to find optimal local patterns. Subsequently, these malicious clients collude to form a malicious coalition and establish a collaborative optimization mechanism within it. In this mechanism, each submits its model updates, and an attention mechanism dynamically aggregates them to explore the best cooperative strategy. The resulting aggregated parameters serve as the initial state for the next round of training within the coalition, thereby effectively mitigating the dilution of backdoor information by benign updates. Experiments on multiple FSSL scenarios and four datasets show that ADCA significantly outperforms existing methods in Attack Success Rate (ASR) and persistence, proving its effectiveness and robustness.

</details>


### [171] [Time-Complexity Characterization of NIST Lightweight Cryptography Finalists](https://arxiv.org/abs/2602.05641)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.CR

TL;DR: 该论文提出了一个符号模型，用于形式化推导NIST轻量级密码项目10个决赛算法的理论时间复杂度，填补了现有经验评估缺乏统一理论理解的空白。


<details>
  <summary>Details</summary>
Motivation: 随着数字身份系统和物联网验证等新兴技术的发展，在计算能力、内存和能源资源有限的设备上需要轻量级密码学。虽然NIST的轻量级密码项目对10个决赛算法提供了经验评估，但缺乏对其时间复杂度的统一理论理解。

Method: 引入一个符号模型，将每个密码方案分解为初始化、数据处理和最终化三个阶段，从而能够对所有10个决赛算法进行形式化的时间复杂度推导。

Result: 结果阐明了设计参数如何在受限的移动和嵌入式环境中影响计算扩展性，提供了区分算法效率的基础。

Conclusion: 该框架为在受限环境中选择能够支持安全系统的密码原语提供了理论基础和指导。

Abstract: Lightweight cryptography is becoming essential as emerging technologies in digital identity systems and Internet of Things verification continue to demand strong cryptographic assurance on devices with limited processing power, memory, and energy resources. As these technologies move into routine use, they demand cryptographic primitives that maintain strong security and deliver predictable performance through clear theoretical models of time complexity. Although NIST's lightweight cryptography project provides empirical evaluations of the ten finalist algorithms, a unified theoretical understanding of their time-complexity behavior remains absent. This work introduces a symbolic model that decomposes each scheme into initialization, data-processing, and finalization phases, enabling formal time-complexity derivation for all ten finalists. The results clarify how design parameters shape computational scaling on constrained mobile and embedded environments. The framework provides a foundation needed to distinguish algorithmic efficiency and guides the choice of primitives capable of supporting security systems in constrained environments.

</details>


### [172] [Characterizing and Modeling the GitHub Security Advisories Review Pipeline](https://arxiv.org/abs/2602.06009)
*Claudio Segal,Paulo Segal,Carlos Eduardo de Schuller Banjar,Felipe Paixão,Hudson Silva Borges,Paulo Silveira Neto,Eduardo Santana de Almeida,Joanna C. S. Santos,Anton Kocheturov,Gaurav Kumar Srivastava,Daniel Sadoc Menasché*

Main category: cs.CR

TL;DR: 对GitHub安全公告(GHSA)审查过程的大规模实证研究，分析了28.8万+公告，揭示了审查机制、延迟模式及双路径队列模型。


<details>
  <summary>Details</summary>
Motivation: GitHub安全公告已成为开源漏洞披露的核心组件，但只有部分公告经过GitHub审查，且审查机制缺乏深入理解。研究旨在揭示GHSA审查过程的特征、延迟模式及内在机制。

Method: 对2019-2025年间超过288,000个安全公告进行大规模实证研究，分析审查概率、量化审查延迟，识别不同的审查延迟机制，并基于公告处理流程结构开发队列模型。

Result: 研究发现存在两种不同的审查延迟机制：快速路径（以GitHub仓库公告为主）和慢速路径（以NVD优先公告为主）。开发了基于公告处理流程结构的队列模型来解释这种二分法。

Conclusion: 该研究首次系统揭示了GHSA审查过程的运作机制，为理解开源漏洞披露生态系统中的审查延迟提供了重要见解，并建立了可解释的队列模型框架。

Abstract: GitHub Security Advisories (GHSA) have become a central component of open-source vulnerability disclosure and are widely used by developers and security tools. A distinctive feature of GHSA is that only a fraction of advisories are reviewed by GitHub, while the mechanisms associated with this review process remain poorly understood. In this paper, we conduct a large-scale empirical study of GHSA review processes, analyzing over 288,000 advisories spanning 2019--2025. We characterize which advisories are more likely to be reviewed, quantify review delays, and identify two distinct review-latency regimes: a fast path dominated by GitHub Repository Advisories (GRAs) and a slow path dominated by NVD-first advisories. We further develop a queueing model that accounts for this dichotomy based on the structure of the advisory processing pipeline.

</details>


### [173] [FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation](https://arxiv.org/abs/2602.05838)
*Mayank Kumar,Qian Lou,Paulo Barreto,Martine De Cock,Sikha Pentyala*

Main category: cs.CR

TL;DR: FHAIM：首个基于全同态加密的框架，用于在加密表格数据上训练基于边际的合成数据生成器，解决隐私保护与数据共享的矛盾


<details>
  <summary>Details</summary>
Motivation: AI发展受限于隐私法规导致的数据孤岛问题，特别是在医疗、教育、金融等重要领域。现有合成数据生成服务需要数据持有者信任服务提供商，存在隐私泄露风险

Method: 提出FHAIM框架，将广泛使用的AIM算法适配到全同态加密环境，开发新的FHE协议，确保私有数据在整个训练过程中保持加密状态，最终输出具有差分隐私保证的合成数据

Result: 实证分析表明，FHAIM在保持AIM算法性能的同时，维持了可行的运行时间

Conclusion: FHAIM为隐私敏感的合成数据生成提供了首个全同态加密解决方案，能够在保护数据隐私的同时实现数据共享，推动AI在关键领域的应用

Abstract: Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.

</details>


### [174] [Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection](https://arxiv.org/abs/2602.05868)
*Ehsan Firouzi,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究发现静态分析工具在评估LLM生成代码安全性时存在显著局限性，提出基于人类反馈的动态检索增强生成框架


<details>
  <summary>Details</summary>
Motivation: 现有研究过度依赖静态分析工具评估LLM的代码安全生成和漏洞检测能力，但缺乏对这些工具可靠性的系统验证

Method: 审查1080个LLM生成的代码样本，构建人工验证的基准真值，比较CodeQL和Semgrep两个静态安全工具的输出结果

Result: 61%样本实际安全，但Semgrep和CodeQL分别报告60%和80%为安全；工具报告与基准真值的匹配率仅65%（Semgrep）和61%（CodeQL）

Conclusion: 静态分析工具不能作为代码安全的唯一评估标准，需要专家反馈；提出基于人类反馈的动态检索增强生成框架，使LLM能复用历史反馈

Abstract: Existing literature heavily relies on static analysis tools to evaluate LLMs for secure code generation and vulnerability detection. We reviewed 1,080 LLM-generated code samples, built a human-validated ground-truth, and compared the outputs of two widely used static security tools, CodeQL and Semgrep, against this corpus. While 61% of the samples were genuinely secure, Semgrep and CodeQL classified 60% and 80% as secure, respectively. Despite the apparent agreement in aggregate statistics, per-sample analysis reveals substantial discrepancies: only 65% of Semgrep's and 61% of CodeQL's reports correctly matched the ground truth. These results question the reliability of static analysis tools as sole evaluators of code security and underscore the need for expert feedback. Building on this insight, we propose a conceptual framework that persistently stores human feedback in a dynamic retrieval-augmented generation pipeline, enabling LLMs to reuse past feedback for secure code generation and vulnerability detection.

</details>


### [175] [Privacy Amplification Persists under Unlimited Synthetic Data Release](https://arxiv.org/abs/2602.04895)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.CR

TL;DR: 该论文研究了通过合成数据发布实现隐私放大的现象，发现在有界参数假设下，即使发布无限数量的合成记录，隐私放大效应仍然存在，改进了Pierquin等人的结果。


<details>
  <summary>Details</summary>
Motivation: Pierquin等人(2025)首次为线性生成器建立了正式的隐私放大保证，但仅适用于模型维度远超过发布的合成记录数量的渐近机制，限制了实际应用价值。本研究旨在探索更实用的隐私放大场景。

Method: 在分析中引入了有界参数假设，研究在此条件下发布合成数据而非私有生成模型本身时的隐私放大效应。通过理论分析证明即使在发布无限数量合成记录的情况下，隐私放大仍然存在。

Result: 研究结果显示，在有界参数假设下，隐私放大效应在发布无限数量合成记录时仍然持续存在，这改进了Pierquin等人的界限，为更复杂的发布机制提供了理论基础。

Conclusion: 该研究提供了结构性的理论见解，可能指导为更复杂的发布机制开发更严格的隐私保证，证明了在更实际的场景下隐私放大的可行性。

Abstract: We study privacy amplification by synthetic data release, a phenomenon in which differential privacy guarantees are improved by releasing only synthetic data rather than the private generative model itself. Recent work by Pierquin et al. (2025) established the first formal amplification guarantees for a linear generator, but they apply only in asymptotic regimes where the model dimension far exceeds the number of released synthetic records, limiting their practical relevance. In this work, we show a surprising result: under a bounded-parameter assumption, privacy amplification persists even when releasing an unbounded number of synthetic records, thereby improving upon the bounds of Pierquin et al. (2025). Our analysis provides structural insights that may guide the development of tighter privacy guarantees for more complex release mechanisms.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [176] [AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval](https://arxiv.org/abs/2602.05524)
*Konosuke Yoshizato,Kazuma Shimizu,Ryota Higa,Takanobu Otsuka*

Main category: cs.MA

TL;DR: 研究探索了基于大语言模型的多智能体系统在库存管理中的应用，通过引入AIM-RM智能体提升系统在不同供应链场景下的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 库存管理是供应链管理的关键组成部分，基于大语言模型的多智能体系统虽然受到关注，但其在库存管理中的有效性仍存在不确定性，特别是能否持续推导最优订购策略并适应多样化供应链场景。

Method: 研究采用基于大语言模型的多智能体系统，使用固定订购策略提示编码问题设置的逐步过程和库存管理中常用的安全库存策略。为进一步提升适应性，提出了名为AIM-RM的新型智能体，该智能体通过相似性匹配利用相似历史经验。

Result: 实证结果表明，即使没有详细的提示调整，基于大语言模型的多智能体系统也能在受限场景中确定最优订购决策。AIM-RM智能体在各种供应链场景中均优于基准方法，展现出卓越的鲁棒性和适应性。

Conclusion: 基于大语言模型的多智能体系统在库存管理中具有应用潜力，通过AIM-RM智能体的相似性匹配机制，系统能够有效适应多样化供应链场景，为智能库存管理提供了新的解决方案。

Abstract: This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.

</details>


### [177] [Learning to Share: Selective Memory for Efficient Parallel Agentic Systems](https://arxiv.org/abs/2602.05965)
*Joseph Fioresi,Parth Parag Kulkarni,Ashmal Vayani,Song Wang,Mubarak Shah*

Main category: cs.MA

TL;DR: LTS提出了一种学习型共享内存机制，用于并行智能体系统，通过选择性跨团队信息复用减少重复计算，在保持任务性能的同时显著降低运行时间。


<details>
  <summary>Details</summary>
Motivation: 并行智能体系统虽然能通过多个团队并行执行来提高鲁棒性和解决方案质量，但不同团队在解决相似子问题或执行类似步骤时会进行大量重复计算，导致显著的计算成本浪费。

Method: LTS引入了一个全局内存库和轻量级控制器，控制器通过基于使用感知信用分配的逐步强化学习进行训练，决定是否将中间智能体步骤添加到内存中，从而实现选择性跨团队信息复用并控制上下文增长。

Result: 在AssistantBench和GAIA基准测试中，LTS在匹配或改进任务性能的同时，显著减少了总体运行时间，相比无内存并行基线表现出更好的效率。

Conclusion: 学习型内存准入是提高并行智能体系统效率的有效策略，LTS通过选择性信息复用减少了重复计算，在保持解决方案质量的同时显著提升了计算效率。

Abstract: Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/

</details>


### [178] [PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling](https://arxiv.org/abs/2602.06030)
*Kavana Venkatesh,Yinhan He,Jundong Li,Jiaming Cui*

Main category: cs.MA

TL;DR: PhysicsAgentABM：一种结合神经符号融合的生成式多智能体建模框架，通过群体级推理和不确定性感知实现可扩展、校准的仿真


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统扩展成本高且时间步对齐的状态转移仿真校准不佳，而经典ABM难以整合丰富的个体级信号和非平稳行为，需要新的可扩展校准仿真范式

Method: 1) 将推理转移到行为一致的智能体集群；2) 状态专用符号智能体编码机制转移先验；3) 多模态神经转移模型捕捉时空和交互动态；4) 不确定性感知认知融合产生校准的集群级转移分布；5) ANCHOR聚类策略基于跨上下文行为响应和对比损失减少LLM调用

Result: 在公共卫生、金融和社会科学实验中，相比机制、神经和LLM基线，在事件时间准确性和校准方面获得一致提升，LLM调用减少6-8倍

Conclusion: 通过围绕群体级推理重新架构生成式ABM，结合不确定性感知的神经符号融合，PhysicsAgentABM建立了LLM可扩展校准仿真的新范式

Abstract: Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [179] [Groups and Inverse Semigroups in Lambda Calculus](https://arxiv.org/abs/2602.05654)
*Antonio Bucciarelli,Arturo De Faveri,Giulio Manzonetto,Antonino Salibra*

Main category: cs.LO

TL;DR: 该论文研究了λ项在λ理论下的可逆性，重点关注有限遗传置换(FHP)和无限遗传置换(HP)两类λ项，通过逆半群理论建立了λ项可逆性与η展开之间的对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究λ项在λ理论下的可逆性问题，特别是理解哪些λ项在特定λ理论中具有逆元。有限遗传置换(FHP)在最小外延λ理论λη中是可逆的，而无限遗传置换(HP)在最大合理λ理论H*中是可逆的。需要建立统一的框架来理解这些可逆性现象。

Method: 采用逆半群理论作为主要工具，逆半群是群和半格的推广。证明FHP模λ理论T总是构成逆半群，HP模包含Böhm树理论的T也构成逆半群。利用逆半群的自然序结构，建立该序与η展开的对应关系：在FHP/T中对应η展开，在HP/T中对应无限η展开。

Result: 1. FHP模任意λ理论T总是逆半群，HP模包含Böhm树理论的T也是逆半群；2. 逆半群的自然序在FHP/T中对应η展开，在HP/T中对应无限η展开；3. 将已有结果置于更广泛的框架中；4. 证明FHP是在λη和H+之间所有λ理论中的可逆λ项，其中H+是Morris的观测λ理论。

Conclusion: 通过逆半群理论成功建立了λ项可逆性的统一框架，揭示了FHP和HP在λ理论中的核心作用，证明了FHP在λη和H+之间所有λ理论中的可逆性，为λ项可逆性研究提供了新的代数视角和工具。

Abstract: We study invertibility of $λ$-terms modulo $λ$-theories. Here a fundamental role is played by a class of $λ$-terms called finite hereditary permutations (FHP) and by their infinite generalisations (HP). More precisely, FHPs are the invertible elements in the least extensional $λ$-theory $λη$ and HPs are those in the greatest sensible $λ$-theory $H^*$. Our approach is based on inverse semigroups, algebraic structures that generalise groups and semilattices. We show that FHP modulo a $λ$-theory $T$ is always an inverse semigroup and that HP modulo $T$ is an inverse semigroup whenever $T$ contains the theory of Böhm trees. An inverse semigroup comes equipped with a natural order. We prove that the natural order corresponds to $η$-expansion in $\mathrm{FHP} /T$, and to infinite $η$-expansion in $\mathrm{HP}/T$. Building on these correspondences we obtain the two main contributions of this work: firstly, we recast in a broader framework the results cited at the beginning; secondly, we prove that the FHPs are the invertible $λ$-terms in all the $λ$-theories lying between $λη$ and $H^+$. The latter is Morris' observational $λ$-theory, defined by using the $β$-normal forms as observables.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [180] [DiLLS: Interactive Diagnosis of LLM-based Multi-agent Systems via Layered Summary of Agent Behaviors](https://arxiv.org/abs/2602.05446)
*Rui Sheng,Yukun Yang,Chuhan Shi,Yanna Lin,Zixin Chen,Huamin Qu,Furui Cheng*

Main category: cs.HC

TL;DR: DiLLS是一个用于分析和理解LLM多智能体系统行为的交互式框架，通过三层结构组织信息，帮助开发者诊断故障


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统在处理复杂任务时表现出色，但其行为复杂性使得系统难以理解。当出现故障时，开发者难以识别根本原因并确定改进路径。传统的原始日志检查方法效率低下，因为数据量大且复杂。

Method: 提出DiLLS框架和交互式系统，通过三层查询完成结构（活动、行动、操作）组织和揭示多智能体系统行为。通过自然语言探测多智能体系统，将规划和执行信息组织成结构化的多层摘要。

Result: 用户研究表明，DiLLS显著提高了开发者在识别、诊断和理解LLM多智能体系统故障方面的有效性和效率。

Conclusion: DiLLS为解决LLM多智能体系统行为理解和故障诊断的挑战提供了一个有效的框架和工具，通过结构化信息组织提高了开发者的工作效率。

Abstract: Large language model (LLM)-based multi-agent systems have demonstrated impressive capabilities in handling complex tasks. However, the complexity of agentic behaviors makes these systems difficult to understand. When failures occur, developers often struggle to identify root causes and to determine actionable paths for improvement. Traditional methods that rely on inspecting raw log records are inefficient, given both the large volume and complexity of data. To address this challenge, we propose a framework and an interactive system, DiLLS, designed to reveal and structure the behaviors of multi-agent systems. The key idea is to organize information across three levels of query completion: activities, actions, and operations. By probing the multi-agent system through natural language, DiLLS derives and organizes information about planning and execution into a structured, multi-layered summary. Through a user study, we show that DiLLS significantly improves developers' effectiveness and efficiency in identifying, diagnosing, and understanding failures in LLM-based multi-agent systems.

</details>


### [181] [AI chatbots versus human healthcare professionals: a systematic review and meta-analysis of empathy in patient care](https://arxiv.org/abs/2602.05628)
*Alastair Howcroft,Amber Bennett-Weston,Ahmad Khan,Joseff Griffiths,Simon Gay,Jeremy Howick*

Main category: cs.HC

TL;DR: AI聊天机器人在文本场景中被认为比人类医疗专业人员更具同理心，标准化平均差异为0.87（P<.00001），相当于10分量表上增加2分


<details>
  <summary>Details</summary>
Motivation: 同理心对改善患者预后至关重要，而AI聊天机器人在医疗领域的应用迅速扩展。现有研究表明AI聊天机器人可能在同理心方面优于人类医疗专业人员，但研究结果不一致且缺乏综合评估

Method: 检索多个数据库，比较使用大型语言模型的AI聊天机器人与人类医疗专业人员在同理心测量上的研究。使用ROBINS-I评估偏倚风险，在可行时使用随机效应荟萃分析综合结果，避免重复计数

Result: 纳入15项研究（2023-2024年），13项研究显示AI同理心评分显著更高，仅2项皮肤病学研究支持人类回答。13项可提取数据的研究荟萃分析显示，标准化平均差异为0.87（95% CI 0.54-1.20），支持AI（P<.00001）

Conclusion: 在纯文本场景中，AI聊天机器人常被认为比人类医疗专业人员更具同理心。未来研究需要患者直接评估验证，并评估新兴语音AI系统是否具有类似优势

Abstract: Background: Empathy is widely recognized for improving patient outcomes, including reduced pain and anxiety and improved satisfaction, and its absence can cause harm. Meanwhile, use of artificial intelligence (AI)-based chatbots in healthcare is rapidly expanding, with one in five general practitioners using generative AI to assist with tasks such as writing letters. Some studies suggest AI chatbots can outperform human healthcare professionals (HCPs) in empathy, though findings are mixed and lack synthesis.
  Sources of data: We searched multiple databases for studies comparing AI chatbots using large language models with human HCPs on empathy measures. We assessed risk of bias with ROBINS-I and synthesized findings using random-effects meta-analysis where feasible, whilst avoiding double counting.
  Areas of agreement: We identified 15 studies (2023-2024). Thirteen studies reported statistically significantly higher empathy ratings for AI, with only two studies situated in dermatology favouring human responses. Of the 15 studies, 13 provided extractable data and were suitable for pooling. Meta-analysis of those 13 studies, all utilising ChatGPT-3.5/4, showed a standardized mean difference of 0.87 (95% CI, 0.54-1.20) favouring AI (P < .00001), roughly equivalent to a two-point increase on a 10-point scale.
  Areas of controversy: Studies relied on text-based assessments that overlook non-verbal cues and evaluated empathy through proxy raters.
  Growing points: Our findings indicate that, in text-only scenarios, AI chatbots are frequently perceived as more empathic than human HCPs.
  Areas timely for developing research: Future research should validate these findings with direct patient evaluations and assess whether emerging voice-enabled AI systems can deliver similar empathic advantages.

</details>


### [182] [Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction](https://arxiv.org/abs/2602.05687)
*Pavithren V S Pakianathan,Rania Islambouli,Diogo Branco,Albrecht Schmidt,Tiago Guerreiro,Jan David Smeddinck*

Main category: cs.HC

TL;DR: LLM驱动的自动摘要和对话界面可帮助医疗专业人员理解患者生成的健康数据，但需解决透明度、隐私和过度依赖等问题


<details>
  <summary>Details</summary>
Motivation: 个人通过可穿戴设备和智能手机生成大量健康数据，这些数据有潜力改变预防性医疗，但临床实践中面临数据规模大、异质性强、医疗专业人员时间压力和数据素养不足等障碍

Method: 采用混合方法研究，以心血管疾病风险降低为用例，让16名医疗专业人员通过原型系统审查多模态患者生成健康数据，该系统整合了常见图表、LLM生成的摘要和对话界面

Result: AI摘要提供快速概览并锚定数据探索，对话交互支持灵活分析并弥合数据素养差距，但医疗专业人员对透明度、隐私和过度依赖表示担忧

Conclusion: 研究为将AI驱动的摘要和对话集成到临床工作流程中提供了实证见解和社会技术设计启示，以支持患者生成健康数据的理解

Abstract: Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data literacy of healthcare professionals (HCPs). We explore how large language models (LLMs) can support sensemaking of patient-generated health data (PGHD) with automated summaries and natural language data exploration. Using cardiovascular disease (CVD) risk reduction as a use case, 16 HCPs reviewed multimodal PGHD in a mixed-methods study with a prototype that integrated common charts, LLM-generated summaries, and a conversational interface. Findings show that AI summaries provided quick overviews that anchored exploration, while conversational interaction supported flexible analysis and bridged data-literacy gaps. However, HCPs raised concerns about transparency, privacy, and overreliance. We contribute empirical insights and sociotechnical design implications for integrating AI-driven summarization and conversation into clinical workflows to support PGHD sensemaking.

</details>


### [183] [ToMigo: Interpretable Design Concept Graphs for Aligning Generative AI with Creative Intent](https://arxiv.org/abs/2602.05825)
*Lena Hegemann,Xinyi Wen,Michael A. Hedderich,Tarmo Nurmi,Hariharan Subramonyam*

Main category: cs.HC

TL;DR: ToMigo是一个通过设计概念图表示用户意图的生成式AI系统，用于解决AI输出与用户意图不对齐的问题，特别针对图形设计领域。


<details>
  <summary>Details</summary>
Motivation: 生成式AI经常产生与用户意图不对齐的结果，特别是在处理模糊提示时。现有方法需要专业知识或严格流程来澄清意图，缺乏通过简单直接输入来理解和影响AI对用户意图解释的方法。

Method: ToMigo将意图表示为设计概念图：节点代表目的、内容或风格的选择，边用可解释的说明连接它们。从预研究数据中推导出节点类型和边的模式，指导多模态大语言模型生成图，使节点外部与用户意图对齐，内部朝向统一的设计目标。系统允许用户探索AI推理并直接操作设计概念。

Result: 用户研究中，ToMigo获得了高对齐评分，能够很好地捕捉大多数用户意图。用户报告了更强的控制感，并发现可编辑图、反思性聊天、概念-设计重新对齐等交互功能对发展和实现设计想法很有用。

Conclusion: ToMigo通过设计概念图表示意图，提供了一种理解、影响和操作AI对用户意图解释的方法，在图形设计领域有效解决了生成式AI的意图对齐问题，增强了用户控制感和设计想法的实现。

Abstract: Generative AI often produces results misaligned with user intentions, for example, resolving ambiguous prompts in unexpected ways. Despite existing approaches to clarify intent, a major challenge remains: understanding and influencing AI's interpretation of user intent through simple, direct inputs requiring no expertise or rigid procedures. We present ToMigo, representing intent as design concept graphs: nodes represent choices of purpose, content, or style, while edges link them with interpretable explanations. Applied to graphic design, ToMigo infers intent from reference images and text. We derived a schema of node types and edges from pre-study data, informing a multimodal large language model to generate graphs aligning nodes externally with user intent and internally toward a unified design goal. This structure enables users to explore AI reasoning and directly manipulate the design concept. In our user studies, ToMigo received high alignment ratings and captured most user intentions well. Users reported greater control and found interactive features-editable graphs, reflective chats, concept-design realignment-useful for evolving and realizing their design ideas.

</details>


### [184] [Prompting Destiny: Negotiating Socialization and Growth in an LLM-Mediated Speculative Gameworld](https://arxiv.org/abs/2602.05864)
*Mandi Yang,Zhiqi Gao,Yibo Meng,Dongyijie Primo Pan*

Main category: cs.HC

TL;DR: LLM角色扮演游戏支持社会化、道德责任和教育角色定位的反思，采用四季节结构让玩家引导儿童王子应对道德情境，通过NPC差异化反应帮助理解教育指导随社会化的变化。


<details>
  <summary>Details</summary>
Motivation: 基于社会化理论，设计能够支持反思社会化过程、道德责任和教育角色定位的AI介导游戏系统，帮助玩家理解教育指导如何随社会化阶段变化。

Method: 采用四季节结构的LLM角色扮演游戏，玩家引导儿童王子应对道德情境；系统隐藏实时评分，提供阶段末成长反馈作为反思提示；通过用户研究（N=12）分析游戏日志和访谈，采用反思性主题分析。

Result: 研究发现玩家如何协商责任和角色定位，揭示了开放表达与持续参与之间的入口负荷张力；系统成功将社会化社会学模型转化为反思性AI介导游戏设计。

Conclusion: 贡献了将社会化社会学模型转化为反思性AI介导游戏系统的设计知识，展示了如何通过隐藏评分和延迟反馈来减少分数追逐，促进深度反思。

Abstract: We present an LLM-mediated role-playing game that supports reflection on socialization, moral responsibility, and educational role positioning. Grounded in socialization theory, the game follows a four-season structure in which players guide a child prince through morally charged situations and compare the LLM-mediated NPC's differentiated responses across stages, helping them reason about how educational guidance shifts with socialization. To approximate real educational contexts and reduce score-chasing, the system hides real-time evaluative scores and provides delayed, end-of-stage growth feedback as reflective prompts. We conducted a user study (N=12) with gameplay logs and post-game interviews, analyzed via reflexive thematic analysis. Findings show how players negotiated responsibility and role positioning, and reveal an entry-load tension between open-ended expression and sustained engagement. We contribute design knowledge on translating sociological models of socialization into reflective AI-mediated game systems.

</details>


### [185] [From Human-Human Collaboration to Human-Agent Collaboration: A Vision, Design Philosophy, and an Empirical Framework for Achieving Successful Partnerships Between Humans and LLM Agents](https://arxiv.org/abs/2602.05987)
*Bingsheng Yao,Chaoran Chen,April Yi Wang,Sherry Tongshuang Wu,Toby Jia-jun Li,Dakuo Wang*

Main category: cs.HC

TL;DR: 该工作坊探讨如何将远程人类协作的研究成果应用于人-智能体协作设计，建立跨学科研究议程


<details>
  <summary>Details</summary>
Motivation: LLM智能体的出现使人-智能体协作成为可能，但需要借鉴数十年远程人类协作研究中的信任、意识感知和共同基础等概念，同时识别AI智能体带来的独特机遇与挑战

Method: 组织180分钟跨学科工作坊，包含主题演讲、邀请闪电演讲和探索性小组设计会议，参与者通过故事板设计新颖的人-智能体协作范式

Result: 工作坊将汇集HCI、CSCW和AI领域的跨学科研究者，建立共享词汇表并制定研究议程，为协作智能体的未来发展指明方向

Conclusion: 通过借鉴远程人类协作的丰富理解，可以启发和指导人-智能体协作的设计与研究，开启人-智能体协作的新时代

Abstract: The emergence of Large Language Model (LLM) agents enables us to build agent-based intelligent systems that move beyond the role of a "tool" to become genuine collaborators with humans, thereby realizing a novel human-agent collaboration paradigm. Our vision is that LLM agents should resemble remote human collaborators, which allows HCI researchers to ground the future exploration in decades of research on trust, awareness, and common ground in remote human collaboration, while also revealing the unique opportunities and challenges that emerge when one or more partners are AI agents. This workshop establishes a foundational research agenda for the new era by posing the question: How can the rich understanding of remote human collaboration inspire and inform the design and study of human-agent collaboration? We will bring together an interdisciplinary group from HCI, CSCW, and AI to explore this critical transition. The 180-minute workshop will be highly interactive, featuring a keynote speaker, a series of invited lightning talks, and an exploratory group design session where participants will storyboard novel paradigms of human-agent partnership. Our goal is to enlighten the research community by cultivating a shared vocabulary and producing a research agenda that charts the future of collaborative agents.

</details>


### [186] [Applying Ground Robot Fleets in Urban Search: Understanding Professionals' Operational Challenges and Design Opportunities](https://arxiv.org/abs/2602.04992)
*Puqi Zhou,Charles R. Twardy,Cynthia Lum,Myeong Lee,David J. Porfirio,Michael R. Hieb,Chris Thomas,Xuesu Xiao,Sungsoo Ray Hong*

Main category: cs.HC

TL;DR: 研究探讨了公共安全专业人员对地面机器人车队与计算机视觉、LLM技术集成的看法，识别了四个关键挑战领域和四个设计机会，旨在减轻城市搜索任务中的认知和体力负担。


<details>
  <summary>Details</summary>
Motivation: 城市搜索任务需要在高压环境下快速决策和持续体力消耗，而现有技术虽先进但缺乏对公共安全专业人员实际需求和接受度的研究，存在开发技术先进但不实用解决方案的风险。

Method: 通过焦点小组访谈，与弗吉尼亚州五个地方部门的八名警察进行讨论，收集他们对地面机器人技术集成的看法和需求。

Result: 发现地面机器人可减少对纸质参考、心算和临时协调的依赖，缓解四个关键挑战：1)跨多个搜索假设分配人力；2)保持群体和情境意识；3)根据失踪者特征规划路线；4)不确定性下的认知和体力疲劳管理。

Conclusion: 提出了四个设计机会：1)可扩展的多机器人规划控制界面；2)机构特定路线优化；3)基于简报更新的实时重规划；4)保持操作信任的视觉辅助提示。最终强调需要可部署、可问责、以人为本的城市搜索支持系统。

Abstract: Urban searches demand rapid, defensible decisions and sustained physical effort under high cognitive and situational load. Incident commanders must plan, coordinate, and document time-critical operations, while field searchers execute evolving tasks in uncertain environments. With recent advances in technology, ground-robot fleets paired with computer-vision-based situational awareness and LLM-powered interfaces offer the potential to ease these operational burdens. However, no dedicated studies have examined how public safety professionals perceive such technologies or envision their integration into existing practices, risking building technically sophisticated yet impractical solutions. To address this gap, we conducted focus-group sessions with eight police officers across five local departments in Virginia. Our findings show that ground robots could reduce professionals' reliance on paper references, mental calculations, and ad-hoc coordination, alleviating cognitive and physical strain in four key challenge areas: (1) partitioning the workforce across multiple search hypotheses, (2) retaining group awareness and situational awareness, (3) building route planning that fits the lost-person profile, and (4) managing cognitive and physical fatigue under uncertainty. We further identify four design opportunities and requirements for future ground-robot fleet integration in public-safety operations: (1) scalable multi-robot planning and control interfaces, (2) agency-specific route optimization, (3) real-time replanning informed by debrief updates, and (4) vision-assisted cueing that preserves operational trust while reducing cognitive workload. We conclude with design implications for deployable, accountable, and human-centered urban-search support systems

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [187] [Doc2Spec: Synthesizing Formal Programming Specifications from Natural Language via Grammar Induction](https://arxiv.org/abs/2602.04892)
*Shihao Xia,Mengting He,Haomin Jia,Linhai Song*

Main category: cs.PL

TL;DR: Doc2Spec：一个多智能体框架，使用LLM从自然语言规则自动推导规范语法，然后基于该语法生成形式化规范，提高API实现与使用的合规性验证效率。


<details>
  <summary>Details</summary>
Motivation: 确保API实现和使用符合自然语言编程规则对软件正确性、安全性和可靠性至关重要。形式化验证能提供强保证但需要精确规范，而手动编写这些规范既困难又成本高昂。

Method: 提出Doc2Spec多智能体框架：1）使用LLM从自然语言规则自动推导规范语法；2）基于推导出的语法生成形式化规范。该语法捕获关键领域知识，约束规范空间，并强制执行一致表示。

Result: 在三种编程语言的七个基准测试中，Doc2Spec优于无语法推导的基线方法，并与使用手动构建语法的技术取得竞争性结果，证明了自动语法推导在形式化自然语言规则方面的有效性。

Conclusion: Doc2Spec通过自动推导规范语法，显著提高了从自然语言规则生成形式化规范的可靠性和质量，为API合规性验证提供了一种高效且可扩展的解决方案。

Abstract: Ensuring that API implementations and usage comply with natural language programming rules is critical for software correctness, security, and reliability. Formal verification can provide strong guarantees but requires precise specifications, which are difficult and costly to write manually. To address this challenge, we present Doc2Spec, a multi-agent framework that uses LLMs to automatically induce a specification grammar from natural-language rules and then generates formal specifications guided by the induced grammar. The grammar captures essential domain knowledge, constrains the specification space, and enforces consistent representations, thereby improving the reliability and quality of generated specifications. Evaluated on seven benchmarks across three programming languages, Doc2Spec outperforms a baseline without grammar induction and achieves competitive results against a technique with a manually crafted grammar, demonstrating the effectiveness of automated grammar induction for formalizing natural-language rules.

</details>


### [188] [Strong Normalisation for Asynchronous Effects](https://arxiv.org/abs/2602.05528)
*Danel Ahman,Ilja Sobolev*

Main category: cs.PL

TL;DR: 该论文研究了Ahman和Pretnar异步效应演算的归一化性质，证明了去除一般递归后的演算具有强归一性，并进一步证明了在引入受控的中断驱动递归行为后，顺序片段仍保持强归一性。


<details>
  <summary>Details</summary>
Motivation: Ahman和Pretnar的异步效应演算为代数计算效应提供了异步处理能力，能够建模抢占式多线程、可取消远程函数调用、多方应用等场景。然而，该演算的归一化性质尚未得到充分研究，特别是其异步特性对终止性的影响需要系统分析。

Method: 采用Lindley和Stark基于⊤⊤-提升的方法的结构化扩展来证明强归一性。首先从原始演算中移除一般递归，证明剩余演算（包括顺序和并行部分）具有强归一性。然后重新引入受控的中断驱动递归行为，证明顺序片段仍保持强归一性。所有证明均在Agda中形式化验证。

Result: 1. 移除一般递归后的异步效应演算具有强归一性（包括顺序和并行部分）；2. 仅适用于简单异步示例的终止性保证；3. 在引入受控中断驱动递归后，顺序片段仍保持强归一性；4. 所有结果均在Agda中形式化验证。

Conclusion: 该研究为异步效应演算的终止性提供了理论基础，证明了在适当限制递归行为的情况下，异步效应演算可以保持强归一性。这为构建可靠异步编程语言提供了理论保障，同时展示了⊤⊤-提升方法在异步效应场景中的可扩展性。

Abstract: Asynchronous effects of Ahman and Pretnar complement the conventional synchronous treatment of algebraic computational effects with asynchrony based on decoupling the execution of algebraic operation calls into signalling that an operation's implementation needs to be executed, and into interrupting a running computation with the operation's result, to which the computation can react by installing matching interrupt handlers. Beyond providing asynchrony for algebraic effects, the resulting core calculus also naturally models examples such as pre-emptive multi-threading, (cancellable) remote function calls, multi-party applications, and even a parallel variant of runners of algebraic effects. In this paper, we study the normalisation properties of this calculus. We prove that if one removes general recursion from the original calculus, then the remaining calculus is strongly normalising, including both its sequential and parallel parts. However, this only guarantees termination for very simple asynchronous examples. To improve on this result, we also prove that the sequential fragment of the calculus remains strongly normalising when a controlled amount of interrupt-driven recursive behaviour is reintroduced. Our strong normalisation proofs are structured compositionally as a natural extension of Lindley and Stark's $\top\top$-lifting based approach for proving strong normalisation of effectful languages. All our results are also formalised in Agda.

</details>


### [189] [An Equational Axiomatization of Dynamic Threads via Algebraic Effects: Presheaves on Finite Relations, Labelled Posets, and Parameterized Algebraic Theories](https://arxiv.org/abs/2602.05850)
*Ohad Kammar,Jack Liell-Cock,Sam Lindley,Cristina Matache,Sam Staton*

Main category: cs.PL

TL;DR: 使用代数效应理论为动态线程提供完整的等式公理化，基于参数化代数理论构建fork和wait原语，证明模型完备性和语法完备性，并扩展到并发编程语言的语义分析。


<details>
  <summary>Details</summary>
Motivation: 为动态线程提供完整的等式公理化，建立并发程序的形式化理论基础，解决线程创建和等待操作的语义建模问题。

Method: 基于参数化代数理论，构建包含fork（创建子线程并传递线程ID）和wait（等待子线程完成）原语的代数理论，包含基本原子操作和结合律等公理。

Result: 1. 模型完备性：闭表达式完全捕获标记偏序集（pomsets）的相等性；2. 语法完备性：开放表达式在所有闭化替换下相等时可证明相等；3. 扩展到并发语言，提供操作和指称语义，证明其健全性、充分性和一阶完全抽象性。

Conclusion: 代数效应为动态线程提供了有效的公理化框架，参数化代数理论是处理名称和绑定的便利工具，建立的语义分析在并发模型中是完备和抽象的。

Abstract: We use the theory of algebraic effects to give a complete equational axiomatization for dynamic threads. Our method is based on parameterized algebraic theories, which give a concrete syntax for strong monads on functor categories, and are a convenient framework for names and binding. Our programs are built from the key primitives `fork' and `wait'. `Fork' creates a child thread and passes its name (thread ID) to the parent thread. `Wait' allows us to wait for given child threads to finish. We provide a parameterized algebraic theory built from fork and wait, together with basic atomic actions and laws such as associativity of `fork'. Our equational axiomatization is complete in two senses. First, for closed expressions, it completely captures equality of labelled posets (pomsets), an established model of concurrency: model complete. Second, any two open expressions are provably equal if they are equal under all closing substitutions: syntactically complete. The benefit of algebraic effects is that the semantic analysis can focus on the algebraic operations of fork and wait. We then extend the analysis to a simple concurrent programming language by giving operational and denotational semantics. The denotational semantics is built using the methods of parameterized algebraic theories and we show that it is sound, adequate, and fully abstract at first order for labelled-poset observations.

</details>
