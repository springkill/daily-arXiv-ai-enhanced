<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.SC](#cs.SC) [Total: 2]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.LG](#cs.LG) [Total: 47]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.RO](#cs.RO) [Total: 24]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Trajectory-Driven Multi-Product Influence Maximization in Billboard Advertising](https://arxiv.org/abs/2601.14737)
*Dildar Ali,Suman Banerjee,Rajibul Islam*

Main category: cs.DB

TL;DR: 该论文研究户外广告牌优化问题，针对多产品推广场景提出两种变体：共享时隙选择和互斥时隙选择，并分别设计了基于多子模覆盖和采样的近似算法。


<details>
  <summary>Details</summary>
Motivation: 户外广告牌广告需要选择有限数量的广告位来最大化影响力。当商业公司需要推广多个产品且每个产品有特定影响力需求时，现有单产品优化方法不再适用，需要开发新的多产品广告位选择算法。

Method: 针对共享时隙变体，建模为多子模覆盖问题，采用连续贪心框架和随机舍入设计双准则近似算法。针对互斥时隙变体，提出基于采样的近似方法，结合高效的原对偶贪心算法自然保证时隙互斥性。

Result: 在真实世界轨迹和广告牌数据集上的大量实验表明，所提出的解决方案在效果和效率方面表现优异，能够有效满足多产品的影响力需求。

Conclusion: 该研究为多产品户外广告牌优化提供了有效的算法框架，解决了共享时隙和互斥时隙两种场景下的影响力最大化问题，并通过实验验证了方法的实用性和高效性。

Abstract: Billboard Advertising has emerged as an effective out-of-home advertising technique, where the goal is to select a limited number of slots and play advertisement content there, with the hope that it will be observed by many people and, effectively, a significant number of them will be influenced towards the brand. Given a trajectory and a billboard database and a positive integer $k$, how can we select $k$ highly influential slots to maximize influence? In this paper, we study a variant of this problem where a commercial house wants to make a promotion of multiple products, and there is an influence demand for each product. We have studied two variants of the problem. In the first variant, our goal is to select $k$ slots such that the respective influence demand of each product is satisfied. In the other variant of the problem, we are given with $\ell$ integers $k_1,k_2, \ldots, k_{\ell}$, the goal here is to search for $\ell$ many set of slots $S_1, S_2, \ldots, S_{\ell}$ such that for all $i \in [\ell]$, $|S_{i}| \leq k_i$ and for all $i \neq j$, $S_i \cap S_j=\emptyset$ and the influence demand of each of the products gets satisfied. We model the first variant of the problem as a multi-submodular cover problem and the second variant as its generalization. To solve the common-slot variant, we formulate the problem as a multi-submodular cover problem and design a bi-criteria approximation algorithm based on the continuous greedy framework and randomized rounding. For the disjoint-slot variant, we proposed a sampling-based approximation approach along with an efficient primal-dual greedy algorithm that enforces disjointness naturally. Extensive experiments with real-world trajectory and billboard datasets highlight the effectiveness and efficiency of the proposed solution approaches.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [2] [Certified Real Eigenvalue Location](https://arxiv.org/abs/2601.14491)
*Baran Solmaz,Tulay Ayyildiz*

Main category: cs.SC

TL;DR: 提出混合符号数值方法进行实数特征值定位认证，结合Gershgorin圆盘分析和Hermite矩阵认证计算包含实数特征值的认证区间，可通过二分法细化精度。


<details>
  <summary>Details</summary>
Motivation: 实数特征值的位置对物理系统的稳定性和共振特性至关重要，需要可靠的方法进行定位和认证。

Method: 混合符号数值方法：结合Gershgorin圆盘分析和Hermite矩阵认证计算包含实数特征值的认证区间，可通过类似二分法的过程进一步细化精度。

Result: 该方法能够在保持计算效率的同时提供可靠的区间认证，通过完整计算示例验证了框架的有效性。

Conclusion: 提出的混合符号数值方法为实数特征值定位提供了可靠且计算高效的认证框架，结合了符号计算的严谨性和数值计算的高效性。

Abstract: The location of real eigenvalues provides critical insights into the stability and resonance properties of physical systems. This paper presents a hybrid symbolic numeric approach for certified real eigenvalue localization. Our method combines Gershgorin disk analysis with Hermite matrix certification to compute certified intervals that enclose the real eigenvalues. These intervals can be further refined through bisectionlike procedures to achieve the desired precision. The proposed approach delivers reliable interval certifications while preserving computational efficiency. The effectiveness of the framework is demonstrated through a concise, fully worked computational example.

</details>


### [3] [Diagonals and algebraicity modulo $p$: a sharper degree bound](https://arxiv.org/abs/2601.14920)
*Boris Adamczewski,Alin Bostan,Xavier Caruso*

Main category: cs.SC

TL;DR: 本文提供了Deligne定理的新证明，首次给出了对角化多元代数幂级数模p约简的代数度多项式上界


<details>
  <summary>Details</summary>
Motivation: Deligne在1984年证明了多元代数幂级数模p约简的代数性，并猜想其代数度d_p应以多项式速度增长。本文旨在提供该定理的新证明，并首次给出明确合理的多项式上界。

Method: 采用初等证明方法，不依赖复杂理论，构造性地给出了代数度d_p的多项式上界估计。

Result: 成功证明了Deligne定理，并首次给出了代数度d_p的显式多项式上界，该上界具有明确且合理的次数。

Conclusion: 本文通过初等方法不仅重新证明了Deligne定理，还解决了Deligne猜想中关于代数度多项式增长的问题，给出了具体的多项式上界。

Abstract: In 1984, Deligne proved that for any prime number $p$, the reduction modulo $p$ of the diagonal of a multivariate algebraic power series with integer coefficients is algebraic over the field of rational functions with coefficients in $\mathbb F_p$. Moreover, he conjectured that the algebraic degrees $d_p$ of these functions should grow at most polynomially in $p$. In this article, we provide a new and elementary proof of Deligne's theorem, which yields the first general polynomial bound on $d_p$ with an explicit and reasonable degree.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [4] [Guardrails for trust, safety, and ethical development and deployment of Large Language Models (LLM)](https://arxiv.org/abs/2601.14298)
*Anjanava Biswas,Wrick Talukdar*

Main category: cs.CR

TL;DR: 提出一种灵活自适应序列机制，结合信任与安全模块，为LLM开发部署提供安全护栏框架


<details>
  <summary>Details</summary>
Motivation: LLM在带来革命性能力的同时，存在隐私泄露、虚假信息生成、恶意滥用等安全、隐私和伦理风险，需要建立有效的安全护栏机制来确保应用的安全性和伦理性

Method: 提出灵活自适应序列机制，整合信任与安全模块，通过应用实现防止模型滥用的部署框架

Result: 论文提出了一种可用于LLM开发部署的安全护栏实现机制，但摘要中未提供具体的实验结果或性能数据

Conclusion: 为应对LLM的安全风险，需要建立有效的安全护栏框架，提出的灵活自适应序列机制为实现这一目标提供了可行方案

Abstract: The AI era has ushered in Large Language Models (LLM) to the technological forefront, which has been much of the talk in 2023, and is likely to remain as such for many years to come. LLMs are the AI models that are the power house behind generative AI applications such as ChatGPT. These AI models, fueled by vast amounts of data and computational prowess, have unlocked remarkable capabilities, from human-like text generation to assisting with natural language understanding (NLU) tasks. They have quickly become the foundation upon which countless applications and software services are being built, or at least being augmented with. However, as with any groundbreaking innovations, the rise of LLMs brings forth critical safety, privacy, and ethical concerns. These models are found to have a propensity to leak private information, produce false information, and can be coerced into generating content that can be used for nefarious purposes by bad actors, or even by regular users unknowingly. Implementing safeguards and guardrailing techniques is imperative for applications to ensure that the content generated by LLMs are safe, secure, and ethical. Thus, frameworks to deploy mechanisms that prevent misuse of these models via application implementations is imperative. In this study, wepropose a Flexible Adaptive Sequencing mechanism with trust and safety modules, that can be used to implement safety guardrails for the development and deployment of LLMs.

</details>


### [5] [DDSA: Dual-Domain Strategic Attack for Spatial-Temporal Efficiency in Adversarial Robustness Testing](https://arxiv.org/abs/2601.14302)
*Jinwei Hu,Shiyuan Meng,Yi Dong,Xiaowei Huang*

Main category: cs.CR

TL;DR: DDSA框架通过时域选择性和空域精确定位，在资源受限的实时应用中实现高效对抗鲁棒性测试


<details>
  <summary>Details</summary>
Motivation: 资源关键型应用中的图像传输与处理系统面临对抗性扰动威胁，现有鲁棒性测试方法计算资源消耗过大，不适用于需要即时处理大规模图像流的实际部署场景

Method: 提出DDSA（双域战略攻击）框架：1）场景感知触发函数基于类别优先级和模型不确定性识别关键帧；2）使用可解释AI技术定位影响像素区域进行针对性扰动；3）双域方法结合时域选择性和空域精度

Result: 在保持攻击有效性的同时，实现了显著的时空资源节约，使对抗鲁棒性测试能够在资源受限的实时应用中实际部署

Conclusion: DDSA框架为资源关键型应用提供了一种实用高效的对抗鲁棒性测试解决方案，计算效率直接影响任务成功率的实时场景中具有重要应用价值

Abstract: Image transmission and processing systems in resource-critical applications face significant challenges from adversarial perturbations that compromise mission-specific object classification. Current robustness testing methods require excessive computational resources through exhaustive frame-by-frame processing and full-image perturbations, proving impractical for large-scale deployments where massive image streams demand immediate processing. This paper presents DDSA (Dual-Domain Strategic Attack), a resource-efficient adversarial robustness testing framework that optimizes testing through temporal selectivity and spatial precision. We introduce a scenario-aware trigger function that identifies critical frames requiring robustness evaluation based on class priority and model uncertainty, and employ explainable AI techniques to locate influential pixel regions for targeted perturbation. Our dual-domain approach achieves substantial temporal-spatial resource conservation while maintaining attack effectiveness. The framework enables practical deployment of comprehensive adversarial robustness testing in resource-constrained real-time applications where computational efficiency directly impacts mission success.

</details>


### [6] [European digital identity: A missed opportunity?](https://arxiv.org/abs/2601.14503)
*Wouter Termont,Beatriz Esteves*

Main category: cs.CR

TL;DR: 本文批评欧盟数字身份(EUDI)法规及其OpenID架构存在概念狭隘、设计缺陷，无法实现真正的自主身份管理，反而可能导致重新中心化、监控风险增加。


<details>
  <summary>Details</summary>
Motivation: 欧盟数字身份(EUDI)法规及其OpenID架构声称要实现用户控制的身份管理，但作者认为其基于狭隘的身份验证概念，存在设计缺陷，无法实现真正的自主身份(SSI)，反而可能带来重新中心化、监控风险增加等问题。

Method: 通过分析OpenID4VCI和OpenID4VP的技术设计，识别不安全实践、静态凭证类型、有限查询语言等问题；对比现有去中心化替代方案，评估其信任模型；分析法规中可信列表制度的经济和政治风险。

Result: 发现OpenID架构存在多个问题：不安全实践、静态绑定凭证类型、有限查询语言限制应用场景；信任模型相比现有去中心化方案未显著提升控制、隐私和可移植性；可信列表制度可能导致排他性、重新中心化的生态系统。

Conclusion: 建议考虑OAuth UMA扩展及其A4DS配置文件，以及GNAP集成作为技术替代方案；未来需要研究统一的查询(元)语言以解决证明和提供者的异构性问题；法规修订应避免排他性设计，真正实现用户导向的身份管理。

Abstract: Recent European efforts around digital identity -- the EUDI regulation and its OpenID architecture -- aim high, but start from a narrow and ill-defined conceptualization of authentication. Based on a broader, more grounded understanding of the term, in we identify several issues in the design of OpenID4VCI and OpenID4VP: insecure practices, static, and subject-bound credential types, and a limited query language restrict their application to classic scenarios of credential exchange -- already supported by existing solutions like OpenID Connect, SIOPv2, OIDC4IDA, and OIDC Claims Aggregation -- barring dynamic, asynchronous, or automated use cases. We also debunk OpenID's 'paradigm-shifting' trust-model, which -- when compared to existing decentralized alternatives -- does not deliver any significant increase in control, privacy, and portability of personal information. Not only the technical choices limit the capabilities of the EUDI framework; also the legislation itself cannot accommodate the promise of self-sovereign identity. In particular, we criticize the introduction of institutionalized trusted lists, and discuss their economical and political risks. Their potential to decline into an exclusory, re-centralized ecosystem endangers the vision of a user-oriented identity management in which individuals are in charge. Instead, the consequences might severely restrict people in what they can do with their personal information, and risk increased linkability and monitoring. In anticipation of revisions to the EUDI regulations, we suggest several technical alternatives that overcome some of the issues with the architecture of OpenID. In particular, OAuth's UMA extension and its A4DS profile, as well as their integration in GNAP, are worth looking into. Future research into uniform query (meta-)languages is needed to address the heterogeneity of attestations and providers.

</details>


### [7] [Tracing the Data Trail: A Survey of Data Provenance, Transparency and Traceability in LLMs](https://arxiv.org/abs/2601.14311)
*Richard Hohensinner,Belgin Mutlu,Inti Gabriel Mendoza Estrada,Matej Vukovic,Simone Kopeinik,Roman Kern*

Main category: cs.CR

TL;DR: 本文对过去十年大语言模型训练数据生命周期研究进行系统性综述，提出了包含数据来源、透明度、可追溯性三个核心轴和偏见与不确定性、数据隐私、工具与技术三个支撑支柱的分类体系。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型已大规模部署，但其训练数据生命周期仍不透明。本文旨在通过系统性综述，整合过去十年相关研究，建立该领域的分类体系，以促进对LLM训练数据生命周期的理解和管理。

Method: 通过分析95篇出版物，提出包含三个核心轴（数据来源、透明度、可追溯性）和三个支撑支柱（偏见与不确定性、数据隐私、工具与技术）的分类法。系统综述了数据生成、水印、偏见测量、数据整理、数据隐私等关键方法，并分析了透明度与不透明性之间的权衡关系。

Result: 建立了该领域的分类体系，明确了各研究领域的定义和对应的人工制品。识别了数据生成、水印、偏见测量、数据整理、数据隐私等关键方法学，并揭示了透明度与不透明性之间的固有权衡关系。

Conclusion: 本文通过系统性综述提出了大语言模型训练数据生命周期研究的分类体系，为该领域提供了结构化的理解框架。分类法有助于组织现有研究，指导未来研究方向，并促进对LLM训练数据生命周期的透明度和可追溯性的改进。

Abstract: Large language models (LLMs) are deployed at scale, yet their training data life cycle remains opaque. This survey synthesizes research from the past ten years on three tightly coupled axes: (1) data provenance, (2) transparency, and (3) traceability, and three supporting pillars: (4) bias \& uncertainty, (5) data privacy, and (6) tools and techniques that operationalize them. A central contribution is a proposed taxonomy defining the field's domains and listing corresponding artifacts. Through analysis of 95 publications, this work identifies key methodologies concerning data generation, watermarking, bias measurement, data curation, data privacy, and the inherent trade-off between transparency and opacity.

</details>


### [8] [SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2601.14323)
*Bingxin Xu,Yuzhang Shang,Binghui Wang,Emilio Ferrara*

Main category: cs.CR

TL;DR: 论文提出SILENTDRIFT攻击方法，利用VLA模型的动作分块和增量位姿表示机制中的安全漏洞，通过构造C2连续扰动实现隐蔽的黑盒后门攻击，在低投毒率下达到高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现代VLA模型在安全关键机器人应用中部署日益增多，但其安全漏洞研究不足。作者发现VLA系统中动作分块与增量位姿表示的结合会产生帧内视觉开环机制，使机器人执行K步动作序列，导致逐步扰动通过积分累积，形成安全漏洞。

Method: 提出SILENTDRIFT攻击方法：1) 利用Smootherstep函数构造具有C2连续性的扰动，确保轨迹边界处速度和加速度为零，满足严格的运动学一致性约束；2) 采用关键帧攻击策略，仅毒化关键的接近阶段，最大化影响同时最小化触发暴露；3) 生成的毒化轨迹在视觉上与成功演示无法区分。

Result: 在LIBERO基准测试中，SILENTDRIFT达到93.2%的攻击成功率，投毒率低于2%，同时保持95.3%的清洁任务成功率。攻击生成的轨迹在视觉上无法与正常轨迹区分。

Conclusion: VLA模型存在由动作分块和增量位姿表示机制引入的安全漏洞，SILENTDRIFT攻击方法成功利用该漏洞实现隐蔽高效的后门攻击，揭示了VLA系统在实际部署中需要考虑的安全风险。

Abstract: Vision-Language-Action (VLA) models are increasingly deployed in safety-critical robotic applications, yet their security vulnerabilities remain underexplored. We identify a fundamental security flaw in modern VLA systems: the combination of action chunking and delta pose representations creates an intra-chunk visual open-loop. This mechanism forces the robot to execute K-step action sequences, allowing per-step perturbations to accumulate through integration. We propose SILENTDRIFT, a stealthy black-box backdoor attack exploiting this vulnerability. Our method employs the Smootherstep function to construct perturbations with guaranteed C2 continuity, ensuring zero velocity and acceleration at trajectory boundaries to satisfy strict kinematic consistency constraints. Furthermore, our keyframe attack strategy selectively poisons only the critical approach phase, maximizing impact while minimizing trigger exposure. The resulting poisoned trajectories are visually indistinguishable from successful demonstrations. Evaluated on the LIBERO, SILENTDRIFT achieves a 93.2% Attack Success Rate with a poisoning rate under 2%, while maintaining a 95.3% Clean Task Success Rate.

</details>


### [9] [Turn-Based Structural Triggers: Prompt-Free Backdoors in Multi-Turn LLMs](https://arxiv.org/abs/2601.14340)
*Yiyang Lu,Jinwen He,Yue Zhao,Kai Chen,Ruigang Liang*

Main category: cs.CR

TL;DR: 本文提出了一种基于对话结构的后门攻击方法TST，利用对话轮次索引作为触发器，在多轮对话LLM系统中实现高效攻击，平均攻击成功率99.52%，且能绕过现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在对话系统和任务导向助手等交互系统中的广泛应用，供应链风险日益凸显。现有后门攻击和防御主要关注用户可见的提示触发器，忽视了多轮对话中的结构信号，因此需要研究基于对话结构的攻击面。

Method: 提出Turn-based Structural Trigger (TST)后门攻击方法，使用对话轮次索引作为触发器，与用户输入内容无关。该方法通过对话结构激活后门，在四个广泛使用的开源LLM模型上进行验证。

Result: TST在四个开源LLM模型上平均攻击成功率(ASR)达99.52%，且效用损失最小。在五种代表性防御机制下仍保持平均98.04%的ASR。在不同指令数据集上泛化良好，平均ASR为99.19%。

Conclusion: 对话结构构成了多轮LLM系统重要且未被充分研究的攻击面，需要在实际应用中采用结构感知的审计和缓解措施。TST攻击的成功表明仅依赖提示中心的防御是不够的。

Abstract: Large Language Models (LLMs) are widely integrated into interactive systems such as dialogue agents and task-oriented assistants. This growing ecosystem also raises supply-chain risks, where adversaries can distribute poisoned models that degrade downstream reliability and user trust. Existing backdoor attacks and defenses are largely prompt-centric, focusing on user-visible triggers while overlooking structural signals in multi-turn conversations. We propose Turn-based Structural Trigger (TST), a backdoor attack that activates from dialogue structure, using the turn index as the trigger and remaining independent of user inputs. Across four widely used open-source LLM models, TST achieves an average attack success rate (ASR) of 99.52% with minimal utility degradation, and remains effective under five representative defenses with an average ASR of 98.04%. The attack also generalizes well across instruction datasets, maintaining an average ASR of 99.19%. Our results suggest that dialogue structure constitutes an important and under-studied attack surface for multi-turn LLM systems, motivating structure-aware auditing and mitigation in practice.

</details>


### [10] [A Survey of Security Challenges and Solutions for Advanced Air Mobility and eVTOL Aircraft](https://arxiv.org/abs/2601.14415)
*Mahyar Ghazanfari,Iman Sharifi,Peng Wei,Noah Dahle,Abel Diaz Gonzalez,Austin Coursey,Bryce Bjorkman,Cailani Lemieux-Mack,Robert Canady,Abenezer Taye,Bryan C. Ward,Xenofon Koutsoukos,Gautam Biswas,Maheed H. Ahmed,Hyeong Tae Kim,Mahsa Ghasemi,Vijay Gupta,Filippos Fotiadis,Ufuk Topcu,Junchi Lu,Alfred Chen,Abdul Kareem Ras,Nischal Aryal,Amer Ibrahim,Amir Shirkhodaie,Heber Herencia-Zapana,Saqib Hasan,Isaac Amundson*

Main category: cs.CR

TL;DR: 本文综述了先进空中交通（AAM）系统（特别是电动垂直起降eVTOL飞机）的安全漏洞与防御机制，提出了攻击分类、分析了缓解策略，并设计了面向未来AAM生态的安全系统架构。


<details>
  <summary>Details</summary>
Motivation: 随着先进空中交通（AAM）系统特别是eVTOL飞机的发展，其自动化、连接性和电子化程度提高，带来了新的安全威胁。需要系统性地分析这些威胁并设计相应的防御机制，以确保未来空中交通系统的安全性。

Method: 基于商业航空电子设备和自动化无人机系统的现有漏洞，采用调查分析方法：1）提出攻击分类法；2）分析缓解策略；3）设计专门针对未来AAM生态系统的安全系统架构；4）识别关键威胁向量；5）描述新兴防御技术。

Result: 识别了AAM系统的关键威胁向量，包括GPS干扰/欺骗、ATC无线电频率滥用、TCAS和ADS-B攻击、通过电子飞行包（EFB）的后门、飞机自动化和连接性引入的新漏洞、飞行管理系统软件、数据库和云服务风险。提出了相应的防御技术和安全架构方案。

Conclusion: AAM系统面临多种安全威胁，需要专门的安全架构和防御机制。论文为未来AAM生态系统提供了全面的安全分析框架，并指出了需要进一步解决的技术问题以改进防御机制。

Abstract: This survey reviews the existing and envisioned security vulnerabilities and defense mechanisms relevant to Advanced Air Mobility (AAM) systems, with a focus on electric vertical takeoff and landing (eVTOL) aircraft. Drawing from vulnerabilities in the avionics in commercial aviation and the automated unmanned aerial systems (UAS), the paper presents a taxonomy of attacks, analyzes mitigation strategies, and proposes a secure system architecture tailored to the future AAM ecosystem. The paper also highlights key threat vectors, including Global Positioning System (GPS) jamming/spoofing, ATC radio frequency misuse, attacks on TCAS and ADS-B, possible backdoor via Electronic Flight Bag (EFB), new vulnerabilities introduced by aircraft automation and connectivity, and risks from flight management system (FMS) software, database and cloud services. Finally, this paper describes emerging defense techniques against these attacks, and open technical problems to address toward better defense mechanisms.

</details>


### [11] [LLM Security and Safety: Insights from Homotopy-Inspired Prompt Obfuscation](https://arxiv.org/abs/2601.14528)
*Luis Lazo,Hamed Jelodar,Roozbeh Razavi-Far*

Main category: cs.CR

TL;DR: 提出基于同伦理论的提示词混淆框架，用于增强对大型语言模型安全漏洞的理解，通过系统化应用精心设计的提示词来影响模型潜在行为。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的安全性和安全性漏洞，揭示当前防护措施的不足，推动更强大的防御机制、可靠的检测策略和改善的韧性。

Method: 采用同伦理论启发的提示词混淆框架，系统化应用精心设计的提示词，在LLama、Deepseek、KIMI（代码生成）和Claude等模型上进行实验，涵盖15,732个提示词（包括10,000个高优先级案例）。

Result: 实验结果揭示了当前LLM防护措施的关键见解，展示了潜在模型行为如何以意外方式被影响，突显了现有安全机制的脆弱性。

Conclusion: 该工作为分析和缓解潜在弱点提供了原则性框架，目标是推进安全、负责任和可信赖的AI技术发展，强调需要更强大的防御机制和改善的模型韧性。

Abstract: In this study, we propose a homotopy-inspired prompt obfuscation framework to enhance understanding of security and safety vulnerabilities in Large Language Models (LLMs). By systematically applying carefully engineered prompts, we demonstrate how latent model behaviors can be influenced in unexpected ways. Our experiments encompassed 15,732 prompts, including 10,000 high-priority cases, across LLama, Deepseek, KIMI for code generation, and Claude to verify. The results reveal critical insights into current LLM safeguards, highlighting the need for more robust defense mechanisms, reliable detection strategies, and improved resilience. Importantly, this work provides a principled framework for analyzing and mitigating potential weaknesses, with the goal of advancing safe, responsible, and trustworthy AI technologies.

</details>


### [12] [Automatically Tightening Access Control Policies with Restricter](https://arxiv.org/abs/2601.14582)
*Ka Lok Wu,Christa Jenkins,Scott D. Stoller,Omar Chowdhury*

Main category: cs.CR

TL;DR: Restricter：一种基于访问日志自动收紧访问控制策略的工具，可在不牺牲系统功能的前提下减少策略规则允许的访问请求数量


<details>
  <summary>Details</summary>
Motivation: 访问控制策略的有效性取决于其强制执行的政策质量。然而，即使是经验丰富的管理员也难以编写满足最小权限原则等属性的有效策略，现实中的许多策略配置错误实例证明了这一挑战。

Method: 提出Restricter工具，通过分析访问日志（包含已执行的访问请求及其对应的允许/拒绝决策）自动收紧每个（允许）策略规则。该方法在不牺牲被监管系统功能的前提下，减少策略规则允许的访问请求数量。

Result: 实现了针对Amazon Cedar策略语言的Restricter，并通过两个现实案例研究证明了其有效性。

Conclusion: Restricter能够有效解决策略配置难题，通过自动收紧策略规则来增强访问控制的安全性，同时保持系统功能完整性。

Abstract: Robust access control is a cornerstone of secure software, systems, and networks. An access control mechanism is as effective as the policy it enforces. However, authoring effective policies that satisfy desired properties such as the principle of least privilege is a challenging task even for experienced administrators, as evidenced by many real instances of policy misconfiguration. In this paper, we set out to address this pain point by proposing Restricter, which automatically tightens each (permit) policy rule of a policy with respect to an access log, which captures some already exercised access requests and their corresponding access decisions (i.e., allow or deny). Restricter achieves policy tightening by reducing the number of access requests permitted by a policy rule without sacrificing the functionality of the underlying system it is regulating. We implement Restricter for Amazon's Cedar policy language and demonstrate its effectiveness through two realistic case studies.

</details>


### [13] [IntelliSA: An Intelligent Static Analyzer for IaC Security Smell Detection Using Symbolic Rules and Neural Inference](https://arxiv.org/abs/2601.14595)
*Qiyue Mei,Michael Fu*

Main category: cs.CR

TL;DR: IntelliSA：结合符号规则与神经推理的IaC安全异味检测智能静态分析器，通过知识蒸馏训练紧凑学生模型，在保持高检测率的同时大幅减少误报


<details>
  <summary>Details</summary>
Motivation: IaC脚本中的安全配置错误可能广泛传播导致严重系统故障和安全风险。现有基于符号规则的静态分析器存在过度近似问题，产生大量误报，增加了人工检查负担。同时，直接使用LLM进行过滤虽然有效，但存在成本高、延迟大、数据治理问题以及离线部署限制等挑战。

Method: 提出IntelliSA智能静态分析器，采用两阶段方法：1) 应用符号规则进行过度近似检测，确保广泛覆盖；2) 使用神经推理过滤误报。为避免依赖LLM API的缺点，采用知识蒸馏方法：使用LLM教师生成伪标签，训练一个比教师模型小500倍以上的紧凑学生模型，学习教师知识并高效分类误报。

Result: 在包含11,814行真实世界IaC代码和241个安全异味的标记数据集上评估，IntelliSA达到最高F1分数（83%），比基线方法高出7-42%。在成本效益方面表现最佳，仅检查不到2%的代码库即可检测60%的安全异味。

Conclusion: IntelliSA通过结合符号规则的广泛覆盖和神经推理的精确过滤，有效解决了IaC安全异味检测中的误报问题。知识蒸馏方法使得系统能够在保持高性能的同时实现低成本、低延迟、可复现和离线部署，为IaC安全分析提供了实用且高效的解决方案。

Abstract: Infrastructure as Code (IaC) enables automated provisioning of large-scale cloud and on-premise environments, reducing the need for repetitive manual setup. However, this automation is a double-edged sword: a single misconfiguration in IaC scripts can propagate widely, leading to severe system downtime and security risks. Prior studies have shown that IaC scripts often contain security smells--bad coding patterns that may introduce vulnerabilities--and have proposed static analyzers based on symbolic rules to detect them. Yet, our preliminary analysis reveals that rule-based detection alone tends to over-approximate, producing excessive false positives and increasing the burden of manual inspection. In this paper, we present IntelliSA, an intelligent static analyzer for IaC security smell detection that integrates symbolic rules with neural inference. IntelliSA applies symbolic rules to over-approximate potential smells for broad coverage, then employs neural inference to filter false positives. While an LLM can effectively perform this filtering, reliance on LLM APIs introduces high cost and latency, raises data governance concerns, and limits reproducibility and offline deployment. To address the challenges, we adopt a knowledge distillation approach: an LLM teacher generates pseudo-labels to train a compact student model--over 500x smaller--that learns from the teacher's knowledge and efficiently classifies false positives. We evaluate IntelliSA against two static analyzers and three LLM baselines (Claude-4, Grok-4, and GPT-5) using a human-labeled dataset including 241 security smells across 11,814 lines of real-world IaC code. Experimental results show that IntelliSA achieves the highest F1 score (83%), outperforming baselines by 7-42%. Moreover, IntelliSA demonstrates the best cost-effectiveness, detecting 60% of security smells while inspecting less than 2% of the codebase.

</details>


### [14] [STEAD: Robust Provably Secure Linguistic Steganography with Diffusion Language Model](https://arxiv.org/abs/2601.14778)
*Yuang Qi,Na Zhao,Qiyi Yao,Benlong Wu,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: 提出一种基于扩散语言模型的鲁棒可证明安全语言隐写方法，能够抵抗主动篡改攻击


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归语言模型的PSLS方法在文本被篡改时会产生严重错误传播，无法抵抗主动篡改攻击

Method: 使用扩散语言模型进行部分并行文本生成，结合纠错码找到鲁棒的隐写嵌入位置，并引入伪随机纠错和邻域搜索纠错策略

Result: 理论证明和实验结果表明该方法安全且鲁棒，能够抵抗隐写文本分割中的标记歧义，并在一定程度上抵抗插入、删除和替换等标记级攻击

Conclusion: 基于扩散语言模型的鲁棒可证明安全语言隐写方法能够有效解决现有ARM-based PSLS方法在主动篡改攻击下的脆弱性问题

Abstract: Recent provably secure linguistic steganography (PSLS) methods rely on mainstream autoregressive language models (ARMs) to address historically challenging tasks, that is, to disguise covert communication as ``innocuous'' natural language communication. However, due to the characteristic of sequential generation of ARMs, the stegotext generated by ARM-based PSLS methods will produce serious error propagation once it changes, making existing methods unavailable under an active tampering attack. To address this, we propose a robust, provably secure linguistic steganography with diffusion language models (DLMs). Unlike ARMs, DLMs can generate text in a partially parallel manner, allowing us to find robust positions for steganographic embedding that can be combined with error-correcting codes. Furthermore, we introduce error correction strategies, including pseudo-random error correction and neighborhood search correction, during steganographic extraction. Theoretical proof and experimental results demonstrate that our method is secure and robust. It can resist token ambiguity in stegotext segmentation and, to some extent, withstand token-level attacks of insertion, deletion, and substitution.

</details>


### [15] [On Implementing Hybrid Post-Quantum End-to-End Encryption](https://arxiv.org/abs/2601.14926)
*Aditi Gandhi,Aakankshya Das,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: 实现了一个结合经典密码学与后量子密码学的混合端到端加密系统，使用CRYSTALS-Kyber进行量子安全密钥交换，AES-256-GCM进行对称加密，SHA-256进行密钥派生，采用零信任架构。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现对当前公钥密码系统构成根本性威胁，需要在所有应用中过渡到量子抗性密码学替代方案。

Method: 实现了一个实用的混合端到端加密系统，结合经典和后量子密码学原语。采用CRYSTALS-Kyber（NIST标准化的基于格的密钥封装机制）进行量子安全密钥交换，AES-256-GCM进行高效认证对称加密，SHA-256进行确定性密钥派生。架构遵循零信任模型，中继服务器仅促进通信而不访问明文消息或加密密钥，所有加密解密操作仅在客户端端点进行。

Result: 系统证明NIST标准化的后量子密码学可以有效地集成到实际消息系统中，具有可接受的性能特征，能够同时防御经典和量子攻击者。

Conclusion: 该工作展示了后量子密码学在实际通信系统中的可行性和实用性，提供了开源实现以促进可重复性和进一步研究。

Abstract: The emergence of quantum computing poses a fundamental threat to current public key cryptographic systems. This threat is necessitating a transition to quantum resistant cryptographic alternatives in all the applications. In this work, we present the implementation of a practical hybrid end-to-end encryption system that combines classical and post-quantum cryptographic primitives to achieve both security and efficiency. Our system employs CRYSTALS-Kyber, a NIST-standardized lattice-based key encapsulation mechanism, for quantum-safe key exchange, coupled with AES-256-GCM for efficient authenticated symmetric encryption and SHA-256 for deterministic key derivation. The architecture follows a zero-trust model where a relay server facilitates communication without accessing plaintext messages or cryptographic keys. All encryption and decryption operations occur exclusively at client endpoints. The system demonstrates that NIST standardized post-quantum cryptography can be effectively integrated into practical messaging systems with acceptable performance characteristics, offering protection against both classical and quantum adversaries. As our focus is on implementation rather than on novelty, we also provide an open-source implementation to facilitate reproducibility and further research in post quantum secure communication systems.

</details>


### [16] [On the Effectiveness of Mempool-based Transaction Auditing](https://arxiv.org/abs/2601.14996)
*Jannik Albrecht,Ghassan Karame*

Main category: cs.CR

TL;DR: 论文分析表明，现有mempool审计方案在检测比特币和以太坊中的交易操纵攻击时存在局限性，可能导致超过25%的误指控率，但特定条件下（交易间隔≥30秒且被所有观察者一致接收）可实现99.9%的成功审计。


<details>
  <summary>Details</summary>
Motivation: 现有区块链防御交易操纵攻击的方案未被主流区块链（比特币、以太坊、Cardano）集成，用户社区依赖Mempool.space等临时性mempool审计方案来检测审查和交易置换攻击。需要首次系统分析mempool审计在检测恶意矿工攻击方面的实际效果。

Method: 首次对比特币和以太坊中mempool审计与检测审查和交易置换攻击能力之间的相互作用进行精确分析，评估审计方案在不同条件下的表现。

Result: 分析显示：1) mempool审计在某些设置下可能导致超过25%的误指控率；2) 如果两个交易被所有观察者一致接收且发送时间间隔至少30秒，审计方案能以99.9%的高概率成功审计任何两个交易的执行；3) 批量顺序公平排序方案在现实部署中只能为有限交易子集提供强公平性保证。

Conclusion: mempool审计方案存在显著局限性，可能导致高误指控率，但在特定条件下可提供强审计保证。批量顺序公平排序方案在现实部署中的公平性保证有限，需要更精确的评估和改进方案。

Abstract: While the literature features a number of proposals to defend against transaction manipulation attacks, existing proposals are still not integrated within large blockchains, such as Bitcoin, Ethereum, and Cardano. Instead, the user community opted to rely on more practical but ad-hoc solutions (such as Mempool.space) that aim at detecting censorship and transaction displacement attacks by auditing discrepancies in the mempools of so-called observers.
  In this paper, we precisely analyze, for the first time, the interplay between mempool auditing and the ability to detect censorship and transaction displacement attacks by malicious miners in Bitcoin and Ethereum. Our analysis shows that mempool auditing can result in mis-accusations against miners with a probability larger than 25% in some settings. On a positive note, however, we show that mempool auditing schemes can successfully audit the execution of any two transactions (with an overwhelming probability of 99.9%) if they are consistently received by all observers and sent at least 30 seconds apart from each other. As a direct consequence, our findings show, for the first time, that batch-order fair-ordering schemes can offer only strong fairness guarantees for a limited subset of transactions in real-world deployments.

</details>


### [17] [SpooFL: Spoofing Federated Learning](https://arxiv.org/abs/2601.15055)
*Isaac Baglin,Xiatian Zhu,Simon Hadfield*

Main category: cs.CR

TL;DR: SpooFL：一种基于欺骗的联邦学习防御方法，通过生成与私有数据无关的合成样本来误导攻击者，防止数据泄露


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习防御方法（如噪声添加、变换或加密）主要关注混淆，但仍会泄露高级信息（如类别分布或特征表示），且容易被强大的去噪攻击破解。需要从根本上不同的防御视角。

Method: 提出SpooFL框架，将联邦学习防御重新定义为欺骗问题。使用在外部数据集上训练的最先进生成模型（与私有数据无类别重叠），生成看似合理但完全无关的合成样本，误导攻击者认为已恢复真实训练数据。

Result: SpooFL成功误导攻击者，防止有意义的数据泄露，同时保持联邦学习训练完整性。与最先进的深度泄露防御方法相比，在不显著影响模型性能的情况下有效防御攻击。

Conclusion: SpooFL提供了一种新颖的欺骗式防御范式，从根本上改变了联邦学习防御的思路，通过生成无关合成数据有效防止数据泄露，同时保持模型性能。

Abstract: Traditional defenses against Deep Leakage (DL) attacks in Federated Learning (FL) primarily focus on obfuscation, introducing noise, transformations or encryption to degrade an attacker's ability to reconstruct private data. While effective to some extent, these methods often still leak high-level information such as class distributions or feature representations, and are frequently broken by increasingly powerful denoising attacks. We propose a fundamentally different perspective on FL defense: framing it as a spoofing problem.We introduce SpooFL (Figure 1), a spoofing-based defense that deceives attackers into believing they have recovered the true training data, while actually providing convincing but entirely synthetic samples from an unrelated task. Unlike prior synthetic-data defenses that share classes or distributions with the private data and thus still leak semantic information, SpooFL uses a state-of-the-art generative model trained on an external dataset with no class overlap. As a result, attackers are misled into recovering plausible yet completely irrelevant samples, preventing meaningful data leakage while preserving FL training integrity. We implement the first example of such a spoofing defense, and evaluate our method against state-of-the-art DL defenses and demonstrate that it successfully misdirects attackers without compromising model performance significantly.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [18] [A full process algebraic representation of Ant Colony Optimization](https://arxiv.org/abs/2601.14436)
*Maria Garcia,Natalia Lopez,Ismael Rodriguez*

Main category: cs.NE

TL;DR: 提出PA²CO过程代数，用于形式化描述并行化蚁群优化算法的详细规范


<details>
  <summary>Details</summary>
Motivation: 需要一种形式化方法来详细规范并行化蚁群优化算法，以便精确描述不同并行实现方案

Method: 基于三种ACO算法（蚁群系统、最大最小蚁群系统、蚁群系统）构建PA²CO过程代数，用于形式化表示具有不同并行方案的实现，包括细粒度和粗粒度规范

Result: 开发了PA²CO过程代数，能够形式化表示多种并行实现类型，包括在不同系统粒度级别利用并行执行的细粒度和粗粒度规范

Conclusion: PA²CO过程代数为并行化蚁群优化算法提供了详细的形式化规范框架，有助于精确描述和分析不同并行实现方案

Abstract: We present a process algebra capable of specifying parallelized Ant Colony Optimization algorithms in full detail: PA$^2$CO. After explaining the basis of three different ACO algorithms (Ant System, MAX-MIN Ant System, and Ant Colony System), we formally define PA$^2$CO and use it for representing several types of implementations with different parallel schemes. In particular fine-grained and coarse-grained specifications, each one taking advantage of parallel executions at different levels of system granularity, are formalized.

</details>


### [19] [Proximal Policy Optimization with Evolutionary Mutations](https://arxiv.org/abs/2601.14705)
*Casimir Czworkowski,Stephen Hornish,Alhassan S. Yasin*

Main category: cs.NE

TL;DR: POEM通过引入进化算法中的自适应变异机制来增强PPO的探索能力，当策略变化停滞时触发参数变异，在多个强化学习环境中显著提升了PPO的性能。


<details>
  <summary>Details</summary>
Motivation: PPO算法虽然稳定高效，但存在探索不足和过早收敛的问题。作者旨在通过引入进化算法思想来增强PPO的探索能力，解决探索-利用权衡问题。

Method: 提出POEM算法，在PPO基础上添加自适应探索机制：监控当前策略与历史策略移动平均之间的KL散度，当策略变化停滞时触发策略参数的自适应变异，促进探索。使用贝叶斯优化进行超参数调优。

Result: 在四个OpenAI Gym环境中测试：BipedalWalker(t=-2.0642, p=0.0495)、CarRacing(t=-6.3987, p=0.0002)、MountainCar(t=-6.2431, p<0.0001)三个任务上POEM显著优于PPO，LunarLander无统计显著性差异(t=-1.8707, p=0.0778)。

Conclusion: 将进化算法原理融入策略梯度方法可以有效克服探索-利用权衡问题，POEM通过自适应变异机制显著提升了PPO在多个强化学习任务中的性能。

Abstract: Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm known for its stability and sample efficiency, but it often suffers from premature convergence due to limited exploration. In this paper, we propose POEM (Proximal Policy Optimization with Evolutionary Mutations), a novel modification to PPO that introduces an adaptive exploration mechanism inspired by evolutionary algorithms. POEM enhances policy diversity by monitoring the Kullback-Leibler (KL) divergence between the current policy and a moving average of previous policies. When policy changes become minimal, indicating stagnation, POEM triggers an adaptive mutation of policy parameters to promote exploration. We evaluate POEM on four OpenAI Gym environments: CarRacing, MountainCar, BipedalWalker, and LunarLander. Through extensive fine-tuning using Bayesian optimization techniques and statistical testing using Welch's t-test, we find that POEM significantly outperforms PPO on three of the four tasks (BipedalWalker: t=-2.0642, p=0.0495; CarRacing: t=-6.3987, p=0.0002; MountainCar: t=-6.2431, p<0.0001), while performance on LunarLander is not statistically significant (t=-1.8707, p=0.0778). Our results highlight the potential of integrating evolutionary principles into policy gradient methods to overcome exploration-exploitation tradeoffs.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [20] [Analog-to-Stochastic Converter Using Magnetic Tunnel Junction Devices for Vision Chips](https://arxiv.org/abs/2601.14640)
*Naoya Onizawa,Daisaku Katagiri,Warren J. Gross,Takahiro Hanyu*

Main category: cs.ET

TL;DR: 提出一种基于磁性隧道结（MTJ）器件的模拟-随机转换器，用于基于随机计算的视觉芯片，实现一步式高效转换


<details>
  <summary>Details</summary>
Motivation: 随机计算在硬件实现中具有面积效率优势，但传统模拟-随机信号转换需要模拟-数字和数字-随机两步转换，功耗和面积开销大。需要开发一步式转换方案以降低信号转换开销。

Method: 利用MTJ器件固有的概率性开关行为（在两个电阻状态间切换），实现模拟信号到随机信号的一步直接转换。考虑MTJ器件的电阻变异性并补偿其对信号转换的影响。基于理论分析，在90nm CMOS和100nm MTJ技术中设计转换器，并使用支持晶体管和MTJ器件的SPICE仿真器（NS-SPICE）进行验证。

Result: 理论上描述了模拟-随机信号转换特性，并使用器件和电路参数评估了转换特性。设计了考虑电阻变异性的转换器，并通过SPICE仿真验证了设计可行性。

Conclusion: 基于MTJ器件的模拟-随机转换器能够实现一步式高效转换，显著降低传统两步转换方案的开销，为基于随机计算的视觉芯片提供有效的信号转换解决方案。

Abstract: This paper introduces an analog-to-stochastic converter using a magnetic tunnel junction (MTJ) device for vision chips based on stochastic computation. Stochastic computation has been recently exploited for area-efficient hardware implementation, such as low-density parity-check (LDPC) decoders and image processors. However, power-and-area hungry two-step (analog-to-digital and digital-to-stochastic) converters are required for the analog to stochastic signal conversion. To realize a one-step conversion, an MTJ device is used as it inherently exhibits a probabilistic switching behavior between two resistance states. Exploiting the device-based probabilistic behavior, analog signals can be directly and area-efficiently converted to stochastic signals to mitigate the signal-conversion overhead. The analog-to-stochastic signal conversion is theoretically described and the conversion characteristic is evaluated using device and circuit parameters. In addition, the resistance variability of the MTJ device is considered in order to compensate the variability effect on the signal conversion. Based on the theoretical analysis, the analog-to-stochastic converter is designed in 90nm CMOS and 100nm MTJ technologies and is verified using a SPICE simulator (NS-SPICE) that handles both transistors and MTJ devices.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [GCG Attack On A Diffusion LLM](https://arxiv.org/abs/2601.14266)
*Ruben Neyroud,Sam Corley*

Main category: cs.LG

TL;DR: 探索GCG风格对抗提示攻击在扩散语言模型LLaDA上的适用性，评估多种攻击变体对有害提示的影响


<details>
  <summary>Details</summary>
Motivation: 虽然大多数LLM是自回归的，但基于扩散的LLM最近成为替代生成方法。GCG攻击对自回归模型有效，但其在扩散语言模型上的适用性尚未充分探索

Method: 对开源扩散LLM LLaDA进行GCG风格对抗提示攻击的探索性研究，评估包括前缀扰动和后缀对抗生成在内的多种攻击变体，使用AdvBench数据集中的有害提示

Result: 提供了关于扩散语言模型鲁棒性和攻击面的初步见解，表明需要为这种设置开发替代优化和评估策略

Conclusion: 扩散语言模型对GCG风格攻击的脆弱性需要进一步研究，需要开发专门针对扩散模型的对抗分析优化和评估方法

Abstract: While most LLMs are autoregressive, diffusion-based LLMs have recently emerged as an alternative method for generation. Greedy Coordinate Gradient (GCG) attacks have proven effective against autoregressive models, but their applicability to diffusion language models remains largely unexplored. In this work, we present an exploratory study of GCG-style adversarial prompt attacks on LLaDA (Large Language Diffusion with mAsking), an open-source diffusion LLM. We evaluate multiple attack variants, including prefix perturbations and suffix-based adversarial generation, on harmful prompts drawn from the AdvBench dataset. Our study provides initial insights into the robustness and attack surface of diffusion language models and motivates the development of alternative optimization and evaluation strategies for adversarial analysis in this setting.

</details>


### [22] [From Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps](https://arxiv.org/abs/2601.14848)
*Mohamed Abouras,Catherine M. Elias*

Main category: cs.LG

TL;DR: 该研究使用多层LSTM架构预测高速公路匝道区域的车辆行为，发现匝道区域预测准确率（76%）显著低于直道区域（94%），但4秒内预测效果良好。


<details>
  <summary>Details</summary>
Motivation: 高速公路匝道区域是研究不足但交通行为变化较大的路段，预测这些区域的车辆行为可以减少不确定性并提高道路安全性。

Method: 使用多层LSTM架构训练匝道区域模型，基于ExiD无人机数据集，测试不同预测时间范围和模型工作流程。

Result: 在4秒预测范围内表现良好，匝道区域最大预测准确率约76%，而普通高速公路场景可达94%，显示匝道区域预测更具挑战性。

Conclusion: 匝道区域与直道高速公路存在显著差异，LSTM模型在匝道区域预测中有效但准确率较低，未来需改进模型以应对匝道区域的复杂性。

Abstract: On and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles' behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models' workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.

</details>


### [23] [Quality or Quantity? Error-Informed Selective Online Learning with Gaussian Processes in Multi-Agent Systems: Extended Version](https://arxiv.org/abs/2601.14275)
*Zewen Yang,Xiaobing Dai,Jiajun Cheng,Yulong Huang,Peng Shi*

Main category: cs.LG

TL;DR: 提出首个分布式高斯过程回归的选择性在线学习框架EIGP，通过评估邻居模型质量进行选择性合作，优先质量而非数量，并引入加速和自适应算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 在分布式多智能体系统的机器学习中，盲目包含所有智能体模型进行联合预测是不合理的。需要优先考虑模型质量而非数量，因为模型数量与质量之间的平衡对合作学习至关重要。

Method: 提出分布式误差感知高斯过程(EIGP)框架：1) 每个智能体评估邻居模型质量；2) 使用选择函数选择预测误差较小的高质量GP模型；3) 嵌入贪婪算法(gEIGP)加速预测；4) 嵌入自适应算法(aEIGP)提高预测精度；5) 结合误差感知量化项迭代和数据删除策略实现快速预测和模型更新。

Result: 数值模拟表明该方法优于现有分布式GP方法，在不同基准测试中展现出优越性能，实现了实时学习操作。

Conclusion: EIGP框架通过选择性合作机制，在分布式高斯过程回归中实现了质量优先于数量的有效合作，为多智能体系统的在线学习提供了高效解决方案。

Abstract: Effective cooperation is pivotal in distributed learning for multi-agent systems, where the interplay between the quantity and quality of the machine learning models is crucial. This paper reveals the irrationality of indiscriminate inclusion of all models on agents for joint prediction, highlighting the imperative to prioritize quality over quantity in cooperative learning. Specifically, we present the first selective online learning framework for distributed Gaussian process (GP) regression, namely distributed error-informed GP (EIGP), that enables each agent to assess its neighboring collaborators, using the proposed selection function to choose the higher quality GP models with less prediction errors. Moreover, algorithmic enhancements are embedded within the EIGP, including a greedy algorithm (gEIGP) for accelerating prediction and an adaptive algorithm (aEIGP) for improving prediction accuracy. In addition, approaches for fast prediction and model update are introduced in conjunction with the error-informed quantification term iteration and a data deletion strategy to achieve real-time learning operations. Numerical simulations are performed to demonstrate the effectiveness of the developed methodology, showcasing its superiority over the state-of-the-art distributed GP methods with different benchmarks.

</details>


### [24] [Which Quantization Should I Use? A Unified Evaluation of llama.cpp Quantization on Llama-3.1-8B-Instruct](https://arxiv.org/abs/2601.14277)
*Uygar Kurt*

Main category: cs.LG

TL;DR: 对llama.cpp量化方案进行统一实证研究，评估Llama-3.1-8B-Instruct模型在3-8位K-quant和传统格式下的性能表现，为实际部署提供选择指南


<details>
  <summary>Details</summary>
Motivation: 量化技术能降低大语言模型的部署门槛，但现有量化格式评估不一致，难以选择合适的方案。llama.cpp虽然能让大模型在普通硬件上运行，但缺乏系统性的量化方案比较研究

Method: 对Llama-3.1-8B-Instruct模型（FP16，GGUF格式）进行统一实证研究，覆盖3-8位K-quant和传统量化格式。评估下游任务性能（推理、知识、指令遵循、真实性基准），同时测量困惑度、CPU吞吐量（预填充/解码）、模型大小、压缩率和量化时间

Result: 提供了不同量化方案在性能、效率和资源消耗方面的详细比较数据，揭示了精度、推理速度、内存占用之间的权衡关系

Conclusion: 本研究为选择llama.cpp量化方案提供了实用指南，帮助用户根据具体使用场景和资源预算做出明智的、情境感知的决策

Abstract: Quantization is a practical technique for making large language models easier to deploy by reducing the precision used to store and operate on model weights. This can lower memory use and improve runtime feasibility on constrained hardware, which is especially relevant for users running models locally. Quantization in llama.cpp enables large language models to run on commodity hardware, but available formats are often evaluated inconsistently, making it hard to choose among schemes. We present a unified empirical study of the llama.cpp quantization on a single modern model, Llama-3.1-8B-Instruct (FP16, GGUF), covering 3-8 bit K-quant and legacy formats. We evaluate downstream task performance across standard reasoning, knowledge, instruction-following, and truthfulness benchmarks, and also measure perplexity and CPU throughput (prefill/decoding) alongside model size, compression, and quantization time. Ultimately, this work is a practical guide for choosing a llama.cpp quantization scheme, helping readers make informed, context-aware decisions for their intended use and resource budget.

</details>


### [25] [On the Limits of Learned Importance Scoring for KV Cache Compression](https://arxiv.org/abs/2601.14279)
*Brady Steele*

Main category: cs.LG

TL;DR: 论文研究了通过推测重要性预测（SIP）进行KV缓存压缩，发现1.7M参数的SIP模型在多个任务和设置下未能超越简单基线方法，包括随机选择和位置启发式方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索基于学习的KV缓存压缩方法，特别是通过推测重要性预测来识别哪些token的KV表示可以安全丢弃以节省内存，同时保持模型性能。

Method: 提出Speculative Importance Prediction (SIP)方法，这是一个1.7M参数的非查询感知评分器，仅从KV表示预测token重要性。方法包含多时间步前瞻和交叉注意力等复杂架构。在5个随机种子、4个保留水平和3个任务上进行评估。

Result: SIP未能超越简单基线方法：1）基于位置的启发式方法（保留前4个+最后N个token）匹配或优于学习方法；2）预填充注意力提供与复杂学习评分器相当的信号；3）KV表示中超出位置和预填充注意力的边际信息对重要性预测有限。

Conclusion: KV缓存压缩中，简单的位置启发式方法已足够有效，复杂学习方法未能提供额外优势。作者假设未来查询与生成轨迹之间的循环依赖关系增加了重要性预测的难度。

Abstract: We investigate learned KV cache compression through Speculative Importance Prediction (SIP), a 1.7M parameter non-query-aware scorer that predicts token importance from KV representations alone. Despite architectural sophistication (multi-horizon lookahead, cross-attention), SIP does not outperform simple baselines, including random selection, across 5 seeds, 4 retention levels, and 3 tasks. Key findings: (1) position-based heuristics (keep first 4 + last N tokens) match or exceed learned approaches; (2) prefill attention provides equivalent signal to complex learned scorers; (3) marginal information in KV representations beyond position and prefill attention appears limited for importance prediction. We hypothesize that circular dependence between future queries and generation trajectories contributes to this difficulty.

</details>


### [26] [Beyond Affinity: A Benchmark of 1D, 2D, and 3D Methods Reveals Critical Trade-offs in Structure-Based Drug Design](https://arxiv.org/abs/2601.14283)
*Kangyu Zheng,Kai Zhang,Jiale Tan,Xuehan Chen,Yingzhou Lu,Zaixi Zhang,Lichao Sun,Marinka Zitnik,Tianfan Fu,Zhiding Liang*

Main category: cs.LG

TL;DR: 该论文建立了首个跨算法类别的结构基药物设计基准，评估了15种不同算法模型在药物性质、对接亲和力和构象方面的表现，揭示了各类算法的独特优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 当前结构基药物设计领域主要存在搜索算法、深度生成模型和强化学习三类算法，但现有研究通常只比较同一算法类别内的模型，缺乏跨算法类别的系统性比较。为填补这一空白，需要建立一个统一的基准来评估不同算法基础模型的性能。

Method: 建立了一个基准测试框架，评估了15个跨不同算法基础的模型。通过评估生成分子的药物性质、与指定靶蛋白的对接亲和力和构象来比较性能。特别强调了1D/2D配体中心药物设计方法可以通过将对接函数视为黑盒预言机而应用于SBDD。

Result: 评估揭示了不同模型类别的明显模式：3D结构基模型在结合亲和力方面表现出色，但在化学有效性和构象质量方面存在不一致性；1D模型在标准分子指标上表现可靠，但很少达到最佳结合亲和力；2D模型提供平衡性能，保持高化学有效性同时获得中等结合分数。

Conclusion: 通过跨多个蛋白质靶点的详细分析，识别了每个模型类别的关键改进领域，为研究人员提供了结合不同方法优势同时解决其局限性的见解。强调了未来SBDD模型设计应考虑整合不同算法类别的优势。

Abstract: Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning. While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce. In this paper, to fill the gap, we establish a benchmark to evaluate the performance of fifteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities and poses with specified target proteins. We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models. We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected. Our evaluation reveals distinct patterns across model categories. 3D structure-based models excel in binding affinities but show inconsistencies in chemical validity and pose quality. 1D models demonstrate reliable performance in standard molecular metrics but rarely achieve optimal binding affinities. 2D models offer balanced performance, maintaining high chemical validity while achieving moderate binding scores. Through detailed analysis across multiple protein targets, we identify key improvement areas for each model category, providing insights for researchers to combine strengths of different approaches while addressing their limitations. All the code that are used for benchmarking is available in https://github.com/zkysfls/2025-sbdd-benchmark

</details>


### [27] [A Comparison of Polynomial-Based Tree Clustering Methods](https://arxiv.org/abs/2601.14285)
*Pengyu Liu,Mariel Vázquez,Nataša Jonoska*

Main category: cs.LG

TL;DR: 比较基于树多项式的不同距离度量在树聚类中的性能，并实现两种基本的自编码器模型用于树聚类


<details>
  <summary>Details</summary>
Motivation: 生命科学中树结构数据（如系统发育、RNA二级结构）日益增多，需要新的树结构数据分析方法。树多项式提供了一种计算高效、可解释且全面的树结构编码方式，但需要评估不同距离度量在聚类中的性能。

Method: 1. 使用树区分多项式编码树结构；2. 比较基于不同距离度量的聚类方法（包括Canberra距离等）；3. 实现两种基本的自编码器模型用于树聚类；4. 评估各方法的聚类准确率。

Result: 基于条目级归一化距离的距离方法在所有比较方法中具有最高的聚类准确率。

Conclusion: 树多项式结合适当的距离度量（特别是条目级归一化距离）能够有效用于树结构的聚类分析，为生命科学中的树结构数据分析提供了实用工具。

Abstract: Tree structures appear in many fields of the life sciences, including phylogenetics, developmental biology and nucleic acid structures. Trees can be used to represent RNA secondary structures, which directly relate to the function of non-coding RNAs. Recent developments in sequencing technology and artificial intelligence have yielded numerous biological data that can be represented with tree structures. This requires novel methods for tree structure data analytics. Tree polynomials provide a computationally efficient, interpretable and comprehensive way to encode tree structures as matrices, which are compatible with most data analytics tools. Machine learning methods based on the Canberra distance between tree polynomials have been introduced to analyze phylogenies and nucleic acid structures. In this paper, we compare the performance of different distances in tree clustering methods based on a tree distinguishing polynomial. We also implement two basic autoencoder models for clustering trees using the polynomial. We find that the distance based methods with entry-level normalized distances have the highest clustering accuracy among the compared methods.

</details>


### [28] [Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity](https://arxiv.org/abs/2601.14300)
*Jun Liu,Leo Yu Zhang,Fengpeng Li,Isao Echizen,Jiantao Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的硬标签黑盒攻击框架，通过零查询频域初始化和模式驱动优化策略，在仅能获取top-1预测标签的受限反馈下实现高效的梯度符号恢复。


<details>
  <summary>Details</summary>
Motivation: 硬标签黑盒设置（仅能观察top-1预测标签）是理解模型行为的重要反馈模型，但面临从离散响应中恢复梯度信息的核心挑战。现有攻击方法多为启发式搜索，缺乏理论解释。

Method: 提出统一理论视角，将现有符号翻转硬标签攻击解释为隐式近似真实损失梯度符号。基于此提出新攻击框架：零查询频域初始化结合模式驱动优化策略，前者提高与真实梯度符号的余弦相似度，后者降低查询复杂度。

Result: 在CIFAR-10、ImageNet、ObjectNet等数据集上，方法在攻击成功率和查询效率上均超越现有SOTA硬标签攻击，特别是在低查询场景。能有效泛化到损坏数据、生物医学数据集和密集预测任务，并能绕过Blacklight防御（0%检测率）。

Conclusion: 硬标签攻击可重构为有限反馈下的梯度符号恢复问题，提出的理论框架和攻击方法在理论和实践上均优于现有方法，展示了在极端受限反馈下恢复梯度信息的可行性。

Abstract: Hard-label black-box settings, where only top-1 predicted labels are observable, pose a fundamentally constrained yet practically important feedback model for understanding model behavior. A central challenge in this regime is whether meaningful gradient information can be recovered from such discrete responses. In this work, we develop a unified theoretical perspective showing that a wide range of existing sign-flipping hard-label attacks can be interpreted as implicitly approximating the sign of the true loss gradient. This observation reframes hard-label attacks from heuristic search procedures into instances of gradient sign recovery under extremely limited feedback. Motivated by this first-principles understanding, we propose a new attack framework that combines a zero-query frequency-domain initialization with a Pattern-Driven Optimization (PDO) strategy. We establish theoretical guarantees demonstrating that, under mild assumptions, our initialization achieves higher expected cosine similarity to the true gradient sign compared to random baselines, while the proposed PDO procedure attains substantially lower query complexity than existing structured search approaches. We empirically validate our framework through extensive experiments on CIFAR-10, ImageNet, and ObjectNet, covering standard and adversarially trained models, commercial APIs, and CLIP-based models. The results show that our method consistently surpasses SOTA hard-label attacks in both attack success rate and query efficiency, particularly in low-query regimes. Beyond image classification, our approach generalizes effectively to corrupted data, biomedical datasets, and dense prediction tasks. Notably, it also successfully circumvents Blacklight, a SOTA stateful defense, resulting in a $0\%$ detection rate. Our code will be released publicly soon at https://github.com/csjunjun/DPAttack.git.

</details>


### [29] [Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2601.14327)
*YuanLab. ai,Shawn Wu,Jiangang Luo,Tong Yu,Darcy Chen,Sean Wang,Xudong Zhao,Louie Li,Claire Wang,Hunter He,Carol Wang,Allen Wang*

Main category: cs.LG

TL;DR: 提出LAEP算法，在MoE LLMs预训练阶段通过层自适应专家剪枝和重组来提高训练效率


<details>
  <summary>Details</summary>
Motivation: MoE LLMs虽然通过减少激活参数提高了准确性，但其预训练阶段存在专家利用不足和训练效率有限的计算瓶颈

Method: 提出层自适应专家剪枝(LAEP)算法，在预训练阶段根据token分布统计选择性剪枝未充分利用的专家，并在计算设备间重组专家

Result: LAEP有效减小模型规模并显著提升预训练效率，在1010B Base模型从头预训练中实现48.3%训练效率提升和33.3%参数减少，同时在多个领域保持优异性能

Conclusion: LAEP算法成功解决了MoE LLMs预训练阶段的效率瓶颈，为大规模MoE模型的高效训练提供了有效解决方案

Abstract: Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.

</details>


### [30] [Hierarchical Contextual Uplift Bandits for Catalog Personalization](https://arxiv.org/abs/2601.14333)
*Anupam Agrawal,Rajesh Mohanty,Shamik Bhattacharjee,Abhimanyu Mittal*

Main category: cs.LG

TL;DR: 提出分层上下文提升赌博机框架，解决幻想体育动态环境中传统上下文赌博机算法频繁重训练的问题，通过动态调整上下文粒度实现有效策略迁移，在Dream11平台实现显著收入提升。


<details>
  <summary>Details</summary>
Motivation: 传统上下文赌博机算法在幻想体育等动态环境中表现不佳，用户行为快速变化、外部因素导致奖励分布剧烈波动，需要频繁重训练。需要一种能适应动态环境、缓解冷启动问题的个性化推荐方法。

Method: 提出分层上下文提升赌博机框架：1）动态调整上下文粒度，从系统级宽泛洞察到用户级详细上下文；2）利用上下文相似性促进有效策略迁移；3）集成提升建模原则；4）缓解冷启动问题。

Result: 在Dream11幻想体育平台的大规模A/B测试中，该方法显著提升推荐质量，实现0.4%的收入提升并改善用户满意度指标。2025年5月部署为默认目录个性化系统后，进一步获得0.5%的收入提升。

Conclusion: 分层上下文提升赌博机框架能有效应对动态环境挑战，通过动态上下文粒度调整和提升建模集成，在幻想体育推荐场景中实现显著性能提升和收入增长，已成功部署到生产环境。

Abstract: Contextual Bandit (CB) algorithms are widely adopted for personalized recommendations but often struggle in dynamic environments typical of fantasy sports, where rapid changes in user behavior and dramatic shifts in reward distributions due to external influences necessitate frequent retraining. To address these challenges, we propose a Hierarchical Contextual Uplift Bandit framework. Our framework dynamically adjusts contextual granularity from broad, system-wide insights to detailed, user-specific contexts, using contextual similarity to facilitate effective policy transfer and mitigate cold-start issues. Additionally, we integrate uplift modeling principles into our approach. Results from large-scale A/B testing on the Dream11 fantasy sports platform show that our method significantly enhances recommendation quality, achieving a 0.4% revenue improvement while also improving user satisfaction metrics compared to the current production system. We subsequently deployed this system to production as the default catalog personalization system in May 2025 and observed a further 0.5% revenue improvement.

</details>


### [31] [VJEPA: Variational Joint Embedding Predictive Architectures as Probabilistic World Models](https://arxiv.org/abs/2601.14354)
*Yongchao Huang*

Main category: cs.LG

TL;DR: VJEPA是一种概率化的联合嵌入预测架构，通过变分目标学习未来潜在状态的预测分布，统一了表示学习与预测状态表示和贝叶斯滤波，为高维噪声环境中的可扩展鲁棒规划提供基础框架。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA使用确定性回归目标，掩盖了概率语义并限制了其在随机控制中的应用。需要一种概率化扩展来学习预测分布，以处理高熵观测和噪声环境。

Method: 提出变分JEPA(VJEPA)，通过变分目标学习未来潜在状态的预测分布。进一步提出贝叶斯JEPA(BJEPA)，将预测信念分解为学习到的动态专家和模块化先验专家，通过专家乘积实现零样本任务迁移和约束满足。

Result: 理论证明VJEPA表示可作为最优控制的充分信息状态，无需像素重构，并提供避免表示坍塌的形式保证。实验显示VJEPA和BJEPA能成功过滤高方差干扰噪声，而生成基线方法会出现表示坍塌。

Conclusion: VJEPA为高维噪声环境中的可扩展、鲁棒、不确定性感知规划提供了基础框架，通过保持观测的似然无关性，同时实现原则性不确定性估计。

Abstract: Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \emph{Variational JEPA (VJEPA)}, a \textit{probabilistic} generalization that learns a predictive distribution over future latent states via a variational objective. We show that VJEPA unifies representation learning with Predictive State Representations (PSRs) and Bayesian filtering, establishing that sequential modeling does not require autoregressive observation likelihoods. Theoretically, we prove that VJEPA representations can serve as sufficient information states for optimal control without pixel reconstruction, while providing formal guarantees for collapse avoidance. We further propose \emph{Bayesian JEPA (BJEPA)}, an extension that factorizes the predictive belief into a learned dynamics expert and a modular prior expert, enabling zero-shot task transfer and constraint (e.g. goal, physics) satisfaction via a Product of Experts. Empirically, through a noisy environment experiment, we demonstrate that VJEPA and BJEPA successfully filter out high-variance nuisance distractors that cause representation collapse in generative baselines. By enabling principled uncertainty estimation (e.g. constructing credible intervals via sampling) while remaining likelihood-free regarding observations, VJEPA provides a foundational framework for scalable, robust, uncertainty-aware planning in high-dimensional, noisy environments.

</details>


### [32] [Adaptive KDE for Real-Time Thresholding: Prioritized Queues for Financial Crime Investigation](https://arxiv.org/abs/2601.14473)
*Danny Butvinik,Nana Boateng,Achi Hackmon*

Main category: cs.LG

TL;DR: 提出一种在线自适应方法，将风险分数流转换为一个或多个审核队列，通过核密度估计和尾质量曲线满足容量约束，无需标签且支持实时多队列路由。


<details>
  <summary>Details</summary>
Motivation: 传统方法如top-K或手动调优的阈值在风险评分流转换中存在局限性，需要一种能适应动态数据流、满足明确容量约束且无需监督标签的自动化方法。

Method: 对分数流拟合在线自适应核密度估计，将密度转换为尾质量曲线以满足容量要求，通过跨带宽检测到的持久密度谷值来"捕捉"阈值，支持滑动窗口或指数遗忘的实时操作。

Result: 在合成、漂移、多模态数据流上，该方法在保持容量依从性的同时减少了阈值抖动，每个事件的更新成本为O(G)，每个活动使用恒定内存。

Conclusion: 该方法提供了一种无标签、自适应的风险分数流队列转换方案，能有效处理动态数据流，满足容量约束，并减少阈值波动，适用于实时多队列路由场景。

Abstract: We study the problem of converting a stream of risk scores into one or more review queues under explicit intake constraints[cite: 6]. Instead of top-$K$ or manually tuned cutoffs, we fit an online adaptive kernel density to the score stream, transform the density into a tail-mass curve to meet capacity, and ``snap'' the resulting cut to a persistent density valley detected across bandwidths[cite: 7]. The procedure is label-free, supports multi-queue routing, and operates in real time with sliding windows or exponential forgetting[cite: 8]. On synthetic, drifting, multimodal streams, the method achieves competitive capacity adherence while reducing threshold jitter[cite: 9]. Updates cost $O(G)$ per event with constant memory per activity

</details>


### [33] [GPU-accelerated simulated annealing based on p-bits with real-world device-variability modeling](https://arxiv.org/abs/2601.14476)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.LG

TL;DR: 基于概率比特的GPU加速模拟退火框架，发现器件变异性可提升算法性能


<details>
  <summary>Details</summary>
Motivation: 传统CMOS逻辑在复杂问题求解（如模拟退火和机器学习）中存在效率限制，概率计算使用概率比特提供了替代方案。然而，使用磁性隧道结等新兴器件实现概率比特会引入器件变异性，通常被认为会损害计算性能，需要研究其实际影响。

Method: 开发了一个基于CUDA的GPU加速开源模拟退火框架，该框架建模了三种关键的器件变异性因素：时序变异性、强度变异性和偏移变异性，以反映真实器件行为。在MAX-CUT基准测试上进行了评估，问题规模从800到20,000个节点。

Result: 研究发现了一个意外结果：器件变异性不仅会降低性能，还能通过利用时序变异性来增强算法性能。GPU加速实现相比CPU实现获得了两个数量级的速度提升，为概率计算研究提供了可扩展且易于使用的工具。

Conclusion: 该研究挑战了器件变异性必然有害的传统观念，展示了变异性在特定条件下可成为计算优势。提供的开源框架将促进概率计算研究，推动优化问题在多个领域的应用。

Abstract: Probabilistic computing using probabilistic bits (p-bits) presents an efficient alternative to traditional CMOS logic for complex problem-solving, including simulated annealing and machine learning. Realizing p-bits with emerging devices such as magnetic tunnel junctions (MTJs) introduces device variability, which was expected to negatively impact computational performance. However, this study reveals an unexpected finding: device variability can not only degrade but also enhance algorithm performance, particularly by leveraging timing variability. This paper introduces a GPU-accelerated, open-source simulated annealing framework based on p-bits that models key device variability factors -- timing, intensity, and offset -- to reflect real-world device behavior. Through CUDA-based simulations, our approach achieves a two-order magnitude speedup over CPU implementations on the MAX-CUT benchmark with problem sizes ranging from 800 to 20,000 nodes. By providing a scalable and accessible tool, this framework aims to advance research in probabilistic computing, enabling optimization applications in diverse fields.

</details>


### [34] [Stabilizing autoregressive forecasts in chaotic systems via multi-rate latent recurrence](https://arxiv.org/abs/2601.14487)
*Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: MSR-HINE：一种用于混沌动力系统长期自回归预测的分层隐式预测器，通过多尺度潜在先验和多速率循环模块减少误差累积，显著提升了预测精度和可预测性范围。


<details>
  <summary>Details</summary>
Motivation: 混沌动力系统的长期自回归预测面临挑战：小的一步误差会迅速放大，导致物理不一致的展开和大尺度统计特性崩溃。现有方法难以同时保持长期上下文和快速尺度变异性。

Method: 提出MSR-HINE（多尺度递归分层隐式编码器），包含：1）多速率循环模块在不同时间尺度运行；2）粗到细循环状态生成潜在先验；3）隐式一步预测器通过多尺度潜在注入细化状态；4）门控融合与后验潜在确保尺度一致性更新；5）轻量级隐藏状态校正对齐循环记忆与融合潜在。

Result: 在两个基准测试中表现优异：在Kuramoto-Sivashinsky系统上，H=400时末端RMSE降低62.8%，末端ACC从-0.155提升至0.828（+0.983），ACC≥0.5的可预测性范围从241步扩展到400步；在Lorenz-96系统上，H=100时RMSE降低27.0%，末端ACC从0.144提升至0.545（+0.402），ACC≥0.5范围从58步扩展到100步。

Conclusion: MSR-HINE通过分层隐式预测架构有效缓解了混沌展开中的误差累积问题，在保持慢流形长期上下文的同时保留了快速尺度变异性，显著提升了混沌动力系统的长期预测性能。

Abstract: Long-horizon autoregressive forecasting of chaotic dynamical systems remains challenging due to rapid error amplification and distribution shift: small one-step inaccuracies compound into physically inconsistent rollouts and collapse of large-scale statistics. We introduce MSR-HINE, a hierarchical implicit forecaster that augments multiscale latent priors with multi-rate recurrent modules operating at distinct temporal scales. At each step, coarse-to-fine recurrent states generate latent priors, an implicit one-step predictor refines the state with multiscale latent injections, and a gated fusion with posterior latents enforces scale-consistent updates; a lightweight hidden-state correction further aligns recurrent memories with fused latents. The resulting architecture maintains long-term context on slow manifolds while preserving fast-scale variability, mitigating error accumulation in chaotic rollouts. Across two canonical benchmarks, MSR-HINE yields substantial gains over a U-Net autoregressive baseline: on Kuramoto-Sivashinsky it reduces end-horizon RMSE by 62.8% at H=400 and improves end-horizon ACC by +0.983 (from -0.155 to 0.828), extending the ACC >= 0.5 predictability horizon from 241 to 400 steps; on Lorenz-96 it reduces RMSE by 27.0% at H=100 and improves end horizon ACC by +0.402 (from 0.144 to 0.545), extending the ACC >= 0.5 horizon from 58 to 100 steps.

</details>


### [35] [On the Runway Cascade of Transformers for Language Modeling](https://arxiv.org/abs/2601.14522)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: 该论文提出了一种名为"跑道感知重连"的机制，通过显式地将间接信息传播路径（跑道）纳入直接路径注意力中，解决因果变换器中信息传播模式不匹配的问题，从而提升语言建模性能。


<details>
  <summary>Details</summary>
Motivation: 因果变换器中存在两种信息传播模式：直接路径注意力和通过中间令牌形成的间接路径（跑道）。研究发现这两种模式之间的不匹配会导致冗余和无关信息在令牌表示中传播，即使注意力模式已经充分学习。这种"跑道级联"现象被认为是导致因果变换器某些失败模式加剧的原因。

Method: 提出跑道感知重连机制，该方法基于每个令牌的跑道景观摘要来重新连接注意力模式。具体来说，该机制将跑道上下文直接纳入每个令牌的直接路径注意力中，使模型能够感知累积的表征影响，实现更平衡的信息传播。该方法不引入额外参数，可以无缝集成到标准注意力机制中。

Result: 实验表明，经过重连的变换器在通用语言建模方面实现了稳定改进，同时在信息检索和外推能力方面表现出明显优于标准变换器的性能。

Conclusion: 跑道感知重连机制通过显式地整合间接信息传播路径，有效解决了因果变换器中信息传播模式不匹配的问题，提升了模型的整体性能，特别是在信息检索和外推任务上表现突出。

Abstract: In decoder-only (causal) transformers, the computation graph created by causal masking routes information through both direct-path attention and indirect paths formed by intermediate tokens. We denote these indirect paths between token pairs as their runways. We argue that certain failure modes of causal transformers as observed by a growing body of recent works are likely exacerbated by a misalignment between these two information propagation modes. We formalize runway cascade as a phenomenon whereby this misalignment results in redundancies and irrelevant information cascading to token representations despite adequately learned attention patterns. As a solution, we propose runway-aware rewiring as a more explicit way of incorporating runway context directly into each token's direct-path attention. This mechanism re-wires the attention pattern for each token based on a summary of its runway landscape, enabling awareness of accumulating representational influences and allowing for more balanced information propagation. Our proposed methodology introduces no additional parameters and can seamlessly be integrated into standard attention mechanism. Empirically, our rewired transformer results in steady improvements in general language modeling as well as noticeably stronger information retrieval and extrapolation abilities compared to standard transformers.

</details>


### [36] [Search over Self-Edit Strategies for LLM Adaptation](https://arxiv.org/abs/2601.14532)
*Alistair Cheong,Haolin Cong,Tyler Yang,Dustin Miao*

Main category: cs.LG

TL;DR: 该研究探索了LLM能否利用任务反馈自主决定如何更新权重，在SEAL框架中放宽固定模板约束，让模型生成自编辑模板以控制训练数据和超参数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM搜索系统通常冻结基础模型，可能限制长期进展。虽然已有研究探索在测试时更新提案模型，但更新策略仍需人工指定。因此本研究旨在探索LLM是否能利用任务反馈自主决定权重更新方式。

Method: 采用SEAL框架作为测试平台，放宽其固定人工模板约束，允许模型生成自编辑模板，从而让模型能控制训练数据和关键NTP超参数。研究两种变体：无存档版本和基于轻量级历史模板存档的条件生成版本。在SEAL的单段落知识整合设置中使用Qwen3-8B模型在SQuAD数据集上进行实验。

Result: 无存档变体表现与较弱的"Implications"基线相当，而存档变体优于"Implications"并接近最强的人工设计"Rewrite"基线但未超越。分析显示朴素存档能提供短期鲁棒性，但也会加速同质化，表明可能需要显式新颖性压力才能持续超越精心优化的人工策略。

Conclusion: LLM能够利用任务反馈自主决定权重更新策略，但需要更精细的探索机制来避免同质化并超越人工优化策略。显式新颖性压力可能是持续进步的关键。

Abstract: Many LLM-based open-ended search systems freeze the foundation model that proposes improvements to existing solutions, which may bottleneck long-run progress. Recent work has explored updating the proposal model at test time [arXiv:2511.23473], but the update strategy is still typically hand-specified. Therefore, this study investigated whether an LLM can use task feedback to decide how it should update its weights. For tractability, we focused on the simpler case where there is only one round of self-improvement, and restricted the update operator to self-supervised next token prediction (NTP), leaving the model freedom in choosing its training data and key NTP hyperparameters. Using the Self-Adapting Language Models (SEAL) [arXiv:2506.10943] framework as a testbed, we relaxed its fixed human template constraint and allowed the model to generate its own self-edit templates, thereby giving it more control over its training data and hyperparameters. Two variants were studied, differing in whether template generation was conditioned on a lightweight archive of past templates. In SEAL's Single-Passage Knowledge Incorporation setting with Qwen3-8B on SQuAD [arXiv:1606.05250], the no-archive variant performed comparably to the weaker "Implications" baseline, while the archive variant outperformed "Implications" and approached the strongest human-designed "Rewrite" baseline without surpassing it. Further analysis of collapse in the model's exploration revealed that a naive archive can confer some short-term robustness but can also accelerate homogenization, suggesting that explicit novelty pressure may be required to consistently advance beyond carefully optimized human strategies. Our code is available at https://github.com/cheongalc/search-self-edit-strategies .

</details>


### [37] [QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design](https://arxiv.org/abs/2601.14549)
*Nilesh Prasad Pandey,Jangseon Park,Onat Gungor,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: QMC提出了一种免重训练的量化与异构内存协同设计方法，通过识别SLM中的内点和离群权重，将内点存储在紧凑的ReRAM中，关键离群值保存在高精度MRAM中，显著提升了边缘设备上小语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 在边缘平台上部署小语言模型面临内存、延迟和能耗限制。传统量化方法受新兴非易失性存储器设备噪声影响，而传统内存层次结构（SRAM密度低、DRAM带宽争用、Flash推理时闲置）限制了效率，需要针对LLM推理的混合内存组织。

Method: 提出离群感知量化与内存协同设计(QMC)：1) 识别SLM中的内点和离群权重；2) 将内点权重存储在紧凑的多级ReRAM中；3) 将关键离群值保存在高精度片上MRAM中，减轻噪声引起的性能下降；4) 免重训练量化与异构内存架构协同设计。

Result: 在语言建模和推理基准测试中，QMC优于或匹配使用先进算法和混合数据格式的SOTA量化方法，在算法评估和实际部署场景下都实现了更高压缩。相比FP16，内存使用减少6.3-7.3倍，外部数据传输减少7.6倍，能耗降低11.7倍，延迟减少12.5倍。

Conclusion: QMC作为一种可扩展、部署就绪的协同设计，通过离群感知量化与异构内存架构的协同优化，有效解决了边缘设备上SLM部署的内存、带宽、能耗和延迟挑战，为高效设备端推理提供了实用解决方案。

Abstract: Deploying Small Language Models (SLMs) on edge platforms is critical for real-time, privacy-sensitive generative AI, yet constrained by memory, latency, and energy budgets. Quantization reduces model size and cost but suffers from device noise in emerging non-volatile memories, while conventional memory hierarchies further limit efficiency. SRAM provides fast access but has low density, DRAM must simultaneously accommodate static weights and dynamic KV caches, which creates bandwidth contention, and Flash, although dense, is primarily used for initialization and remains inactive during inference. These limitations highlight the need for hybrid memory organizations tailored to LLM inference. We propose Outlier-aware Quantization with Memory Co-design (QMC), a retraining-free quantization with a novel heterogeneous memory architecture. QMC identifies inlier and outlier weights in SLMs, storing inlier weights in compact multi-level Resistive-RAM (ReRAM) while preserving critical outliers in high-precision on-chip Magnetoresistive-RAM (MRAM), mitigating noise-induced degradation. On language modeling and reasoning benchmarks, QMC outperforms and matches state-of-the-art quantization methods using advanced algorithms and hybrid data formats, while achieving greater compression under both algorithm-only evaluation and realistic deployment settings. Specifically, compared against SoTA quantization methods on the latest edge AI platform, QMC reduces memory usage by 6.3x-7.3x, external data transfers by 7.6x, energy by 11.7x, and latency by 12.5x when compared to FP16, establishing QMC as a scalable, deployment-ready co-design for efficient on-device inference.

</details>


### [38] [Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation](https://arxiv.org/abs/2601.14590)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 该研究评估了使用大语言模型（LLMs）生成反事实解释（CFEs）的效果，在临床数据上验证了LLMs能够生成高质量、可操作的反事实，并可用于数据增强以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 反事实解释通过识别最小、可操作的改变来改变机器学习模型的预测，具有人类中心的可解释性。然而，传统优化方法生成的CFEs在临床场景中可能缺乏语义连贯性和临床可操作性。本研究旨在探索LLMs在生成高质量反事实解释方面的潜力，特别是在传感器数字健康领域的应用。

Method: 研究使用多模态AI-READI临床数据集，评估了多种LLMs生成反事实解释的能力，包括GPT-4（零样本和少样本）、BioMistral-7B和LLaMA-3.1-8B（预训练和微调配置）。评估从三个维度进行：干预质量、特征多样性和增强效果。同时与DiCE、CFNOW、NICE等优化基线方法进行比较。

Result: 微调后的LLMs（特别是LLaMA-3.1-8B）能够生成高可信度（高达99%）、强有效性（高达0.99）且具有现实行为可修改特征调整的反事实。在标签稀缺设置下，LLM生成的反事实用于数据增强能显著恢复分类器性能，在三种稀缺场景中平均实现20%的F1恢复。相比优化基线方法，LLMs提供了更灵活、模型无关的方法，生成更具临床可操作性和语义连贯性的反事实。

Conclusion: 这项工作展示了LLM驱动的反事实解释在传感器数字健康领域中，对于可解释干预设计和数据高效模型训练的巨大潜力。SenseCF方法通过微调LLM生成有效、有代表性的反事实解释，并补充不平衡数据集中的少数类，从而改善模型训练、提升模型鲁棒性和预测性能。

Abstract: Counterfactual explanations (CFEs) provide human-centric interpretability by identifying the minimal, actionable changes required to alter a machine learning model's prediction. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. We conduct a comprehensive evaluation of CF generation using large language models (LLMs), including GPT-4 (zero-shot and few-shot) and two open-source models-BioMistral-7B and LLaMA-3.1-8B, in both pretrained and fine-tuned configurations. Using the multimodal AI-READI clinical dataset, we assess CFs across three dimensions: intervention quality, feature diversity, and augmentation effectiveness. Fine-tuned LLMs, particularly LLaMA-3.1-8B, produce CFs with high plausibility (up to 99%), strong validity (up to 0.99), and realistic, behaviorally modifiable feature adjustments. When used for data augmentation under controlled label-scarcity settings, LLM-generated CFs substantially restore classifier performance, yielding an average 20% F1 recovery across three scarcity scenarios. Compared with optimization-based baselines such as DiCE, CFNOW, and NICE, LLMs offer a flexible, model-agnostic approach that generates more clinically actionable and semantically coherent counterfactuals. Overall, this work demonstrates the promise of LLM-driven counterfactuals for both interpretable intervention design and data-efficient model training in sensor-based digital health.
  Impact: SenseCF fine-tunes an LLM to generate valid, representative counterfactual explanations and supplement minority class in an imbalanced dataset for improving model training and boosting model robustness and predictive performance

</details>


### [39] [Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective](https://arxiv.org/abs/2601.14599)
*Xiao Hu,Hong Xie,Tao Tan,Defu Lian,Jianyu Han*

Main category: cs.LG

TL;DR: 该论文提出了一种自下而上的实验框架来分析强化学习微调LLM中的各种优化选择，通过最小化配置逐步扩展来理解每个设计选择的作用和瓶颈。


<details>
  <summary>Details</summary>
Motivation: 目前LLM强化学习微调领域存在大量启发式方法，但各种主张相互矛盾，缺乏对两个基本问题的清晰理解：1) 每个优化选择的具体作用是什么？2) 哪些是性能瓶颈？

Method: 提出自下而上的实验流程：底层采用最小化配置（单一训练数据、每轮单次rollout、直接使用奖励作为学习信号而无优势函数设计），该配置连接到大离散动作空间的多臂老虎机学习，为实验结果提供理论支持。然后逐层扩展最小化配置，系统性地检验每个设计选择的作用。

Result: 在三个LLM和两个推理数据集上的实验不仅揭示了设计选择的新理解，还为该领域提供了重要的洞见。

Conclusion: 通过自下而上的实验框架，论文澄清了LLM强化学习微调中各种优化选择的作用，识别了性能瓶颈，为该领域提供了理论指导和实践洞见。

Abstract: A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.

</details>


### [40] [Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum](https://arxiv.org/abs/2601.14603)
*Jingru Li,Yibo Fan,Huan Li*

Main category: cs.LG

TL;DR: Muon-NSR和Muon-VS是两种基于正交动量更新的LLM预训练加速方法，通过方差自适应归一化改进原始Muon，在GPT-2和LLaMA预训练中显著加速收敛并降低验证损失。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在各种NLP任务中表现出色，但预训练计算成本高昂，优化器效率成为重要实践考量。现有Adam优化器可视为方差自适应符号更新算法，这启发了对正交动量更新方法Muon的改进。

Method: 提出Muon-NSR和Muon-VS两种变体，在正交化前对动量应用方差自适应归一化。Muon-NSR使用信噪比调制，Muon-VS进行基于方差的缩放而不引入额外超参数。这些方法将正交动量更新作为元素级符号算子的矩阵模拟。

Result: 在GPT-2和LLaMA预训练实验中，Muon-NSR和Muon-VS加速收敛，始终获得比精心调优的AdamW和原始Muon基线更低的验证损失。对于LLaMA-1.2B模型，达到目标验证损失所需的迭代次数相比基准Muon减少1.36倍。

Conclusion: 通过方差自适应归一化改进的正交动量更新方法Muon-NSR和Muon-VS能够显著加速LLM预训练，提高优化器效率，为大规模语言模型训练提供了有效的优化方案。

Abstract: Large Language Models (LLMs) achieve competitive performance across diverse natural language processing (NLP) tasks, yet pretraining is computationally demanding, making optimizer efficiency an important practical consideration. Muon accelerates LLM pretraining via orthogonal momentum updates that serve as a matrix analogue of the element-wise sign operator. Motivated by the recent perspective that Adam is a variance-adaptive sign update algorithm, we propose two variants of Muon, Muon-NSR and Muon-VS, which apply variance-adaptive normalization to momentum before orthogonalization. Muon-NSR applies noise-to-signal ratio (NSR) modulation, while Muon-VS performs variance-based scaling without introducing additional hyperparameters. Experiments on GPT-2 and LLaMA pretraining demonstrate that our proposed methods accelerate convergence and consistently achieve lower validation loss than both competitive, well-tuned AdamW and Muon baselines. For example, on the LLaMA-1.2B model, Muon-NSR and Muon-VS reduce the iterations required to reach the target validation loss by $1.36\times$ relative to the well-tuned Muon following the recent benchmark.

</details>


### [41] [Efficient Imputation for Patch-based Missing Single-cell Data via Cluster-regularized Optimal Transport](https://arxiv.org/abs/2601.14653)
*Yuyu Liu,Jiannan Yang,Ziyang Yu,Weishen Pan,Fei Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: CROT是一种基于最优传输的插补算法，专门处理表格数据中的块状缺失数据，在保持高精度的同时显著减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序数据中的缺失数据对提取生物学见解构成挑战，现有插补方法假设数据均匀且完整，难以处理大块缺失数据的情况。

Method: 提出CROT算法，基于最优传输理论处理表格格式中的块状缺失数据，有效捕捉存在显著缺失情况下的底层数据结构。

Result: CROT实现了优越的插补精度，同时显著减少了运行时间，展示了其在大规模数据集上的可扩展性和效率。

Conclusion: 该工作为异质高维数据集中的结构化数据缺失提供了稳健的插补解决方案，解决了生物和临床数据分析中的关键挑战。

Abstract: Missing data in single-cell sequencing datasets poses significant challenges for extracting meaningful biological insights. However, existing imputation approaches, which often assume uniformity and data completeness, struggle to address cases with large patches of missing data. In this paper, we present CROT, an optimal transport-based imputation algorithm designed to handle patch-based missing data in tabular formats. Our approach effectively captures the underlying data structure in the presence of significant missingness. Notably, it achieves superior imputation accuracy while significantly reducing runtime, demonstrating its scalability and efficiency for large-scale datasets. This work introduces a robust solution for imputation in heterogeneous, high-dimensional datasets with structured data absence, addressing critical challenges in both biological and clinical data analysis. Our code is available at Anomalous Github.

</details>


### [42] [Beyond Denial-of-Service: The Puppeteer's Attack for Fine-Grained Control in Ranking-Based Federated Learning](https://arxiv.org/abs/2601.14687)
*Zhihao Chen,Zirui Gong,Jianting Ning,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种针对联邦排序学习（FRL）的新型细粒度控制攻击——边缘控制攻击（ECA），该攻击能够精确控制目标模型的准确率，同时保持正常的收敛轨迹以避免检测。


<details>
  <summary>Details</summary>
Motivation: 虽然联邦排序学习（FRL）因其基于离散排序的更新机制而具有抗模型中毒攻击的鲁棒性，但作者发现FRL仍然容易受到新型细粒度控制攻击。现有攻击多为拒绝服务（DoS）攻击，容易被检测，而ECA能够在不引起注意的情况下精确控制模型性能。

Method: ECA攻击分为两个阶段：1）识别并操纵上升和下降边缘，使全局模型与目标模型对齐；2）扩大选择边界间隙，使全局模型稳定在目标准确率水平。攻击在七个基准数据集和九种拜占庭鲁棒聚合规则下进行了验证。

Result: 实验表明，ECA能够实现细粒度准确率控制，平均误差仅为0.224%，比基线方法提升高达17倍。攻击能够在不引起检测的情况下精确控制竞争对手的模型准确率。

Conclusion: 尽管FRL减少了攻击面，但仍易受ECA等细粒度控制攻击。研究结果强调了需要针对高级中毒攻击开发更强的防御机制，特别是在基于排序的联邦学习框架中。

Abstract: Federated Rank Learning (FRL) is a promising Federated Learning (FL) paradigm designed to be resilient against model poisoning attacks due to its discrete, ranking-based update mechanism. Unlike traditional FL methods that rely on model updates, FRL leverages discrete rankings as a communication parameter between clients and the server. This approach significantly reduces communication costs and limits an adversary's ability to scale or optimize malicious updates in the continuous space, thereby enhancing its robustness. This makes FRL particularly appealing for applications where system security and data privacy are crucial, such as web-based auction and bidding platforms. While FRL substantially reduces the attack surface, we demonstrate that it remains vulnerable to a new class of local model poisoning attack, i.e., fine-grained control attacks. We introduce the Edge Control Attack (ECA), the first fine-grained control attack tailored to ranking-based FL frameworks. Unlike conventional denial-of-service (DoS) attacks that cause conspicuous disruptions, ECA enables an adversary to precisely degrade a competitor's accuracy to any target level while maintaining a normal-looking convergence trajectory, thereby avoiding detection. ECA operates in two stages: (i) identifying and manipulating Ascending and Descending Edges to align the global model with the target model, and (ii) widening the selection boundary gap to stabilize the global model at the target accuracy. Extensive experiments across seven benchmark datasets and nine Byzantine-robust aggregation rules (AGRs) show that ECA achieves fine-grained accuracy control with an average error of only 0.224%, outperforming the baseline by up to 17x. Our findings highlight the need for stronger defenses against advanced poisoning attacks. Our code is available at: https://github.com/Chenzh0205/ECA

</details>


### [43] [CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation](https://arxiv.org/abs/2601.14695)
*Yutong Chen,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: CoScale-RL是一种针对大型推理模型的新型扩展策略，通过扩展解决方案而非简单扩大数据集，以及扩展rollout计算来稳定强化学习，显著提高了数据和计算效率。


<details>
  <summary>Details</summary>
Motivation: 训练大型推理模型通常不稳定且不可预测，特别是在困难问题或基础模型较弱的情况下。当前的后训练扩展策略在这些情况下仍有改进空间。

Method: 提出CoScale-RL扩展策略：1）扩展解决方案：为每个问题收集多个解决方案，而非简单扩大数据集；2）扩展rollout计算以稳定强化学习；3）使用Re-distillation模型合并技术来维持或提高计算效率。

Result: 在四个基准测试上平均获得3.76倍的准确率提升，显著提高了数据和计算效率，能够在不依赖大量监督微调数据集的情况下提升大型推理模型的能力边界。

Conclusion: CoScale-RL为改进大型推理模型的推理能力提供了新的扩展方向，通过更好的数据和计算效率解决了训练不稳定问题。

Abstract: Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.

</details>


### [44] [Case-Guided Sequential Assay Planning in Drug Discovery](https://arxiv.org/abs/2601.14710)
*Tianchi Chen,Jan Bima,Sean L. Wu,Otto Ritter,Bingjia Yang,Xiang Yu*

Main category: cs.LG

TL;DR: 提出IBMDP框架，用于无模拟器的药物发现实验序列优化，通过隐式贝叶斯模型和集成MCTS规划，显著减少资源消耗并提高决策质量。


<details>
  <summary>Details</summary>
Motivation: 药物发现中的实验序列优化面临严重不确定性和资源约束，标准强化学习缺乏环境模拟器或转移数据，需要仅依赖静态历史数据库进行规划。

Method: 引入隐式贝叶斯马尔可夫决策过程（IBMDP）框架，通过相似历史结果构建非参数信念分布形成隐式转移动态模型，采用贝叶斯信念更新和集成蒙特卡洛树搜索规划。

Result: 在真实CNS药物发现任务中，IBMDP相比现有启发式方法减少92%资源消耗；在合成环境中，与确定性值迭代相比，显著提高与最优策略的对齐度。

Conclusion: IBMDP为数据丰富但模拟器稀缺领域的序列实验设计提供了实用解决方案，其集成规划器优于确定性替代方案。

Abstract: Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s')$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.

</details>


### [45] [PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning](https://arxiv.org/abs/2601.14716)
*Yao Lu,Dengdong Fan,Jianzheng Nie,Fan Xu,Jie Chen,Bin Zhou,Yonghong Tian*

Main category: cs.LG

TL;DR: PCL-Reasoner-V1.5是一个基于Qwen2.5-32B构建的320亿参数数学推理大语言模型，采用监督微调+强化学习训练，提出离线RL方法提升训练稳定性和效率，在AIME竞赛中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发更稳定高效的强化学习方法用于大语言模型的数学推理能力提升，解决在线RL方法（如GRPO）训练不稳定和效率低的问题。

Method: 基于Qwen2.5-32B构建320亿参数模型，采用监督微调(SFT)后接强化学习(RL)的两阶段训练策略，核心创新是提出的离线RL方法，相比标准在线RL方法提供更好的训练稳定性和效率。

Result: 在基于Qwen2.5-32B后训练的模型中达到最先进性能：AIME 2024平均准确率90.9%，AIME 2025平均准确率85.6%。所有实验在华为昇腾910C NPU上进行。

Conclusion: 离线RL方法为LLM推理能力提升提供了稳定高效的训练范式，PCL-Reasoner-V1.5在数学推理任务上表现出色，验证了离线RL方法的有效性。

Abstract: We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.

</details>


### [46] [Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models](https://arxiv.org/abs/2601.14758)
*Injin Kong,Hyoungjoon Lee,Yohan Jo*

Main category: cs.LG

TL;DR: 该研究通过对比分析发现，将自回归模型后训练为掩码扩散模型会引发"机制转变"：对于局部因果依赖任务保留自回归电路，对于全局规划任务则重构电路，实现从局部专业化到分布式整合的转变。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究将预训练自回归模型后训练为掩码扩散模型时，模型内部算法转变的本质。需要明确这种后训练是否真正赋予模型双向推理能力，还是仅仅重新包装了自回归启发式方法。

Method: 采用对比电路分析方法，对自回归模型及其对应的掩码扩散模型进行系统比较，分析它们在结构和语义层面的差异。

Result: 研究发现存在系统性的"机制转变"：结构上，对于局部因果依赖任务，MDMs基本保留自回归电路；对于全局规划任务，MDMs放弃初始化路径，表现出早期层处理增强的重新布线特征。语义上，观察到从ARMs的尖锐局部专业化向MDMs的分布式整合的转变。

Conclusion: 扩散后训练不仅仅是调整模型参数，而是从根本上重组内部计算以支持非顺序的全局规划，表明MDMs确实获得了真正的双向推理能力而非简单重新包装自回归方法。

Abstract: Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative circuit analysis of ARMs and their MDM counterparts. Our analysis reveals a systematic "mechanism shift" dependent on the structural nature of the task. Structurally, we observe a distinct divergence: while MDMs largely retain autoregressive circuitry for tasks dominated by local causal dependencies, they abandon initialized pathways for global planning tasks, exhibiting distinct rewiring characterized by increased early-layer processing. Semantically, we identify a transition from sharp, localized specialization in ARMs to distributed integration in MDMs. Through these findings, we conclude that diffusion post-training does not merely adapt model parameters but fundamentally reorganizes internal computation to support non-sequential global planning.

</details>


### [47] [Anytime Optimal Decision Tree Learning with Continuous Features](https://arxiv.org/abs/2601.14765)
*Harold Kiossou,Pierre Schaus,Siegfried Nijssen*

Main category: cs.LG

TL;DR: 提出一种基于有限差异搜索的随时完整算法，用于学习具有连续特征的最优决策树，改善现有深度优先搜索方法的随时性能


<details>
  <summary>Details</summary>
Motivation: 现有学习连续特征最优决策树的深度优先搜索方法虽然能找到最优解，但计算时间增长快且随时性能差，早期中断时找到的树往往高度不平衡且次优，而纯贪心方法反而可能得到更好结果

Method: 采用有限差异搜索策略，将计算努力更均匀地分布在整个树结构中，而不是像深度优先搜索那样先完全优化左子树再探索右子树

Result: 实验结果表明，该方法在随时性能方面优于现有方法，确保在任何中断点都能获得高质量的决策树

Conclusion: 通过有限差异搜索实现的随时完整方法有效解决了连续特征最优决策树学习中的随时性能问题，在计算效率和解决方案质量之间取得了更好平衡

Abstract: In recent years, significant progress has been made on algorithms for learning optimal decision trees, primarily in the context of binary features. Extending these methods to continuous features remains substantially more challenging due to the large number of potential splits for each feature. Recently, an elegant exact algorithm was proposed for learning optimal decision trees with continuous features; however, the rapidly increasing computational time limits its practical applicability to shallow depths (typically 3 or 4). It relies on a depth-first search optimization strategy that fully optimizes the left subtree of each split before exploring the corresponding right subtree. While effective in finding optimal solutions given sufficient time, this strategy can lead to poor anytime behavior: when interrupted early, the best-found tree is often highly unbalanced and suboptimal. In such cases, purely greedy methods such as C4.5 may, paradoxically, yield better solutions. To address this limitation, we propose an anytime, yet complete approach leveraging limited discrepancy search, distributing the computational effort more evenly across the entire tree structure, and thus ensuring that a high-quality decision tree is available at any interruption point. Experimental results show that our approach outperforms the existing one in terms of anytime performance.

</details>


### [48] [Reflecting in the Reflection: Integrating a Socratic Questioning Framework into Automated AI-Based Question Generation](https://arxiv.org/abs/2601.14798)
*Ondřej Holub,Essi Ryymin,Rodrigo Alves*

Main category: cs.LG

TL;DR: 提出基于大语言模型的"反思中的反思"框架，通过两个角色专业化智能体（学生-教师和教师-教育者）进行苏格拉底式多轮对话，自动生成高质量的反思问题。


<details>
  <summary>Details</summary>
Motivation: 设计好的反思问题对教学很重要，但耗时且教师支持不均衡，需要自动化工具来辅助教师生成高质量的反思问题。

Method: 采用"反思中的反思"框架，协调两个角色专业化智能体：学生-教师提出候选问题及简要理由，教师-教育者从清晰度、深度、相关性、参与度和概念关联性五个维度评估，通过苏格拉底式多轮对话迭代优化单个问题。

Result: 动态停止机制结合上下文信息（学生水平和教学材料）显著优于固定5步或10步迭代；双智能体协议生成的问题在相关性、深度和整体质量上明显优于单次提示基线；过长对话容易导致问题偏离或过度复杂化。

Conclusion: 该框架能有效生成高质量的反思问题，动态停止机制和上下文信息是关键成功因素，为教师提供了实用的自动化反思问题生成工具。

Abstract: Designing good reflection questions is pedagogically important but time-consuming and unevenly supported across teachers. This paper introduces a reflection-in-reflection framework for automated generation of reflection questions with large language models (LLMs). Our approach coordinates two role-specialized agents, a Student-Teacher and a Teacher-Educator, that engage in a Socratic multi-turn dialogue to iteratively refine a single question given a teacher-specified topic, key concepts, student level, and optional instructional materials. The Student-Teacher proposes candidate questions with brief rationales, while the Teacher-Educator evaluates them along clarity, depth, relevance, engagement, and conceptual interconnections, responding only with targeted coaching questions or a fixed signal to stop the dialogue. We evaluate the framework in an authentic lower-secondary ICT setting on the topic, using GPT-4o-mini as the backbone model and a stronger GPT- 4-class LLM as an external evaluator in pairwise comparisons of clarity, relevance, depth, and overall quality. First, we study how interaction design and context (dynamic vs.fixed iteration counts; presence or absence of student level and materials) affect question quality. Dynamic stopping combined with contextual information consistently outperforms fixed 5- or 10-step refinement, with very long dialogues prone to drift or over-complication. Second, we show that our two-agent protocol produces questions that are judged substantially more relevant and deeper, and better overall, than a one-shot baseline using the same backbone model.

</details>


### [49] [Statistical Learning Theory for Distributional Classification](https://arxiv.org/abs/2601.14818)
*Christian Fiedler*

Main category: cs.LG

TL;DR: 本文研究了两阶段采样设置下分布输入的监督学习问题，特别关注使用支持向量机（SVM）进行分布输入分类的理论分析，建立了新的oracle不等式、一致性结果和学习速率，并针对高斯核和铰链损失提出了新的噪声假设。


<details>
  <summary>Details</summary>
Motivation: 在基于学习的医学筛查或因果学习等应用中，输入是概率分布，但在学习阶段只能获得这些分布的样本（两阶段采样设置）。虽然基于核的方法（如核均值嵌入）已被用于处理此类问题，但对其理论分析，特别是使用SVM进行分类的理论基础仍需深入研究。

Method: 采用核均值嵌入（KMEs）将分布或样本嵌入到希尔伯特空间，然后在该嵌入空间上应用标准核方法（如SVM）。研究重点是对这种方法的理论分析，特别是针对分布输入分类的SVM。建立了新的oracle不等式，推导了一致性和学习速率结果，并针对高斯核和铰链损失提出了新的噪声假设变体。

Result: 建立了新的oracle不等式，证明了该方法的一致性，并推导了学习速率。特别针对使用高斯核和铰链损失的SVM，提出了新的噪声假设变体，并在此假设下建立了学习速率。此外，为希尔伯特空间上的高斯核开发的新特征空间技术具有独立的研究价值。

Conclusion: 本文为两阶段采样设置下分布输入的SVM分类提供了坚实的理论基础，建立了理论保证并推导了学习速率。提出的新噪声假设和针对高斯核的技术工具扩展了现有理论框架，为实际应用提供了理论支持。

Abstract: In supervised learning with distributional inputs in the two-stage sampling setup, relevant to applications like learning-based medical screening or causal learning, the inputs (which are probability distributions) are not accessible in the learning phase, but only samples thereof. This problem is particularly amenable to kernel-based learning methods, where the distributions or samples are first embedded into a Hilbert space, often using kernel mean embeddings (KMEs), and then a standard kernel method like Support Vector Machines (SVMs) is applied, using a kernel defined on the embedding Hilbert space. In this work, we contribute to the theoretical analysis of this latter approach, with a particular focus on classification with distributional inputs using SVMs. We establish a new oracle inequality and derive consistency and learning rate results. Furthermore, for SVMs using the hinge loss and Gaussian kernels, we formulate a novel variant of an established noise assumption from the binary classification literature, under which we can establish learning rates. Finally, some of our technical tools like a new feature space for Gaussian kernels on Hilbert spaces are of independent interest.

</details>


### [50] [Adaptive Exponential Integration for Stable Gaussian Mixture Black-Box Variational Inference](https://arxiv.org/abs/2601.14855)
*Baojun Che,Yifan Chen,Daniel Zhengyu Huang,Xinying Mao,Weijie Wang*

Main category: cs.LG

TL;DR: 提出一种稳定高效的黑盒变分推断框架，结合仿射不变预处理、指数积分器和自适应时间步长，用于高斯混合族近似复杂后验分布


<details>
  <summary>Details</summary>
Motivation: 传统黑盒变分推断方法在使用高斯混合族近似复杂后验时，标准数值优化方法存在不稳定和效率低下的问题，需要更稳定高效的优化框架

Method: 结合三个关键组件：1) 通过自然梯度公式实现仿射不变预处理；2) 无条件保持协方差矩阵正定性的指数积分器；3) 确保稳定性并适应预热和收敛阶段的自适应时间步长

Result: 对于高斯后验，证明了在无噪声设置下的指数收敛性，以及在蒙特卡洛估计下的几乎必然收敛；数值实验在多种分布、Neal的多尺度漏斗和基于PDE的贝叶斯反问题中验证了方法的有效性

Conclusion: 提出的框架为黑盒变分推断提供了一种稳定高效的优化方法，具有与流形优化和镜像下降的自然联系，自适应时间步长被严格证明是必要的

Abstract: Black-box variational inference (BBVI) with Gaussian mixture families offers a flexible approach for approximating complex posterior distributions without requiring gradients of the target density. However, standard numerical optimization methods often suffer from instability and inefficiency. We develop a stable and efficient framework that combines three key components: (1) affine-invariant preconditioning via natural gradient formulations, (2) an exponential integrator that unconditionally preserves the positive definiteness of covariance matrices, and (3) adaptive time stepping to ensure stability and to accommodate distinct warm-up and convergence phases. The proposed approach has natural connections to manifold optimization and mirror descent. For Gaussian posteriors, we prove exponential convergence in the noise-free setting and almost-sure convergence under Monte Carlo estimation, rigorously justifying the necessity of adaptive time stepping. Numerical experiments on multimodal distributions, Neal's multiscale funnel, and a PDE-based Bayesian inverse problem for Darcy flow demonstrate the effectiveness of the proposed method.

</details>


### [51] [Strategic Doctrine Language Models (sdLM): A Learning-System Framework for Doctrinal Consistency and Geopolitical Forecasting](https://arxiv.org/abs/2601.14862)
*Olaf Yunus Laitinen Imanov,Taner Yilmaz,Derya Umut Kulali*

Main category: cs.LG

TL;DR: sdLM框架通过多文档注意力、时间编码和教义一致性层，在战略推理中提升长期预测准确性、计划合理性和教义一致性，同时校准不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有通用大语言模型在战略推理任务中存在教义一致性不足、长期预测准确性差、不确定性校准不佳等问题，需要专门框架来处理多文档战略分析并确保符合特定教义约束。

Method: 结合多文档注意力机制、时间编码技术和教义一致性层，构建战略教义语言模型框架，通过结构化处理多个战略文档来提升推理质量。

Result: 在三个基准测试中表现优异：(1)专家小组对47个战略场景评分显示更高战略质量；(2)在336份教义出版物(12,847条陈述)上保持更好教义一致性；(3)在127个历史反事实(1945-2020)的12-60个月预测中优于基线模型，与人类专家在长期判断上竞争力相当。

Conclusion: sdLM框架显著提升了战略推理的教义一致性、预测准确性和不确定性校准，为战略分析和决策支持提供了有效的技术解决方案，并通过消融实验和部署特性分析明确了各组件贡献。

Abstract: We introduce Strategic Doctrine Language Models (sdLM), a learning-system framework for multi-document strategic reasoning with doctrinal consistency constraints and calibrated uncertainty. The approach combines multi-document attention, temporal encoding, and a doctrine-consistency layer to improve long-horizon forecasting and plan plausibility while reducing severe doctrinal violations. We evaluate sdLM using (i) expert-panel scoring of strategic scenarios (N=47), (ii) doctrine consistency on 336 doctrine publications (12,847 statements), and (iii) geopolitical forecasting on 127 historical counterfactuals (1945-2020) across 12-60 month horizons. Across these benchmarks, sdLM achieves higher strategic quality and better calibration than strong general-purpose LLM baselines, and remains competitive with human experts on long-horizon judgments. We further report ablations, scaling trends, and deployment-oriented performance/latency characteristics to clarify which components drive improvements and how they translate to operational settings.

</details>


### [52] [Tailoring Adverse Event Prediction in Type 1 Diabetes with Patient-Specific Deep Learning Models](https://arxiv.org/abs/2601.14917)
*Giorgia Rigamonti,Mirko Paolo Barbato,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的个性化血糖预测方法，通过利用患者特定数据提高预测准确性，相比传统通用模型能更好地适应个体差异，在真实场景中实现更精准的血糖管理。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病管理需要连续血糖监测和精确胰岛素调整，随着可穿戴血糖监测设备和移动健康应用的普及，准确的血糖预测对于增强自动化胰岛素输送和决策支持系统至关重要。传统通用模型无法充分适应个体差异，需要开发能够考虑患者特定动态的个性化预测方法。

Method: 提出基于深度学习的个性化血糖预测方法，利用患者特定数据。采用两种评估策略：留一受试者交叉验证和微调策略，比较它们对患者特定动态的建模能力。实验比较了多模态患者特定方法与传统仅使用连续血糖监测的方法，并进行消融研究，探索模型在不同规模训练集下的性能，确定有效个性化所需的最小数据量。

Result: 个性化模型显著提高了不良事件的预测准确性，能够在真实场景中实现更精确和及时的干预。多模态患者特定方法优于传统CGM-only方法。消融研究确定了有效个性化所需的最小数据量，这对于实际应用中数据收集受限的情况具有重要意义。

Conclusion: 自适应个性化血糖预测模型具有推动下一代糖尿病管理的潜力，特别是在可穿戴和移动健康平台中，能够增强面向消费者的糖尿病护理解决方案。个性化方法通过考虑个体差异，相比传统通用模型能提供更有效的患者特定预测。

Abstract: Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.

</details>


### [53] [Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed Learning](https://arxiv.org/abs/2601.14942)
*Hang Zhao,Hongru Li,Dongfang Xu,Shenghui Song,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出三阶段通信感知分布式学习框架，用于多模态边缘推理，通过自监督学习、分布式微调和不确定性引导反馈机制，在减少通信开销的同时保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态边缘推理面临两大挑战：1) 多模态特性导致分布式学习在带宽有限无线链路上的通信开销巨大；2) 在变化信道和噪声多模态输入下鲁棒性有限。需要一种通信高效的训练和鲁棒推理方法。

Method: 三阶段框架：阶段I - 设备本地多模态自监督学习，无需设备-服务器交换，获得共享和模态特定编码器；阶段II - 分布式微调结合集中式证据融合，校准每模态不确定性并可靠聚合噪声或信道衰落扭曲的特征；阶段III - 不确定性引导反馈机制，为不确定样本选择性请求额外特征，优化分布式设置中的通信-精度权衡。

Result: 在RGB-深度室内场景分类实验中，所提框架以更少的训练通信轮数获得更高精度，对模态退化或信道变化保持鲁棒性，优于现有自监督和全监督基线方法。

Conclusion: 该三阶段通信感知分布式学习框架有效解决了多模态边缘推理的通信效率和鲁棒性挑战，通过自监督学习减少通信开销，证据融合增强鲁棒性，不确定性反馈优化通信-精度权衡。

Abstract: Semantic communication is emerging as a key enabler for distributed edge intelligence due to its capability to convey task-relevant meaning. However, achieving communication-efficient training and robust inference over wireless links remains challenging. This challenge is further exacerbated for multi-modal edge inference (MMEI) by two factors: 1) prohibitive communication overhead for distributed learning over bandwidth-limited wireless links, due to the \emph{multi-modal} nature of the system; and 2) limited robustness under varying channels and noisy multi-modal inputs. In this paper, we propose a three-stage communication-aware distributed learning framework to improve training and inference efficiency while maintaining robustness over wireless channels. In Stage~I, devices perform local multi-modal self-supervised learning to obtain shared and modality-specific encoders without device--server exchange, thereby reducing the communication cost. In Stage~II, distributed fine-tuning with centralized evidential fusion calibrates per-modality uncertainty and reliably aggregates features distorted by noise or channel fading. In Stage~III, an uncertainty-guided feedback mechanism selectively requests additional features for uncertain samples, optimizing the communication--accuracy tradeoff in the distributed setting. Experiments on RGB--depth indoor scene classification show that the proposed framework attains higher accuracy with far fewer training communication rounds and remains robust to modality degradation or channel variation, outperforming existing self-supervised and fully supervised baselines.

</details>


### [54] [Improving Regret Approximation for Unsupervised Dynamic Environment Generation](https://arxiv.org/abs/2601.14957)
*Harry Mead,Bruno Lacerda,Jakob Foerster,Nick Hawes*

Main category: cs.LG

TL;DR: 提出DEGen方法解决UED中的信用分配问题，引入MNA作为改进的遗憾近似度量，提升大规模环境中课程生成效果


<details>
  <summary>Details</summary>
Motivation: 当前无监督环境设计方法在环境参数空间较大时面临信用分配困难，难以识别真正具有挑战性的环境配置，限制了课程生成效果和智能体泛化能力

Method: 提出动态环境生成方法，通过更密集的生成器奖励信号缓解信用分配问题；引入最大化负优势作为改进的遗憾近似度量，能更好识别困难环境配置

Result: 实验表明MNA优于现有遗憾近似方法，DEGen与MNA结合后在不同规模环境中均优于现有方法，尤其在大规模环境中表现更显著

Conclusion: DEGen通过改进奖励信号密度和信用分配机制，结合MNA对困难环境的更好识别能力，显著提升了UED在大规模环境中的课程生成效果

Abstract: Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.

</details>


### [55] [InstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement](https://arxiv.org/abs/2601.14968)
*Mingyue Cheng,Xiaoyu Tao,Huajian Zhang,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: InstructTime++：将时间序列分类重构为多模态生成任务，通过离散化时间序列、跨模态对齐和隐式特征建模提升分类性能


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类方法采用判别式范式，直接将输入序列映射到one-hot编码的类别标签。这种方法难以融入上下文特征，也无法捕捉类别间的语义关系。需要一种能够更好地整合多模态信息并理解类别语义的新框架。

Method: 1. 将时间序列分类重构为多模态生成任务：连续数值序列、上下文文本特征和任务指令作为多模态输入，类别标签作为语言模型生成的文本输出。2. 引入时间序列离散化模块，将连续序列转换为离散时间标记。3. 使用对齐投影层和生成式自监督预训练策略增强跨模态表示对齐。4. InstructTime++进一步加入隐式特征建模：使用专用工具包从原始时间序列和上下文输入中挖掘信息性隐式模式（包括统计特征提取和基于视觉语言的图像描述），并将其转换为文本描述进行无缝整合。

Result: 在多个基准数据集上的广泛实验表明，InstructTime++具有优越的性能表现。

Conclusion: 通过将时间序列分类重构为多模态生成任务，并引入隐式特征建模来弥补语言模型有限的归纳偏置，InstructTime++能够更好地整合上下文特征和捕捉类别语义关系，从而在时间序列分类任务上取得优异性能。

Abstract: Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.

</details>


### [56] [Lineup Regularized Adjusted Plus-Minus (L-RAPM): Basketball Lineup Ratings with Informed Priors](https://arxiv.org/abs/2601.15000)
*Christos Petridis,Konstantinos Pelechrinis*

Main category: cs.LG

TL;DR: 提出L-RAPM回归方法，通过控制对手阵容影响并利用球员信息，解决篮球阵容数据稀疏性问题，提高小样本阵容表现预测能力


<details>
  <summary>Details</summary>
Motivation: 篮球等运动中识别表现良好的阵容组合是体育分析的重要任务，但频繁换人导致数据高度稀疏（NBA球队每赛季使用600+阵容，平均每个阵容仅25-30次进攻回合），现有统计数据噪声大、预测价值低，且公开领域缺乏解决方案

Method: 提出基于回归的方法L-RAPM，该方法控制每个阵容面对的对手影响，同时利用构成阵容的球员信息

Result: 实验表明L-RAPM比当前使用的基线方法具有更好的预测能力，且随着阵容样本量减小，改进效果更加明显

Conclusion: L-RAPM方法能有效解决阵容数据稀疏性问题，在小样本情况下尤其能提升阵容表现预测的准确性

Abstract: Identifying combinations of players (that is, lineups) in basketball - and other sports - that perform well when they play together is one of the most important tasks in sports analytics. One of the main challenges associated with this task is the frequent substitutions that occur during a game, which results in highly sparse data. In particular, a National Basketball Association (NBA) team will use more than 600 lineups during a season, which translates to an average lineup having seen the court in approximately 25-30 possessions. Inevitably, any statistics that one collects for these lineups are going to be noisy, with low predictive value. Yet, there is no existing work (in the public at least) that addresses this problem. In this work, we propose a regression-based approach that controls for the opposition faced by each lineup, while it also utilizes information about the players making up the lineups. Our experiments show that L-RAPM provides improved predictive power than the currently used baseline, and this improvement increases as the sample size for the lineups gets smaller.

</details>


### [57] [Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control](https://arxiv.org/abs/2601.15015)
*Jannis Becktepe,Aleksandra Franz,Nils Thuerey,Sebastian Peitz*

Main category: cs.LG

TL;DR: FluidGym：首个独立、完全可微的强化学习主动流控制基准套件，基于PyTorch和GPU加速PICT求解器，无需外部CFD软件，提供标准化评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在主动流控制领域的研究进展难以评估，因为现有研究使用异构的观测与执行方案、数值设置和评估协议。现有基准严重依赖外部CFD求解器，不完全可微，且3D和多智能体支持有限。

Method: 基于GPU加速的PICT求解器，完全在PyTorch中构建，形成单一Python堆栈，无需外部CFD软件。提供标准化评估协议，支持3D和多智能体场景，实现完全可微性。

Result: 发布了FluidGym基准套件，包含所有环境、数据集和训练模型作为公共资源。提供了PPO和SAC的基线结果，建立了可扩展的学习型流控制研究基础。

Conclusion: FluidGym解决了现有基准的局限性，实现了控制方法的系统比较，为学习型流控制研究提供了标准化、可扩展的平台，所有资源已在GitHub开源。

Abstract: Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.

</details>


### [58] [Mixture-of-Experts Models in Vision: Routing, Optimization, and Generalization](https://arxiv.org/abs/2601.15021)
*Adam Rokah,Daniel Veress,Caleb Caulk,Sourav Sharan*

Main category: cs.LG

TL;DR: 研究在图像分类任务中比较了密集模型与两种MoE变体（SoftMoE和SparseMoE），发现MoE模型在验证准确率上略有优势，但条件计算在实际硬件上未能实现推理加速。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在图像分类任务中的行为，而非传统的大语言模型扩展场景，重点关注预测性能、专家利用率和泛化特性。

Method: 在CIFAR10数据集上，在可比模型容量下比较密集、SoftMoE和SparseMoE分类器头；使用正则化保持专家平衡利用；通过Hessian矩阵的谱半径和迹评估损失曲面的锐度；进行损失曲面扰动分析；评估实际推理效率。

Result: 两种MoE变体在验证准确率上略高于密集基线，同时通过正则化避免了专家崩溃；SoftMoE在Hessian锐度指标上表现出更高的锐度，而密集和SparseMoE处于相似的曲率状态；损失曲面扰动分析揭示了密集和MoE模型在有限参数扰动下的非局部行为差异；实际硬件上条件路由未能实现推理加速。

Conclusion: MoE架构在图像分类中能提供轻微的性能优势并保持专家平衡利用，但曲率分析和损失曲面扰动未能完全解释泛化差异；实际硬件上稀疏MoE的理论效率与实际效率存在差距，条件计算在小规模模型中未能实现推理加速。

Abstract: Mixture-of-Experts (MoE) architectures enable conditional computation by routing inputs to multiple expert subnetworks and are often motivated as a mechanism for scaling large language models. In this project, we instead study MoE behavior in an image classification setting, focusing on predictive performance, expert utilization, and generalization. We compare dense, SoftMoE, and SparseMoE classifier heads on the CIFAR10 dataset under comparable model capacity. Both MoE variants achieve slightly higher validation accuracy than the dense baseline while maintaining balanced expert utilization through regularization, avoiding expert collapse. To analyze generalization, we compute Hessian-based sharpness metrics at convergence, including the largest eigenvalue and trace of the loss Hessian, evaluated on both training and test data. We find that SoftMoE exhibits higher sharpness by these metrics, while Dense and SparseMoE lie in a similar curvature regime, despite all models achieving comparable generalization performance. Complementary loss surface perturbation analyses reveal qualitative differences in non-local behavior under finite parameter perturbations between dense and MoE models, which help contextualize curvature-based measurements without directly explaining validation accuracy. We further evaluate empirical inference efficiency and show that naively implemented conditional routing does not yield inference speedups on modern hardware at this scale, highlighting the gap between theoretical and realized efficiency in sparse MoE models.

</details>


### [59] [LoRAP: Low-Rank Aggregation Prompting for Quantized Graph Neural Networks Training](https://arxiv.org/abs/2601.15079)
*Chenyu Liu,Haige Li,Luca Rossi*

Main category: cs.LG

TL;DR: 提出LoRAP方法，通过低秩聚合提示优化GNN量化训练，在多个数据集和框架上提升低比特量化GNN性能


<details>
  <summary>Details</summary>
Motivation: GNN量化可减少模型大小、加速推理，但相比LLMs量化更强调图特征量化。现有方法仅提示节点特征只能使部分量化聚合结果最优，需要更全面的优化方案

Method: 提出低秩聚合提示(LoRAP)，将轻量级、输入相关的提示注入每个聚合特征中，优化量化聚合结果。该方法在4个主流QAT框架和9个图数据集上进行评估

Result: LoRAP在多个数据集和框架上一致提升低比特量化GNN性能，同时引入的计算开销极小

Conclusion: LoRAP通过优化量化聚合过程，有效提升了GNN量化感知训练的性能，为资源受限环境中的高效GNN部署提供了实用解决方案

Abstract: Graph Neural Networks (GNNs) are neural networks that aim to process graph data, capturing the relationships and interactions between nodes using the message-passing mechanism. GNN quantization has emerged as a promising approach for reducing model size and accelerating inference in resource-constrained environments. Compared to quantization in LLMs, quantizing graph features is more emphasized in GNNs. Inspired by the above, we propose to leverage prompt learning, which manipulates the input data, to improve the performance of quantization-aware training (QAT) for GNNs. To mitigate the issue that prompting the node features alone can only make part of the quantized aggregation result optimal, we introduce Low-Rank Aggregation Prompting (LoRAP), which injects lightweight, input-dependent prompts into each aggregated feature to optimize the results of quantized aggregations. Extensive evaluations on 4 leading QAT frameworks over 9 graph datasets demonstrate that LoRAP consistently enhances the performance of low-bit quantized GNNs while introducing a minimal computational overhead.

</details>


### [60] [Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning](https://arxiv.org/abs/2601.15086)
*Oleg Shchendrigin,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 该论文针对强化学习中记忆重写能力的研究空白，提出了一个在部分可观测环境下测试持续记忆更新的基准，并发现经典循环模型在记忆重写任务中比现代结构化记忆和基于Transformer的智能体表现更优。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的有效决策需要记忆既稳定又适应性强：环境随时间变化，智能体必须在长时间范围内保留相关信息，同时在情况变化时更新或覆盖过时内容。现有的强化学习基准和记忆增强智能体主要关注记忆保留，而同样关键的记忆重写能力在很大程度上未被探索。

Method: 引入一个基准，在部分可观测性下明确测试持续记忆更新（智能体必须依赖记忆而非当前观察），并使用该基准比较循环、基于Transformer和结构化记忆架构。基准旨在评估记忆重写能力，而非单纯记忆保留。

Result: 实验表明，尽管经典循环模型简单，但在记忆重写任务中表现出比现代结构化记忆更大的灵活性和鲁棒性。结构化记忆仅在狭窄条件下成功，而基于Transformer的智能体通常在非平凡保留情况下失败。这些发现揭示了当前方法的基本局限性。

Conclusion: 强调了平衡稳定保留与自适应更新的记忆机制的必要性。该工作突出了这一被忽视的挑战，引入了评估基准，并为设计具有显式和可训练遗忘机制的未来强化学习智能体提供了见解。

Abstract: Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/

</details>


### [61] [Field-Space Autoencoder for Scalable Climate Emulators](https://arxiv.org/abs/2601.15102)
*Johannes Meuer,Maximilian Witte,Étiénne Plésiat,Thomas Ludwig,Christopher Kadow*

Main category: cs.LG

TL;DR: 提出Field-Space Autoencoder框架，通过球面压缩模型解决千米尺度地球系统模型计算成本高、输出数据量大的问题，支持零样本超分辨率和生成式扩散模型训练。


<details>
  <summary>Details</summary>
Motivation: 千米尺度地球系统模型计算成本高昂且产生PB级输出，限制了其在概率风险评估等应用中的实用性，需要开发可扩展的气候模拟框架来克服这些挑战。

Method: 提出Field-Space Autoencoder框架，基于球面压缩模型，利用Field-Space Attention直接在原生气候模型输出上操作，避免将球面数据强制映射到欧几里得网格造成的几何失真。该方法产生结构化压缩场，作为下游生成式模拟的良好基线，并能进行零样本超分辨率。

Result: 该方法比卷积基线更好地保留了物理结构，通过结构化压缩场支持生成式扩散模型训练，能够同时从丰富的低分辨率数据学习内部变异性，从稀疏的高分辨率数据学习精细尺度物理。

Conclusion: 该工作填补了低分辨率集合统计的高数据量与高分辨率物理细节稀缺之间的鸿沟，为可扩展的气候模拟提供了有效框架。

Abstract: Kilometer-scale Earth system models are essential for capturing local climate change. However, these models are computationally expensive and produce petabyte-scale outputs, which limits their utility for applications such as probabilistic risk assessment. Here, we present the Field-Space Autoencoder, a scalable climate emulation framework based on a spherical compression model that overcomes these challenges. By utilizing Field-Space Attention, the model efficiently operates on native climate model output and therefore avoids geometric distortions caused by forcing spherical data onto Euclidean grids. This approach preserves physical structures significantly better than convolutional baselines. By producing a structured compressed field, it serves as a good baseline for downstream generative emulation. In addition, the model can perform zero-shot super-resolution that maps low-resolution large ensembles and scarce high-resolution data into a shared representation. We train a generative diffusion model on these compressed fields. The model can simultaneously learn internal variability from abundant low-resolution data and fine-scale physics from sparse high-resolution data. Our work bridges the gap between the high volume of low-resolution ensemble statistics and the scarcity of high-resolution physical detail.

</details>


### [62] [Auditing Language Model Unlearning via Information Decomposition](https://arxiv.org/abs/2601.15111)
*Anmol Goel,Alan Ritter,Iryna Gurevych*

Main category: cs.LG

TL;DR: 当前机器遗忘方法存在关键局限：尽管遗忘算法表面成功，但被遗忘数据的信息仍可从内部表示中线性解码。论文引入基于部分信息分解的信息论框架来审计遗忘效果，发现冗余信息构成残余知识，并提出基于表示的风险评分来缓解隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 揭示当前语言模型机器遗忘方法的根本缺陷：尽管遗忘算法在表面上看起来成功，但被遗忘数据的信息仍然可以从模型的内部表示中解码出来。这种表面成功与实际信息残留之间的差距需要系统性的评估框架。

Method: 引入基于部分信息分解（PID）的可解释信息论框架来审计遗忘效果。通过比较遗忘前后的模型表示，将与被遗忘数据的互信息分解为不同成分，形式化定义"已遗忘知识"和"残余知识"的概念。分析发现冗余信息（两个模型共享）构成残余知识，并提出基于表示的风险评分来指导推理时对敏感输入的回避。

Result: 分析表明冗余信息构成残余知识，这些知识在遗忘后仍然存在，并与已知对抗重建攻击的易感性相关。基于这些洞察，提出了基于表示的风险评分，能够在推理时指导对敏感输入的回避，提供缓解隐私泄露的实际机制。

Conclusion: 该工作引入了原则性的表示层面遗忘审计方法，为语言模型机器遗忘提供了理论洞察和可操作工具。揭示了当前遗忘方法的局限性，并提出了基于信息论的评估框架和实际缓解措施，有助于语言模型更安全的部署。

Abstract: We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.

</details>


### [63] [Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.15124)
*Haonan Yuan,Qingyun Sun,Jiacheng Tao,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: RAG-GFM：基于检索增强生成的图基础模型，通过外部化图知识解决现有GFM的内存瓶颈问题，在跨域节点和图分类任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型（GFMs）存在内存瓶颈问题：它们试图将知识编码到模型参数中，这限制了语义容量，引入了严重的损失压缩和冲突，并且以阻碍高效适应的方式将图表示与知识纠缠在一起，从而影响了可扩展性和可解释性。

Method: 提出RAG-GFM（检索增强生成的图基础模型），将知识从参数中卸载并补充参数化学习。方法包括：1）构建双模态统一检索模块，包含基于前缀结构文本的语义存储和基于中心性基元的结构存储；2）设计双视图对齐目标，对比两种模态以捕捉内容和关系模式；3）执行上下文增强，用检索到的文本和基元作为上下文证据来丰富支持实例。

Result: 在五个基准图数据集上的广泛实验表明，RAG-GFM在跨域节点和图分类任务中始终优于13个最先进的基线方法，实现了卓越的有效性和效率。

Conclusion: RAG-GFM通过外部化图知识、构建双模态检索模块和设计对齐目标，成功解决了图基础模型的内存瓶颈问题，为图学习提供了更高效、可扩展和可解释的解决方案。

Abstract: Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.

</details>


### [64] [CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning](https://arxiv.org/abs/2601.15141)
*Tianshi Xu,Yuteng Chen,Meng Li*

Main category: cs.LG

TL;DR: CLEANER是一种用于参数受限LLM的强化学习方法，通过内在自我修正能力净化噪声轨迹，解决了探索阶段的执行失败和信用分配问题，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 参数受限模型（4B-7B）在工具使用强化学习中面临探索阶段频繁执行失败的问题，产生噪声轨迹阻碍策略优化。标准基于结果的奖励设置导致信用分配问题，错误动作与成功结果同时被强化。现有方法面临密集奖励导致奖励攻击或超采样计算成本过高的困境。

Method: 提出CLEANER方法，利用模型内在自我修正能力在数据收集阶段直接消除错误污染上下文。核心是相似性感知自适应回滚（SAAR）机制，通过回顾性地用成功的自我修正替换失败，自主构建干净、净化的轨迹。基于语义相似性，SAAR自适应调节替换粒度，从浅层执行修复到深层推理替换。通过在这些自我净化的路径上训练，模型内化正确的推理模式而非错误恢复循环。

Result: 在AIME24/25、GPQA和LiveCodeBench上的实验结果显示，相比基线平均准确率分别提升6%、3%和5%。特别值得注意的是，CLEANER仅使用三分之一的训练步骤就达到了最先进性能，突显了轨迹净化作为高效智能体强化学习的可扩展解决方案。

Conclusion: CLEANER通过利用LLM的内在自我修正能力净化噪声轨迹，有效解决了参数受限模型在工具使用强化学习中的信用分配问题。该方法不仅提升了性能，还显著提高了训练效率，为高效智能体强化学习提供了可扩展的解决方案。

Abstract: Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub

</details>


### [65] [Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data](https://arxiv.org/abs/2601.15158)
*Yuval Ran-Milo,Yotam Alexander,Shahar Mendel,Nadav Cohen*

Main category: cs.LG

TL;DR: 论文分析了Transformer模型在稀疏奖励强化学习下如何自发产生思维链推理能力，通过图遍历任务的理论分析揭示了梯度流驱动模型学习可解释迭代算法的机制。


<details>
  <summary>Details</summary>
Motivation: 尽管基于结果监督的强化学习训练Transformer能够自发产生思维链推理能力，但稀疏奖励如何驱动梯度下降发现这种系统性推理的机制尚不清楚。本研究旨在揭示这一机制。

Method: 通过分析单层Transformer在合成图遍历任务上的梯度流动态，该任务无法在没有思维链的情况下解决但允许简单的迭代解决方案。从理论上证明梯度流驱动模型收敛到结构化、可解释的迭代算法。

Result: 证明了仅基于最终答案正确性的训练能够驱动模型学习逐顶点遍历图的算法。识别了"简单示例"（需要较少推理步骤的实例）在分布特性中的关键作用：当训练分布包含足够多的简单实例时，模型学习可泛化的遍历策略；当这些实例消失时，基于梯度的学习变得不可行。

Conclusion: 研究揭示了稀疏奖励下思维链推理能力涌现的理论机制，并通过合成数据和真实世界语言模型在数学推理任务上的实验验证了理论发现对实际场景的适用性。

Abstract: Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of "simple examples": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.

</details>


### [66] [ZENITH: Automated Gradient Norm Informed Stochastic Optimization](https://arxiv.org/abs/2601.15212)
*Dhrubo Saha*

Main category: cs.LG

TL;DR: ZENITH优化器通过梯度范数的时间演化自适应调整学习率，在图像分类、目标检测等任务中实现更高精度和更快训练速度，且与正则化兼容。


<details>
  <summary>Details</summary>
Motivation: 现有自适应优化器存在计算内存开销大、与正则化不兼容、学习率选择次优等问题，需要手动调整学习率计划。

Method: 提出ZENITH优化器，利用梯度范数的时间演化来自适应调整学习率，实现零开销的自动学习率调度。

Result: 在6种CNN架构和6个基准测试的图像分类实验中，ZENITH在更短时间内获得更高测试精度；在MS COCO上的目标检测、关键点检测和实例分割任务中，使用R-CNN模型获得更优mAP；与正则化兼容进一步提升泛化能力。

Conclusion: ZENITH优化器通过梯度范数演化自适应调整学习率，在保持零开销的同时实现优于现有方法的性能，且与正则化兼容，具有实用价值。

Abstract: Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.

</details>


### [67] [Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism](https://arxiv.org/abs/2601.15249)
*Garrett G. Wen,Buxin Su,Natalie Collina,Zhun Deng,Weijie Su*

Main category: cs.LG

TL;DR: 提出一种基于等渗机制的作者辅助最佳论文评选方法，通过作者对自身论文的排名评估来调整原始评审分数，从而提高奖项评选质量。


<details>
  <summary>Details</summary>
Motivation: 随着NeurIPS、ICML等AI会议投稿量激增至数万篇，同行评审质量和一致性面临严峻挑战，特别是最佳论文奖项的评选过程近年来争议不断，需要更有效的评选机制。

Method: 采用等渗机制（Isotonic Mechanism）让作者对自己的投稿进行排名评估，利用这些排名信息调整原始评审分数，以更准确地估计论文的真实质量。该方法在作者效用函数为凸可加函数时能激励真实报告，特别在作者只有单一配额时，即使效用函数仅为非递减可加函数也能保证真实性。

Result: 通过ICLR（2019-2023）和NeurIPS（2021-2023）的公开评审数据验证了凸性假设的合理性。仿真结果表明，该机制显著提高了最佳论文奖项的评选质量。特别重要的是，在作者只有单一配额的情况下，对效用函数的假设要求大幅放宽。

Conclusion: 提出的作者辅助机制能有效改善大规模AI会议中最佳论文奖项的评选过程，通过激励作者真实报告自身论文的相对质量，结合原始评审分数，可获得更准确的质量估计，从而提高奖项评选的公正性和质量。

Abstract: Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [68] [From Agent Simulation to Social Simulator: A Comprehensive Review (Part 2)](https://arxiv.org/abs/2601.14296)
*Xiao Xue,Deyu Zhou,Ming Zhang,Xiangning Yu,Fei-Yue Wang*

Main category: cs.MA

TL;DR: 计算实验方法通过反事实实验和系统变量调整，为复杂系统研究提供因果推断工具，弥补传统基于主体建模的局限性


<details>
  <summary>Details</summary>
Motivation: 传统基于主体建模方法主要侧重于模拟而非实验，难以深入揭示复杂系统的运行原理和根本原因，需要新的方法论来提供因果解释

Method: 提出计算实验方法，强调反事实实验，通过创建平行世界模拟现实事件的替代演化路径，系统调整输入变量并观察输出变量变化

Result: 计算实验为复杂系统研究提供了强大的因果推断工具，能够揭示系统动态演化的因果机制

Conclusion: 计算实验方法结合传统ABM，能够为复杂系统研究提供因果洞察，帮助理解系统复杂性的根本原因和运行原理

Abstract: The study of system complexity primarily has two objectives: to explore underlying patterns and to develop theoretical explanations. Pattern exploration seeks to clarify the mechanisms behind the emergence of system complexity, while theoretical explanations aim to identify the fundamental causes of this complexity. Laws are generally defined as mappings between variables, whereas theories offer causal explanations of system behavior. Agent Based Modeling(ABM) is an important approach for studying complex systems, but it tends to emphasize simulation over experimentation. As a result, ABM often struggles to deeply uncover the governing operational principles. Unlike conventional scenario analysis that relies on human reasoning, computational experiments emphasize counterfactual experiments-that is, creating parallel worlds that simulate alternative "evolutionary paths" of real-world events. By systematically adjusting input variables and observing the resulting changes in output variables, computational experiments provide a robust tool for causal inference, thereby addressing the limitations of traditional ABM. Together, these methods offer causal insights into the dynamic evolution of systems. This part can help readers gain a preliminary understanding of the entire computational experiment method, laying the foundation for the subsequent study.

</details>


### [69] [Predicting Long-Term Self-Rated Health in Small Areas Using Ordinal Regression and Microsimulation](https://arxiv.org/abs/2601.14335)
*Seán Caulfield Curley,Karl Mason,Patrick Mannion*

Main category: cs.MA

TL;DR: 使用微观模拟和序数回归预测爱尔兰未来人口自评健康，考虑社会经济特征和空间差异，提出对齐技术提高预测准确性，发现老龄化可能轻微恶化平均自评健康


<details>
  <summary>Details</summary>
Motivation: 为地方当局提供预测和应对未来人口健康问题的工具，通过细粒度健康建模分析社会经济特征对自评健康的影响，特别关注老龄化对健康分布的潜在影响

Method: 使用开源微观模拟预测爱尔兰未来人口，在选举分区层面进行空间分解，采用序数回归基于社会经济特征预测个体自评健康，提出对齐技术解决健康微数据与国家数据分布差异问题

Result: 序数回归方法能很好匹配爱尔兰2022年健康状态分布；在一个潜在未来人口情景中，老龄化效应可能超过社会经济改善，导致爱尔兰平均自评健康轻微恶化

Conclusion: 细粒度健康建模能为地方当局提供预测和应对未来健康问题的能力，老龄化可能对平均自评健康产生负面影响，需要政策关注

Abstract: This paper presents an approach for predicting the self-rated health of individuals in a future population utilising the individuals' socio-economic characteristics. An open-source microsimulation is used to project Ireland's population into the future where each individual is defined by a number of demographic and socio-economic characteristics. The model is disaggregated spatially at the Electoral Division level, allowing for analysis of results at that, or any broader geographical scales. Ordinal regression is utilised to predict an individual's self-rated health based on their socio-economic characteristics and this method is shown to match well to Ireland's 2022 distribution of health statuses. Due to differences in the health status distributions of the health microdata and the national data, an alignment technique is proposed to bring predictions closer to real values. It is illustrated for one potential future population that the effects of an ageing population may outweigh other improvements in socio-economic outcomes to disimprove Ireland's mean self-rated health slightly. Health modelling at this kind of granular scale could offer local authorities a chance to predict and combat health issues which may arise in their local populations in the future.

</details>


### [70] [INFA-Guard: Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.14667)
*Yijin Zhou,Xiaoya Lu,Dongrui Liu,Junchi Yan,Jing Shao*

Main category: cs.MA

TL;DR: INFA-Guard是一个针对LLM多智能体系统的新型防御框架，通过感染感知检测和拓扑约束来识别和处理被感染智能体，有效降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM多智能体系统安全防护通常采用二元范式，严格区分良性智能体和攻击智能体，但忽视了被感染智能体（即被攻击智能体转化的良性实体）这一重要威胁类别，导致安全漏洞。

Method: 提出INFA-Guard防御框架，包含感染感知检测和拓扑约束两个核心组件。通过感染感知检测准确识别攻击源和感染范围，在修复阶段替换攻击者并恢复被感染智能体，同时保持拓扑完整性。

Result: 实验表明INFA-Guard达到最先进性能，平均降低攻击成功率33%，同时表现出跨模型鲁棒性、优越的拓扑泛化能力和高成本效益。

Conclusion: INFA-Guard通过明确识别和处理被感染智能体这一独特威胁类别，有效解决了传统二元防御范式的局限性，为LLM多智能体系统提供了更全面的安全防护。

Abstract: The rapid advancement of Large Language Model (LLM)-based Multi-Agent Systems (MAS) has introduced significant security vulnerabilities, where malicious influence can propagate virally through inter-agent communication. Conventional safeguards often rely on a binary paradigm that strictly distinguishes between benign and attack agents, failing to account for infected agents i.e., benign entities converted by attack agents. In this paper, we propose Infection-Aware Guard, INFA-Guard, a novel defense framework that explicitly identifies and addresses infected agents as a distinct threat category. By leveraging infection-aware detection and topological constraints, INFA-Guard accurately localizes attack sources and infected ranges. During remediation, INFA-Guard replaces attackers and rehabilitates infected ones, avoiding malicious propagation while preserving topological integrity. Extensive experiments demonstrate that INFA-Guard achieves state-of-the-art performance, reducing the Attack Success Rate (ASR) by an average of 33%, while exhibiting cross-model robustness, superior topological generalization, and high cost-effectiveness.

</details>


### [71] [Game-Theoretic Lens on LLM-based Multi-Agent Systems](https://arxiv.org/abs/2601.15047)
*Jianing Hao,Han Ding,Yuanjian Xu,Tianze Sun,Ran Chen,Wanbo Zhang,Guang Zhang,Siguang Li*

Main category: cs.MA

TL;DR: 该论文通过博弈论视角对基于大语言模型的多智能体系统进行了系统性综述，建立了统一的理论框架。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体系统研究呈现碎片化状态，缺乏统一的理论基础。虽然多智能体系统在适应性、协调性方面优于单智能体系统，但现有研究缺乏系统性框架来理解和比较不同方法。

Method: 采用博弈论视角，围绕博弈论的四个核心要素（玩家、策略、收益、信息）对现有LLM-based多智能体系统研究进行系统性组织和分析，建立统一的理论框架。

Result: 提出了一个基于博弈论的系统性框架，能够有效理解、比较和指导未来LLM-based多智能体系统的设计与分析研究，为该领域提供了统一的理论基础。

Conclusion: 通过博弈论视角对LLM-based多智能体系统进行系统性综述，填补了该领域缺乏统一理论框架的空白，为未来研究提供了清晰的组织结构和指导方向。

Abstract: Large language models (LLMs) have demonstrated strong reasoning, planning, and communication abilities, enabling them to operate as autonomous agents in open environments. While single-agent systems remain limited in adaptability and coordination, recent progress has shifted attention toward multi-agent systems (MAS) composed of interacting LLMs that pursue cooperative, competitive, or mixed objectives. This emerging paradigm provides a powerful testbed for studying social dynamics and strategic behaviors among intelligent agents. However, current research remains fragmented and lacks a unifying theoretical foundation. To address this gap, we present a comprehensive survey of LLM-based multi-agent systems through a game-theoretic lens. By organizing existing studies around the four key elements of game theory: players, strategies, payoffs, and information, we establish a systematic framework for understanding, comparing, and guiding future research on the design and analysis of LLM-based MAS.

</details>


### [72] [From Who They Are to How They Act: Behavioral Traits in Generative Agent-Based Models of Social Media](https://arxiv.org/abs/2601.15114)
*Valerio La Gatta,Gian Marco Orlando,Marco Perillo,Ferdinando Tammaro,Vincenzo Moscato*

Main category: cs.MA

TL;DR: 该研究提出在生成智能体建模中引入行为特质层，以模拟社交媒体中用户差异化的参与模式，解决现有方法中智能体行为同质化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成智能体建模框架通过人口统计属性、人格特质和兴趣来表征智能体，但缺乏对平台行为倾向的编码机制，导致智能体表现出同质化的参与模式，而非真实社交媒体中观察到的差异化参与风格。

Method: 引入行为特质作为显式表征层，调节智能体在发帖、转发、评论、反应和不活动等行为上的倾向性。通过涉及980个智能体的大规模模拟，并与真实社交媒体数据进行验证。

Result: 研究表明行为特质对于维持异质性、符合用户画像的参与模式至关重要，能够通过放大导向型和互动导向型用户画像的相互作用实现真实的内容传播动态。

Conclusion: 建模智能体如何行动（而不仅仅是他们是谁）对于推进生成智能体建模作为研究社交媒体现象的工具是必要的。

Abstract: Generative Agent-Based Modeling (GABM) leverages Large Language Models to create autonomous agents that simulate human behavior in social media environments, demonstrating potential for modeling information propagation, influence processes, and network phenomena. While existing frameworks characterize agents through demographic attributes, personality traits, and interests, they lack mechanisms to encode behavioral dispositions toward platform actions, causing agents to exhibit homogeneous engagement patterns rather than the differentiated participation styles observed on real platforms. In this paper, we investigate the role of behavioral traits as an explicit characterization layer to regulate agents' propensities across posting, re-sharing, commenting, reacting, and inactivity. Through large-scale simulations involving 980 agents and validation against real-world social media data, we demonstrate that behavioral traits are essential to sustain heterogeneous, profile-consistent participation patterns and enable realistic content propagation dynamics through the interplay of amplification- and interaction-oriented profiles. Our findings establish that modeling how agents act-not only who they are-is necessary for advancing GABM as a tool for studying social media phenomena.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [73] [When Generative AI Is Intimate, Sexy, and Violent: Examining Not-Safe-For-Work (NSFW) Chatbots on FlowGPT](https://arxiv.org/abs/2601.14324)
*Xian Li,Yuanning Han,Di Liu,Pengcheng An,Shuo Niu*

Main category: cs.HC

TL;DR: 该研究分析了FlowGPT平台上376个NSFW聊天机器人和307个公开对话会话，识别了四种聊天机器人类型，发现NSFW内容在用户提示和机器人输出中普遍存在，揭示了虚拟亲密、性幻想、暴力表达和不安全内容获取的组合体验。


<details>
  <summary>Details</summary>
Motivation: 生成式AI驱动的用户创建聊天机器人为分享和互动NSFW内容提供了新途径，但目前对这些聊天机器人的特征及其用户互动了解甚少，需要基于社交媒体的NSFW功能理论进行系统分析。

Method: 基于社交媒体的NSFW功能理论，对FlowGPT平台上的376个NSFW聊天机器人和307个公开对话会话进行内容分析，识别聊天机器人类型和互动模式。

Result: 识别出四种聊天机器人类型：角色扮演角色、故事生成器、图像生成器和"do-anything-now"机器人；AI角色最普遍，常使用露骨头像吸引互动；性、暴力和侮辱性内容在用户提示和机器人输出中均出现，部分机器人在用户未创建色情提示时仍生成露骨内容。

Conclusion: FlowGPT上的NSFW体验可理解为虚拟亲密、性幻想、暴力表达和不安全内容获取的组合，研究为聊天机器人设计、创作者支持、用户安全和内容审核提供了重要启示。

Abstract: User-created chatbots powered by generative AI offer new ways to share and interact with Not-Safe-For-Work (NSFW) content. However, little is known about the characteristics of these GenAI-based chatbots and their user interactions. Drawing on the functional theory of NSFW on social media, this study analyzes 376 NSFW chatbots and 307 public conversation sessions on FlowGPT. Findings identify four chatbot types: roleplay characters, story generators, image generators, and do-anything-now bots. AI Characters portraying fantasy personas and enabling hangout-style interactions are most common, often using explicit avatar images to invite engagement. Sexual, violent, and insulting content appears in both user prompts and chatbot outputs, with some chatbots generating explicit material even when users do not create erotic prompts. In sum, the NSFW experience on FlowGPT can be understood as a combination of virtual intimacy, sexual delusion, violent thought expression, and unsafe content acquisition. We conclude with implications for chatbot design, creator support, user safety, and content moderation.

</details>


### [74] [Loss Aversion Online: Emotional Responses to Financial Booms and Crashes](https://arxiv.org/abs/2601.14423)
*Aryan Ramchandra Kapadia,Niharika Bhattacharjee,Mung Yao Jia,Ishq Gupta,Dong Wang,Koustuv Saha*

Main category: cs.HC

TL;DR: 该研究通过分析Reddit社区在金融繁荣与崩溃期间的情感表达，发现金融崩溃导致一致负面情绪变化，而繁荣期反应较弱且混合，符合损失厌恶理论。


<details>
  <summary>Details</summary>
Motivation: 金融事件对情感健康有负面影响，但利用实时社交媒体数据大规模研究其对在线情感表达影响的工作仍有限。本研究旨在填补这一空白，通过分析Reddit社区在金融事件期间的情感表达变化。

Method: 采用准实验方法：差异中的差异（DiD）和因果影响分析，在两个案例研究（金融崩溃和繁荣）期间，分析金融与非金融Reddit社区的每日情感、情绪和LIWC词频计数。

Result: 金融崩溃期间观察到一致、负面的情绪反应变化，而繁荣期间的反应较弱且混合，这与损失厌恶理论一致。金融与非金融社区在情感和心理语言反应上存在差异。

Conclusion: 通过探索金融事件期间的情感和心理语言表达，为理解在线用户心理健康和建设互联健康社区提供了未来启示。研究证实了金融事件对在线情感表达的显著影响，特别是崩溃期间的负面情绪变化。

Abstract: Financial events negatively affect emotional well-being, but large-scale studies examining their impact on online emotional expression using real-time social media data remain limited. To address this gap, we propose analyzing Reddit communities (financial and non-financial) across two case studies: a financial crash and a boom. We investigate how emotional and psycholinguistic responses differ between financial and non-financial communities, and the extent to which the type of financial event affects user behavior during the two case study periods. To examine the effect of these events on expressed language, we analyze daily sentiment, emotion, and LIWC counts using quasi-experimental methods: Difference-in-Differences (DiD) and Causal Impact analyses during a financial boom and a financial crash. Overall, we find coherent, negative shifts in emotional responses during financial crashes, but weaker, mixed responses during booms, consistent with loss aversion. By exploring emotional and psycholinguistic expressions during financial events, we identify future implications for understanding online users' mental health and building connected, healthy communities.

</details>


### [75] [SPIRIT: A Design Framework To Support Technology Interventions for Spiritual Care Within and Beyond the Clinic](https://arxiv.org/abs/2601.14435)
*C. Estelle Smith,Alemitu Bezabih,Shadi Nourriz,Jesan Ahammed Ovi*

Main category: cs.HC

TL;DR: 该论文提出了SPIRIT设计框架，用于指导数字技术在灵性关怀领域的应用，通过修订灵性支持定义和识别关键设计维度来弥合临床灵性关怀中的技术应用差距。


<details>
  <summary>Details</summary>
Motivation: 灵性关怀对健康福祉至关重要，但在人机交互领域研究不足，且临床灵性关怀中的技术应用落后于其他医疗领域。需要建立系统化的设计框架来指导数字技术在灵性关怀中的有效应用。

Method: 通过成员检查法修订先前定义的灵性支持概念，结合重新分析先前数据和新访谈专业灵性关怀提供者，识别灵性关怀的三个先决条件和六个设计维度，最终形成SPIRIT设计框架。

Result: 识别出灵性关怀的三个先决条件：开放接受关怀、安全空间、识别和表达灵性需求的能力；提出六个设计维度：爱的临在、意义建构、技术使用的适当程度、地点、关系亲近程度、时间性。

Conclusion: SPIRIT框架为设计和评估数字灵性关怀干预系统提供了系统指导，有助于扩展灵性关怀的交付模式，适用于临床和非临床环境，推动灵性关怀领域的技术创新。

Abstract: Despite its importance for well-being, spiritual care remains under-explored in HCI, while the adoption of technology in clinical spiritual care lags behind other healthcare fields. Prior work derived a definition of "spiritual support" through co-design workshops with stakeholders in online health communities. This paper contributes: (1) a revision of that definition through member checking with professional spiritual care providers (SCPs); (2) a novel design framework -- SPIRIT -- which can help to expand models of delivery for spiritual care using digital technologies. Through re-analysis of previous data and new interviews with SCPs, we identify three prerequisites for meaningful spiritual care: openness to care, safe space, and the ability to discern and articulate spiritual needs. We also propose six design dimensions: loving presence, meaning-making, appropriate degree of technology use, location, degree of relational closeness, and temporality. We discuss how SPIRIT offers guidance for designing impactful digital spiritual care intervention systems within and beyond clinical settings.

</details>


### [76] [Evaluating Preattentive Features for Detecting Changes in Virtual Environments](https://arxiv.org/abs/2601.14561)
*DongHoon Kim,Isaac Cho*

Main category: cs.HC

TL;DR: 该研究探讨了前注意处理如何影响沉浸式VR环境中的变化检测，发现空间隔离的物体变化检测更可靠，而视觉相似物体包围的物体变化检测更困难。


<details>
  <summary>Details</summary>
Motivation: 随着VR环境中视觉复杂度的增加，感知性能下降，变化检测变得困难。研究旨在探索前注意处理特征（视觉特征、物体属性、空间邻近性）如何影响沉浸式3D环境中的变化检测任务。

Method: 研究在沉浸式VR环境中进行变化检测任务实验，重点关注视觉复杂度、物体属性和空间邻近性。通过比较空间隔离物体与视觉相似物体包围的物体的变化检测表现，分析前注意处理的影响。

Result: 前注意处理显著增强变化检测，特别是当变化物体空间隔离且不与周围相似物体形成感知分组时。空间隔离物体的变化检测更可靠，而视觉相似物体包围的物体变化检测更困难，表明感知分组阻碍复杂场景中的个体物体识别。

Conclusion: 研究结果为VR应用设计提供了指导：通过战略性地利用空间隔离和视觉特征，可以减少认知负荷，提高变化检测性能，从而改善用户体验。

Abstract: Visual perception plays a critical role in detecting changes within immersive Virtual Reality (VR) environments. However, as visual complexity increases, perceptual performance declines, making it more difficult to detect changes quickly and accurately. This study examines how visual features, known for facilitating preattentive processing, impact a change detection task in immersive 3D environments, with a focus on visual complexity, object attributes, and spatial proximity. Our results demonstrate that preattentive processing enhances change detection, particularly when the altered object is spatially isolated and not perceptually grouped with similar surrounding objects. Changes to isolated objects were detected more reliably, suggesting that perceptual isolation reduces cognitive load and draws more attention. Conversely, when a changed object was surrounded by visually similar elements, participants were less likely to detect the change, indicating that perceptual grouping hinders individual object recognition in complex scenes. These results provide guidelines for designing VR applications that strategically utilize spatial isolation and visual features to improve the user experience.

</details>


### [77] [Explainable OOHRI: Communicating Robot Capabilities and Limitations as Augmented Reality Affordances](https://arxiv.org/abs/2601.14587)
*Lauren W. Wang,Mohamed Kari,Parastoo Abtahi*

Main category: cs.HC

TL;DR: X-OOHRI：一个增强现实界面，通过视觉符号、径向菜单、色彩编码和解释标签传达机器人动作可能性与约束，实现可解释的人机交互


<details>
  <summary>Details</summary>
Motivation: 当前机器人系统对用户来说大多是黑盒，缺乏对其能力与限制的透明性，这阻碍了用户发出个性化指令并在可能失败时提供协助

Method: 开发可解释的面向对象人机交互（X-OOHRI）AR界面，使用视觉语言模型将物体属性和机器人限制编码为面向对象结构，支持实时解释生成和虚拟孪生体的直接操作

Result: 将端到端管道与物理机器人集成，展示了从低级拾取放置到高级指令的多样化用例；用户研究表明参与者能有效发出面向对象指令，准确理解机器人限制，并进行混合主动式问题解决

Conclusion: X-OOHRI系统通过增强现实界面成功提高了机器人系统的透明性和可解释性，使用户能够更好地理解机器人能力限制并有效进行交互

Abstract: Human interaction is essential for issuing personalized instructions and assisting robots when failure is likely. However, robots remain largely black boxes, offering users little insight into their evolving capabilities and limitations. To address this gap, we present explainable object-oriented HRI (X-OOHRI), an augmented reality (AR) interface that conveys robot action possibilities and constraints through visual signifiers, radial menus, color coding, and explanation tags. Our system encodes object properties and robot limits into object-oriented structures using a vision-language model, allowing explanation generation on the fly and direct manipulation of virtual twins spatially aligned within a simulated environment. We integrate the end-to-end pipeline with a physical robot and showcase diverse use cases ranging from low-level pick-and-place to high-level instructions. Finally, we evaluate X-OOHRI through a user study and find that participants effectively issue object-oriented commands, develop accurate mental models of robot limitations, and engage in mixed-initiative resolution.

</details>


### [78] [Designing KRIYA: An AI Companion for Wellbeing Self-Reflection](https://arxiv.org/abs/2601.14589)
*Shanshan Zhu,Wenxuan Song,Jiayue Melissa Shi,Dong Whi Yoo,Karthik S. Bhat,Koustuv Saha*

Main category: cs.HC

TL;DR: KRIYA是一款AI健康伴侣，通过协同解释的方式帮助用户理解健康数据，强调自我反思而非绩效比较，包含舒适区、侦探模式和假设规划等功能。


<details>
  <summary>Details</summary>
Motivation: 现有健康应用通常展示总结性仪表板，用户难以将数据转化为有意义的理解。这些应用通过目标、提醒和结构化指标来促进参与，但可能强化比较、评判和表现焦虑。需要探索一种强调自我反思的补充方法。

Method: 设计KRIYA AI健康伴侣，支持与用户协同解释个人健康数据。包含舒适区、侦探模式和假设规划等功能。对18名大学生进行半结构化访谈，让他们使用假设数据与KRIYA原型交互。

Result: 用户将健康数据互动视为解释而非绩效表现；反思体验取决于情感框架，可以是支持性或压力性的；通过透明度建立信任。KRIYA促进了好奇心、自我同情和个人健康数据的反思性意义建构。

Conclusion: AI伴侣应支持好奇心、自我同情和个人健康数据的反思性意义建构。设计需关注情感框架和透明度，以促进支持性反思而非压力性体验。

Abstract: Most personal wellbeing apps present summative dashboards of health and physical activity metrics, yet many users struggle to translate this information into meaningful understanding. These apps commonly support engagement through goals, reminders, and structured targets, which can reinforce comparison, judgment, and performance anxiety. To explore a complementary approach that prioritizes self-reflection, we design KRIYA, an AI wellbeing companion that supports co-interpretive engagement with personal wellbeing data. KRIYA aims to collaborate with users to explore questions, explanations, and future scenarios through features such as Comfort Zone, Detective Mode, and What-If Planning. We conducted semi-structured interviews with 18 college students interacting with a KRIYA prototype using hypothetical data. Our findings show that through KRIYA interaction, users framed engaging with wellbeing data as interpretation rather than performance, experienced reflection as supportive or pressuring depending on emotional framing, and developed trust through transparency. We discuss design implications for AI companions that support curiosity, self-compassion, and reflective sensemaking of personal health data.

</details>


### [79] [Seeing to Think? How Source Transparency Design Shapes Interactive Information Seeking and Evaluation in Conversational AI](https://arxiv.org/abs/2601.14611)
*Jiangen He,Jiqun Liu*

Main category: cs.HC

TL;DR: 研究探讨了对话AI系统中源透明度设计如何影响信息寻求、信任和批判性参与，通过四种界面设计比较发现不同设计在流程流畅性和反思验证之间存在权衡


<details>
  <summary>Details</summary>
Motivation: 对话AI系统日益成为信息寻求的主要接口，但其如何呈现来源以支持信息评估仍未被充分探索。需要研究源透明度设计如何影响交互式信息寻求、信任和批判性参与。

Method: 采用受控组间实验设计(N=372)，比较四种源呈现界面：可折叠式、悬停卡片、页脚和对齐侧边栏，这些界面在可见性和可访问性上有所不同。使用细粒度行为分析和自动化批判性思维评估方法。

Result: 界面设计从根本上改变了探索策略和证据整合方式。悬停卡片界面在任务期间促进了无缝的按需验证，而对齐侧边栏独特地减轻了信息过载的负面影响：随着引用密度增加，侧边栏用户的批判性思维和综合得分显著高于其他条件。

Conclusion: 研究结果突显了支持工作流程流畅性的设计与强制执行反思验证的设计之间的权衡，为设计能够促进对AI生成内容进行批判性参与的适应性和负责任的对话AI提供了实际意义。

Abstract: Conversational AI systems increasingly function as primary interfaces for information seeking, yet how they present sources to support information evaluation remains under-explored. This paper investigates how source transparency design shapes interactive information seeking, trust, and critical engagement. We conducted a controlled between-subjects experiment (N=372) comparing four source presentation interfaces - Collapsible, Hover Card, Footer, and Aligned Sidebar - varying in visibility and accessibility. Using fine-grained behavioral analysis and automated critical thinking assessment, we found that interface design fundamentally alters exploration strategies and evidence integration. While the Hover Card interface facilitated seamless, on-demand verification during the task, the Aligned Sidebar uniquely mitigated the negative effects of information overload: as citation density increased, Sidebar users demonstrated significantly higher critical thinking and synthesis scores compared to other conditions. Our results highlight a trade-off between designs that support workflow fluency and those that enforce reflective verification, offering practical implications for designing adaptive and responsible conversational AI that fosters critical engagement with AI generated content.

</details>


### [80] [DesignBridge: Bridging Designer Expertise and User Preferences through AI-Enhanced Co-Design for Fashion](https://arxiv.org/abs/2601.14639)
*Yuheng Shao,Yuansong Xu,Yifan Jin,Shuhao Zhang,Wenxin Gu,Quan Li*

Main category: cs.HC

TL;DR: DesignBridge是一个多平台AI增强交互系统，通过三个阶段连接设计师专业知识和用户偏好，解决时尚设计协作中的挑战。


<details>
  <summary>Details</summary>
Motivation: 时尚设计中设计师与用户的有效协作对于提高产品接受度和创造价值很重要，但传统方法存在限制：设计师中心方法限制了用户参与，用户驱动方法需要设计能力且边缘化专业创意判断。现有协同设计实践存在用户参与度低、偏好收集效率低、难以平衡用户反馈与设计考量等问题。

Method: 首先进行形成性研究（N=7），识别协同设计过程中设计师与用户协作的关键挑战和需求。基于这些洞察，开发DesignBridge系统，包含三个阶段：1)初始设计框架（设计师定义初始概念）；2)偏好表达收集（用户通过交互工具直观表达偏好）；3)偏好整合设计（设计师使用AI辅助分析将反馈整合到连贯设计中）。

Result: 用户研究表明，DesignBridge显著增强了用户偏好收集和分析能力，使设计师能够将多样化偏好与专业专长相结合。

Conclusion: DesignBridge通过多平台AI增强交互系统有效解决了时尚设计协作中的挑战，成功连接了设计师专业知识和用户偏好，提高了协同设计的效果。

Abstract: Effective collaboration between designers and users is important for fashion design, which can increase the user acceptance of fashion products and thereby create value. However, it remains an enduring challenge, as traditional designer-centric approaches restrict meaningful user participation, while user-driven methods demand design proficiency, often marginalizing professional creative judgment. Current co-design practices, including workshops and AI-assisted frameworks, struggle with low user engagement, inefficient preference collection, and difficulties in balancing user feedback with design considerations. To address these challenges, we conducted a formative study with designers and users experienced in co-design (N=7), identifying critical challenges for current collaboration between designers and users in the co-design process, and their requirements. Informed by these insights, we introduce DesignBridge, a multi-platform AI-enhanced interactive system that bridges designer expertise and user preferences through three stages: (1) Initial Design Framing, where designers define initial concepts. (2) Preference Expression Collection, where users intuitively articulate preferences via interactive tools. (3) Preference-Integrated Design, where designers use AI-assisted analytics to integrate feedback into cohesive designs. A user study demonstrates that DesignBridge significantly enhances user preference collection and analysis, enabling designers to integrate diverse preferences with professional expertise.

</details>


### [81] [MIND: Empowering Mental Health Clinicians with Multimodal Data Insights through a Narrative Dashboard](https://arxiv.org/abs/2601.14641)
*Ruishi Zou,Shiyu Xu,Margaret E Morris,Jihan Ryu,Timothy D. Becker,Nicholas Allen,Anne Marie Albano,Randy Auerbach,Dan Adler,Varun Mishra,Lace Padilla,Dakuo Wang,Ryan Sultan,Xuhai "Orson" Xu*

Main category: cs.HC

TL;DR: MIND：基于大语言模型的心理健康多模态数据仪表板，通过叙事文本和图表呈现患者生成数据与临床数据的整合分析，提升临床决策支持


<details>
  <summary>Details</summary>
Motivation: 随着患者生成数据（可穿戴设备、智能手机、生态瞬时评估等）的收集能力增强，这些数据在心理健康护理中具有重要价值。然而，如何将这些多模态数据流与临床数据（如临床记录）有效整合并呈现给临床医生，以支持临床决策，仍面临重大挑战。

Method: 通过五位临床医生的协同设计会议，开发了MIND系统——一个基于大语言模型的仪表板，旨在呈现心理健康护理中临床相关的多模态数据洞察。MIND通过叙事文本呈现多模态洞察，辅以图表展示底层数据。进行了用户研究（N=16）评估系统效果。

Result: 用户研究表明，临床医生认为MIND相比基线方法有显著改进：在揭示隐藏的临床相关数据洞察方面表现更好（p<.001），在支持临床决策方面也更有效（p=.004）。

Conclusion: MIND展示了基于大语言模型的数据叙事方法在整合多模态患者生成数据和临床数据方面的潜力。研究结果为将数据叙事整合到更广泛的临床实践中提供了未来研究方向。

Abstract: Advances in data collection enable the capture of rich patient-generated data: from passive sensing (e.g., wearables and smartphones) to active self-reports (e.g., cross-sectional surveys and ecological momentary assessments). Although prior research has demonstrated the utility of patient-generated data in mental healthcare, significant challenges remain in effectively presenting these data streams along with clinical data (e.g., clinical notes) for clinical decision-making. Through co-design sessions with five clinicians, we propose MIND, a large language model-powered dashboard designed to present clinically relevant multimodal data insights for mental healthcare. MIND presents multimodal insights through narrative text, complemented by charts communicating underlying data. Our user study (N=16) demonstrates that clinicians perceive MIND as a significant improvement over baseline methods, reporting improved performance to reveal hidden and clinically relevant data insights (p<.001) and support their decision-making (p=.004). Grounded in the study results, we discuss future research opportunities to integrate data narratives in broader clinical practices.

</details>


### [82] [Talk Me Through It: Developing Effective Systems for Chart Authoring](https://arxiv.org/abs/2601.14707)
*Nazar Ponochevnyi,Young-Ho Kim,Joseph Jay Williams,Anastasia Kuzminykh*

Main category: cs.HC

TL;DR: 该研究探讨了图表创建系统中不同类型指令数据的结构差异，发现语音想象图表指令比文本现有图表指令包含更丰富的命令格式和语言特征，且基于语音想象图表数据训练的系统性能更优。


<details>
  <summary>Details</summary>
Motivation: 当前图表创建系统主要基于用户查看现有图表时输入的文本指令进行训练，但实际应用中用户更多是通过语音描述想象中的图表来创建新图表。这两种场景的认知过程和指令结构差异尚未得到充分研究，需要探索不同类型指令数据的特性及其对系统性能的影响。

Method: 研究对比分析了三种指令类型：语音想象图表指令、文本想象图表指令和文本现有图表指令。通过实证方法分析这些指令在命令格式、元素规范和语言特征等方面的结构差异。然后比较了基于语音想象图表数据训练的系统与基于文本现有图表数据训练的系统的性能表现。

Result: 研究发现想象图表指令（特别是语音形式）包含更丰富的命令格式、更详细的元素规范和更复杂的语言特征。性能测试表明，基于语音想象图表数据训练的系统在语音和文本输入上都优于基于文本现有图表数据训练的系统。

Conclusion: 针对语音想象图表数据进行专门训练对于提高图表创建系统在实际应用场景中的性能至关重要。研究提出了相应的设计指南，帮助系统更好地处理用户通过语音描述想象中的图表来创建新图表的真实需求。

Abstract: Recent chart-authoring systems increasingly focus on natural-language input, enabling users to form a mental image of the chart they wish to create and express this intent using spoken instructions (spoken imagined-chart data). Yet these systems are predominantly trained on typed instructions written while viewing the target chart (typed existing-chart data). While the cognitive processes for describing an existing chart arguably differ from those for creating a new chart, the structural differences in the corresponding prompts remain underexplored. We present empirical findings on the structural differences among spoken imagined-chart instructions, typed imagined-chart instructions, and typed existing-chart instructions for chart creation, showing that imagined-chart prompts contain richer command formats, element specifications, and complex linguistic features, especially in spoken instructions. We then compare the performance of systems trained on spoken imagined-chart data versus typed existing-chart data, finding that the first system outperforms the second one on both voice and text input, highlighting the necessity of targeted training on spoken imagined-chart data. We conclude with design guidelines for chart-authoring systems to improve performance in real-world scenarios.

</details>


### [83] [The CHI26 Workshop on the Future of Cognitive Personal Informatics](https://arxiv.org/abs/2601.14891)
*Christina Schneegass,Francesco Chiossi,Anna L. Cox,Dimitra Dritsa,Teodora Mitrevska,Stephen Rainey,Max L. Wilson*

Main category: cs.HC

TL;DR: 关于认知个人信息学（CPI）的研讨会，探讨如何将复杂认知数据转化为有意义指标、AI如何支持数据理解而不过度简化认知洞察，以及如何设计考虑个体差异和神经多样性的包容性CPI技术。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴认知追踪技术在消费市场的兴起，以及生成式AI为分析、可视化和解释认知数据提供新方法，认知追踪可能很快会像测量跑步时心率一样简单。然而，认知数据本质上比身体活动数据更复杂、更依赖上下文且理解不足。

Method: 通过研讨会形式，汇集HCI专家讨论关键问题，包括：如何将复杂认知数据转化为有意义指标；AI如何支持用户数据理解而不过度简化认知洞察；如何设计考虑个体差异和神经多样性的包容性CPI技术。

Result: 该研讨会旨在绘制认知个人信息学领域的未来研究方向图景，但目前尚未提供具体研究结果，因为这是一个前瞻性的研讨会提案。

Conclusion: 认知个人信息学是一个新兴且快速发展的领域，需要跨学科合作来解决认知数据的复杂性、AI辅助解释的平衡问题，以及确保技术设计的包容性，以应对个体差异和神经多样性。

Abstract: Research on Cognitive Personal Informatics (CPI) is steadily growing as new wearable cognitive tracking technologies emerge on the consumer market, claiming to measure stress, focus, and other cognitive factors. At the same time, with generative AI offering new ways to analyse, visualize, and interpret cognitive data, we hypothesize that cognitive tracking will soon become as simple as measuring your heart rate during a run. Yet, cognitive data remains inherently more complex, context-dependent, and less well understood than physical activity data. This workshop brings together HCI experts to discuss critical questions, including: How can complex cognitive data be translated into meaningful metrics? How can AI support users' data sensemaking without over-simplifying cognitive insights? How can we design inclusive CPI technologies that consider inter-personal variance and neurodiversity? We will map

</details>


### [84] [Incentive-Tuning: Understanding and Designing Incentives for Empirical Human-AI Decision-Making Studies](https://arxiv.org/abs/2601.15064)
*Simran Kaur,Sara Salimzadeh,Ujwal Gadiraju*

Main category: cs.HC

TL;DR: 该论文探讨了人机协作决策研究中激励设计的关键作用，通过主题综述分析了现有实践、挑战和机遇，并提出了激励调优框架来指导研究者设计有效的激励方案。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在决策领域取得了革命性进展，但在高风险决策中人类判断仍然至关重要。人机协作决策研究通常通过众包平台招募参与者，而激励设计直接影响参与者行为和研究有效性。目前缺乏系统性的激励设计指导，影响了研究的可靠性和可推广性。

Method: 采用主题综述方法，系统分析现有研究中激励设计的实践、挑战和机遇。识别了激励方案的组成要素、研究者如何操纵激励方案以及激励对研究结果的影响等主题模式。基于这些发现，开发了激励调优框架。

Result: 识别了激励设计的三个核心主题：激励方案的组成要素、研究者的操纵方式以及激励对研究结果的影响。提出了激励调优框架，为研究者提供了设计、反思和记录激励设计过程的系统化指南。

Conclusion: 激励设计在人机决策实证研究中具有关键作用。通过标准化的激励设计方法和实用工具，可以提高研究的可靠性和可推广性，推动人机协作决策领域知识的积累。

Abstract: AI has revolutionised decision-making across various fields. Yet human judgement remains paramount for high-stakes decision-making. This has fueled explorations of collaborative decision-making between humans and AI systems, aiming to leverage the strengths of both. To explore this dynamic, researchers conduct empirical studies, investigating how humans use AI assistance for decision-making and how this collaboration impacts results. A critical aspect of conducting these studies is the role of participants, often recruited through crowdsourcing platforms. The validity of these studies hinges on the behaviours of the participants, hence effective incentives that can potentially affect these behaviours are a key part of designing and executing these studies. In this work, we aim to address the critical role of incentive design for conducting empirical human-AI decision-making studies, focusing on understanding, designing, and documenting incentive schemes. Through a thematic review of existing research, we explored the current practices, challenges, and opportunities associated with incentive design for human-AI decision-making empirical studies. We identified recurring patterns, or themes, such as what comprises the components of an incentive scheme, how incentive schemes are manipulated by researchers, and the impact they can have on research outcomes. Leveraging the acquired understanding, we curated a set of guidelines to aid researchers in designing effective incentive schemes for their studies, called the Incentive-Tuning Framework, outlining how researchers can undertake, reflect on, and document the incentive design process. By advocating for a standardised yet flexible approach to incentive design and contributing valuable insights along with practical tools, we hope to pave the way for more reliable and generalizable knowledge in the field of human-AI decision-making.

</details>


### [85] [Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface](https://arxiv.org/abs/2601.15209)
*Paige S. DeVries,Michaela Okosi,Ming Li,Nora Dunphy,Gidey Gezae,Dante Conway,Abraham Glasser,Raja Kushalnagar,Christian Vogler*

Main category: cs.HC

TL;DR: 研究比较了智能个人助手对聋人和听力障碍者的可访问性，对比了语音输入（包括Alexa自动语音识别和人工转述）与LLM辅助触摸界面两种方法，发现两者在可用性上没有显著差异。


<details>
  <summary>Details</summary>
Motivation: 智能个人助手无法理解包括聋人语音在内的多样化口音，这使得不会手语但能说话的聋人和听力障碍者难以使用这些助手，需要探索替代交互方式来提高可访问性。

Method: 使用Echo Show设备，通过混合方法研究比较三种输入方式：1）自然语言语音输入（Alexa自动语音识别）；2）Wizard-of-Oz设置中训练有素的协助者转述命令；3）LLM辅助的触摸界面（通过"任务提示器"整合用户历史和智能环境来建议上下文相关命令）。

Result: 定量结果显示两种语音输入条件与LLM辅助触摸界面之间没有显著差异。定性结果显示参与者对不同方法的可用性意见存在差异。最终表明需要让智能个人助手原生支持对聋人口音的稳健识别。

Conclusion: 虽然LLM辅助触摸界面为聋人和听力障碍者提供了可行的替代方案，但根本解决方案是让智能个人助手能够原生识别和理解聋人口音，以实现真正的可访问性。

Abstract: We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa's automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered "task prompter," which integrated the user's history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.

</details>


### [86] [LLM-based Multimodal Feedback Produces Equivalent Learning and Better Student Perceptions than Educator Feedback](https://arxiv.org/abs/2601.15280)
*Chloe Qianhui Zhao,Jie Cao,Jionghao Lin,Kenneth R. Koedinger*

Main category: cs.HC

TL;DR: AI多模态反馈系统整合文本解释、幻灯片引用和音频讲解，在保持学习效果的同时显著提升反馈感知质量，降低认知负荷，并展现不同的参与模式。


<details>
  <summary>Details</summary>
Motivation: 提供及时、有针对性的多模态反馈有助于学生快速纠正错误、建立深刻理解和保持学习动力，但在大规模教学中实现这一目标仍然面临挑战。

Method: 开发实时AI辅助多模态反馈系统，整合结构化文本解释、动态多媒体资源（包括检索最相关幻灯片页面引用）和流式AI音频讲解。通过在线众包实验，将该系统与教育者提供的固定常规反馈在三个维度进行比较：学习效果、学习者参与度、感知反馈质量和价值。

Result: AI多模态反馈在学习效果上与原始教育者反馈相当，同时在感知清晰度、针对性、简洁性、动机、满意度方面显著优于后者，并降低了认知负荷，在正确性、信任度和接受度方面表现相当。过程日志显示不同的参与模式：对于选择题，教育者反馈鼓励更多提交；对于开放式问题，AI辅助的针对性建议降低了修订障碍，促进了迭代改进。

Conclusion: AI多模态反馈具有提供可扩展、实时、上下文感知支持的潜力，既能减少教师工作量，又能增强学生学习体验。

Abstract: Providing timely, targeted, and multimodal feedback helps students quickly correct errors, build deep understanding and stay motivated, yet making it at scale remains a challenge. This study introduces a real-time AI-facilitated multimodal feedback system that integrates structured textual explanations with dynamic multimedia resources, including the retrieved most relevant slide page references and streaming AI audio narration. In an online crowdsourcing experiment, we compared this system against fixed business-as-usual feedback by educators across three dimensions: (1) learning effectiveness, (2) learner engagement, (3) perceived feedback quality and value. Results showed that AI multimodal feedback achieved learning gains equivalent to original educator feedback while significantly outperforming it on perceived clarity, specificity, conciseness, motivation, satisfaction, and reducing cognitive load, with comparable correctness, trust, and acceptance. Process logs revealed distinct engagement patterns: for multiple-choice questions, educator feedback encouraged more submissions; for open-ended questions, AI-facilitated targeted suggestions lowered revision barriers and promoted iterative improvement. These findings highlight the potential of AI multimodal feedback to provide scalable, real-time, and context-aware support that both reduces instructor workload and enhances student experience.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [87] [Unpacking Security Scanners for GitHub Actions Workflows](https://arxiv.org/abs/2601.14455)
*Madjda Fares,Yogya Gamage,Benoit Baudry*

Main category: cs.SE

TL;DR: 对9个GitHub Actions工作流安全扫描器进行首次系统性比较，分析其范围、检测能力和可用性，基于596个工作流的测试结果提供可操作的安全建议。


<details>
  <summary>Details</summary>
Motivation: GitHub Actions作为广泛使用的自动化平台，已成为软件供应链攻击的主要目标。攻击者利用过度权限、版本模糊和缺乏完整性检查等弱点来破坏工作流。虽然已出现多个安全扫描器帮助开发者加固工作流，但缺乏对这些工具的系统性比较和评估。

Method: 1. 建立包含10种安全弱点的分类法；2. 收集并策划596个工作流作为测试集；3. 从范围（目标安全弱点）、检测能力（检测到的弱点数量）和可用性（扫描时间）三个维度比较9个扫描器；4. 分析扫描器对安全弱点的不同解释方式。

Result: 1. GitHub Actions工作流安全扫描器生态多样化，既有范围广泛的工具，也有高度专注的工具；2. 不同扫描器对安全弱点的解释存在显著差异，导致报告的类型和数量不一致；3. 扫描器的检测能力和覆盖范围各不相同，没有单一工具能全面覆盖所有安全弱点。

Conclusion: 基于实证证据，为开发者提供加固GitHub Actions工作流的可操作建议。研究揭示了当前扫描器生态的多样性以及安全弱点解释的不一致性，强调了需要更统一的安全标准和多工具组合使用的重要性。

Abstract: GitHub Actions is a widely used platform that allows developers to automate the build and deployment of their projects through configurable workflows. As the platform's popularity continues to grow, it has become a target of choice for recent software supply chain attacks. These attacks exploit excessive permissions, ambiguous versions, or the absence of artifact integrity checks to compromise workflows. In response to these attacks, several security scanners have emerged to help developers harden their workflows.
  In this paper, we perform the first systematic comparison of 9 GitHub Actions workflow security scanners. We compare them in terms of scope (which security weaknesses they target), detection capabilities (how many weaknesses they detect), and usability (how long they take to scan a workflow). To compare scanners on a common ground, we first establish a taxonomy of 10 security weaknesses that can occur in GitHub Actions workflows. Then, we run the scanners against a curated set of 596 workflows.
  Our study reveals that the landscape of GitHub Actions workflow security scanners is diverse, with both broad-scope tools and very focused ones. More importantly, we show that scanners interpret security weaknesses differently, leading to significant differences in the type and number of reported weaknesses. Based on this empirical evidence, we make actionable recommendations for developers to harden their GitHub Actions workflows.

</details>


### [88] [Tokenomics: Quantifying Where Tokens Are Used in Agentic Software Engineering](https://arxiv.org/abs/2601.14470)
*Mohamad Salim,Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 该研究分析了基于LLM的多智能体系统在软件开发生命周期中的令牌消耗模式，发现代码审查阶段消耗了大部分令牌（平均59.4%），输入令牌占比最大（平均53.9%），揭示了智能体协作中的效率问题。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统在软件工程任务中应用日益广泛，但其运行效率和资源消耗缺乏深入理解，导致实际应用受到不可预测的成本和环境影响的阻碍。需要分析SDLC中的令牌消耗模式，为优化提供依据。

Method: 使用ChatDev框架和GPT-5推理模型执行30个软件开发任务，分析执行轨迹，将内部阶段映射到标准开发阶段（设计、编码、代码完成、代码审查、测试、文档），建立标准化评估框架，量化比较各阶段的令牌分布（输入、输出、推理）。

Result: 迭代的代码审查阶段消耗了大部分令牌（平均59.4%），输入令牌在所有阶段中占比最大（平均53.9%），表明智能体协作存在显著效率问题。主要成本不在初始代码生成，而在自动化精炼和验证过程。

Conclusion: 该研究提供了量化分析LLM多智能体系统令牌消耗的方法论，帮助从业者预测成本、优化工作流，并指导未来研究开发更令牌高效的智能体协作协议。代码审查阶段的过度消耗是需要优化的关键领域。

Abstract: LLM-based Multi-Agent (LLM-MA) systems are increasingly applied to automate complex software engineering tasks such as requirements engineering, code generation, and testing. However, their operational efficiency and resource consumption remain poorly understood, hindering practical adoption due to unpredictable costs and environmental impact. To address this, we conduct an analysis of token consumption patterns in an LLM-MA system within the Software Development Life Cycle (SDLC), aiming to understand where tokens are consumed across distinct software engineering activities. We analyze execution traces from 30 software development tasks performed by the ChatDev framework using a GPT-5 reasoning model, mapping its internal phases to distinct development stages (Design, Coding, Code Completion, Code Review, Testing, and Documentation) to create a standardized evaluation framework. We then quantify and compare token distribution (input, output, reasoning) across these stages.
  Our preliminary findings show that the iterative Code Review stage accounts for the majority of token consumption for an average of 59.4% of tokens. Furthermore, we observe that input tokens consistently constitute the largest share of consumption for an average of 53.9%, providing empirical evidence for potentially significant inefficiencies in agentic collaboration. Our results suggest that the primary cost of agentic software engineering lies not in initial code generation but in automated refinement and verification. Our novel methodology can help practitioners predict expenses and optimize workflows, and it directs future research toward developing more token-efficient agent collaboration protocols.

</details>


### [89] [Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback](https://arxiv.org/abs/2601.15188)
*Stephan Wallraven,Tim Köhne,Hartmut Westenberger,Andreas Moser*

Main category: cs.SE

TL;DR: 该研究系统评估了大型语言模型在生成ABAP代码方面的性能，通过180个任务的基准测试发现，强大LLM在多次迭代后成功率约75%，并能有效利用编译器反馈进行改进。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在许多编程语言中已有成功应用，但目前缺乏对ABAP代码生成的系统性分析。研究旨在实证分析各种LLM生成语法正确、功能正常的ABAP代码的能力，以及它们如何有效利用编译器反馈进行迭代改进，并识别哪些任务类型存在特殊挑战。

Method: 研究采用包含180个任务的基准测试，包括改编的HumanEval任务和实际SAP场景。通过实验评估不同LLM生成ABAP代码的性能，特别关注它们利用编译器反馈进行迭代改进的能力。

Result: 结果显示模型间存在显著性能差异：更强大的LLM经过多次迭代后成功率可达约75%，并能从编译器反馈中大幅受益；而较小模型表现明显较弱。研究还识别了特定任务类型带来的挑战。

Conclusion: 该研究强调了强大LLM在ABAP开发流程中的巨大潜力，特别是在迭代错误修正方面。这为ABAP开发中的AI辅助编程提供了实证基础。

Abstract: This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.

</details>


### [90] [AQUA: an Agile Process to Develop Quantum Annealing Applications](https://arxiv.org/abs/2601.14501)
*Lodovica Marchesi,Amal Nasharti,Michele Marchesi*

Main category: cs.SE

TL;DR: AQUA是一个为QUBO/量子退火开发定制的敏捷生命周期框架，通过行业-学术合作开发，采用设计科学研究方法，将工作分为四个阶段，并在真实信用评分案例中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: QUBO领域因量子硬件的发展而受到关注，但实际应用受到数学复杂性、硬件限制以及缺乏健全的QUBO开发软件工程流程的阻碍。需要系统化的工程框架来促进QUBO/量子退火的实际采用。

Method: 采用设计科学研究方法，通过NetService S.p.A和卡利亚里大学的行业-学术合作，定制Scrum以适应QUBO/量子退火开发需求。将工作结构化分为四个阶段：初始评估与形式建模、原型驱动的算法选择、敏捷实现、部署与持续维护，每个阶段都有里程碑控制。

Result: 在真实的信用评分案例中验证了AQUA的可行性，提供了一个明确、系统的量子退火工程框架。主要贡献包括：专门的QUBO/量子退火软件流程、使用DSR方法创建和设计该流程，以及在简单但真实的案例研究中进行实证验证。

Conclusion: AQUA为QUBO/量子退火开发提供了一个可行的敏捷生命周期框架，解决了该领域缺乏系统化工程流程的问题，有助于促进量子计算在实际业务问题中的应用。

Abstract: Quadratic unconstrained binary optimization (QUBO) is a field of operations research that is attracting growing interest due to the recent availability of quantum hardware targeted at solving QUBO problems. However, practical adoption is hindered by mathematical intricacy, hardware constraints, and a lack of sound software engineering processes for QUBO development. This work presents AQUA (Agile QUantum Annealing), an agile lifecycle for QUBO/QA development created through an industry-academia partnership between NetService S.p.A and the University of Cagliari. Using the Design Science Research (DSR) approach, AQUA customizes Scrum to the needs of QUBO/QA development, structuring work into four stages: initial assessment with formal modeling, prototype-driven algorithm selection, agile implementation, and deployment with ongoing maintenance, each gated by milestones. Validated on a real credit-scoring case, AQUA shows feasibility and offers an explicit, systematic QA engineering framework. Key contributions of our work are: a dedicated QUBO/QA software process, its creation and design using DSR approach, and its empirical validation on a simple yet real case study.

</details>


### [91] [HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation](https://arxiv.org/abs/2601.14598)
*Yonatan Gizachew Achamyeleh,Harsh Thomare,Mohammad Abdullah Al Faruque*

Main category: cs.SE

TL;DR: HELIOS框架通过结构化表示控制流图来提升LLM二进制反编译质量，将编译成功率从45%提升至85%以上，并保持跨架构的高语法正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM反编译方法将代码视为纯文本，忽略了程序控制流图，导致对优化二进制文件产生语法脆弱和逻辑不一致的输出。

Method: HELIOS将二进制控制流和函数调用总结为分层文本表示，包含基本块、后继节点以及高级模式（如循环和条件语句），结合原始反编译器输出，并可选地使用编译器反馈循环。

Result: 在x86_64上，将Gemini 2.0的对象文件编译成功率从45.0%提升至85.2%，GPT-4.1 Mini从71.4%提升至89.6%；使用编译器反馈后编译成功率超过94%，功能正确性提升5.6个百分点；在六种架构上保持高语法正确性并减少功能正确性差异。

Conclusion: HELIOS无需微调即可作为安全逆向工程工作流的实用构建块，为分析师提供跨多样硬件目标的可重新编译、语义忠实的代码。

Abstract: Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.
  On HumanEval-Decompile for \texttt{x86\_64}, \textsc{HELIOS} raises average object file compilability from 45.0\% to 85.2\% for Gemini~2.0 and from 71.4\% to 89.6\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.

</details>


### [92] [ARFT-Transformer: Modeling Metric Dependencies for Cross-Project Aging-Related Bug Prediction](https://arxiv.org/abs/2601.14731)
*Shuning Ge,Fangyun Qin,Xiaohui Wan,Yang Liu,Qian Dai,Zheng Zheng*

Main category: cs.SE

TL;DR: ARFT-Transformer：基于Transformer的跨项目软件老化相关缺陷预测框架，通过度量级多头注意力机制捕捉度量交互，结合Focal Loss处理类别不平衡问题，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 软件系统长期运行会出现软件老化现象，主要由老化相关缺陷（ARBs）引起。跨项目ARB预测面临两大挑战：1）源项目与目标项目间的分布差异导致的领域适应问题；2）ARB易发样本与ARB无关样本间的严重类别不平衡。现有方法将输入度量独立处理，忽略了丰富的度量间依赖关系，且通常使用交叉熵损失函数，无法区分样本分类难度。

Method: 提出ARFT-Transformer框架，采用基于Transformer的架构，引入度量级多头注意力机制来捕捉度量间的交互关系。同时，在训练过程中采用Focal Loss函数替代传统的交叉熵损失，以有效处理类别不平衡问题。该框架支持单源和多源跨项目预测场景。

Result: 在三个大规模开源项目上的实验表明，ARFT-Transformer在单源和多源情况下均优于现有最先进的跨项目ARB预测方法。在Balance指标上分别实现了最高29.54%和19.92%的提升。

Conclusion: ARFT-Transformer通过有效捕捉度量间依赖关系和处理类别不平衡问题，显著提升了跨项目ARB预测的性能。该框架为解决软件老化相关缺陷预测中的数据稀缺和分布差异问题提供了有效解决方案。

Abstract: Software systems that run for long periods often suffer from software aging, which is typically caused by Aging-Related Bugs (ARBs). To mitigate the risk of ARBs early in the development phase, ARB prediction has been introduced into software aging research. However, due to the difficulty of collecting ARBs, within-project ARB prediction faces the challenge of data scarcity, leading to the proposal of cross-project ARB prediction. This task faces two major challenges: 1) domain adaptation issue caused by distribution difference between source and target projects; and 2) severe class imbalance between ARB-prone and ARB-free samples. Although various methods have been proposed for cross-project ARB prediction, existing approaches treat the input metrics independently and often neglect the rich inter-metric dependencies, which can lead to overlapping information and misjudgment of metric importance, potentially affecting the model's performance. Moreover, they typically use cross-entropy as the loss function during training, which cannot distinguish the difficulty of sample classification. To overcome these limitations, we propose ARFT-Transformer, a transformer-based cross-project ARB prediction framework that introduces a metric-level multi-head attention mechanism to capture metric interactions and incorporates Focal Loss function to effectively handle class imbalance. Experiments conducted on three large-scale open-source projects demonstrate that ARFT-Transformer on average outperforms state-of-the-art cross-project ARB prediction methods in both single-source and multi-source cases, achieving up to a 29.54% and 19.92% improvement in Balance metric.

</details>


### [93] [ARISE -- Adaptive Refinement and Iterative Scenario Engineering](https://arxiv.org/abs/2601.14743)
*Konstantin Poddubnyy,Igor Vozniak,Nils Lipp,Ivan Burmistrov,Davit Hovhannisyan,Christian Mueller,Philipp Slusallek*

Main category: cs.SE

TL;DR: ARISE：一种通过迭代LLM引导精炼将自然语言提示转换为可执行Scenic脚本的多阶段工具，用于生成多样化的交通场景训练数据


<details>
  <summary>Details</summary>
Motivation: 当前碰撞自由轨迹规划器的有效性依赖于训练数据的质量和多样性，特别是罕见场景。现有方法存在局限性：手动脚本编写需要高精度，单次生成困难，现有文本到仿真管道依赖静态片段检索、有限语法、单次解码或缺乏鲁棒的可执行性检查，且过度依赖受限的LLM提示和最小后处理

Method: ARISE（自适应精炼和迭代场景工程）是一个多阶段工具，通过迭代LLM引导精炼将自然语言提示转换为可执行Scenic脚本。每次生成后，ARISE在仿真软件中测试脚本可执行性，将结构化诊断反馈给LLM，直到满足语法和功能要求

Result: ARISE在生成语义准确且可执行的交通场景方面优于基线方法，具有更高的可靠性和鲁棒性，显著减少了手动干预需求

Conclusion: ARISE通过迭代精炼和可执行性检查的方法，有效解决了现有文本到仿真管道的局限性，为生成多样化交通场景训练数据提供了更可靠和鲁棒的解决方案

Abstract: The effectiveness of collision-free trajectory planners depends on the quality and diversity of training data, especially for rare scenarios. A widely used approach to improve dataset diversity involves generating realistic synthetic traffic scenarios. However, producing such scenarios remains difficult due to the precision required when scripting them manually or generating them in a single pass. Natural language offers a flexible way to describe scenarios, but existing text-to-simulation pipelines often rely on static snippet retrieval, limited grammar, single-pass decoding, or lack robust executability checks. Moreover, they depend heavily on constrained LLM prompting with minimal post-processing. To address these limitations, we introduce ARISE - Adaptive Refinement and Iterative Scenario Engineering, a multi-stage tool that converts natural language prompts into executable Scenic scripts through iterative LLM-guided refinement. After each generation, ARISE tests script executability in simulation software, feeding structured diagnostics back to the LLM until both syntactic and functional requirements are met. This process significantly reduces the need for manual intervention. Through extensive evaluation, ARISE outperforms the baseline in generating semantically accurate and executable traffic scenarios with greater reliability and robustness.

</details>


### [94] [FastFI: Enhancing API Call-Site Robustness in Microservice-Based Systems with Fault Injection](https://arxiv.org/abs/2601.14800)
*Yuzhen Tan,Jian Wang,Shuaiyu Xie,Bing Li,Yunqing Yong,Neng Zhang,Shaolin Tan*

Main category: cs.SE

TL;DR: FastFI是一个用于增强微服务系统API调用站点鲁棒性的故障注入引导框架，通过动态故障注入和DFS求解器发现所有有效组合故障，相比现有方法平均减少76.12%的端到端故障注入时间。


<details>
  <summary>Details</summary>
Motivation: 微服务架构复杂性导致故障注入空间指数增长，传统随机注入效率低下。现有基于谱系的方法通过启发式剪枝缓解问题，但仍面临两个限制：1)组合故障发现受限于通用SAT求解器，无法利用CNF公式的单调和低重叠结构；2)现有技术仅报告检测到的故障，缺乏注入后指导。

Method: FastFI采用基于DFS的求解器配合动态故障注入来发现所有有效组合故障，并利用故障注入结果识别需要加固的关键API调用站点。框架包含两个核心组件：1)动态故障注入机制；2)基于故障注入结果的API重要性分析。

Result: 在四个代表性微服务基准测试上，FastFI相比最先进的基线方法平均减少76.12%的端到端故障注入时间，同时保持可接受的资源开销。框架能准确识别高影响API，并为调用站点加固提供可操作的指导。

Conclusion: FastFI通过创新的动态故障注入和DFS求解器解决了微服务系统中组合故障发现的效率瓶颈，不仅显著提升了故障注入性能，还提供了有价值的API加固指导，增强了微服务系统的鲁棒性。

Abstract: Fault injection is a key technique for assessing software reliability, enabling proactive detection of system defects before they manifest in production. However, the increasing complexity of microservice architectures leads to exponential growth in the fault-injection space, rendering traditional random injection inefficient. Recent lineage-driven approaches mitigate this problem through heuristic pruning, but they face two limitations. First, combinatorial-fault discovery remains bottlenecked by general-purpose SAT solvers, which fail to exploit the monotone and low-overlap structure of derived CNF formulas and typically rely on a static upper bound on fault size. Second, existing techniques provide limited post-injection guidance beyond reporting detected faults. To address these challenges, we propose FastFI, a fault-injection-guided framework to enhance the robustness of API call sites in microservice-based systems. FastFI features a DFS-based solver with dynamic fault injection to discover all valid combinatorial faults, and it leverages fault-injection results to identify critical APIs whose call sites should be hardened for robustness. Experiments on four representative microservice benchmarks show that FastFI reduces end-to-end fault-injection time by an average of 76.12\% compared to state-of-the-art baselines while maintaining acceptable resource overhead. Moreover, FastFI accurately identifies high-impact APIs and provides actionable guidance for call-site hardening.

</details>


### [95] [Understanding Usefulness in Developer Explanations on Stack Overflow](https://arxiv.org/abs/2601.14865)
*Martin Obaidi,Kushtrim Qengaj,Hannah Deters,Jakob Droste,Marc Herrmann,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 该研究通过分析Stack Overflow上的59,398个回答，识别了影响开发者解释感知有用性的关键因素，发现结构性和情境性因素（如长度、代码包含、时机和作者声誉）比情感极性更重要。


<details>
  <summary>Details</summary>
Motivation: 在软件工程和需求沟通中，解释对于澄清歧义、证明设计选择和建立共同理解至关重要。虽然Stack Overflow等在线问答论坛提供了大规模的解释生成和评估环境，但关于哪些具体特征使解释真正有用的研究仍然不足。先前研究主要关注答案接受和投票行为，而对解释有用性的具体驱动因素（如结构、情境和语言因素）的相对影响尚不清楚。

Method: 研究分析了Stack Overflow上的3,323个问题和59,398个答案，结合文本分析和统计建模方法，考察解释属性如何与感知有用性（标准化点赞数）相关。研究关注结构因素（如长度、代码包含）、情境因素（如时机、作者声誉）和语言因素（如情感极性）。

Result: 结构性和情境性因素，特别是解释长度、代码包含、时机和作者声誉，显示出小到中等的正向影响。情感极性的影响可以忽略不计，表明在技术沟通中，清晰度和实质内容比语气更重要。研究提供了开发者解释感知有用性的实证分析。

Conclusion: 该研究为开发者和需求工程从业者如何构建更清晰有效的解释提供了基于证据的指导，支持在开放和组织环境中实现更公平的沟通。从需求工程角度看，这些决定因素可被解释为日常需求沟通中减少歧义和阐明理由的实际信号。研究通过开放数据和复制材料提供了方法透明度，并通过将观察到的沟通模式与需求沟通原则联系起来提供了概念性见解。

Abstract: Explanations are essential in software engineering (SE) and requirements communication, helping stakeholders clarify ambiguities, justify design choices, and build shared understanding. Online Q&A forums such as Stack Overflow provide large-scale settings where such explanations are produced and evaluated, offering valuable insights into what makes them effective. While prior work has explored answer acceptance and voting behavior, little is known about which specific features make explanations genuinely useful. The relative influence of structural, contextual, and linguistic factors, such as content richness, timing, and sentiment, remains unclear. We analyzed 3,323 questions and 59,398 answers from Stack Overflow, combining text analysis and statistical modeling to examine how explanation attributes relate to perceived usefulness (normalized upvotes). Structural and contextual factors, especially explanation length, code inclusion, timing, and author reputation, show small to moderate positive effects. Sentiment polarity has negligible influence, suggesting that clarity and substance outweigh tone in technical communication. This study provides an empirical account of what drives perceived usefulness in developer explanations. It contributes methodological transparency through open data and replication materials, and conceptual insight by relating observed communication patterns to principles of requirements communication. The findings offer evidence-based implications for how developers and RE practitioners can craft clearer and more effective explanations, potentially supporting fairer communication in both open and organizational contexts. From an RE perspective, these determinants can be interpreted as practical signals for ambiguity reduction and rationale articulation in day-to-day requirements communication.

</details>


### [96] [LLM-Based Repair of C++ Implicit Data Loss Compiler Warnings: An Industrial Case Study](https://arxiv.org/abs/2601.14936)
*Chansong You,Hyun Deok Choi,Jingun Hong*

Main category: cs.SE

TL;DR: 使用大型语言模型自动修复C++项目中隐式数据丢失警告的方法，通过LSP收集上下文、Tree-sitter提取代码、LLM决策生成修复，在大型C++项目中测试获得92.73%的人工接受率。


<details>
  <summary>Details</summary>
Motivation: 减少处理编译器警告的手动工作量，在保持代码质量和性能的同时自动化修复过程，提高C++软件项目的代码维护效率。

Method: 使用语言服务器协议(LSP)收集代码上下文，Tree-sitter提取相关代码片段，大型语言模型(LLM)评估范围检查的必要性并生成修复方案，考虑性能影响。

Result: 在大型C++项目中测试，人工代码审查接受率达92.73%；相比基线修复策略，LLM生成的修复减少了39.09%因范围检查和异常处理引入的额外指令；与人工最优解差距为13.56%。

Conclusion: LLM方法能有效减少处理编译器警告的手动工作量，保持代码质量和性能，有望集成到现有开发工作流中，改善复杂C++项目的代码维护实践。

Abstract: This paper presents a method to automatically fix implicit data loss warnings in large C++ projects using Large Language Models (LLMs). Our approach uses the Language Server Protocol (LSP) to gather context, Tree-sitter to extract relevant code, and LLMs to make decisions and generate fixes. The method evaluates the necessity of range checks concerning performance implications and generates appropriate fixes. We tested this method in a large C++ project, resulting in a 92.73% acceptance rate of the fixes by human developers during the code review. Our LLM-generated fixes reduced the number of warning fix changes that introduced additional instructions due to range checks and exception handling by 39.09% compared to a baseline fix strategy. This result was 13.56% behind the optimal solutions created by human developers. These findings demonstrate that our LLM-based approach can reduce the manual effort to address compiler warnings while maintaining code quality and performance in a real-world scenario. Our automated approach shows promise for integration into existing development workflows, potentially improving code maintenance practices in complex C++ software projects.

</details>


### [97] [DeLog: An Efficient Log Compression Framework with Pattern Signature Synthesis](https://arxiv.org/abs/2601.15084)
*Siyu Yu,Yifan Wu,Junjielong Xu,Ying Fu,Ning Wang,Maoyin Liu,Pancheng Jiang,Xiang Zhang,Tong Jia,Pinjia He,Ying Li*

Main category: cs.SE

TL;DR: DeLog：一种基于模式签名合成的新型日志压缩器，通过有效的模式分组而非单纯提高解析精度来优化压缩比


<details>
  <summary>Details</summary>
Motivation: 现有基于解析器的日志压缩方法在复杂生产日志上性能不佳，这与日志解析组件精度下降有关。研究旨在验证一个基础但未经验证的问题：更高的解析精度是否必然带来更好的压缩比？

Method: 首先通过实证研究量化解析精度与压缩比的关系，发现压缩比取决于有效的模式分组和编码。基于此设计DeLog，采用模式签名合成机制实现高效模式分组

Result: 实证研究发现更高的解析精度并不保证更好的压缩比。DeLog在16个公共和10个生产数据集上实现了最先进的压缩比和速度

Conclusion: 日志压缩的关键在于实现有效的模式分组和编码，而非单纯追求解析精度。DeLog通过模式签名合成机制成功实现了这一目标，在压缩比和速度上均达到最优

Abstract: Parser-based log compression, which separates static templates from dynamic variables, is a promising approach to exploit the unique structure of log data. However, its performance on complex production logs is often unsatisfactory. This performance gap coincides with a known degradation in the accuracy of its core log parsing component on such data, motivating our investigation into a foundational yet unverified question: does higher parsing accuracy necessarily lead to better compression ratio?
  To answer this, we conduct the first empirical study quantifying this relationship and find that a higher parsing accuracy does not guarantee a better compression ratio. Instead, our findings reveal that compression ratio is dictated by achieving effective pattern-based grouping and encoding, i.e., the partitioning of tokens into low entropy, highly compressible groups.
  Guided by this insight, we design DeLog, a novel log compressor that implements a Pattern Signature Synthesis mechanism to achieve efficient pattern-based grouping. On 16 public and 10 production datasets, DeLog achieves state-of-the-art compression ratio and speed.

</details>


### [98] [Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks](https://arxiv.org/abs/2601.15094)
*Md Zahidul Haque,Saima Afrin,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 多任务QLoRA微调在代码生成、翻译和摘要任务中表现优异，能有效利用迁移学习，在功能正确性和代码质量方面达到或超越单任务QLoRA和多任务全微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件工程任务中表现出色，但全微调计算成本高昂。参数高效微调方法如QLoRA能以较低资源实现专业化。虽然QLoRA优化的代码模型在单一任务上表现良好，但多任务QLoRA微调的有效性、迁移学习对生成工件正确性和质量的影响尚未充分探索。

Method: 研究多任务QLoRA微调在三个代表性代码任务上的应用：代码生成、翻译和摘要。通过基于执行的正确性评估和相似性度量，结合全面的代码质量分析（先前研究常忽视的方面）来评估模型性能。

Result: 多任务QLoRA能有效利用迁移学习，在功能正确性和代码质量方面达到或超越单任务QLoRA和多任务全微调。较大模型在正确性和质量间保持更一致的平衡，而较小模型虽能保持功能但出现更多质量问题。

Conclusion: 多任务QLoRA微调是一种高效的方法，能在多个代码相关任务上实现竞争性性能，同时保持参数效率。模型规模影响性能平衡，较大模型在保持功能正确性的同时提供更好的代码质量。

Abstract: Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.

</details>


### [99] [Why Authors and Maintainers Link (or Don't Link) Their PyPI Libraries to Code Repositories and Donation Platforms](https://arxiv.org/abs/2601.15139)
*Alexandros Tsakpinis,Nicolas Raube,Alexander Pretschner*

Main category: cs.SE

TL;DR: 对PyPI库元数据（源代码仓库链接和捐赠平台链接）缺失原因的大规模实证研究，通过5万名作者/维护者的调查和LLM主题建模分析，揭示了链接动机、障碍及跨领域挑战。


<details>
  <summary>Details</summary>
Motivation: PyPI库的元数据（特别是源代码仓库链接和捐赠平台链接）对开源库的透明度、信任度和可持续性至关重要，但许多包缺乏这些元数据，且缺失的根本原因尚不清楚。

Method: 大规模实证研究：向5万名PyPI作者和维护者发送针对性调查，收集1400多份回复；使用基于大语言模型（LLM）的主题建模分析回复内容，评估30次运行的鲁棒性（84%词汇相似度，89%语义相似度），并通过23位专家评估主题质量（Randolph's kappa = 0.55）。

Result: 仓库链接的主要动机是促进协作、增加透明度和启用问题跟踪，缺失原因包括疏忽、懒惰或认为与项目无关；捐赠平台链接旨在支持开源工作或获得财务贡献，但受到怀疑、技术摩擦和组织约束的阻碍；跨领域挑战包括链接过时、缺乏意识和指导不明确。

Conclusion: 研究提供了PyPI元数据实践的实证见解，提出了改进建议，同时证明了基于LLM的主题建模方法在分析短文本调查回复中的有效性。

Abstract: Metadata of libraries on the Python Package Index (PyPI)-including links to source code repositories and donation platforms-plays a critical role in supporting the transparency, trust, and sustainability of open-source libraries. Yet, many packages lack such metadata, and little is known about the underlying reasons. This paper presents a large-scale empirical study combining two targeted surveys sent to 50,000 PyPI authors and maintainers. We analyze more than 1,400 responses using large language model (LLM)-based topic modeling to uncover key motivations and barriers related to linking repositories and donation platforms. While repository URLs are often linked to foster collaboration, increase transparency, and enable issue tracking, some maintainers omit them due to oversight, laziness, or the perceived irrelevance to their project. Donation platform links are reported to support open source work or receive financial contributions, but are hindered by skepticism, technical friction, and organizational constraints. Cross-cutting challenges-such as outdated links, lack of awareness, and unclear guidance-affect both types of metadata. We further assess the robustness of our topic modeling pipeline across 30 runs (84% lexical and 89% semantic similarity) and validate topic quality with 23 expert raters (Randolph's kappa = 0.55). The study contributes empirical insights into PyPI's metadata practices and provides recommendations for improving them, while also demonstrating the effectiveness of our topic modeling approach for analyzing short-text survey responses.

</details>


### [100] [SAGA: Detecting Security Vulnerabilities Using Static Aspect Analysis](https://arxiv.org/abs/2601.15154)
*Yoann Marquer,Domenico Bianculli,Lionel C. Briand*

Main category: cs.SE

TL;DR: SAGA：一种用于Python源代码漏洞检测的静态分析方法，通过符号控制流图和领域特定语言实现高精度、高效率的漏洞检测


<details>
  <summary>Details</summary>
Motivation: Python作为流行编程语言，其项目中的安全漏洞类型日益多样化，但现有分析工具仅支持少数漏洞类型，需要一种能检测多种漏洞的方法

Method: 提出SAGA方法，包括：1）源代码解析器提取控制流和数据流信息，构建符号控制流图；2）领域特定语言定义静态属性及其在图遍历中的演化；3）基于该语言构建完整性、机密性等安全属性的静态属性库

Result: 在108个漏洞数据集上评估，获得100%敏感度和99.15%特异度，仅1个误报，优于4个常用安全分析工具；分析时间小于31秒，比基线工具快2.5-512.1倍

Conclusion: SAGA能够以高精度、高效率检测Python源代码中的多种安全漏洞，解决了现有工具覆盖范围有限的问题，为Python项目安全分析提供了有效解决方案

Abstract: Python is one of the most popular programming languages; as such, projects written in Python involve an increasing number of diverse security vulnerabilities. However, existing state-of-the-art analysis tools for Python only support a few vulnerability types. Hence, there is a need to detect a large variety of vulnerabilities in Python projects.
  In this paper, we propose the SAGA approach to detect and locate vulnerabilities in Python source code in a versatile way. SAGA includes a source code parser able to extract control- and data-flow information and to represent it as a symbolic control-flow graph, as well as a domain-specific language defining static aspects of the source code and their evolution during graph traversals. We have leveraged this language to define a library of static aspects for integrity, confidentiality, and other security-related properties.
  We have evaluated SAGA on a dataset of 108 vulnerabilities, obtaining 100% sensitivity and 99.15% specificity, with only one false positive, while outperforming four common security analysis tools. This analysis was performed in less than 31 seconds, i.e., between 2.5 and 512.1 times faster than the baseline tools.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [101] [RoboBrain 2.5: Depth in Sight, Time in Mind](https://arxiv.org/abs/2601.14352)
*Huajie Tan,Enshen Zhou,Zhiyu Li,Yijie Xu,Yuheng Ji,Xiansheng Chen,Cheng Chi,Pengwei Wang,Huizhu Jia,Yulong Ao,Mingyu Cao,Sixiang Chen,Zhe Li,Mengzhen Liu,Zixiao Wang,Shanyu Rong,Yaoxu Lyu,Zhongxia Zhao,Peterson Co,Yibo Li,Yi Han,Shaoxuan Xie,Guocai Yao,Songjing Wang,Leiduo Zhang,Xi Yang,Yance Jiao,Donghai Shi,Kunchang Xie,Shaokai Nie,Chunlei Men,Yonghua Lin,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboBrain 2.5是一个新一代具身AI基础模型，通过高质量时空监督训练，提升了通用感知、空间推理和时间建模能力，专注于复杂精细操作任务。


<details>
  <summary>Details</summary>
Motivation: 构建更物理接地、执行感知的具身智能系统，解决现有模型在3D空间推理和时间建模方面的不足，实现复杂精细操作任务。

Method: 引入两大能力升级：1) 精确3D空间推理：从2D像素相对定位转向深度感知坐标预测和绝对度量约束理解，生成完整3D操作轨迹作为有序关键点序列；2) 密集时间价值估计：提供密集、步骤感知的进度预测和执行状态理解，生成稳定反馈信号。

Result: 模型实现了更物理接地和感知执行的具身智能，能够处理复杂精细操作任务，代码和检查点已公开。

Conclusion: RoboBrain 2.5通过精确3D空间推理和密集时间价值估计两大升级，显著提升了具身AI在复杂精细操作任务中的能力，推动了物理接地和执行感知的具身智能发展。

Abstract: We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io

</details>


### [102] [Agentic AI Meets Edge Computing in Autonomous UAV Swarms](https://arxiv.org/abs/2601.14437)
*Thuan Minh Nguyen,Vu Tuan Truong,Long Bao Le*

Main category: cs.RO

TL;DR: 该论文研究将基于LLM的智能体AI与边缘计算集成到无人机群中，以实现可扩展和弹性的自主性，特别针对野火搜救等高风险场景。


<details>
  <summary>Details</summary>
Motivation: 将具备自主推理、规划和执行能力的LLM智能体AI集成到无人机群中，为无人机物联网带来新可能性，但基础设施限制、动态环境和多智能体协调的计算需求限制了高风险场景的实际部署。

Method: 提出三种支持无人机群的架构：独立部署、边缘使能和边缘-云混合部署，每种针对不同自主性和连接级别进行优化。设计野火搜救用例来展示边缘使能架构的效率。

Result: 边缘使能架构相比传统方法实现了更高的搜救覆盖率、更短的任务完成时间和更高水平的自主性，证明了LLM与边缘计算集成的有效性。

Conclusion: LLM智能体AI与边缘计算的集成为无人机群提供了可扩展和弹性的自主性，但关键任务应用中仍存在开放挑战需要解决。

Abstract: The integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We first discuss three architectures for supporting UAV swarms - standalone, edge-enabled, and edge-cloud hybrid deployment - each optimized for varying autonomy and connectivity levels. Then, a use case for wildfire search and rescue (SAR) is designed to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion times, and a higher level of autonomy compared to traditional approaches. Finally, we highlight open challenges in integrating LLMs and edge computing for mission-critical UAV-swarm applications.

</details>


### [103] [Robust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery](https://arxiv.org/abs/2601.14445)
*Aiden Mazidi,Majid Roshanfar,Amir Sayadi,Javad Dargahi,Jake Barralet,Liane S. Feldman,Amir Hooshiar*

Main category: cs.RO

TL;DR: 提出非线性阻抗匹配方法(NIMA)用于机器人辅助微创手术的触觉反馈，相比传统IMA方法显著提升力反馈精度并消除触觉"回弹"现象


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术(RAMIS)的触觉反馈长期受限于力渲染精度和系统安全性问题，需要鲁棒、高保真的触觉系统来提升远程操作手术工具的精确性和可靠性

Method: 提出非线性阻抗匹配方法(NIMA)，在先前验证的阻抗匹配方法(IMA)基础上，引入非线性动力学来准确建模和渲染工具-组织相互作用力

Result: NIMA将力反馈的平均绝对误差(MAE)降低至0.01N(SD 0.02)，相比IMA减少了95%的误差；有效消除触觉"回弹"现象，确保用户释放手柄时触觉设备不会对手施加力

Conclusion: NIMA通过考虑工具-组织相互作用的非线性特性，在各种手术条件下提升了力保真度、响应性和精确性，推动了机器人手术触觉反馈系统的发展

Abstract: Background: The integration of haptic feedback into robot-assisted minimally invasive surgery (RAMIS) has long been limited by challenges in accurately rendering forces and ensuring system safety. The need for robust, high-fidelity haptic systems is critical for enhancing the precision and reliability of teleoperated surgical tools. Methods: In this study, we present a Nonlinear Impedance Matching Approach (NIMA) designed to improve force rendering by accurately modelling complex tool-tissue interactions. Based on our previously validated Impedance Matching Approach (IMA), our novel NIMA method includes nonlinear dynamics to capture and render tool-tissue forces effectively. Results: NIMA improves force feedback accuracy with a mean absolute error (MAE) of 0.01 (SD 0.02) N, achieving a 95% reduction in MAE compared to IMA. Furthermore, NIMA effectively eliminates haptic "kickback" by ensuring no force is applied by the haptic device to the user's hand when they release the handle, enhancing both patient safety and user comfort. Conclusion: NIMA's ability to account for nonlinearities in tool-tissue interactions provides an improvement in force fidelity, responsiveness, and precision across various surgical conditions. Our findings promote the advancement of haptic feedback systems for robotic surgery, offering a realistic and reliable interface for robot-assisted surgical procedures.

</details>


### [104] [UNCLE-Grasp: Uncertainty-Aware Grasping of Leaf-Occluded Strawberries](https://arxiv.org/abs/2601.14492)
*Malak Mansour,Ali Abouzeid,Zezhou Sun,Qinbo Sun,Dezhen Song,Abdalla Swikir*

Main category: cs.RO

TL;DR: 提出一种针对部分遮挡草莓的感知不确定性感知抓取方法，通过多假设形状补全和保守决策机制，在严重遮挡时可靠放弃高风险抓取，在几何置信度足够时保持稳健执行


<details>
  <summary>Details</summary>
Motivation: 机器人草莓采摘在部分遮挡条件下面临挑战，叶片遮挡导致显著的几何不确定性，基于单一确定性形状估计的抓取决策不可靠。从单一局部观测可能产生多个不相容的3D补全假设，导致在一个补全上可行的抓取在另一个上失败

Method: 使用蒙特卡洛dropout点云补全生成多个形状假设，为每个补全生成候选抓取，基于物理基础的力闭合度量评估抓取可行性。通过聚合所有补全的可行性，应用保守的下置信界准则决定是否尝试抓取或安全放弃

Result: 在模拟和物理机器人实验中，不确定性感知决策能够在严重遮挡下可靠放弃高风险抓取尝试，同时在几何置信度足够时保持稳健抓取执行，优于确定性基线方法

Conclusion: 提出的不确定性感知抓取管道通过显式建模补全不确定性，实现了在部分遮挡草莓采摘中的可靠决策，能够在高风险条件下安全放弃，在可行条件下稳健执行，提高了机器人采摘的可靠性

Abstract: Robotic strawberry harvesting is challenging under partial occlusion, where leaves induce significant geometric uncertainty and make grasp decisions based on a single deterministic shape estimate unreliable. From a single partial observation, multiple incompatible 3D completions may be plausible, causing grasps that appear feasible on one completion to fail on another. We propose an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models completion uncertainty arising from both occlusion and learned shape reconstruction. Our approach uses point cloud completion with Monte Carlo dropout to sample multiple shape hypotheses, generates candidate grasps for each completion, and evaluates grasp feasibility using physically grounded force-closure-based metrics. Rather than selecting a grasp based on a single estimate, we aggregate feasibility across completions and apply a conservative lower confidence bound (LCB) criterion to decide whether a grasp should be attempted or safely abstained. We evaluate the proposed method in simulation and on a physical robot across increasing levels of synthetic and real leaf occlusion. Results show that uncertainty-aware decision making enables reliable abstention from high-risk grasp attempts under severe occlusion while maintaining robust grasp execution when geometric confidence is sufficient, outperforming deterministic baselines in both simulated and physical robot experiments.

</details>


### [105] [TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks](https://arxiv.org/abs/2601.14550)
*Tailai Cheng,Kejia Chen,Lingyun Chen,Liding Zhang,Yue Zhang,Yao Ling,Mahdi Hamad,Zhenshan Bing,Fan Wu,Karan Sharma,Alois Knoll*

Main category: cs.RO

TL;DR: TacUMI：集成触觉视觉传感器、力扭矩传感器和姿态跟踪器的多模态数据采集系统，用于接触丰富的长时程操作任务，结合多模态分割框架实现90%以上的分割准确率。


<details>
  <summary>Details</summary>
Motivation: 对于涉及丰富物理交互的复杂长时程操作任务，仅依靠视觉观察和机器人本体感受信息往往无法揭示底层事件转换，需要高效收集高质量多模态数据以及鲁棒的分割方法来将演示分解为有意义的模块。

Method: 基于手持演示设备UMI，引入TacUMI多模态数据采集系统，集成ViTac传感器、力扭矩传感器和姿态跟踪器到紧凑的机器人兼容夹爪设计中，实现人类演示期间所有模态的同步采集；提出多模态分割框架，利用时序模型检测顺序操作中的语义事件边界。

Result: 在具有挑战性的电缆安装任务上评估显示超过90%的分割准确率，并突显了更多模态带来的显著改进，验证了TacUMI为接触丰富任务中多模态演示的可扩展收集和分割建立了实用基础。

Conclusion: TacUMI系统成功解决了接触丰富长时程操作任务中多模态数据采集和事件分割的挑战，通过集成多种传感器模态和时序分割框架，实现了高精度的任务分解，为复杂操作任务的学习和理解提供了有效工具。

Abstract: Task decomposition is critical for understanding and learning complex long-horizon manipulation tasks. Especially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions. This raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules. Building on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, force-torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations. We then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations. Evaluation on a challenging cable mounting task shows more than 90 percent segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.

</details>


### [106] [UniCon: A Unified System for Efficient Robot Learning Transfers](https://arxiv.org/abs/2601.14617)
*Yunfeng Lin,Li Xu,Yong Yu,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: UniCon是一个轻量级框架，通过标准化状态、控制流和仪器化来解决异构机器人部署中的平台差异、接口不一致和中间件效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 基于学习的控制器在异构机器人上部署面临挑战，包括平台差异、接口不一致和中间件效率低下。传统方法难以实现跨平台的即插即用部署和高效的sim-to-real迁移。

Method: UniCon将工作流分解为具有可重用组件的执行图，将系统状态与控制逻辑分离。采用批处理和向量化数据流来最小化通信开销，提高推理延迟。采用模块化、数据导向的设计方法。

Result: UniCon在转移工作流时减少了代码冗余，相比基于ROS的系统获得了更高的推理效率。已在7个制造商的12个以上机器人模型上成功部署，并集成到实际研究项目中。

Conclusion: UniCon通过标准化状态和控制流，实现了跨异构机器人平台的即插即用部署，提高了部署效率和sim-to-real迁移的便利性，在实际应用中证明了其有效性。

Abstract: Deploying learning-based controllers across heterogeneous robots is challenging due to platform differences, inconsistent interfaces, and inefficient middleware. To address these issues, we present UniCon, a lightweight framework that standardizes states, control flow, and instrumentation across platforms. It decomposes workflows into execution graphs with reusable components, separating system states from control logic to enable plug-and-play deployment across various robot morphologies. Unlike traditional middleware, it prioritizes efficiency through batched, vectorized data flow, minimizing communication overhead and improving inference latency. This modular, data-oriented approach enables seamless sim-to-real transfer with minimal re-engineering. We demonstrate that UniCon reduces code redundancy when transferring workflows and achieves higher inference efficiency compared to ROS-based systems. Deployed on over 12 robot models from 7 manufacturers, it has been successfully integrated into ongoing research projects, proving its effectiveness in real-world scenarios.

</details>


### [107] [Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models](https://arxiv.org/abs/2601.14622)
*Ling Xiao,Toshihiko Yamasaki*

Main category: cs.RO

TL;DR: 研究探索了基于认知理论的社会机器人导航提示设计，发现竞争性动机框架和系统指导提示能显著提升小型视觉语言模型的决策准确性，而非仅增强表征能力。


<details>
  <summary>Details</summary>
Motivation: 当前社会机器人导航基准缺乏原则性的提示设计方法，而实际应用中多依赖小型视觉语言模型（VLMs）以实现效率。相比大型语言模型，小型VLMs决策能力较弱，因此有效的提示设计对准确导航至关重要。研究受人类学习和动机的认知理论启发，旨在探索如何通过提示设计提升社会合规导航性能。

Method: 研究从两个维度探索提示设计：系统指导（动作导向、推理导向、感知-推理提示）和动机框架（模型与人类竞争、与其他AI系统竞争、与自身过去版本竞争）。在两个社会合规导航数据集上进行实验，分析不同提示设计对非微调GPT-4o和微调模型的影响，评估语义级指标（感知、预测、推理）和动作准确性。

Result: 1. 对于非微调GPT-4o，与人类竞争表现最佳，与其他AI系统竞争表现最差；对于微调模型，与自身过去版本竞争效果最好，其次是与人竞争。2. 不恰当的系统提示设计会显著降低性能，甚至不如直接微调。3. 直接微调大幅提升语义级指标（感知、预测、推理），但对动作准确性提升有限；而系统提示设计对动作准确性改善更大，表明提示设计主要作为决策级约束而非表征增强。

Conclusion: 基于认知理论的提示设计，特别是竞争性动机框架和系统指导提示，能有效提升小型视觉语言模型在社会机器人导航中的决策准确性。提示设计主要作为决策级约束机制，而非表征增强手段，为实际应用中优化社会合规导航系统提供了重要指导。

Abstract: Language models are increasingly used for social robot navigation, yet existing benchmarks largely overlook principled prompt design for socially compliant behavior. This limitation is particularly relevant in practice, as many systems rely on small vision language models (VLMs) for efficiency. Compared to large language models, small VLMs exhibit weaker decision-making capabilities, making effective prompt design critical for accurate navigation. Inspired by cognitive theories of human learning and motivation, we study prompt design along two dimensions: system guidance (action-focused, reasoning-oriented, and perception-reasoning prompts) and motivational framing, where models compete against humans, other AI systems, or their past selves. Experiments on two socially compliant navigation datasets reveal three key findings. First, for non-finetuned GPT-4o, competition against humans achieves the best performance, while competition against other AI systems performs worst. For finetuned models, competition against the model's past self yields the strongest results, followed by competition against humans, with performance further influenced by coupling effects among prompt design, model choice, and dataset characteristics. Second, inappropriate system prompt design can significantly degrade performance, even compared to direct finetuning. Third, while direct finetuning substantially improves semantic-level metrics such as perception, prediction, and reasoning, it yields limited gains in action accuracy. In contrast, our system prompts produce a disproportionately larger improvement in action accuracy, indicating that the proposed prompt design primarily acts as a decision-level constraint rather than a representational enhancement.

</details>


### [108] [A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control](https://arxiv.org/abs/2601.14628)
*Weiyu Guo,He Zhang,Pengteng Li,Tiefu Cai,Ziyang Chen,Yandong Guo,Xiao He,Yongkui Yang,Ying Sun,Hui Xiong*

Main category: cs.RO

TL;DR: NeuroVLA是一个受生物神经系统启发的机器人控制框架，通过模拟大脑皮层、小脑和脊髓的结构组织，实现了动态稳定性、反射响应和时间记忆等生物运动特性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人策略难以复制生物运动的动态稳定性、反射响应性和时间记忆能力，而生物系统却能从稀疏经验中快速习得技能。需要开发能模拟生物神经系统结构组织的机器人控制框架。

Method: 采用系统级仿生设计：高层模型规划目标，自适应小脑模块利用高频传感器反馈稳定运动，仿生脊髓层执行快速动作生成。这是首个在物理机器人上部署的神经形态VLA框架。

Result: 实现了最先进的性能，观察到生物运动特性的涌现：消除了机械臂抖动，显著节能（神经形态处理器仅需0.4w），具备时间记忆能力，安全反射触发时间小于20毫秒。

Conclusion: NeuroVLA框架成功模拟了生物神经系统的结构组织，在物理机器人上实现了生物运动特性，为机器人控制提供了新的仿生方法。

Abstract: Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.

</details>


### [109] [Landing-Induced Viscoelastic Changes in an Anthropomimetic Foot Joint Structure are Modulated by Foot Structure and Posture](https://arxiv.org/abs/2601.14634)
*Satoru Hashimoto,Yinlai Jiang,Hiroshi Yokoi,Shunta Togo*

Main category: cs.RO

TL;DR: 开发仿人足部关节结构研究骨骼结构对落地冲击衰减的影响，发现多关节拱形结构比简化平足具有更高阻尼比，踝背屈和趾伸展可调节衰减与回弹的权衡


<details>
  <summary>Details</summary>
Motivation: 尸体研究难以重复测试落地冲击后的姿势依赖性粘弹性响应，导致骨骼结构对落地动力学的贡献不完全清楚，需要开发仿人足部结构来研究骨骼几何对冲击衰减的影响

Method: 开发仿人足部关节结构复制人类足部骨骼几何，使用垂直下落装置模拟落地过程，结合粘弹性系统辨识模型，研究骨骼结构和姿势如何调节冲击后的表观粘弹性响应

Result: 多关节仿人结构比简化平足和刚性足表现出更高的阻尼比；踝背屈和趾伸展系统性改变辨识参数，在测试条件下降低阻尼比；拱形多关节骨骼结构可增强冲击衰减，形态和被动姿势可调节衰减与回弹的权衡

Conclusion: 骨骼结构可能部分解释人类落地策略的差异，解剖学启发的骨骼复制具有工程优势，可通过姿势调整实现类人表观粘弹性行为

Abstract: Cadaveric studies have provided important insights into the mechanics of the human foot arch and plantar fascia. However, repeatedly probing posture-dependent viscoelastic responses immediately after landing impact is difficult in biological specimens, leaving the contribution of skeletal architecture to landing dynamics incompletely understood. In this study, we developed an anthropomimetic foot joint structure aimed at replicating the skeletal geometry of the human foot. Using a vertical drop apparatus that simulates landing and a viscoelastic system-identification model, we investigated how skeletal structure and posture modulate the apparent post-impact viscoelastic response. The results show that the multi-jointed anthropomimetic structure exhibited a higher damping ratio than simplified flat and rigid feet. Moreover, ankle dorsiflexion and toe extension systematically shifted the identified parameters, reducing the damping ratio under the tested conditions. Taken together, these findings indicate that an arch-like, multi-jointed skeletal architecture can enhance impact attenuation in an anthropomimetic mechanical foot, and that morphology and passive posture alone can tune the trade-off between attenuation and rebound. The observed posture-dependent trends are qualitatively consistent with reported differences in human landing strategies, suggesting that skeletal architecture may partly account for the modulation. Furthermore, these results highlight the engineering advantage of anatomically informed skeletal replication for achieving human-like apparent viscoelastic behavior through postural adjustment during landing.

</details>


### [110] [FARE: Fast-Slow Agentic Robotic Exploration](https://arxiv.org/abs/2601.14681)
*Shuhao Liao,Xuxin Lv,Jeric Lew,Shizhe Zhang,Jingsong Liang,Peizhuo Li,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: FARE：一种分层自主探索框架，将大语言模型的全局语义推理与强化学习的局部决策相结合，遵循快慢思维范式，显著提升了机器人探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有自主探索方法通常缺乏高级语义推理能力，难以在复杂未知环境中进行高效探索。需要将agent级别的语义理解与快速局部控制相结合，实现更智能的探索策略。

Method: 提出FARE框架，包含慢思维LLM模块和快思维RL模块。LLM模块解析环境文本描述，生成agent级探索策略，并通过拓扑图将其转化为全局路径点，采用模块化剪枝减少冗余结构。RL模块基于局部观测执行探索，受LLM生成的全局路径点指导，并通过奖励项确保与全局策略的一致性。

Result: 在具有挑战性的仿真环境中，FARE相比最先进的基线方法在探索效率上取得了显著提升。在硬件部署中，成功验证了FARE在复杂大规模（200m×130m）建筑环境中的有效性。

Conclusion: FARE通过将语义推理与几何决策解耦，使各模块在适当的时空尺度上运行，实现了更高效、更智能的自主探索。该框架为机器人探索提供了新的层次化架构范式。

Abstract: This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale $200m\times130m$ building environment.

</details>


### [111] [Stochastic Decision-Making Framework for Human-Robot Collaboration in Industrial Applications](https://arxiv.org/abs/2601.14809)
*Muhammad Adel Yusuf,Ali Nasir,Zeeshan Hameed Khan*

Main category: cs.RO

TL;DR: 提出一种基于随机建模的人机协作决策方法，利用概率模型和控制策略预测人类行为和情绪，使协作机器人能相应调整行为


<details>
  <summary>Details</summary>
Motivation: 在工业和服务环境中，协作机器人需要与人类高效安全地协作，但现有研究主要关注检测人类意图，缺乏对人类动机水平和攻击性水平等人类因素的推理能力

Method: 采用随机建模方法，结合概率模型和控制策略，构建人机协作决策框架，能够预测人类行为和情绪

Result: 提出了理论框架、实施策略和仿真结果，展示了双边协作方法在协作机器人安全性和效率方面的潜在应用

Conclusion: 基于随机建模的决策方法能够使协作机器人更好地适应人类行为和情绪变化，提高人机协作的安全性和效率，为协作机器人领域提供了新的研究方向

Abstract: Collaborative robots, or cobots, are increasingly integrated into various industrial and service settings to work efficiently and safely alongside humans. However, for effective human-robot collaboration, robots must reason based on human factors such as motivation level and aggression level. This paper proposes an approach for decision-making in human-robot collaborative (HRC) environments utilizing stochastic modeling. By leveraging probabilistic models and control strategies, the proposed method aims to anticipate human actions and emotions, enabling cobots to adapt their behavior accordingly. So far, most of the research has been done to detect the intentions of human co-workers. This paper discusses the theoretical framework, implementation strategies, simulation results, and potential applications of the bilateral collaboration approach for safety and efficiency in collaborative robotics.

</details>


### [112] [Moving Beyond Compliance in Soft-Robotic Catheters Through Modularity for Precision Therapies](https://arxiv.org/abs/2601.14837)
*B. Calmé,N. J. Greenidge,A. Metcalf,A. Bacchetti,G. Loza,D. Kpeglo,P. Lloyd,V. Pensabene,J. H. Chandler,P. Valdastri*

Main category: cs.RO

TL;DR: 研究人员开发了一种直径1.47毫米的模块化软体机器人导管，集成了传感、驱动和治疗功能，能够在腔内导航中实现半自主部署和精准操作，特别适用于胰腺导管等难以进入的区域。


<details>
  <summary>Details</summary>
Motivation: 软体机器人器械在脆弱、曲折的解剖结构中导航比刚性工具更安全，但临床应用受到尖端功能化和实时组织界面反馈不足的限制。现有传感和治疗模块不够紧凑、鲁棒和适应性强，难以在腔内手术中测量和响应微妙的生理信号。

Method: 开发了一种直径1.47毫米的模块化软体机器人导管架构，支持多达四个独立控制的功能单元，可定制组合锚定、操纵、传感和靶向药物输送功能。系统结合了学习模型、磁驱动、板载形状传感和视觉标记跟踪，实现了闭环自主/共享控制系统。

Result: 在活体猪模型中，成功演示了半自主部署到胰腺导管内，并在其中进行了7.5厘米的内窥镜导航，这是目前标准导管无法进入的区域。闭环自主/共享控制系统进一步提高了插管准确性。

Conclusion: 该研究建立了一个可扩展的多功能软体机器人导管平台，为复杂的腔内介入手术提供了新范式，具有减少辐射暴露、缩短培训时间和加速软体机器人技术临床转化的潜力。

Abstract: Soft robotic instruments could navigate delicate, tortuous anatomy more safely than rigid tools, but clinical adoption is limited by insufficient tip functionalization and real-time feedback at the tissue interface. Few sensing and therapeutic modules are compact, robust, and adaptable enough to measure, and respond to, subtle physiological cues during intraluminal procedures. We present a 1.47 mm diameter modular soft robotic catheter that integrates sensing, actuation, and therapy while retaining the compliance needed for safe endoluminal navigation. Validated across multiple in vivo settings, we emphasize its utility in endoscopic retrograde cholangiopancreatography (ERCP), a highly technical procedure and a key access route to the pancreas, an organ that is fragile, difficult to instrument, and central to diseases such as pancreatic cancer. Our architecture supports up to four independently controlled functional units, allowing customizable combinations of anchoring, manipulation, sensing, and targeted drug delivery. In a live porcine model, we demonstrate semi-autonomous deployment into the pancreatic duct and 7.5 cm of endoscopic navigation within it, a region currently inaccessible with standard catheters. A closed-loop autonomous/shared-control system that combines a learned model, magnetic actuation, onboard shape sensing, and visual marker tracking further improves cannulation accuracy. Together, these results establish a scalable platform for multifunctional soft robotic catheters and a new paradigm for complex endoluminal interventions, with potential to reduce radiation exposure, shorten training, and accelerate clinical translation of soft robotic technologies.

</details>


### [113] [On-the-fly hand-eye calibration for the da Vinci surgical robot](https://arxiv.org/abs/2601.14871)
*Zejian Cui,Ferdinando Rodriguez y Baena*

Main category: cs.RO

TL;DR: 提出一种用于达芬奇手术机器人的在线手眼标定框架，通过特征关联和标定算法实现无需预训练的工具定位，显著降低定位误差并提高时间效率。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助微创手术中，电缆驱动机器人（如达芬奇机器人）由于编码器读数误差导致工具定位不准确，这对患者安全和手术成功至关重要。现有方法存在局限性，需要一种能够适应不同手术场景的在线标定方案。

Method: 提出一个包含两个相互关联算法的标定框架：1) 特征关联模块 - 在单目图像上检测关键点并提供鲁棒的对应关系，无需预训练；2) 手眼标定模块 - 采用多种滤波方法以适应不同手术场景，在线计算手眼变换矩阵。

Result: 在公开视频数据集上进行广泛测试，涵盖体外和离体场景、不同光照条件和关键点测量精度。结果显示，该框架显著降低了工具定位误差，精度与最先进方法相当，同时时间效率更高。

Conclusion: 该在线手眼标定框架能够有效解决电缆驱动手术机器人的工具定位问题，通过实时计算手眼变换矩阵，在多种手术场景下实现准确、高效的工具定位，具有临床应用潜力。

Abstract: In Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both in vitro and ex vivo scenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.

</details>


### [114] [HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation](https://arxiv.org/abs/2601.14874)
*Yara Mahmoud,Yasheerah Yaqoot,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: HumanoidVLM：基于视觉语言模型的检索框架，使Unitree G1人形机器人能够从第一视角RGB图像中直接选择任务相关的笛卡尔阻抗参数和夹爪配置，实现自适应操作。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人的控制器大多依赖固定的、手动调整的阻抗增益和夹爪设置，无法适应多样化的物体和任务需求。需要一种能够根据语义感知自动选择合适控制参数的方法。

Method: 结合视觉语言模型进行语义任务推断，使用基于FAISS的检索增强生成（RAG）模块从两个自定义数据库中检索经过实验验证的刚度-阻尼对和物体特定的抓取角度，并通过任务空间阻抗控制器执行。

Result: 在14个视觉场景中达到93%的检索准确率。实际实验中显示稳定的交互动力学特性，z轴跟踪误差通常在1-3.5厘米内，虚拟力与任务相关的阻抗设置一致。

Conclusion: 该研究证明了将语义感知与基于检索的控制相结合是可行的，为自适应人形机器人操作提供了一条可解释的路径。

Abstract: Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-appropriate Cartesian impedance parameters and gripper configurations directly from an egocentric RGB image. The system couples a vision-language model for semantic task inference with a FAISS-based Retrieval-Augmented Generation (RAG) module that retrieves experimentally validated stiffness-damping pairs and object-specific grasp angles from two custom databases, and executes them through a task-space impedance controller for compliant manipulation. We evaluate HumanoidVLM on 14 visual scenarios and achieve a retrieval accuracy of 93%. Real-world experiments show stable interaction dynamics, with z-axis tracking errors typically within 1-3.5 cm and virtual forces consistent with task-dependent impedance settings. These results demonstrate the feasibility of linking semantic perception with retrieval-based control as an interpretable path toward adaptive humanoid manipulation.

</details>


### [115] [Vision-Language Models on the Edge for Real-Time Robotic Perception](https://arxiv.org/abs/2601.14921)
*Sarat Ahmad,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: cs.RO

TL;DR: 该论文研究了在6G边缘智能（ORAN/MEC）基础设施上部署视觉语言模型（VLMs），以解决机器人系统中云计算的延迟、资源限制和隐私问题，使用Unitree G1人形机器人作为测试平台。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在机器人感知和交互中具有重要作用，但在实际部署中面临延迟、有限板载资源和云卸载隐私风险等挑战。6G边缘智能（特别是Open RAN和MEC）通过将计算靠近数据源，为解决这些问题提供了途径。

Method: 设计基于WebRTC的管道，将多模态数据流传输到边缘节点，在Unitree G1人形机器人平台上进行实验。比较LLaMA-3.2-11B-Vision-Instruct模型在边缘与云端的实时部署性能，并评估专为资源受限环境优化的紧凑模型Qwen2-VL-2B-Instruct。

Result: 边缘部署在保持接近云端准确性的同时，将端到端延迟降低了5%。Qwen2-VL-2B-Instruct模型实现了亚秒级响应，将延迟减少了一半以上，但以牺牲准确性为代价。

Conclusion: 在ORAN/MEC边缘基础设施上部署VLMs是可行的解决方案，能够在保持准确性的同时显著降低延迟，紧凑模型在资源受限环境中提供了延迟与准确性的权衡选择。

Abstract: Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.

</details>


### [116] [TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control](https://arxiv.org/abs/2601.14945)
*Yuteng Sun,Haoran Wang,Ruofei Bai,Zhengguo Li,Jun Li,Meng Yee,Chuah,Wei Yun Yau*

Main category: cs.RO

TL;DR: TIDAL提出了一种双频率层次框架，通过解耦语义推理与高频执行来解决VLA模型推理延迟问题，实现约9Hz控制更新，在动态拦截任务中性能提升2倍。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言-动作模型存在高推理延迟问题，只能采用低频批处理执行范式，导致在动态环境中目标移动时出现执行盲点，限制了实时应用。

Method: TIDAL采用层次化双频率架构：低频宏意图循环缓存语义嵌入，高频微控制循环交错单步流集成与执行。引入时间错位训练策略，让策略学习使用过时语义意图和实时本体感知进行预测补偿。还加入差分运动预测器处理静态视觉编码器对速度不敏感的问题。

Result: 在边缘硬件上实现约9Hz控制更新（基线约2.4Hz），动态拦截任务性能提升2倍，反馈频率提高4倍，语义嵌入有效视野超越原生动作块大小。在非暂停推理协议下保持鲁棒性。

Conclusion: TIDAL通过架构层面的双频率解耦设计，有效解决了VLA模型的延迟问题，实现了语义推理与高频执行的平衡，为动态环境中的实时控制提供了可行方案。

Abstract: Large-scale Vision-Language-Action (VLA) models offer semantic generalization but suffer from high inference latency, limiting them to low-frequency batch-and-execute paradigm. This frequency mismatch creates an execution blind spot, causing failures in dynamic environments where targets move during the open-loop execution window. We propose TIDAL (Temporally Interleaved Diffusion and Action Loop), a hierarchical framework that decouples semantic reasoning from high-frequency actuation. TIDAL operates as a backbone-agnostic module for diffusion-based VLAs, using a dual-frequency architecture to redistribute the computational budget. Specifically, a low-frequency macro-intent loop caches semantic embeddings, while a high-frequency micro-control loop interleaves single-step flow integration with execution. This design enables approximately 9 Hz control updates on edge hardware (vs. approximately 2.4 Hz baselines) without increasing marginal overhead. To handle the resulting latency shift, we introduce a temporally misaligned training strategy where the policy learns predictive compensation using stale semantic intent alongside real-time proprioception. Additionally, we address the insensitivity of static vision encoders to velocity by incorporating a differential motion predictor. TIDAL is architectural, making it orthogonal to system-level optimizations. Experiments show a 2x performance gain over open-loop baselines in dynamic interception tasks. Despite a marginal regression in static success rates, our approach yields a 4x increase in feedback frequency and extends the effective horizon of semantic embeddings beyond the native action chunk size. Under non-paused inference protocols, TIDAL remains robust where standard baselines fail due to latency.

</details>


### [117] [HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV](https://arxiv.org/abs/2601.14973)
*Faryal Batool,Iana Zhura,Valerii Serpiva,Roohan Ahmed Khan,Ivan Valuev,Issatay Tokmurziyev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: HumanDiffusion：一种基于RGB图像的轻量级扩散规划器，用于无人机在紧急场景中实现人类感知的导航和医疗援助


<details>
  <summary>Details</summary>
Motivation: 紧急场景下可靠的人机协作需要能够检测人类、推断导航目标并在动态环境中安全操作的自主系统。现有方法通常依赖先验地图或计算密集的规划流程，限制了在时间关键援助场景中的实用性。

Method: 结合YOLO-11人类检测与图像条件扩散规划器，直接从RGB图像生成人类感知的导航轨迹。轨迹在像素空间预测，确保平滑运动并保持与人类的安全距离，无需先验地图或复杂规划管道。

Result: 在300个样本测试集上实现像素空间轨迹重建的均方误差为0.02。真实世界室内模拟灾难场景实验中，在事故响应和搜索定位任务中总体任务成功率达到80%（包括部分遮挡情况）。

Conclusion: 人类条件扩散规划为时间关键援助场景中的人类感知无人机导航提供了实用且鲁棒的解决方案，能够有效处理动态环境和部分遮挡情况。

Abstract: Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.

</details>


### [118] [Graph-Based Adaptive Planning for Coordinated Dual-Arm Robotic Disassembly of Electronic Devices (eGRAP)](https://arxiv.org/abs/2601.14998)
*Adip Ranjan Das,Maria Koskinopoulou*

Main category: cs.RO

TL;DR: 提出eGRAP系统，通过视觉、动态规划和双机械臂执行实现电子设备的自主拆解，在硬盘驱动器上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 电子废弃物快速增长而回收率低，需要自动化拆解解决方案来提高回收效率和安全性。

Method: 采用电子设备图基自适应规划(eGRAP)：视觉系统识别零件和姿态，有向图编码拆解顺序，调度器使用拓扑排序选择有效下一步并分配给双机械臂并行执行，系统在线更新图和计划。

Result: 在3.5英寸硬盘驱动器上成功演示，实现完整拆解，具有高成功率和高效循环时间，展示了实时自适应协调双机械臂任务的能力。

Conclusion: eGRAP系统能够有效协调双机械臂进行实时自适应拆解，为电子废弃物自动化回收提供了可行解决方案。

Abstract: E-waste is growing rapidly while recycling rates remain low. We propose an electronic-device Graph-based Adaptive Planning (eGRAP) that integrates vision, dynamic planning, and dual-arm execution for autonomous disassembly. A camera-equipped arm identifies parts and estimates their poses, and a directed graph encodes which parts must be removed first. A scheduler uses topological ordering of this graph to select valid next steps and assign them to two robot arms, allowing independent tasks to run in parallel. One arm carries a screwdriver (with an eye-in-hand depth camera) and the other holds or handles components. We demonstrate eGRAP on 3.5in hard drives: as parts are unscrewed and removed, the system updates its graph and plan online. Experiments show consistent full disassembly of each HDD, with high success rates and efficient cycle times, illustrating the method's ability to adaptively coordinate dual-arm tasks in real time.

</details>


### [119] [DWPP: Dynamic Window Pure Pursuit Considering Velocity and Acceleration Constraints](https://arxiv.org/abs/2601.15006)
*Fumiya Ohnishi,Masaki Takahashi*

Main category: cs.RO

TL;DR: DWPP（动态窗口纯追踪）是一种改进的路径跟踪方法，通过在速度空间（v-ω平面）中考虑速度和加速度约束来避免传统纯追踪方法的速度指令与实际能力不匹配问题，从而提升跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 传统纯追踪方法及其变体虽然简单且计算高效，但通常没有明确考虑速度和加速度约束，导致指令速度与实际速度之间存在差异，造成超调和跟踪性能下降。

Method: DWPP从根本上重新设计了速度指令计算过程，在速度空间（v-ω平面）中制定速度指令计算，并选择动态窗口内最接近ω=κv直线的点作为指令速度，从而显式地纳入速度和加速度约束。

Result: 实验结果表明，DWPP能够避免违反约束的指令，相比传统纯追踪方法实现了更优的路径跟踪精度。该方法已集成到官方Nav2仓库并公开可用。

Conclusion: DWPP通过显式考虑机器人动力学约束，解决了传统纯追踪方法的速度指令与实际能力不匹配问题，显著提升了移动机器人路径跟踪性能。

Abstract: Pure pursuit and its variants are widely used for mobile robot path tracking owing to their simplicity and computational efficiency. However, many conventional approaches do not explicitly account for velocity and acceleration constraints, resulting in discrepancies between commanded and actual velocities that result in overshoot and degraded tracking performance. To address this problem, this paper proposes dynamic window pure pursuit (DWPP), which fundamentally reformulates the command velocity computation process to explicitly incorporate velocity and acceleration constraints. Specifically, DWPP formulates command velocity computation in the velocity space (the $v$-$ω$ plane) and selects the command velocity as the point within the dynamic window that is closest to the line $ω= κv$. Experimental results demonstrate that DWPP avoids constraint-violating commands and achieves superior path-tracking accuracy compared with conventional pure pursuit methods. The proposed method has been integrated into the official Nav2 repository and is publicly available (https://github.com/ros-navigation/navigation2).

</details>


### [120] [Risk Estimation for Automated Driving](https://arxiv.org/abs/2601.15018)
*Leon Tolksdorf,Arturo Tejada,Jonas Bauernfeind,Christian Birkner,Nathan van de Wouw*

Main category: cs.RO

TL;DR: 提出一种结合碰撞概率估计和碰撞严重性概念的风险评估通用方法，用于自动驾驶车辆运动规划


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全评估需要准确的风险评估方法，现有方法要么依赖经验模型，要么采用严重近似，缺乏普适性和准确性

Method: 结合碰撞概率估计的最新进展与碰撞严重性概念，开发通用风险估计方法，可为不同碰撞类型分配个体严重性函数

Result: 提出的方法计算效率高，适用于实时运动规划，并提供了高斯不确定性的示例实现代码

Conclusion: 该方法为自动驾驶风险评估提供了通用、准确且计算高效的解决方案，支持不同碰撞类型的个体化严重性评估

Abstract: Safety is a central requirement for automated vehicles. As such, the assessment of risk in automated driving is key in supporting both motion planning technologies and safety evaluation. In automated driving, risk is characterized by two aspects. The first aspect is the uncertainty on the state estimates of other road participants by an automated vehicle. The second aspect is the severity of a collision event with said traffic participants. Here, the uncertainty aspect typically causes the risk to be non-zero for near-collision events. This makes risk particularly useful for automated vehicle motion planning. Namely, constraining or minimizing risk naturally navigates the automated vehicle around traffic participants while keeping a safety distance based on the level of uncertainty and the potential severity of the impending collision. Existing approaches to calculate the risk either resort to empirical modeling or severe approximations, and, hence, lack generalizability and accuracy. In this paper, we combine recent advances in collision probability estimation with the concept of collision severity to develop a general method for accurate risk estimation. The proposed method allows us to assign individual severity functions for different collision constellations, such as, e.g., frontal or side collisions. Furthermore, we show that the proposed approach is computationally efficient, which is beneficial, e.g., in real-time motion planning applications. The programming code for an exemplary implementation of Gaussian uncertainties is also provided.

</details>


### [121] [CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes](https://arxiv.org/abs/2601.15039)
*Jiyao Zhang,Zhiyuan Ma,Tianhao Wu,Zeyuan Chen,Hao Dong*

Main category: cs.RO

TL;DR: CADGrasp：一种两阶段算法，使用单视角点云输入实现通用灵巧抓取，通过预测稀疏IBS表示和优化生成高质量无碰撞抓取姿态


<details>
  <summary>Details</summary>
Motivation: 解决灵巧抓取在杂乱环境中的挑战，包括灵巧手的高自由度、遮挡问题，以及由于多样物体几何形状和复杂布局导致的潜在碰撞问题

Method: 两阶段算法：第一阶段预测稀疏IBS（场景解耦、接触感知和碰撞感知表示）作为优化目标，使用具有体素级条件引导和力闭合分数过滤的占用扩散模型；第二阶段基于稀疏IBS开发能量函数和排序策略进行优化，生成高质量灵巧抓取姿态

Result: 在模拟和真实世界环境中进行广泛实验验证了方法的有效性，能够减少碰撞同时在不同物体和复杂场景中保持高抓取成功率

Conclusion: CADGrasp通过稀疏IBS表示和两阶段优化方法，有效解决了杂乱环境中的灵巧抓取问题，实现了无碰撞的高质量抓取

Abstract: Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict sparse IBS, a scene-decoupled, contact- and collision-aware representation, as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes.

</details>


### [122] [Systematic Evaluation of Hip Exoskeleton Assistance Parameters for Enhancing Gait Stability During Ground Slip Perturbations](https://arxiv.org/abs/2601.15056)
*Maria T. Tagliaferri,Inseung Kang*

Main category: cs.RO

TL;DR: 研究系统调节髋关节外骨骼辅助力矩大小和持续时间对滑倒扰动下稳定性的影响，发现时间参数决定辅助的稳定/失稳效果，稳定性最优参数比能量优化控制器减少25.7%的全身角动量范围，存在显著的个体差异。


<details>
  <summary>Details</summary>
Motivation: 跌倒对老年人是主要伤害原因，现有外骨骼控制器主要优化步行能量消耗而非稳定性，辅助参数（如力矩大小和持续时间）对稳定性的影响尚未探索。

Method: 在8名健康成年人中，使用双侧髋关节外骨骼系统调节辅助力矩的大小和持续时间，在滑倒扰动下量化稳定性（使用全身角动量WBAM），并与现有能量优化控制器比较。

Result: 辅助力矩大小和持续时间存在显著交互作用，持续时间决定辅助是稳定还是失稳；稳定性最优参数比能量优化控制器平均减少25.7%的WBAM范围；观察到显著的个体间差异。

Conclusion: 仅优化能量消耗不足以改善步态扰动下的反应稳定性；稳定性导向的外骨骼控制应优先考虑时间参数并包含用户个性化定制，这对改善老年人稳定性和降低跌倒风险有直接意义。

Abstract: Falls are the leading cause of injury related hospitalization and mortality among older adults. Consequently, mitigating age-related declines in gait stability and reducing fall risk during walking is a critical goal for assistive devices. Lower-limb exoskeletons have the potential to support users in maintaining stability during walking. However, most exoskeleton controllers are optimized to reduce the energetic cost of walking rather than to improve stability. While some studies report stability benefits with assistance, the effects of specific parameters, such as assistance magnitude and duration, remain unexplored. To address this gap, we systematically modulated the magnitude and duration of torque provided by a bilateral hip exoskeleton during slip perturbations in eight healthy adults, quantifying stability using whole-body angular momentum (WBAM). WBAM responses were governed by a significant interaction between assistance magnitude and duration, with duration determining whether exoskeleton assistance was stabilizing or destabilizing relative to not wearing the exoskeleton device. Compared to an existing energy-optimized controller, experimentally identified stability-optimal parameters reduced WBAM range by 25.7% on average. Notably, substantial inter-subject variability was observed in the parameter combinations that minimized WBAM during perturbations. We found that optimizing exoskeleton assistance for energetic outcomes alone is insufficient for improving reactive stability during gait perturbations. Stability-focused exoskeleton control should prioritize temporal assistance parameters and include user-specific personalization. This study represents an important step toward personalized, stability-focused exoskeleton control, with direct implications for improving stability and reducing fall risk in older adults.

</details>


### [123] [Influence of Operator Expertise on Robot Supervision and Intervention](https://arxiv.org/abs/2601.15069)
*Yanran Jiang,Pavan Sikka,Leimin Tian,Dana Kuliic,Cecile Paris*

Main category: cs.RO

TL;DR: 研究探索不同专业水平的用户如何监督自主机器人，分析其干预时机和决策策略差异


<details>
  <summary>Details</summary>
Motivation: 随着机器人自主性提高，监督用户的专业知识水平差异增大，需要理解不同专业水平用户如何执行监督任务及其对人机团队绩效的影响

Method: 进行用户研究（N=27），参与者在模拟器中监督自主探索未知隧道环境的机器人，在认为机器人遇到困难时通过提供航点进行干预，分析交互数据和问卷响应

Result: 识别了新手、中级和专家用户在干预时机和决策策略方面的不同模式

Conclusion: 不同专业水平的用户在监督自主机器人时表现出系统性的行为差异，这对设计适应不同用户水平的机器人监督系统具有重要意义

Abstract: With increasing levels of robot autonomy, robots are increasingly being supervised by users with varying levels of robotics expertise. As the diversity of the user population increases, it is important to understand how users with different expertise levels approach the supervision task and how this impacts performance of the human-robot team. This exploratory study investigates how operators with varying expertise levels perceive information and make intervention decisions when supervising a remote robot. We conducted a user study (N=27) where participants supervised a robot autonomously exploring four unknown tunnel environments in a simulator, and provided waypoints to intervene when they believed the robot had encountered difficulties. By analyzing the interaction data and questionnaire responses, we identify differing patterns in intervention timing and decision-making strategies across novice, intermediate, and expert users.

</details>


### [124] [V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks](https://arxiv.org/abs/2601.15164)
*Yaru Liu,Ao-bo Wang,Nanyang Ye*

Main category: cs.RO

TL;DR: V-CAGE是一个闭环框架，用于大规模生成物理合理、语义对齐的机器人操作数据集，通过上下文感知实例化、分层指令分解和VLM验证循环解决合成数据中的物理不可行、语义不对齐和长时程规划问题。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法存在三个主要问题：1) 生成的场景物理上不可行；2) 语言驱动程序经常"成功"但未满足任务语义；3) 高级指令需要接地到可执行动作序列。这些问题限制了从合成数据学习长时程具身行为的效果。

Method: V-CAGE采用三阶段方法：1) 上下文感知实例化机制，通过动态维护禁止空间区域地图确保几何一致性，防止物体穿透并保证可达、无冲突配置；2) 分层指令分解模块，将高级目标分解为组合动作基元；3) VLM验证循环，作为视觉评判器进行严格拒绝采样，过滤"静默失败"。

Result: 实验表明V-CAGE生成的数集具有优越的物理和语义保真度，相比未验证基线显著提升了下游策略的成功率和泛化能力。

Conclusion: V-CAGE通过集成几何一致性约束、分层规划和语义验证，有效解决了合成数据生成中的关键挑战，为大规模、高质量的机器人操作数据集创建提供了可靠框架。

Abstract: Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently "succeed" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., "get ready for work") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out "silent failures" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [125] [A Category-Theoretic Framework for Dependent Effect Systems](https://arxiv.org/abs/2601.14846)
*Satoshi Kura,Marco Gaboardi,Taro Sekiyama,Hiroshi Unno*

Main category: cs.LO

TL;DR: 论文提出索引分级单子，扩展传统分级单子以支持依赖程序值的效应，并应用于依赖效应的精化类型系统


<details>
  <summary>Details</summary>
Motivation: 现有分级单子框架无法处理依赖程序值的效应（依赖效应），限制了表达能力

Method: 引入索引分级单子，受纤维化"索引"视角和依赖类型理论经典范畴语义启发，提供依赖效应精化类型系统的语义

Result: 索引分级单子为依赖效应类型系统提供语义，可实例化为多种形式系统：代价分析、概率边界推理、期望边界推理、时序安全验证

Conclusion: 索引分级单子解决了分级单子对依赖效应的支持不足问题，扩展了效应推理的表达能力

Abstract: Graded monads refine traditional monads using effect annotations in order to describe quantitatively the computational effects that a program can generate. They have been successfully applied to a variety of formal systems for reasoning about effectful computations. However, existing categorical frameworks for graded monads do not support effects that may depend on program values, which we call dependent effects, thereby limiting their expressiveness. We address this limitation by introducing indexed graded monads, a categorical generalization of graded monads inspired by the fibrational "indexed" view and by classical categorical semantics of dependent type theories. We show how indexed graded monads provide semantics for a refinement type system with dependent effects. We also show how this type system can be instantiated with specific choices of parameters to obtain several formal systems for reasoning about specific program properties. These instances include, in particular, cost analysis, probability-bound reasoning, expectation-bound reasoning, and temporal safety verification.

</details>


### [126] [Efficient reversal of transductions of sparse graph classes](https://arxiv.org/abs/2601.14906)
*Jan Dreier,Jakub Gajarský,Michał Pilipczuk*

Main category: cs.LO

TL;DR: 提出一种高效算法方法，用于近似反转一阶转换的应用，假设源图是稀疏的。对于任何具有结构有界扩张的图类，提供O(n⁴)时间算法，计算顶点着色图H，使得G可以从H通过一阶解释恢复，且H属于有界扩张图类。


<details>
  <summary>Details</summary>
Motivation: 解决Gajarský等人提出的开放问题：如何有效地反转图转换过程。研究具有结构有界扩张的图类，这些图类可以从有界扩张图类通过一阶转换得到。目标是开发算法方法，将转换后的图近似恢复为原始图的形式。

Method: 提出O(n⁴)时间算法，对于具有结构有界扩张的图类C中的任意图G，计算顶点着色图H。算法假设源图是稀疏的，且图类C满足两个条件：单调稳定（不转换所有半图类）和具有固有线性邻域复杂度（在所有可从C转换的图类中邻域复杂度为线性）。

Result: 证明了满足单调稳定和固有线性邻域复杂度这两个性质的图类恰好与结构有界扩张的图类重合。算法能够在O(n⁴)时间内计算顶点着色图H，使得G可以从H通过一阶解释恢复，且H属于有界扩张图类D。

Conclusion: 该工作解决了反转图转换的算法问题，建立了单调稳定性和固有线性邻域复杂度与结构有界扩张之间的等价关系。提出的算法为处理具有结构有界扩张的图类提供了有效的工具，回答了Gajarský等人提出的开放问题。

Abstract: (First-order) transductions are a basic notion capturing graph modifications that can be described in first-order logic. In this work, we propose an efficient algorithmic method to approximately reverse the application of a transduction, assuming the source graph is sparse. Precisely, for any graph class $\mathcal{C}$ that has structurally bounded expansion (i.e., can be transduced from a class of bounded expansion), we give an $O(n^4)$-time algorithm that given a graph $G\in \mathcal{C}$, computes a vertex-colored graph $H$ such that $G$ can be recovered from $H$ using a first-order interpretation and $H$ belongs to a graph class $\mathcal{D}$ of bounded expansion. This answers an open problem raised by Gajarský et al. In fact, for our procedure to work we only need to assume that $\mathcal{C}$ is monadically stable (i.e., does not transduce the class of all half-graphs) and has inherently linear neighborhood complexity (i.e., the neighborhood complexity is linear in all graph classes transducible from $\mathcal{C}$). This renders the conclusion that the graph classes satisfying these two properties coincide with classes of structurally bounded expansion.

</details>


### [127] [A Complete Propositional Dynamic Logic for Regular Expressions with Lookahead](https://arxiv.org/abs/2601.15214)
*Yoshiki Nakamura*

Main category: cs.LO

TL;DR: 该论文为具有向前看功能的正则表达式（REwLA）提供了逻辑推理的公理化表征，通过扩展命题动态逻辑（PDL）并建立完备的公理系统来捕捉REwLA的等价关系。


<details>
  <summary>Details</summary>
Motivation: 研究具有向前看功能的正则表达式（REwLA）的逻辑推理问题，旨在为REwLA的（匹配）语言等价性以及最大的替换封闭等价关系提供公理化表征，从而形式化地描述这些等价关系。

Method: 引入在有限线性序上的命题动态逻辑（PDL）变体，扩展了两个操作符：限制到恒等关系和限制到其补集。通过希尔伯特风格的有限公理化系统，建立该逻辑的完备性，并通过归约到无恒等关系的PDL变体来证明完备性。

Result: 成功建立了REwLA等价关系的完备公理化系统，证明了扩展PDL与REwLA具有相同的计算复杂度，为REwLA的逻辑推理提供了形式化基础。

Conclusion: 通过扩展PDL并建立完备的公理系统，成功捕捉了REwLA的等价关系，为具有向前看功能的正则表达式的逻辑推理提供了坚实的理论基础，同时保持了计算复杂度的可控性。

Abstract: We consider (logical) reasoning for regular expressions with lookahead (REwLA). In this paper, we give an axiomatic characterization for both the (match-)language equivalence and the largest substitution-closed equivalence that is sound for the (match-)language equivalence. To achieve this, we introduce a variant of propositional dynamic logic (PDL) on finite linear orders, extended with two operators: the restriction to the identity relation and the restriction to its complement. Our main contribution is a sound and complete Hilbert-style finite axiomatization for the logic, which captures the equivalences of REwLA. Using the extended operators, the completeness is established via a reduction into an identity-free variant of PDL on finite strict linear orders. Moreover, the extended PDL has the same computational complexity as REwLA.

</details>


### [128] [How to Verify a Turing Machine with Dafny](https://arxiv.org/abs/2601.15230)
*Edgar F. A. Lederer*

Main category: cs.LO

TL;DR: 使用Dafny程序验证器对两个图灵机进行形式化验证，证明其作为判定器的完全正确性


<details>
  <summary>Details</summary>
Motivation: 传统图灵机正确性证明通常依赖非正式的描述和示例执行，缺乏严格的数学证明或机械化形式验证。由于证明所需的循环不变式和变式等构件规模庞大且技术细节复杂，没有机械支持几乎不可能完成验证

Method: 使用Dafny程序验证器对两个典型的图灵机进行形式化验证。第一个是课程中常见的示例机器，第二个直接取自相关教科书。通过构建复杂的循环不变式和变式等验证构件，利用Dafny的自动化验证能力完成证明

Result: 成功验证了两个图灵机的完全正确性（作为判定器）。证明了即使对于看似简单的图灵机，形式化验证也需要处理大量技术细节，而Dafny等现代程序验证器能够有效处理这些复杂性

Conclusion: 现代程序验证器如Dafny能够有效处理图灵机验证中的复杂技术细节，展示了其在形式化验证领域的强大能力。这类验证任务正是程序验证器展示其价值和应用潜力的理想场景

Abstract: This paper describes the formal verification of two Turing machines using the program verifier Dafny. Both machines are deciders, so we prove total correctness. They are typical first examples of Turing machines used in any course of Theoretical Computer Science; in fact, the second machine is literally taken from a relevant textbook. Usually, the correctness of such machines is made plausible by some informal explanations of their basic ideas, augmented with a few sample executions, but neither by rigorous mathematical nor mechanized formal proof. No wonder: The invariants (and variants) required for such proofs are big artifacts, peppered with overpowering technical details. Finding and checking these artifacts without mechanical support is practically impossible, and such support is only available since recent times. But nowadays, just because of these technicalities, with such subjects under proof a program verifier can really show off and demonstrate its capabilities.

</details>


### [129] [Feasibility Preservation under Monotone Retrieval Truncation](https://arxiv.org/abs/2601.15241)
*Sean Plummer*

Main category: cs.LO

TL;DR: 该论文从结构角度研究检索系统，将查询回答建模为截断下的可行性问题，分析在何种条件下无限检索深度下的可行性能保证有限深度下的可行性。


<details>
  <summary>Details</summary>
Motivation: 基于检索的系统通过暴露可用证据的截断子集来近似访问语料库。即使相关信息存在于语料库中，截断也可能阻止兼容证据同时出现，导致基于相关性的评估无法捕捉到的失败。论文旨在从结构角度研究检索，将查询回答建模为截断下的可行性问题。

Method: 将检索形式化为候选证据序列，并刻画在无限检索深度下可行性意味着有限检索深度下可行性的条件。证明单调截断足以保证单个查询的有限可证性。对于查询类，识别有限生成见证证书作为获得统一检索边界的附加条件，并证明该条件是必要的。进一步展示在非单调截断、非有限生成查询类和纯槽位覆盖下的失败反例。

Result: 建立了检索的可行性理论框架，证明单调截断保证单个查询的有限可证性，识别有限生成见证证书作为查询类统一检索边界的充分必要条件。提供了在非单调截断、非有限生成查询类和纯槽位覆盖下的尖锐反例，展示了这些条件下的失败。

Conclusion: 将可行性保持作为独立于相关性评分或优化的检索正确性标准，阐明了基于截断的检索固有的结构限制。为检索系统设计提供了理论基础，强调了超越传统相关性评估的结构考虑的重要性。

Abstract: Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibility problem under truncation.
  We formalize retrieval as a sequence of candidate evidence sets and characterize conditions under which feasibility in the limit implies feasibility at finite retrieval depth. We show that monotone truncation suffices to guarantee finite witnessability for individual queries. For classes of queries, we identify finite generation of witness certificates as the additional condition required to obtain a uniform retrieval bound, and we show that this condition is necessary. We further exhibit sharp counterexamples demonstrating failure under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage.
  Together, these results isolate feasibility preservation as a correctness criterion for retrieval independent of relevance scoring or optimization, and clarify structural limitations inherent to truncation-based retrieval.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [130] [DeGAS: Gradient-Based Optimization of Probabilistic Programs without Sampling](https://arxiv.org/abs/2601.15167)
*Francesca Randone,Romina Doz,Mirco Tribastone,Luca Bortolussi*

Main category: cs.PL

TL;DR: DeGAS是一种用于无循环概率程序的可微分高斯近似语义，支持在包含连续和离散分量的模型中进行无样本、基于梯度的优化。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的概率程序优化方法（如MCMC、变分推断）在处理涉及连续变量的条件约束时可能收敛困难，需要一种能够进行端到端梯度优化的无样本方法。

Method: 采用高斯混合语义评估程序，将测度为零的谓词和离散分支替换为渐近平滑，得到后验和路径概率的闭式表达式，证明这些量对程序参数的可微性，利用自动微分进行优化。

Result: 在13个基准程序上，DeGAS的准确性和运行时间与变分推断和MCMC相当，在涉及连续变量条件约束的优化问题上，能够可靠地解决基于采样的基线方法无法收敛的情况。

Conclusion: DeGAS为概率程序提供了一种有效的可微分优化框架，特别适用于包含连续和离散混合分量的模型，避免了蒙特卡洛估计器的不稳定性，实现了端到端的梯度优化。

Abstract: We present DeGAS, a differentiable Gaussian approximate semantics for loopless probabilistic programs that enables sample-free, gradient-based optimization in models with both continuous and discrete components. DeGAS evaluates programs under a Gaussian-mixture semantics and replaces measure-zero predicates and discrete branches with a vanishing smoothing, yielding closed-form expressions for posterior and path probabilities. We prove differentiability of these quantities with respect to program parameters, enabling end-to-end optimization via standard automatic differentiation, without Monte Carlo estimators. On thirteen benchmark programs, DeGAS achieves accuracy and runtime competitive with variational inference and MCMC. Importantly, it reliably tackles optimization problems where sampling-based baselines fail to converge due to conditioning involving continuous variables.

</details>


### [131] [Contextual Metaprogramming for Session Types](https://arxiv.org/abs/2601.15180)
*Pedro Ângelo,Atsushi Igarashi,Yuito Murase,Vasco T. Vasconcelos*

Main category: cs.PL

TL;DR: 将分阶段元编程集成到会话类型的消息传递函数式语言中，通过上下文模态类型理论支持代码传输和执行


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中需要通过会话类型消息动态准备和传输代码的需求，如服务器按需准备和发送代码的场景

Method: 基于多级上下文的上下文模态类型理论，支持上下文值的装箱和消息传输，区分线性和非受限资源类型，实现类型检查器

Result: 证明了类型保持性、顺序计算的进展性、并发运行时环境无运行时错误，以及类型检查器的正确性

Conclusion: 成功将分阶段元编程集成到会话类型消息传递语言中，为动态代码传输和执行提供了类型安全的框架

Abstract: We propose the integration of staged metaprogramming into a session-typed message passing functional language. We build on a model of contextual modal type theory with multi-level contexts, where contextual values, closing arbitrary terms over a series of variables, may be boxed and transmitted in messages. Once received, one such value may then be unboxed and locally applied before being run. To motivate this integration, we present examples of real-world use cases, for which our system would be suitable, such as servers preparing and shipping code on demand via session typed messages. We present a type system that distinguishes linear (used exactly once) from unrestricted (used an unbounded number of times) resources, and further define a type checker, suitable for a concrete implementation. We show type preservation, a progress result for sequential computations and absence of runtime errors for the concurrent runtime environment, as well as the correctness of the type checker.

</details>
